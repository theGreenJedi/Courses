{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this example, we will use the Naive Bayes implementation from the Scikit-learn  machine learning library to classify newsgroup postings. We have chosen two newsgroups from the datasets provided by Scikit-learn (alt.atheism and sci.med) and we shall use Naive Bayes to predict which newsgroup a particular posting is from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "import numpy as np\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn import metrics,cross_validation\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "cats = ['alt.atheism', 'sci.med']\n",
      "newsgroups= fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'), categories=cats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first loads the newsgroup data using the utility function provided by scikit-learn (this downloads the dataset from the internet and may take some time). The newsgroup object is a map, the newsgroup postings are saved against 'data', and the target variables are in newsgroups.target. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "newsgroups.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the features are words, we transform them to another representation using Term Frequency-Inverse Document Frequency (Tfidf). The purpose of tfidf is to de-emphasize words that occur in all postings (such as 'the','by,'for' etc) and instead emphasize words that are unique to a particular class (such as religion, creationism which are from the alt.atheism newsgroup).\n",
      "We can do the same by creating a TfidfVectorizer and then transforming all the newsgroup data to a vector representation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer()\n",
      "vectors = vectorizer.fit_transform(newsgroups.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Vectors now contains features that we can use as input data to the Naive Bayes classifier. A shape query reveals that it contains 1789 instances, and each instance contains about 24k features. However, many of those features can be 0, indicating words that do no appear in that particular posting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectors.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "(1789, 24202)"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn provides a few versions of Naive Bayes classifier, the one we use is called MultinomialNB. Since using a classifier typically involves splitting the dataset into train, test and validation sets, then training on the 'train' set and testing the efficacy on the 'validation' set, we can use the scikit-learn provided utility to do the same for us.\n",
      "The cross_validation.cross_val_score automatically splits the data into multiple sets and returns the F1 score (a metric that measures a classifier's accuracy)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = MultinomialNB(alpha=.01)\n",
      "print \"CrossValidation Score: \", np.mean(cross_validation.cross_val_score(clf,vectors, newsgroups.target, scoring='f1'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CrossValidation Score:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.954618416381\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that despite the assumption that all features are conditionally independent, the classifier maintains a decent F1 score of 95%. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}