{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us attempt to use constraint based approaches to a larger dataset. The \"Logical Alarm Reduction Mechanism\" network is a Bayesian network designed to provide an alarm message system for patient monitoring. This network has 37 vertices and 46 edges, considerably larger than the toy job interview network we have been using so far that had 5 vertices and 4 edges.\n",
      "\n",
      "The dataset can be found here (http://www.cs.ru.nl/~peterl/BN/) and is commonly referred to as the Alarm network. More information on the dataset(such as column descriptions) can be found here (http://www.bnlearn.com/documentation/man/alarm.html).\n",
      "\n",
      "Let us load the alarm.csv file using the Pandas library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "df=pd.read_csv(\"alarm.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The alarm.csv file has records that we should convert into a format that libpgm can consume. Each instance should be a dictionary, where keys are column names and values are column values. The following function does the same, and returns a list of dictionaries that are sampled without replacement from the original dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import randint,sample\n",
      "\n",
      "def rand_index(dframe,n_samples=100):\n",
      "    rindex =  np.array(sample(xrange(len(dframe)) ,n_samples if n_samples <=len(dframe) else len(dframe)))\n",
      "    return [{i:j.values()[0] for i,j in dframe.iloc[[k]].to_dict().items()} for k in rindex ]\n",
      "\n",
      "#Lets examine a single sample:\n",
      "rand_index(df,n_samples=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[{'Anaphylaxis': 'b',\n",
        "  'ArtCOb': 'b',\n",
        "  'BP': 'c',\n",
        "  'CO': 'b',\n",
        "  'CVP': 'b',\n",
        "  'Catechol': 'a',\n",
        "  'Disconnect': 'b',\n",
        "  'ErrCauter': 'b',\n",
        "  'ErrLowOutput': 'b',\n",
        "  'ExpCOb': 'c',\n",
        "  'FiOb': 'b',\n",
        "  'HR': 'b',\n",
        "  'HRBP': 'b',\n",
        "  'HREKG': 'b',\n",
        "  'HRSat': 'b',\n",
        "  'History': 'b',\n",
        "  'Hypovolemia': 'b',\n",
        "  'InsuffAnesth': 'b',\n",
        "  'Intubation': 'a',\n",
        "  'KinkedTube': 'b',\n",
        "  'LVEDVolume': 'b',\n",
        "  'LVFailure': 'b',\n",
        "  'MinVol': 'c',\n",
        "  'MinVolSet': 'b',\n",
        "  'PAP': 'b',\n",
        "  'PCWP': 'b',\n",
        "  'PVSat': 'b',\n",
        "  'Press': 'c',\n",
        "  'PulmEmbolus': 'b',\n",
        "  'SaOb': 'b',\n",
        "  'Shunt': 'a',\n",
        "  'StrokeVolume': 'b',\n",
        "  'TPR': 'b',\n",
        "  'VentAlv': 'c',\n",
        "  'VentLung': 'c',\n",
        "  'VentMach': 'c',\n",
        "  'VentTube': 'c'}]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets load the data, create an instance of the learner object and estimate the structure with a small number of samples (100). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "from libpgm.nodedata import NodeData\n",
      "from libpgm.graphskeleton import GraphSkeleton\n",
      "from libpgm.discretebayesiannetwork import DiscreteBayesianNetwork\n",
      "from libpgm.pgmlearner import PGMLearner\n",
      "\n",
      "data=rand_index(df,n_samples=100)\n",
      "learner = PGMLearner()\n",
      "result=learner.discrete_constraint_estimatestruct(data)\n",
      "print result.E"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['HR', 'HRSat'], ['CVP', 'LVEDVolume'], ['ExpCOb', 'ArtCOb'], ['StrokeVolume', 'CO'], ['TPR', 'BP'], ['CO', 'BP']]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To compare the performance of the structure found to the correct alarm network, let's load the alarm network. The file parent-child.txt contains, on each line, the parent vertex, followed by the child vertices (in some cases, some nodes are leaf nodes that have no children)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = open('parent-child.txt', 'r')\n",
      "\n",
      "def edges(line):\n",
      "    st=line.strip('\\n').strip(' ').split(' ')\n",
      "    #print st\n",
      "    return [[st[0],i] for i in st[1:] ]\n",
      "\n",
      "all_edges=[l for line in file for l in edges(line)]\n",
      "#a set containing the correct edges\n",
      "ground_truth=set([tuple(i) for i in all_edges])\n",
      "print all_edges[:5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['HISTORY', 'LVFAILURE'], ['CVP', 'LVEDVOLUME'], ['PCWP', 'LVEDVOLUME'], ['LVEDVOLUME', 'HYPOVOLEMIA'], ['LVEDVOLUME', 'LVFAILURE']]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets define a diagnostic function that compares the found network with the correct network. \n",
      "\n",
      "The learned structure tries to find edges between nodes, and its performance can be categorized as follows:\n",
      " 1. It finds edges that exist in the correct network. \n",
      " 2. Some of these have wrong directionality.\n",
      " 3. It also finds spurios edges that don't exist in the correct network.\n",
      " \n",
      "We define a function that can quantify the above errors in the learned network."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printdiag(result):\n",
      "    found=set([tuple([j.upper() for j in i]) for i in result.E])\n",
      "    correct=ground_truth.intersection(found)\n",
      "    undirected_common_edges=[(i,j) for i,j in found for k,l in ground_truth if i.find(k)!=-1 and j.find(l)!=-1]\n",
      "    print \"Number of edges in learnt network \",len(found)\n",
      "    print \"Total number of edges in true network \",len(ground_truth)\n",
      "    print \"Number of edges with correct directionality \",len(correct)\n",
      "    print \"Number of edges with incorrect directionality \",len(undirected_common_edges)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets define a function that prints the resulting statistics when the dataset picks a specific number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn_structure(n_samples):\n",
      "    data=rand_index(df,n_samples)\n",
      "    learner = PGMLearner()\n",
      "    result1=learner.discrete_constraint_estimatestruct(data)\n",
      "    printdiag(result1)\n",
      "    \n",
      "learn_structure(1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of edges in learnt  36\n",
        "Total number of edges in true network  46\n",
        "Number of edges with correct directionality  6\n",
        "Number of edges with incorrect directionality  7\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets examine the performance of the learn_structure when provided with 1000 samples. It finds less than half of the edges, and only a few edges  correctly connect nodes. Does increasing the sample size help? (Caution: this may take a few minutes to run)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "learn_structure(10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of edges in learnt network  30\n",
        "Total number of edges in true network  46\n",
        "Number of edges with correct directionality  2\n",
        "Number of edges with incorrect directionality  3\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the number of samples is still not enough to improve the number of edges that are correctly identified. The complexity of the algorithm has a running time of O(n^(d+2)) where n is the number of vertices and d is the upper bound on the witness set. Various versions of this algorithm try to constrain the witness set to improve its performance(described in Chapter 2 of this paper (http://arxiv.org/pdf/1111.6925.pdf))"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}