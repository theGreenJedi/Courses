{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us attempt to use constraint based approaches to a larger dataset. The \"Logical Alarm Reduction Mechanism\" network is a Bayesian network designed to provide an alarm message system for patient monitoring. This network has 37 vertices and 46 edges, considerably larger than the toy job interview network we have been using so far that had 5 vertices and 4 edges.\n",
      "\n",
      "The dataset can be found here (http://www.cs.ru.nl/~peterl/BN/) and is commonly referred to as the Alarm network. More information on the dataset(such as column descriptions) can be found here (http://www.bnlearn.com/documentation/man/alarm.html).\n",
      "\n",
      "Let us load the alarm.csv file using the Pandas library."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "df=pd.read_csv(\"alarm.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The alarm.csv file has records that we should convert into a format that libpgm can consume. Each instance should be a dictionary, where keys are column names and values are column values. The following function does the same, and returns a list of dictionaries that are sampled without replacement from the original dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import randint,sample\n",
      "\n",
      "def rand_index(dframe,n_samples=100):\n",
      "    rindex =  np.array(sample(xrange(len(dframe)) ,n_samples if n_samples <=len(dframe) else len(dframe)))\n",
      "    return [{i:j.values()[0] for i,j in dframe.iloc[[k]].to_dict().items()} for k in rindex ]\n",
      "\n",
      "#Lets examine a single sample:\n",
      "rand_index(df,n_samples=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[{'Anaphylaxis': 'b',\n",
        "  'ArtCOb': 'b',\n",
        "  'BP': 'c',\n",
        "  'CO': 'a',\n",
        "  'CVP': 'a',\n",
        "  'Catechol': 'a',\n",
        "  'Disconnect': 'b',\n",
        "  'ErrCauter': 'b',\n",
        "  'ErrLowOutput': 'b',\n",
        "  'ExpCOb': 'c',\n",
        "  'FiOb': 'b',\n",
        "  'HR': 'b',\n",
        "  'HRBP': 'b',\n",
        "  'HREKG': 'b',\n",
        "  'HRSat': 'b',\n",
        "  'History': 'b',\n",
        "  'Hypovolemia': 'a',\n",
        "  'InsuffAnesth': 'b',\n",
        "  'Intubation': 'a',\n",
        "  'KinkedTube': 'b',\n",
        "  'LVEDVolume': 'a',\n",
        "  'LVFailure': 'b',\n",
        "  'MinVol': 'c',\n",
        "  'MinVolSet': 'b',\n",
        "  'PAP': 'b',\n",
        "  'PCWP': 'a',\n",
        "  'PVSat': 'b',\n",
        "  'Press': 'c',\n",
        "  'PulmEmbolus': 'b',\n",
        "  'SaOb': 'b',\n",
        "  'Shunt': 'a',\n",
        "  'StrokeVolume': 'a',\n",
        "  'TPR': 'c',\n",
        "  'VentAlv': 'c',\n",
        "  'VentLung': 'c',\n",
        "  'VentMach': 'c',\n",
        "  'VentTube': 'c'}]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets load the data, create an instance of the learner object and estimate the structure with a small number of samples (100). "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "from libpgm.nodedata import NodeData\n",
      "from libpgm.graphskeleton import GraphSkeleton\n",
      "from libpgm.discretebayesiannetwork import DiscreteBayesianNetwork\n",
      "from libpgm.pgmlearner import PGMLearner\n",
      "\n",
      "data=rand_index(df,n_samples=100)\n",
      "learner = PGMLearner()\n",
      "result=learner.discrete_constraint_estimatestruct(data)\n",
      "print result.E"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['VentLung', 'MinVol'], ['HR', 'HRBP'], ['LVFailure', 'History'], ['BP', 'TPR'], ['SaOb', 'Shunt'], ['Intubation', 'Shunt'], ['PCWP', 'LVEDVolume'], ['Hypovolemia', 'LVEDVolume']]\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To compare the performance of the structure found to the correct alarm network, let's load the alarm network. The file parent-child.txt contains, on each line, the parent vertex, followed by the child vertices (in some cases, some nodes are leaf nodes that have no children)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file = open('parent-child.txt', 'r')\n",
      "\n",
      "def edges(line):\n",
      "    st=line.strip('\\n').strip(' ').split(' ')\n",
      "    #print st\n",
      "    return [[st[0],i] for i in st[1:] ]\n",
      "\n",
      "all_edges=[l for line in file for l in edges(line)]\n",
      "#a set containing the correct edges\n",
      "ground_truth=set([tuple(i) for i in all_edges])\n",
      "print all_edges[:5]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['HISTORY', 'LVFAILURE'], ['CVP', 'LVEDVOLUME'], ['PCWP', 'LVEDVOLUME'], ['LVEDVOLUME', 'HYPOVOLEMIA'], ['LVEDVOLUME', 'LVFAILURE']]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets define a diagnostic function that compares the found network with the correct network. We wish to enquire about the number of edges in both, as well as the number of edges with right directedness, and the edges that connect nodes correctly but have wrong directionality."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def printdiag(result):\n",
      "    found=set([tuple([j.upper() for j in i]) for i in result.E])\n",
      "    correct=ground_truth.intersection(found)\n",
      "    undirected_common_edges=[(i,j) for i,j in found for k,l in ground_truth if i.find(k)!=-1 and j.find(l)!=-1]\n",
      "    print \"number of edges in found network \",len(found),\"out of total number of edges \",len(ground_truth),\\\n",
      "    \" number of edges with correct directionality\",len(correct),\\\n",
      "    \" number of correct undirected edges\",len(undirected_common_edges)\n",
      "\n",
      "printdiag(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of edges in found network  8 out of total number of edges  46  number of edges with correct directionality 3  number of correct undirected edges 3\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets define a function that prints the resulting statistics when the dataset picks a specific number of samples."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn_structure(n_samples):\n",
      "    data=rand_index(df,n_samples)\n",
      "    learner = PGMLearner()\n",
      "    result1=learner.discrete_constraint_estimatestruct(data)\n",
      "    printdiag(result1)\n",
      "    \n",
      "learn_structure(1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of edges in found network  20 out of total number of edges  46  number of edges with correct directionality 6  number of correct undirected edges 6\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lets examine the performance of the learn_structure when provided with 1000 samples. It finds less than half of the edges, and only a few edges  correctly connect nodes. Does increasing the sample size help? (Caution: this may take a few minutes to run)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "learn_structure(10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of edges in found network  30 out of total number of edges  46  number of edges with correct directionality 2  number of correct undirected edges 3\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the number of samples is still not enough to improve the number of edges that are correctly identified. The complexity of the algorithm has a running time of O(n^(d+2)) where n is the number of vertices and d is the upper bound on the witness set. Various versions of this algorithm try to constrain the witness set to improve its performance(described in Chapter 2 of this paper (http://arxiv.org/pdf/1111.6925.pdf))"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}