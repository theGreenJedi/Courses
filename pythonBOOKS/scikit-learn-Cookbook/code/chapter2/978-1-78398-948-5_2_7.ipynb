{
 "metadata": {
  "name": "",
  "signature": "sha256:718eb3e11e828c103a43b59705c9e7847d3c625f8fd6438aa24b806e82b4e980"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.stats as sps"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def log_func(x):\n",
      "    return 1 / (1 + np.exp(-x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting Ready\n",
      "\n",
      "Linear models can actually be used for classification tasks.  This involves fitting a linear model to the probability of a certain class, then using a function to create a threshold at which we specify the outcome one of the classes.\n",
      "\n",
      "The function used here is typically the logistic function (surprise!)  It's a pretty simple function.\n",
      "\n",
      "$$ f(x) = \\frac{1}{1 + e^{-t}} $$\n",
      "\n",
      "Visually it looks like this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, ax = plt.subplots(figsize=(5, 3))\n",
      "rng = np.linspace(-5, 5)\n",
      "log_f = np.apply_along_axis(log_func, 0, rng)\n",
      "ax.set_title(\"Logistic Function between [-5, 5]\")\n",
      "ax.plot(rng, log_f)\n",
      "f.savefig(\"log_regress.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAToAAADSCAYAAADaIgwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIiIiIiIiIiIiIiIiDjx/2Ecw6OQAYzcAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x106fbda50>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the make_classification method and create a dataset and get to classifying!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_classification?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Object `make_classification` not found.\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_classification\n",
      "X, y = make_classification(n_samples=1000, n_features=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#How to do it\n",
      "\n",
      "The `LogisticRegression` object work similarily to the other linear models."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "lr = LogisticRegression()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because we're good data scientists, we're going to pull out the last 200 samples to test the trained model on.  Since this is a random dataset it's fine to hold out the last 200, but if you're dealing with structured data don't do this.  For example, if you were dealing with time series data.  More on cross validation later in the book."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = X[:-200]\n",
      "X_test = X[-200:]\n",
      "y_train = y[:-200]\n",
      "y_test = y[-200:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we need to fit the model with the Logistic Regression.  We'll keep around the predictions on the train set, just like the test set.  It's a good idea to see how often you were correct on both sets.  Often you'll be better on the train set, it's a matter of how much worse you are the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.fit(X_train, y_train)\n",
      "y_train_predictions = lr.predict(X_train)\n",
      "y_test_predictions = lr.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the predictions let's take a look at how good our predictions were.  Here we'll simply look at the number of time we were correct, later we'll talk about evaluating classification models in more detail.\n",
      "\n",
      "This calcuation is simple: it's the number of times we were correct over the total sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_train_predictions == y_train).sum().astype(float) / y_train.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "0.93374999999999997"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now the the test sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_test_predictions == y_test).sum().astype(float) / y_test.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "0.93500000000000005"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So here we were correct about as often in the test set as we were in the train set.  Sadly, in practice, this often isn't the case."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#How it works\n",
      "\n",
      "The question then becomes, how do we go from the logistic function, $ f(x) = \\frac{1}{1 + e^{-t}} $, to a method by which we can classify groups.\n",
      "\n",
      "First, recall that linear regression hopes fo find the linear function that fits the expected value of $Y$ given the values of $X$.  This is then, $E(Y|X) = \\beta X$.  Here the $Y$s are the probabilities of the classes.  Therefore, the the problem we're trying to solve $E(p|X) = \\beta X$.  Then once the threshold is applied, this becomes: $Logit(p) = \\beta X$.  This idea expanded is how other forms of regression work, e.g. Poisson."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#There's more"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Specifying Class Weights\n",
      "\n",
      "We'll see this again, with other classification problems are situation where one class is weighted differently than other classes.  For example once class may be be 99% of cases.  This situation will pop up all over the place in classification work.  The canonical example is fraud detection where most transactions aren't fraud.\n",
      "\n",
      "Let's create a classification problem with 95% imbalance and see how the basic stock logistic regression handles this case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = make_classification(n_samples=5000, n_features=4, weights=[.95])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(y) / (len(y)*1.) #to confirm the class imbalance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "0.054600000000000003"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = X[:-500]\n",
      "X_test = X[-500:]\n",
      "y_train = y[:-500]\n",
      "y_test = y[-500:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.fit(X_train, y_train)\n",
      "y_train_predictions = lr.predict(X_train)\n",
      "y_test_predictions = lr.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_train_predictions == y_train).sum().astype(float) / y_train.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "0.97688888888888892"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_test_predictions == y_test).sum().astype(float) / y_test.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.98599999999999999"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At first it looks like we did amazing, but it turns out, if we had just always guessed not fraud (or class 0 in general) we'd have been write around 95% of the time.  If we look at how well we did in classifying the 1 class it's not nearly as good."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_test[y_test==1] == y_test_predictions[y_test==1]).sum().astype(float) / y_test[y_test==1].shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "0.69999999999999996"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hypothentically, we may care more about identifying fraud cases and are ok we misclassifying non fraud cases we can change our class weights.\n",
      "\n",
      "By default, the classes are weighted (and thusly resampled) in accordence with the inverse of the class weights of the training set.  But because we care more about fraud cases, let's oversample fraud relative to non fraud.\n",
      "\n",
      "We know our relative weighting right now is 95% non fraud, let's change that to overweight fraud cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr = LogisticRegression(class_weight={0: .15, 1: .85})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lr.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "LogisticRegression(C=1.0, class_weight={0: 0.15, 1: 0.85}, dual=False,\n",
        "          fit_intercept=True, intercept_scaling=1, penalty='l2',\n",
        "          random_state=None, tol=0.0001)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_predictions = lr.predict(X_train)\n",
      "y_test_predictions = lr.predict(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can that we did a much better job on classifying fraud cases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_test[y_test==1] == y_test_predictions[y_test==1]).sum().astype(float) / y_test[y_test==1].shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "0.80000000000000004"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But at what expense?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(y_test_predictions == y_test).sum().astype(float) / y_test.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "0.95799999999999996"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, only about 1% less accuracy.  Whether that's ok, depends on your problem."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}