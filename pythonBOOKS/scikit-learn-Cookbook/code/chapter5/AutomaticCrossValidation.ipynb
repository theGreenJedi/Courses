{
 "metadata": {
  "name": "",
  "signature": "sha256:366ea064fff66e1aad474b6f548bf3ed8bb5f1979844d135ce8596164ee579b9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We've looked at the using cross validation iterators that scikit-learn comes with, but we can also use a helper function to do this for use automatically.  This is similar to how other objects in scikit-learn are wrapped by helper functions, Pipeline for instance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting Ready"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we'll need to create a sample classifier, this can really be anything, a decision tree, a random forest, whatever.  For us it'll be a random forest.  We'll then create a dataset, and use the cross validation functions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#How to do it"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import ensemble\n",
      "rf = ensemble.RandomForestRegressor(max_features='auto')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so now let's create some regression data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "X, y = datasets.make_regression(10000, 10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the data we can import the `cross_validation` module and get access to the functions we'll use."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import cross_validation"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_validation.cross_val_score(rf, X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.86823874  0.86763225  0.86986129]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How it works"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For the most part, this will deligate to the cross validation objects.  One nice thing is that the function will handle doing the cross validation in parallel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can activate verbose mode play-by-play."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = cross_validation.cross_val_score(rf, X, y, verbose=3, cv=4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[CV] no parameters to be set .........................................\n",
        "[CV] ................ no parameters to be set, score=0.872866 -   0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[CV] no parameters to be set .........................................\n",
        "[CV] ................ no parameters to be set, score=0.873679 -   0.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[CV] no parameters to be set .........................................\n",
        "[CV] ................ no parameters to be set, score=0.878018 -   0.7s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[CV] no parameters to be set .........................................\n",
        "[CV] ................ no parameters to be set, score=0.871598 -   0.6s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=1)]: Done   1 jobs       | elapsed:    0.7s\n",
        "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    2.6s finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see during each iteration we scored the function.  We also get an idea of how long the model took to run."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's also worth knowing that we can score our function predicated on which kind of model we're trying to fit.  In other recipes we've talked about how to create your own scoring function."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}