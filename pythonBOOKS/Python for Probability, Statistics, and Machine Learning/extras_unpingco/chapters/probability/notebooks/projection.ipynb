{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Methods\n",
    "\n",
    "The concept of projection is key to developing an intuition about conditional\n",
    "probability.  We already have a natural intuition of projection from looking at\n",
    "the shadows of objects on a sunny day. As we will see, this simple idea\n",
    "consolidates many abstract ideas in optimization and mathematics.  Consider\n",
    "[Figure](#fig:probability_001) where we want to find a point along the blue\n",
    "line (namely, $\\mathbf{x}$) that is closest to the black square (namely,\n",
    "$\\mathbf{y}$). In other words, we want to inflate the gray circle until it just\n",
    "touches the black line. Recall that the circle boundary is the set of points for\n",
    "which"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sqrt{(\\mathbf{y}-\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x})} =\\|\\mathbf{y}-\\mathbf{x} \\| = \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for some value of $\\epsilon$. So we want a point $\\mathbf{x}$ along\n",
    "the line that satisfies this for the smallest $\\epsilon$.  Then, that point\n",
    "will be the closest point on the black line to the black square.\n",
    " It may be obvious from the diagram, but the closest point on the line\n",
    "occurs where the line segment from the black square to the black line is\n",
    "perpedicular to the line. At this point, the gray circle just touches the black\n",
    "line. This is illustrated below in [Figure](#fig:probability_002).\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/probability_001.png, width=500 frac=0.90] Given the point $\\mathbf{y}$ (black square) we want to find the $\\mathbf{x}$ along the line that is closest to it. The gray circle is the locus of points within a fixed distance from $\\mathbf{y}$. <div id=\"fig:probability_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:probability_001\"></div>\n",
    "\n",
    "<p>Given the point $\\mathbf{y}$ (black square) we want to find the $\\mathbf{x}$ along the line that is closest to it. The gray circle is the locus of points within a fixed distance from $\\mathbf{y}$.</p>\n",
    "<img src=\"fig-probability/probability_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "[Figure](#fig:probability_001) uses the `matplotlib.patches` module.  This\n",
    "module contains primitive shapes like circles, ellipses, and rectangles that\n",
    "can be assembled into complex graphics.  As shown in the code in the IPython\n",
    "Notebook corresponding to this chapter, after importing a particular shape, you\n",
    "can apply that shape to an existing axis  using the `add_patch` method.  The\n",
    "patches themselves can by styled using the usual formatting keywords like\n",
    "`color` and `alpha`.\n",
    "\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/probability_002.png, width=500 frac=0.90] The closest point on the line occurs when the line is tangent to the circle. When this happens, the black line and the line (minimum distance) are perpedicular.  <div id=\"fig:probability_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:probability_002\"></div>\n",
    "\n",
    "<p>The closest point on the line occurs when the line is tangent to the circle. When this happens, the black line and the line (minimum distance) are perpedicular.</p>\n",
    "<img src=\"fig-probability/probability_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    " Now that we can see what's going on, we can construct the the solution\n",
    "analytically. We can represent an arbitrary point along the black line as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}=\\alpha\\mathbf{v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\alpha\\in\\mathbb{R}$ slides the point up and down the line with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v} = \\left[ 1,1 \\right]^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Formally, $\\mathbf{v}$ is the *subspace* onto which we want to\n",
    "*project* $\\mathbf{y}$. At the closest point, the vector between\n",
    "$\\mathbf{y}$ and $\\mathbf{x}$ (the *error* vector above) is\n",
    "perpedicular to the line. This means that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(\\mathbf{y}-\\mathbf{x} )^T \\mathbf{v} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and by substituting and working out the terms, we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\alpha = \\frac{\\mathbf{y}^T\\mathbf{v}}{ \\|\\mathbf{v} \\|^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The *error* is the distance between $\\alpha\\mathbf{v}$ and $\n",
    "\\mathbf{y}$.  This is a right triangle, and we can use the Pythagorean\n",
    "theorem to compute the squared length of this error as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon^2 = \\|( \\mathbf{y}-\\mathbf{x} )\\|^2 = \\|\\mathbf{y}\\|^2 - \\alpha^2 \\|\\mathbf{v}\\|^2 = \\|\\mathbf{y}\\|^2 - \\frac{\\|\\mathbf{y}^T\\mathbf{v}\\|^2}{\\|\\mathbf{v}\\|^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $ \\|\\mathbf{v}\\|^2 = \\mathbf{v}^T \\mathbf{v} $. Note that since $\\epsilon^2 \\ge 0 $, this also shows that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\| \\mathbf{y}^T\\mathbf{v}\\| \\le \\|\\mathbf{y}\\|  \\|\\mathbf{v}\\|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is the famous and useful Cauchy-Schwarz inequality which we\n",
    "will exploit later. Finally, we can assemble all of this into the *projection*\n",
    "operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}_v = \\frac{1}{\\|\\mathbf{v}\\|^2 } \\mathbf{v v}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With this operator, we can take any $\\mathbf{y}$ and find the closest\n",
    "point on $\\mathbf{v}$ by doing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}_v \\mathbf{y} = \\mathbf{v} \\left( \\frac{  \\mathbf{v}^T \\mathbf{y} }{\\|\\mathbf{v}\\|^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where we recognize the term in parenthesis as the $\\alpha$ we\n",
    "computed earlier.  It's called an *operator* because it takes a vector\n",
    "($\\mathbf{y}$) and produces another vector ($\\alpha\\mathbf{v}$). Thus,\n",
    "projection unifies geometry and optimization.\n",
    "\n",
    "## Weighted distance\n",
    "\n",
    "We can easily extend this projection operator to cases where the measure of\n",
    "distance between $\\mathbf{y}$ and the subspace $\\mathbf{v}$ is weighted. We can\n",
    "accommodate these weighted distances by re-writing the projection operator as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:weightedProj\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_v=\\mathbf{v}\\frac{\\mathbf{v}^T\\mathbf{Q}^T}{\\mathbf{v}^T\\mathbf{Q v}}\n",
    "\\end{equation}\n",
    "\\label{eq:weightedProj} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\mathbf{Q}$ is positive definite matrix.  In the previous\n",
    "case, we started with a point $\\mathbf{y}$ and inflated a circle centered at\n",
    "$\\mathbf{y}$ until it just touched the line defined by $\\mathbf{v}$ and this\n",
    "point was closest point on the line to $\\mathbf{y}$. The same thing happens\n",
    "in the general case with a weighted distance except now we inflate an\n",
    "ellipse, not a circle, until the ellipse touches the line.\n",
    "\n",
    "<!-- # @@@CODE src-probability/Projection.py fromto: ^theta@^fig,ax -->\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/probability_003.png, width=500 frac=0.95] In the weighted case, the closest point on the line is tangent to the ellipse and is still perpedicular in the sense of the weighted distance. <div id=\"fig:probability_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:probability_003\"></div>\n",
    "\n",
    "<p>In the weighted case, the closest point on the line is tangent to the ellipse and is still perpedicular in the sense of the weighted distance.</p>\n",
    "<img src=\"fig-probability/probability_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "Note that the error vector ($\\mathbf{y}-\\alpha\\mathbf{v}$) in [Figure](#fig:probability_003) is still perpendicular to the line (subspace\n",
    "$\\mathbf{v}$), but in the space of the weighted distance.  The difference\n",
    "between the first projection (with the uniform circular distance) and the\n",
    "general case (with the elliptical weighted distance) is the inner product\n",
    "between the two cases.  For example, in the first case we have $\\mathbf{y}^T\n",
    "\\mathbf{v}$ and in the weighted case we have $\\mathbf{y}^T \\mathbf{Q}^T\n",
    "\\mathbf{v}$. To move from the uniform circular case to the weighted ellipsoidal\n",
    "case, all we had to do was change all of the vector inner products.  Before we\n",
    "finish, we need a formal property  of projections:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{P}_v \\mathbf{P}_v = \\mathbf{P}_v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " known as the *idempotent* property which basically says that once we\n",
    "have projected onto a subspace, subsequent projections leave us in the\n",
    "same subspace. You can verify this by computing Equation ref{eq:weightedProj}.\n",
    "\n",
    "Thus, projection ties a minimization problem (closest point to a line) to an\n",
    "algebraic concept (inner product). It turns out that these same geometric ideas\n",
    "from linear algebra [[strang2006linear]](#strang2006linear) can be translated to the conditional\n",
    "expectation. How this works is the subject of our next  section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
