{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We considered Maximum Likelihood Estimation (MLE) and Maximum A-Posteriori\n",
    "(MAP) estimation and in each case we started out with a probability density\n",
    "function of some kind and we further assumed that the samples were identically\n",
    "distributed and independent (iid). The idea behind robust statistics\n",
    "[[maronna2006robust]](#maronna2006robust) is to construct estimators that can survive the\n",
    "weakening of either or both of these assumptions. More concretely, suppose you\n",
    "have a model that works great except for a few outliers. The temptation is to\n",
    "just ignore the outliers and proceed. Robust estimation methods provide a\n",
    "disciplined way to handle outliers without cherry-picking data that works for\n",
    "your favored model.\n",
    "\n",
    "### The Notion of Location\n",
    "\n",
    "The first notion we need is *location*, which is  a generalization of the idea\n",
    "of *central value*. Typically, we just use an estimate of the mean for this,\n",
    "but we will see later why this could be a bad idea.  The general idea of\n",
    "location satisfies the following requirements Let $X$ be a random variable with\n",
    "distribution $F$, and let $\\theta(X)$ be some descriptive measure of $F$. Then\n",
    "$\\theta(X)$ is said to be a measure of *location* if for any constants *a* and\n",
    "*b*, we have the following:\n",
    "\n",
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\theta(X+b)  = \\theta(X) +b \n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "\\theta(-X)   = -\\theta(X)  \n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "X \\ge 0 \\Rightarrow \\theta(X)  \\ge 0  \n",
    "\\label{_auto3} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "\\theta(a X) = a\\theta(X)\n",
    "\\label{_auto4} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first condition is called *location equivariance* (or *shift-invariance* in\n",
    "signal processing lingo). The fourth condition is called *scale equivariance*,\n",
    "which means that the units that $X$ is measured in should not effect the value\n",
    "of the  location estimator.  These requirements capture the intuition of\n",
    "*centrality* of a distribution, or where most of the\n",
    "probability mass is located.\n",
    "\n",
    "For example, the sample mean estimator is $ \\hat{\\mu}=\\frac{1}{n}\\sum X_i $. The first\n",
    "requirement is obviously satisfied as $ \\hat{\\mu}=\\frac{1}{n}\\sum (X_i+b) = b +\n",
    "\\frac{1}{n}\\sum X_i =b+\\hat{\\mu}$. Let us consider the second requirement:$\n",
    "\\hat{\\mu}=\\frac{1}{n}\\sum -X_i = -\\hat{\\mu}$. Finally, the last requirement is\n",
    "satisfied with $ \\hat{\\mu}=\\frac{1}{n}\\sum a X_i =a \\hat{\\mu}$.\n",
    "\n",
    "### Robust Estimation and Contamination\n",
    "\n",
    "Now that we have the generalized location of centrality embodied in the\n",
    "*location* parameter, what can we do with it?  Previously, we assumed that our samples\n",
    "were all identically distributed. The key idea is that the samples might be\n",
    "actually coming from a *single* distribution that is contaminated by another nearby\n",
    "distribution, as in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X) = \\epsilon G(X) + (1-\\epsilon)H(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $ \\epsilon $ randomly toggles between zero and one. This means\n",
    "that our data samples $\\lbrace X_i \\rbrace$ actually derived from two separate\n",
    "distributions, $ G(X) $ and $ H(X) $. We just don't know how they are mixed\n",
    "together. What we really want  is an estimator  that captures the location of $\n",
    "G(X) $ in the face of random intermittent contamination by $ H(X)$.  For\n",
    "example, it may be that this contamination is responsible for the outliers in a\n",
    "model that otherwise works well with the dominant $F$ distribution. It can get\n",
    "even worse than that because we don't know that there is only one contaminating\n",
    "$H(X)$ distribution out there. There may be a whole family of distributions\n",
    "that are contaminating $G(X)$. This means that whatever estimators we construct\n",
    "have to be derived from a more generalized family of distributions instead of\n",
    "from a single distribution,  as the maximum-likelihood method assumes.  This is\n",
    "what makes robust estimation so difficult --- it has to deal with *spaces* of\n",
    "function distributions instead of parameters from a particular probability\n",
    "distribution.\n",
    "\n",
    "### Generalized Maximum Likelihood Estimators\n",
    "\n",
    "M-estimators are generalized maximum likelihood estimators. Recall that for\n",
    "maximum likelihood, we want to maximize the likelihood function as in the\n",
    "following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{\\mu}(x_i) = \\prod f_0(x_i-\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then to find the estimator $\\hat{\\mu}$ so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mu} = \\arg \\max_{\\mu} L_{\\mu}(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  So far, everything is the same as our usual maximum-likelihood\n",
    "derivation except for the fact that we don't assume a specific $f_0$ as the\n",
    "distribution of the $\\lbrace X_i\\rbrace$. Making the definition of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho = -\\log f_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we obtain the more convenient form of the likelihood product and the\n",
    "optimal $\\hat{\\mu}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mu} = \\arg \\min_{\\mu} \\sum \\rho(x_i-\\mu)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If $\\rho$ is differentiable, then differentiating  this with respect\n",
    "to $\\mu$ gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:muhat\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum \\psi(x_i-\\hat{\\mu}) = 0 \n",
    "\\label{eq:muhat} \\tag{5}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with $\\psi = \\rho^\\prime$, the first derivative of $\\rho$ , and for technical reasons we will assume that\n",
    "$\\psi$ is increasing. So far, it looks like we just pushed some definitions\n",
    "around, but the key idea is we want to consider general $\\rho$ functions that\n",
    "may not be maximum likelihood estimators for *any* distribution. Thus, our\n",
    "focus is now on uncovering the nature of $\\hat{\\mu}$.\n",
    "\n",
    "### Distribution of M-estimates\n",
    "\n",
    "For a given distribution $F$, we define $\\mu_0=\\mu(F)$ as the solution to the\n",
    "following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}_F(\\psi(x-\\mu_0))= 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is technical to show, but it turns out that $\\hat{\\mu} \\sim\n",
    "\\mathcal{N}(\\mu_0,\\frac{v}{n})$ with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v = \\frac{\\mathbb{E}_F(\\psi(x-\\mu_0)^2)}{(\\mathbb{E}_F(\\psi^\\prime(x-\\mu_0)))^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, we can say that $\\hat{\\mu}$ is asymptotically normal with asymptotic\n",
    "value $\\mu_0$ and asymptotic variance $v$. This leads to the efficiency ratio\n",
    "which is defined as  the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{Eff}(\\hat{\\mu})= \\frac{v_0}{v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $v_0$ is the asymptotic variance of the MLE and measures how\n",
    "near $\\hat{\\mu}$ is to the optimum. In other words, this provides a sense of\n",
    "how much outlier contamination costs in terms of samples. For example, if for\n",
    "two estimates with asymptotic variances $v_1$ and $v_2$, we have $v_1=3v_2$,\n",
    "then first estimate requires three times as many observations to obtain the\n",
    "same variance as the second. Furthermore, for the sample mean (i.e.,\n",
    "$\\hat{\\mu}=\\frac{1}{n} \\sum X_i$) with $F=\\mathcal{N}$, we have $\\rho=x^2/2$\n",
    "and $\\psi=x$ and also $\\psi'=1$. Thus, we have $v=\\mathbb{V}(x)$.\n",
    "Alternatively, using the sample median as the estimator for the location, we\n",
    "have $v=1/(4 f(\\mu_0)^2)$.  Thus, if we have $F=\\mathcal{N}(0,1)$, for the\n",
    "sample median, we obtain $v={2\\pi}/{4} \\approx 1.571$. This means that the\n",
    "sample median takes approximately 1.6 times as many samples to obtain the same\n",
    "variance for the location as the sample mean. The sample median is \n",
    "far more immune to the effects of outliers than the sample mean, so this \n",
    "gives a sense of how much this robustness costs in samples.\n",
    "\n",
    "** M-Estimates as Weighted Means.** One way to think about M-estimates is a\n",
    "weighted means. Operationally, this\n",
    "means that we want weight functions that can circumscribe the\n",
    "influence of the individual data points, but, when taken as a whole,\n",
    "still provide good estimated parameters. Most of the time, we have $\\psi(0)=0$ and $\\psi'(0)$ exists so\n",
    "that $\\psi$ is approximately linear at the origin. Using the following\n",
    "definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W(x)  =  \\begin{cases}\n",
    "                \\psi(x)/x & \\text{if} \\: x \\neq 0 \\\\\\\n",
    "                \\psi'(x)  & \\text{if} \\: x =0 \n",
    "            \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can write our Equation ref{eq:muhat} as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:Wmuhat\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sum W(x_i-\\hat{\\mu})(x_i-\\hat{\\mu}) = 0 \n",
    "\\label{eq:Wmuhat} \\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Solving this for $\\hat{\\mu} $ yields the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mu} = \\frac{\\sum w_{i} x_i}{\\sum w_{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $w_{i}=W(x_i-\\hat{\\mu})$. This is not practically useful\n",
    "because the $w_i$ contains $\\hat{\\mu}$, which is what we are trying to solve\n",
    "for. The question that remains is how to pick the $\\psi$ functions. This is\n",
    "still an open question, but the Huber functions are a well-studied choice.\n",
    "\n",
    "### Huber Functions\n",
    "\n",
    "The family of Huber functions is defined by the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho_k(x ) = \\begin{cases}\n",
    "                x^2         & \\mbox{if }  |x|\\leq k \\\\\\\n",
    "                2 k |x|-k^2 & \\mbox{if }  |x| >  k\n",
    "                \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with corresponding derivatives $2\\psi_k(x)$ with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\psi_k(x ) = \\begin{cases}\n",
    "                x              & \\mbox{if } \\: |x| \\leq k \\\\\\\n",
    "                \\text{sgn}(x)k & \\mbox{if } \\: |x| >  k\n",
    "                \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the limiting cases $k \\rightarrow \\infty$ and $k \\rightarrow 0$\n",
    "correspond to the mean and median, respectively. To see this, take\n",
    "$\\psi_{\\infty} = x$ and therefore $W(x) = 1$ and thus the defining Equation\n",
    "ref{eq:Wmuhat} results in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{i=1}^{n} (x_i-\\hat{\\mu}) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then solving this leads to $\\hat{\\mu} = \\frac{1}{n}\\sum x_i$.\n",
    "Note that choosing $k=0$ leads to  the sample median, but that is not so\n",
    "straightforward to solve for. Nonetheless, Huber functions provide a way\n",
    "to move between two extremes of estimators for location (namely, \n",
    "the mean vs. the median) with a tunable parameter $k$. \n",
    "The $W$ function corresponding to Huber's $\\psi$ is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W_k(x) = \\min\\Big{\\lbrace} 1, \\frac{k}{|x|} \\Big{\\rbrace}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Figure](#fig:Robust_Statistics_0001) shows the Huber weight\n",
    "function for $k=2$ with some sample points. The idea is that the computed\n",
    "location, $\\hat{\\mu}$ is computed from Equation ref{eq:Wmuhat} to lie somewhere\n",
    "in the middle of the weight function so that those terms (i.e., *insiders*)\n",
    "have their values fully reflected in the location estimate. The black circles\n",
    "are the *outliers* that have their values attenuated by the weight function so\n",
    "that only a fraction of their presence is represented in the location estimate.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Robust_Statistics_0001.png, width=500 frac=0.80] This shows the Huber weight function, $W_2(x)$ and some cartoon data points that are insiders or outsiders as far as the robust location estimate is concerned.  <div id=\"fig:Robust_Statistics_0001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Robust_Statistics_0001\"></div>\n",
    "\n",
    "<p>This shows the Huber weight function, $W_2(x)$ and some cartoon data points that are insiders or outsiders as far as the robust location estimate is concerned.</p>\n",
    "<img src=\"fig-statistics/Robust_Statistics_0001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "### Breakdown Point\n",
    "\n",
    "So far, our discussion of robustness has been very abstract.  A more concrete\n",
    "concept of robustness comes from the breakdown point.  In the simplest terms,\n",
    "the breakdown point describes what happens when a single data point in an\n",
    "estimator is changed in the most damaging way possible. For example, suppose we\n",
    "have the sample mean, $\\hat{\\mu}=\\sum x_i/n$, and we take one of the $x_i$\n",
    "points to be infinite. What happens to this estimator? It also goes infinite.\n",
    "This means that the breakdown point of the estimator is 0%. On the other hand,\n",
    "the median has a breakdown point of 50%, meaning that half of the data for\n",
    "computing the median could go infinite without affecting the median value. The median\n",
    "is a *rank* statistic that cares more about the relative ranking of the data\n",
    "than the values of the data, which explains its robustness.\n",
    "\n",
    "The simpliest but still formal way to express the breakdown point is to\n",
    "take $n$ data points, $\\mathcal{D} = \\lbrace (x_i,y_i) \\rbrace$. Suppose $T$\n",
    "is a regression estimator that yields a vector of regression coefficients,\n",
    "$\\boldsymbol{\\theta}$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "T(\\mathcal{D}) = \\boldsymbol{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Likewise, consider all possible corrupted samples of the data\n",
    "$\\mathcal{D}^\\prime$. The maximum *bias* caused by this contamination is\n",
    "the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{bias}_{m} = \\sup_{\\mathcal{D}^\\prime} \\Vert T(\\mathcal{D^\\prime})-T(\\mathcal{D}) \\Vert\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where the $\\sup$ sweeps over all possible sets of $m$ contaminated samples.\n",
    "Using this, the breakdown point is defined as the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\epsilon_m = \\min \\Big\\lbrace \\frac{m}{n} \\colon \\texttt{bias}_{m} \\rightarrow \\infty \\Big\\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, in our least-squares regression, even one point at\n",
    "infinity causes an infinite $T$. Thus, for least-squares regression,\n",
    "$\\epsilon_m=1/n$. In the limit $n \\rightarrow \\infty$, we have $\\epsilon_m\n",
    "\\rightarrow 0$.\n",
    "\n",
    "### Estimating Scale\n",
    "\n",
    "In robust statistics, the concept of *scale* refers to a measure of the\n",
    "dispersion of the data. Usually, we use the\n",
    "estimated standard deviation for this, but this has a terrible breakdown point.\n",
    "Even more troubling, in order to get a good estimate of location, we have to\n",
    "either somehow know the scale ahead of time, or jointly estimate it. None of\n",
    "these methods have easy-to-compute closed form solutions and must be computed\n",
    "numerically.\n",
    "\n",
    "The most popular method for estimating scale is the *median absolute deviation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{MAD} = \\texttt{Med} (\\vert \\mathbf{x} - \\texttt{Med}(\\mathbf{x})\\vert)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In words, take the median of the data $\\mathbf{x}$ and\n",
    "then subtract that median from the data itself, and then take the median of the\n",
    "absolute value of the result. Another good dispersion estimate is the *interquartile range*,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{IQR} = x_{(n-m+1)} - x_{(n)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $m= [n/4]$. The $x_{(n)}$ notation means the $n^{th}$ data\n",
    "element after the data have been sorted. Thus, in this notation,\n",
    "$\\texttt{max}(\\mathbf{x})=x_{(n)}$. In the case where $x \\sim\n",
    "\\mathcal{N}(\\mu,\\sigma^2)$, then $\\texttt{MAD}$ and $\\texttt{IQR}$ are constant\n",
    "multiples of $\\sigma$ such that the normalized $\\texttt{MAD}$ is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\texttt{MADN}(x) = \\frac{\\texttt{MAD} }{0.675}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The number comes from the inverse CDF of the normal distribution\n",
    "corresponding to the $0.75$ level. Given the complexity of the\n",
    "calculations, *jointly* estimating both location and scale is a purely\n",
    "numerical matter. Fortunately, the Statsmodels module has many of these\n",
    "ready to use. Let's create some contaminated data in the following code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "data=np.hstack([stats.norm(10,1).rvs(10),stats.norm(0,1).rvs(100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These data correspond to our model of contamination that we started\n",
    "this section with. As shown in the  histogram in [Figure](#fig:Robust_Statistics_0002), there are two normal distributions, one\n",
    "centered neatly at zero, representing the majority of the samples, and another\n",
    "coming less regularly from the normal distribution on the right. Notice that\n",
    "the group of infrequent samples on the right separates the mean and median\n",
    "estimates (vertical dotted and dashed lines).  In the absence of the\n",
    "contaminating distribution on the right, the standard deviation for this data\n",
    "should be close to one. However, the usual non-robust estimate for standard\n",
    "deviation (`np.std`) comes out to approximately three.  Using the\n",
    "$\\texttt{MADN}$ estimator (`sm.robust.scale.mad(data)`) we obtain approximately\n",
    "1.25. Thus, the robust estimate of dispersion is less moved by the presence of\n",
    "the  contaminating distribution.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Robust_Statistics_0002.png, width=500 frac=0.85] Histogram of sample data. Notice that the group of infrequent samples on the right separates the mean and median estimates indicated by the vertical lines.  <div id=\"fig:Robust_Statistics_0002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Robust_Statistics_0002\"></div>\n",
    "\n",
    "<p>Histogram of sample data. Notice that the group of infrequent samples on the right separates the mean and median estimates indicated by the vertical lines.</p>\n",
    "<img src=\"fig-statistics/Robust_Statistics_0002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "The generalized maximum likelihood M-estimation extends to joint\n",
    "scale and location estimation using Huber functions. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "huber = sm.robust.scale.Huber()\n",
    "loc,scl=huber(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which implements Huber's *proposal two* method of joint estimation of\n",
    "location and scale. This kind of estimation is the key ingredient to robust\n",
    "regression methods, many of which are implemented in Statsmodels in\n",
    "`statsmodels.formula.api.rlm`. The corresponding documentation has more\n",
    "information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
