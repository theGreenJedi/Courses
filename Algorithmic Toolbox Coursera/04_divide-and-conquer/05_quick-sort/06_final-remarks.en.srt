1
00:00:00,250 --> 00:00:03,770
In this last video of
the Quicksort lesson,

2
00:00:03,770 --> 00:00:07,680
I would like to address
two implementation issues.

3
00:00:07,680 --> 00:00:12,980
So the first issue is about space
complexity of the QuickSort algorithm.

4
00:00:12,980 --> 00:00:18,750
So on one hand, when sorting
an array by a Quicksort algorithm,

5
00:00:18,750 --> 00:00:21,365
we do not use any additional space.

6
00:00:21,365 --> 00:00:26,120
We just partition the array and
with small elements inside the array.

7
00:00:26,120 --> 00:00:30,190
On the other hand, the QuickSort
algorithm is a recursive algorithm.

8
00:00:30,190 --> 00:00:34,660
And when we make a recursive call we
store some information on this tech.

9
00:00:34,660 --> 00:00:35,980
Right?

10
00:00:35,980 --> 00:00:39,880
So on one hand it is possible to show that

11
00:00:39,880 --> 00:00:45,430
the average recurrent
depths is logarithmic.

12
00:00:45,430 --> 00:00:49,090
Meaning that we need only
a logarithmic additional space.

13
00:00:49,090 --> 00:00:51,600
On the other hand,
there is a very nice and

14
00:00:51,600 --> 00:00:56,960
elegant trick that allows to
re-implement the QuickSort algorithm,

15
00:00:56,960 --> 00:01:01,930
such that it's worst case space
complexity is at most logarithmic.

16
00:01:03,250 --> 00:01:08,730
So for this, let's recall that
the QuickSort algorithm contains

17
00:01:08,730 --> 00:01:13,850
of the call to the partition procedure and
then of two recursive calls.

18
00:01:14,980 --> 00:01:19,550
So the situation when we have
a recursive call is and,

19
00:01:19,550 --> 00:01:24,340
if the procedure is called tail recursion.

20
00:01:24,340 --> 00:01:27,317
And there is a known way to
eliminate such a recursive call.

21
00:01:27,317 --> 00:01:32,519
Namely, instead of making this
recursive call, let's just update.

22
00:01:32,519 --> 00:01:39,507
Well, in the second recursive call,
we sort the right part of our array.

23
00:01:39,507 --> 00:01:42,665
I mean,
the part from index n+1 to index r.

24
00:01:42,665 --> 00:01:47,726
Instead of making this recursive call,
let's replace the with a while loop,

25
00:01:47,726 --> 00:01:52,255
inside this while loop we call the
partition procedure as shown on the slide.

26
00:01:52,255 --> 00:01:56,004
Then we make a recursive
call to the left part, but

27
00:01:56,004 --> 00:02:00,292
instead of making the recursive call for
the right part,

28
00:02:00,292 --> 00:02:04,366
we'll just update the value
of l to be equal to m+1.

29
00:02:04,366 --> 00:02:08,380
And then we go to the beginning
of this while loop, and

30
00:02:08,380 --> 00:02:11,520
this essentially mimics
our recursive call.

31
00:02:12,680 --> 00:02:14,160
So far so good.

32
00:02:14,160 --> 00:02:18,950
We've just realized that we can
eliminate the last recursive call.

33
00:02:18,950 --> 00:02:21,660
At the same time let's also
realize the following thing.

34
00:02:22,660 --> 00:02:27,090
In our QuickSort algorithm we first
call the partition precision,

35
00:02:27,090 --> 00:02:29,800
then we make two recursive calls.

36
00:02:29,800 --> 00:02:33,540
And these two recursive calls
are in a sense independent.

37
00:02:33,540 --> 00:02:36,450
Well it doesn't matter which comes first,
right?

38
00:02:36,450 --> 00:02:39,180
So they do not depend on each other.

39
00:02:39,180 --> 00:02:44,820
This means that we can as well eliminate
a recursive call through the first part.

40
00:02:44,820 --> 00:02:49,620
Well, and this in turn means that we can
always select which one to eliminate.

41
00:02:49,620 --> 00:02:54,680
And for us, it is better to remove
a recursive call to a longer part.

42
00:02:54,680 --> 00:02:59,760
And this is why, if we always make
a recursive call during the rate

43
00:02:59,760 --> 00:03:05,100
which is shorter, then we make
a recursive call during the rate

44
00:03:05,100 --> 00:03:10,090
which is at least twice shorter
than the initial already, right?

45
00:03:10,090 --> 00:03:15,072
And this in turn means that the depths of
our recursion will be at most logarithmic.

46
00:03:15,072 --> 00:03:18,090
Because well,
the first recursive call is made for

47
00:03:18,090 --> 00:03:23,300
an array of size of at most n over 2,
then at most n over 4 and so on.

48
00:03:23,300 --> 00:03:26,550
So the depth is logarithmic,
which is good.

49
00:03:26,550 --> 00:03:28,460
And this can be implemented as follows.

50
00:03:28,460 --> 00:03:32,750
So we first call the partition procedure.

51
00:03:32,750 --> 00:03:36,530
It gives us a value of m.

52
00:03:36,530 --> 00:03:40,530
At this point,
we know the length of two parts.

53
00:03:40,530 --> 00:03:42,140
And we just compare them.

54
00:03:42,140 --> 00:03:46,430
If, for example,
the lengths of the first part is shorter,

55
00:03:46,430 --> 00:03:48,860
then we make a recursive
call to this part.

56
00:03:48,860 --> 00:03:52,170
And instead of making the recursive
call for the second part,

57
00:03:52,170 --> 00:03:54,470
we just update the value of l.

58
00:03:54,470 --> 00:03:59,421
In the other case when the right part is
shorter, we make the recursive call for

59
00:03:59,421 --> 00:04:03,697
this part, and instead of making
the recursive call for this part,

60
00:04:03,697 --> 00:04:06,610
we'll just update the value of r.

61
00:04:06,610 --> 00:04:07,294
Right?
So

62
00:04:07,294 --> 00:04:12,835
overall this gives us an implementation
of the QuickSort algorithm

63
00:04:12,835 --> 00:04:18,001
which uses in the worst case
an additional logarithmic space.

64
00:04:18,001 --> 00:04:24,317
So the next implementation issue concerns
the random bits used by our algorithm.

65
00:04:24,317 --> 00:04:27,126
So I assume that we would like to have

66
00:04:27,126 --> 00:04:31,538
a deterministic version of
our randomized QuickSort.

67
00:04:31,538 --> 00:04:36,088
And this is a reasonable thing to want
because in practice we would like to have

68
00:04:36,088 --> 00:04:40,950
such a thing as reproducibility, which
is for example essential for debugging.

69
00:04:40,950 --> 00:04:47,380
So we would like our program to always
output the same, on the same dataset.

70
00:04:47,380 --> 00:04:53,550
And this is why we would probably not
like to use random numbers, okay?

71
00:04:53,550 --> 00:04:55,510
Then we can do the following.

72
00:04:55,510 --> 00:04:57,850
The following algorithm is
known as intro sort and

73
00:04:57,850 --> 00:05:02,050
is used in many practical
implementation of QuickSort.

74
00:05:02,050 --> 00:05:05,220
So instead of selecting
the pivot element randomly,

75
00:05:05,220 --> 00:05:10,140
let's select it as follows using, for
example, the following simple heuristic.

76
00:05:10,140 --> 00:05:12,370
Each time when we're given a summary, and

77
00:05:12,370 --> 00:05:15,770
we need to partition it
with respect to some pivot.

78
00:05:15,770 --> 00:05:18,860
So for this we need to select pivot,
and let's select it as follows.

79
00:05:18,860 --> 00:05:21,610
We take the first element of the summary,
the last element and

80
00:05:21,610 --> 00:05:24,140
the middle element, for example.

81
00:05:24,140 --> 00:05:26,560
Then we have three elements,
and we sort them.

82
00:05:26,560 --> 00:05:30,050
We just compare them and
we select the medium value of these.

83
00:05:31,460 --> 00:05:34,570
And we use this element
as our pivot element.

84
00:05:34,570 --> 00:05:38,590
So this is a very simple heuristic,
it can be implemented very efficiently.

85
00:05:38,590 --> 00:05:43,800
We just need three comparisons
to select this median.

86
00:05:43,800 --> 00:05:50,690
And in many cases this is enough for the
QuickSort algorithm to work effectively.

87
00:05:50,690 --> 00:05:53,080
However, this is not what we want, right.

88
00:05:53,080 --> 00:05:56,200
We are not happy with the statement
that this algorithm works.

89
00:05:56,200 --> 00:05:57,640
Works well in many cases.

90
00:05:57,640 --> 00:06:02,250
We would like our algorithm to works
well just on every possible input.

91
00:06:03,510 --> 00:06:09,190
Unfortunately there are pathological cases
in which these heuristics works badly.

92
00:06:09,190 --> 00:06:12,240
But we can overcome this
in the following way.

93
00:06:12,240 --> 00:06:17,440
While running our QuickSort algorithm,

94
00:06:17,440 --> 00:06:21,820
well let's count what is the current
depths of our recursion three.

95
00:06:21,820 --> 00:06:27,120
And at some point when it exceeds some
values here again, for some constant c,

96
00:06:27,120 --> 00:06:31,700
then we just stop the current algorithm
and switch to some other algorithm.

97
00:06:31,700 --> 00:06:34,160
For example for the heap sort algorithm.

98
00:06:34,160 --> 00:06:38,528
This is another efficient algorithm,
which is,

99
00:06:38,528 --> 00:06:45,496
asymptotically as good as MergeSort I
mean, it has asymptotic again.

100
00:06:45,496 --> 00:06:50,750
However Greek sort is
usually faster in practice.

101
00:06:50,750 --> 00:06:53,850
So, at this point,
we switch to the quick sort algorithm.

102
00:06:55,240 --> 00:06:58,770
Which means that for
these pathological bad instances,

103
00:06:58,770 --> 00:07:03,250
for the QuickSort with this simple
heuristic of selecting the pivot element,

104
00:07:03,250 --> 00:07:08,040
we still work in the worst
case in time n log m.

105
00:07:08,040 --> 00:07:15,250
Because before we exceeded the depth
c log n, we spend time n log m.

106
00:07:15,250 --> 00:07:17,510
And after this,
we'll start this algorithm.

107
00:07:17,510 --> 00:07:20,990
Immediately and
we run the heap sort algorithm.

108
00:07:20,990 --> 00:07:25,680
So overall, we've spent time n log n.

109
00:07:25,680 --> 00:07:31,398
So this gives us an algorithm
which in many cases performs like

110
00:07:31,398 --> 00:07:36,860
the QuickSort algorithm and in any case,

111
00:07:36,860 --> 00:07:43,990
just in the worst case, its running
time is bounded above by n log n.

112
00:07:43,990 --> 00:07:48,800
So to conclude, the QuickSort algorithm
is a comparison based algorithm

113
00:07:48,800 --> 00:07:52,910
whose running time is big O of n
log n in the average case, and

114
00:07:52,910 --> 00:07:54,780
big O of n squared in the worst case.

115
00:07:55,780 --> 00:08:00,170
What is important in this algorithm is
that it is very efficient in practice.

116
00:08:00,170 --> 00:08:04,350
It is more efficient than the north
shore algorithim for example.

117
00:08:04,350 --> 00:08:08,030
For this reason it is commonly
used in practice and for

118
00:08:08,030 --> 00:08:09,860
this reason it is called QuickSort