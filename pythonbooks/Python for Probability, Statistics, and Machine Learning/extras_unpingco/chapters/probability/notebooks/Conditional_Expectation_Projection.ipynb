{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Expectation as Projection\n",
    "\n",
    "Now that we understand projection methods geometrically, we can apply\n",
    "them to conditional probability. This is the *key* concept that ties\n",
    "probability to geometry, optimization, and linear algebra. \n",
    "\n",
    "### Inner Product for Random Variables\n",
    "\n",
    " From our previous work on projection for vectors in\n",
    "$\\mathbb{R}^n$, we have a good geometric grasp on how projection is related to\n",
    "Minimum Mean Squared Error (MMSE). By one abstract step, we can carry\n",
    "all of our geometric interpretations to the space of random variables.\n",
    "For example, we previously noted that at the point of projection, we had the\n",
    "following orthogonal (i.e.,  perpendicular vectors) condition,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "( \\mathbf{y} - \\mathbf{v}_{opt} )^T \\mathbf{v} = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which by noting the inner product slightly more abstractly as\n",
    "$\\langle\\mathbf{x},\\mathbf{y} \\rangle = \\mathbf{x}^T \\mathbf{y}$, we can\n",
    "express as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle \\mathbf{y} - \\mathbf{v}_{opt},\\mathbf{v} \\rangle = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and by defining the inner product for the random variables\n",
    "$X$ and $Y$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle X,Y \\rangle = \\mathbb{E}(X Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we have the same relationship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle X-h_{opt}(Y),Y \\rangle = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which holds not for vectors in $\\mathbb{R}^n$, but for random\n",
    "variables $X$ and $Y$ and functions of those random variables. Exactly why this\n",
    "is true is technical, but it turns out that one can build up the *entire theory\n",
    "of probability* this way [[edward1987radically]](#edward1987radically), by using the expectation as\n",
    "an inner product.\n",
    "\n",
    "Furthermore, by abstracting out the inner product concept, we have connected\n",
    "minimum-mean-squared-error (MMSE) optimization problems, geometry, and random\n",
    "variables.  That's  a lot of mileage to get a out of an abstraction and it\n",
    "enables us to shift between these interpretations to address real problems.\n",
    "Soon, we'll do this with some examples, but first we collect the most important\n",
    "result that flows naturally from this abstraction.\n",
    "\n",
    "### Conditional Expectation as Projection\n",
    "\n",
    "The conditional expectation is the minimum mean squared error (MMSE) solution\n",
    "to the following problem [^proof]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_h \\int_{\\mathbb{R}} (x - h(y) )^2 dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with the minimizing $h_{opt}(Y) $ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt}(Y) = \\mathbb{E}(X|Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^proof]: See appendix for proof using the Cauchy-Schwarz inequality.\n",
    "\n",
    " which is another way of saying that among all possible functions\n",
    "$h(Y)$, the one that minimizes the MSE is $ \\mathbb{E}(X|Y)$. From our previous discussion on projection, we noted that\n",
    "these MMSE solutions can be thought of as projections onto a subspace that\n",
    "characterizes $Y$. For example, we previously noted that at the point of\n",
    "projection, we have perpendicular terms,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ortho\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\langle X-h_{opt}(Y),Y \\rangle = 0\n",
    "\\end{equation}\n",
    "\\label{eq:ortho} \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " but since we know that the MMSE solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt}(Y) = \\mathbb{E}(X|Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we have by direct substitution,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:ortho_001\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathbb{E}(X-\\mathbb{E}(X|Y),Y) = 0\n",
    "\\end{equation}\n",
    "\\label{eq:ortho_001} \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That last step seems pretty innocuous, but it ties MMSE to\n",
    "conditional expectation to the inner project abstraction, and in so doing,\n",
    "reveals the conditional expectation to be a projection operator for random\n",
    "variables. Before we develop this further, let's grab some quick dividends.\n",
    "From the previous equation, by linearity of the expectation, we obtain,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X Y) =  \\mathbb{E}(Y \\mathbb{E}(X|Y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is the so-called *tower property* of the expectation. Note that\n",
    "we could have found this by using the formal definition of conditional\n",
    "expectation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X|Y) = \\int_{\\mathbb{R}^2} x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and brute-force direct integration,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(Y \\mathbb{E}(X|Y)) = \\int_{\\mathbb{R}} y \\int_{\\mathbb{R}} x \\frac{f_{X,Y}(x,y)}{f_Y(y)}  f_Y(y) dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\n",
    "                              =\\int_{\\mathbb{R}^2} x y f_{X,Y}(x,y) dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\n",
    "                              =\\mathbb{E}( X Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is not very geometrically intuitive. This lack of geometric\n",
    "intuition makes it hard to apply these concepts and keep track of these\n",
    "relationships. \n",
    "\n",
    "We can keep pursuing this analogy and obtain the length of the error term \n",
    "from the orthogonality property of the MMSE solution as,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle X-h_{opt}(Y),X-h_{opt}(Y)\\rangle = \\langle X,X  \\rangle - \\langle h_{opt}(Y),h_{opt}(Y)  \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then by substituting all the notation we obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X-  \\mathbb{E}(X|Y))^2 = \\mathbb{E}(X)^2 - \\mathbb{E}(\\mathbb{E}(X|Y) )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which would be tough to compute by direct integration.  \n",
    "\n",
    "To formally establish that $\\mathbb{E}(X|Y)$ *is* in fact *a projection operator* we\n",
    "need to show idempotency.  Recall that idempotency means that once we project\n",
    "something onto a subspace, further projections do nothing. In the space of\n",
    "random variables, $\\mathbb{E}(X|\\cdot$) is the idempotent projection as we can\n",
    "show by noting that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt} = \\mathbb{E}(X|Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " is purely a function of $Y$, so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(h_{opt}(Y)|Y) = h_{opt}(Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " because $Y$ is fixed, this verifies idempotency. Thus, conditional\n",
    "expectation is the corresponding projection operator for random variables. We\n",
    "can continue to carry over our geometric interpretations of projections for\n",
    "vectors ($\\mathbf{v}$) into random variables ($X$).  With this important\n",
    "result, let's consider some examples of conditional expectations obtained by\n",
    "using brute force to find the optimal MMSE function $h_{opt}$ as well as by\n",
    "using our new perspective on conditional expectation.\n",
    "\n",
    "**Example.** Suppose we have a random variable, $X$, then what constant is closest to $X$ in\n",
    "the sense of the mean-squared-error (MSE)? In other words, which $c \\in\n",
    "\\mathbb{R}$ minimizes the following mean squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mbox{MSE} = \\mathbb{E}( X - c )^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we can work this out many ways. First, using calculus-based optimization,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X-c)^2=\\mathbb{E}(c^2-2 c X + X^2)=c^2-2 c \\mathbb{E}(X) + \\mathbb{E}(X^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then take the first derivative with respect to $c$ and solve:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "c_{opt}=\\mathbb{E}(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Remember that $X$ may potentially take on many values, but this says\n",
    "that the closest number to $X$ in the MSE sense is $\\mathbb{E}(X)$.  This is\n",
    "intuitively pleasing.  Coming at this same problem using our inner product,\n",
    "from Equation ref{eq:ortho_001} we know that at the point of projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}((X-c_{opt}) 1) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  where the $1$ represents the space of constants \n",
    "we are projecting onto. By linearity of the expectation, gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "c_{opt}=\\mathbb{E}(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the projection approach, because $\\mathbb{E}(X|Y)$ is\n",
    "the projection operator, with $Y=\\Omega$ (the entire underlying\n",
    "probability space), we have, using the definition of conditional\n",
    "expectation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X|Y=\\Omega) = \\mathbb{E}(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is because of the subtle fact that a random variable over the entire\n",
    "$\\Omega$ space can only be a constant.  Thus, we just worked the same problem\n",
    "three ways (optimization, orthogonal inner products, projection).\n",
    "\n",
    "**Example.** Let's consider the following example with probability density\n",
    "$f_{X,Y}= x + y $ where $(x,y) \\in [0,1]^2$ and compute the conditional\n",
    "expectation straight from the definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{ E}(X|Y) = \\int_0^1 x \\frac{f_{X,Y}(x,y)}{f_Y(y)} dx=  \\int_0^1 x \\frac{x+y}{y+1/2} dx =\\frac{3 y + 2}{6 y + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That was pretty easy because the density function was so simple. Now,\n",
    "let's do it the hard way by going directly for the MMSE solution $h(Y)$. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mbox{ MSE } = \\underset{h}\\min \\int_0^1\\int_0^1 (x - h(y) )^2 f_{X,Y}(x,y)dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\n",
    "             = \\underset{h}\\min \\int_0^1 y h^2 {\\left (y \\right )} - y h{\\left (y \\right )} + \\frac{1}{3} y + \\frac{1}{2} h^{2}{\\left (y \\right )} - \\frac{2}{3} h{\\left (y \\right )} + \\frac{1}{4} dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we have to find a function $h$ that is going to minimize this.\n",
    "Solving for a function, as opposed to solving for a number, is generally very,\n",
    "very hard, but because we are integrating over a finite interval, we can use\n",
    "the Euler-Lagrange method from variational calculus to take the derivative of\n",
    "the integrand with respect to the function $h(y)$ and set it to zero. Using\n",
    "Euler-Lagrange methods, we obtain the following result,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "2 y h{\\left (y \\right )} - y + h{\\left (y \\right )} - \\frac{2}{3} =0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Solving this gives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt}(y)= \\frac{3 y + 2}{6 y + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  which is what we obtained before. Finally, we can solve this\n",
    "using our inner product in Equation ref{eq:ortho} as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}((X-h(Y)) Y)=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Writing this out gives,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_0^1\\int_0^1 (x-h(y))y(x+y) dx dy = \\int_0^1\\frac{1}{6}y(-3(2 y+1) h(y)+3 y+2) dy=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and the integrand must be zero,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "2 y + 3 y^2 - 3 y h(y) - 6 y^2 h(y)=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and solving this for $h(y)$ gives the same solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt}(y)= \\frac{3 y + 2}{6 y + 3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, doing it by the brute force integration from the definition,\n",
    "optimization, or inner product gives us the same answer; but, in general, no\n",
    "method is necessarily easiest because they both involve potentially difficult\n",
    "or impossible integration, optimization, or functional equation solving.  The\n",
    "point is that now that we have a deep toolbox, we can pick and choose which\n",
    "tools we want to apply for different problems.\n",
    "\n",
    "Before we leave this example, let's use Sympy to verify the length of the error\n",
    "function we found earlier for this example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}(X-\\mathbb{E}(X|Y))^2=\\mathbb{E}(X)^2-\\mathbb{E}(\\mathbb{E}(X|Y))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " that is based on the Pythagorean theorem. First, we \n",
    "need to compute the marginal densities,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sympy.abc import y,x\n",
    "from sympy import integrate, simplify\n",
    "fxy = x + y                 # joint density\n",
    "fy = integrate(fxy,(x,0,1)) # marginal density\n",
    "fx = integrate(fxy,(y,0,1)) # marginal density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we need to write out the conditional expectation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EXY = (3*y+2)/(6*y+3) # conditional expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Next, we can compute the left side, $\\mathbb{E}(X-\\mathbb{E}(X|Y))^2$,\n",
    "as the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-log(216)/144 + log(72)/144 + 1/12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from the definition\n",
    "LHS=integrate((x-EXY)**2*fxy,(x,0,1),(y,0,1)) \n",
    "LHS # left-hand-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can similarly compute the right side, $\\mathbb{E}(X)^2-\\mathbb{E}(\\mathbb{E}(X|Y))^2$,\n",
    "as the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-log(216)/144 + log(72)/144 + 1/12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Pythagorean theorem\n",
    "RHS=integrate((x)**2*fx,(x,0,1))-integrate((EXY)**2*fy,(y,0,1))\n",
    "RHS # right-hand-side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we can verify that the left and right sides match,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print simplify(LHS-RHS)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we have pulled together all the projection and least-squares\n",
    "optimization ideas from the previous sections to connect geometric notions of\n",
    "projection from vectors in $\\mathbb{R}^n$ to random variables. This resulted in\n",
    "the remarkable realization that the conditional expectation is in fact a\n",
    "projection operator for random variables.  Knowing this allows to approach\n",
    "difficult problems in multiple ways, depending on which way is more intuitive\n",
    "or tractable in a particular situation. Indeed, finding the right problem to\n",
    "solve is the hardest part, so having many ways of looking at the same concepts\n",
    "is crucial.\n",
    "\n",
    "For much more detailed development, the book by Mikosch\n",
    "[[mikosch1998elementary]](#mikosch1998elementary) has some excellent sections covering much of this\n",
    "material with a similar geometric interpretation. Kobayashi\n",
    "[[kobayashi2011probability]](#kobayashi2011probability) does too.  Nelson [[edward1987radically]](#edward1987radically) also\n",
    "has a similar presentation based on hyper-real numbers.\n",
    "\n",
    "## Appendix\n",
    "\n",
    "We want to prove that we the conditional expectation is the\n",
    "minimum mean squared error minimizer of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "J= \\min_h \\int_{ \\mathbb{R}^2 } \\lvert X - h(Y) \\rvert^2 f_{X,Y}(x,y) dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can expand this as follows,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{multline*}\n",
    "J=\\min_h \\int_{ \\mathbb{R}^2 } \\lvert X \\rvert^2 f_{X,Y}(x,y) dx dy + \\int_{ \\mathbb{R}^2 } \\lvert h(Y) \\rvert^2 f_{X,Y}(x,y) dx dy \\\\\\\n",
    "- \\int_{ \\mathbb{R}^2 } 2 X h(Y) f_{X,Y}(x,y) dx dy\n",
    "\\end{multline*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To minimize this, we have to maximize the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "A=\\max_h \\int_{ \\mathbb{R}^2 }  X h(Y) f_{X,Y}(x,y) dx dy\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Breaking up the integral using the definition of conditional expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A =\\max_h \\int_\\mathbb{R} \\left(\\int_\\mathbb{R} X  f_{X|Y}(x|y) dx \\right)h(Y) f_Y(y) dy \n",
    "\\label{_auto1} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation} \\\n",
    "=\\max_h \\int_\\mathbb{R} \\mathbb{E}(X|Y) h(Y)f_Y(Y) dy \n",
    "\\label{_auto2} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From properties of the Cauchy-Schwarz inequality, we know that the\n",
    "maximum happens when $h_{opt}(Y) = \\mathbb{E}(X|Y)$, so we have found the\n",
    "optimal $h(Y)$ function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h_{opt}(Y) = \\mathbb{E}(X|Y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which shows that the optimal function is the conditional expectation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
