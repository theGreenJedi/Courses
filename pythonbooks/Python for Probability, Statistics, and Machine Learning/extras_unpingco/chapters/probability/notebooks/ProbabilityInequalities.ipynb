{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import textwrap\n",
    "import sys, re\n",
    "old_displayhook = sys.displayhook\n",
    "def displ(x):\n",
    "   if x is None: return\n",
    "   print \"\\n\".join(textwrap.wrap(repr(x).replace(' ',''),width=60))\n",
    "\n",
    "sys.displayhook=displ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Inequalities\n",
    "\n",
    "In practice, few quantities can be analytically calculated. Some knowledge\n",
    "of bounding inequalities helps find the ballpark for potential solutions. This\n",
    "sections discusses three key inequalities that are important for \n",
    "probability, statistics, and machine learning.\n",
    "\n",
    "## Markov's Inequality\n",
    "\n",
    "Let $X$ be a non-negative random variable\n",
    "and suppose that $\\mathbb{E}(X) < \\infty$. Then,\n",
    "for any $t>0$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(X>t)\\leq \\frac{\\mathbb{E}(X)}{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is a foundational inequality that is\n",
    "used as a stepping stone to other inequalities. It is easy\n",
    "to prove. Because $X>0$, we have the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(X)&=\\int_0^\\infty x f_x(x)dx =\\underbrace{\\int_0^t x f_x(x)dx}_{\\text{omit this}}+\\int_t^\\infty x f_x(x)dx \\\\\\ \n",
    "             &\\ge\\int_t^\\infty x f_x(x)dx \\ge t\\int_t^\\infty x f_x(x)dx = t \\mathbb{P}(X>t)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The step that establishes the inequality is the part where the\n",
    "$\\int_0^t x f_x(x)dx$ is omitted.  For a particular $f_x(x)$ that my be\n",
    "concentrated around the $[0,t]$ interval, this could be a lot to throw out.\n",
    "For that reason, the Markov Inequality is considered a *loose* inequality,\n",
    "meaning that there is a substantial gap between both sides of the inequality.\n",
    "For example, as shown in [Figure](#fig:ProbabilityInequalities_001), the\n",
    "$\\chi^2$ distribution has a lot of its mass on the left, which would be omitted\n",
    "in the  Markov Inequality. [Figure](#fig:ProbabilityInequalities_002) shows\n",
    "the two curves established by the Markov Inequality. The gray shaded region is\n",
    "the gap between the two terms and indicates that looseness of the bound\n",
    "(fatter shaded region) for this case.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_001.png, width=500 frac=0.75] The $\\chi_1^2$ density has much of its weight on the left, which is excluded in the establishment of the Markov Inequality. <div id=\"fig:ProbabilityInequalities_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_001\"></div>\n",
    "\n",
    "<p>The $\\chi_1^2$ density has much of its weight on the left, which is excluded in the establishment of the Markov Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_002.png, width=500 frac=0.75] The shaded area shows the region between the curves on either side of the Markov Inequality.  <div id=\"fig:ProbabilityInequalities_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_002\"></div>\n",
    "\n",
    "<p>The shaded area shows the region between the curves on either side of the Markov Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Chebyshev's Inequality\n",
    "\n",
    "Chebyshev's Inequality drops out directly from the Markov Inequality.  Let\n",
    "$\\mu=\\mathbb{E}(X)$ and $\\sigma^2=\\mathbb{V}(X)$. Then, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\vert X-\\mu\\vert \\ge t) \\le \\frac{\\sigma^2}{t^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that if we normalize so that $Z=(X-\\mu)/\\sigma$, we\n",
    "have $\\mathbb{P}(\\vert Z\\vert \\ge k) \\le 1/k^2$. In particular,\n",
    "$\\mathbb{P}(\\vert Z\\vert \\ge 2) \\le 1/4$. We can illustrate this\n",
    "inequality using Sympy statistics module,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "import sympy.stats as ss\n",
    "t=sympy.symbols('t',real=True)\n",
    "x=ss.ChiSquared('x',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  To get the left side of the Chebyshev inequality, we\n",
    "have to write this out as the following conditional probability,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = ss.P((x-1) > t,x>1)+ss.P(-(x-1) > t,x<1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This is because of certain limitations in the statistics module at\n",
    "this point in its development regarding the absolute value function. We could\n",
    "take the above expression, which is a function of $t$ and attempt to compute\n",
    "the integral, but that would take a very long time (the expression is very long\n",
    "and complicated, which is why we did not print it out above). This is because\n",
    "Sympy is a pure-python module that does not utilize any C-level optimizations\n",
    "under the hood.  In this situation, it's better to use the built-in cumulative\n",
    "density function as in the following (after some rearrangement of the terms),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w=(1-ss.cdf(x)(t+1))+ss.cdf(x)(1-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To plot this, we can evaluated at a variety of `t` values by using\n",
    "the `.subs` substitution method, but it is more convenient to use the\n",
    "`lambdify` method to convert the expression to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fw=sympy.lambdify(t,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, we can evaluate this function using something like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.157299207050285,\n",
       " 0.08326451666355039,\n",
       " 0.04550026389635875,\n",
       " 0.0253473186774682]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(fw,[0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " to produce the following [Figure](#fig:ProbabilityInequalities_003). \n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_003.png,width=500 frac=0.85] The shaded area shows the region between the curves on either side of the Chebyshev Inequality.  <div id=\"fig:ProbabilityInequalities_003\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_003\"></div>\n",
    "\n",
    "<p>The shaded area shows the region between the curves on either side of the Chebyshev Inequality.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_003.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "**Programming Tip.**\n",
    "\n",
    "Note that we cannot use vectorized inputs for the `lambdify` function because\n",
    "it contains embedded functions that are only available in Sympy. Otherwise, we\n",
    "could have used `lambdify(t,fw,numpy)` to specify the corresponding functions\n",
    "in Numpy to use for the expression.\n",
    "\n",
    "\n",
    "\n",
    "## Hoeffding's Inequality\n",
    "<div id=\"ch:prob:sec:ineq\"></div>\n",
    "\n",
    "Hoeffding's Inequality is similar, but less loose, than Markov's Inequality.\n",
    "Let $X_1,\\ldots,X_n$ be iid observations such that $\\mathbb{E}(X_i)=\\mu$ and\n",
    "$a\\le X_i \\le b$. Then, for any $\\epsilon>0$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\vert \\overline{X}_n -\\mu\\vert \\ge \\epsilon) \\le 2 \\exp(-2 n\\epsilon^2/(b-a)^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $\\overline{X}_n = \\tfrac{1}{n}\\sum_i^n X_i$. Note that we\n",
    "further assume that the individual random variables are bounded.\n",
    "\n",
    "**Corollary.** If $X_1,\\ldots,X_n$ are independent with $\\mathbb{P}(a\\le X_i\\le b)=1$\n",
    "and all with $\\mathbb{E}(X_i)=\\mu$. Then, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vert\\overline{X}_n-\\mu\\vert \\le \\sqrt{\\frac{c}{2 n}\\log \\frac{2}{\\delta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " where $c=(b-a)^2$. We will see this inequality again in the machine\n",
    "learning chapter. [Figure](#fig:ProbabilityInequalities_004) shows the Markov\n",
    "and Hoeffding bounds for the case of ten identically and uniformly distributed\n",
    "random variables, $X_i \\sim \\mathcal{U}[0,1]$.  The solid line shows\n",
    "$\\mathbb{P}(\\vert \\overline{X}_n - 1/2 \\vert > \\epsilon)$.  Note that the\n",
    "Hoeffding Inequality is tighter than the Markov Inequality and that both of\n",
    "them merge when $\\epsilon$ gets big enough.\n",
    "\n",
    "<!-- dom:FIGURE: [fig-probability/ProbabilityInequalities_004.png,width=500 frac=0.75] This shows the Markov and Hoeffding bounds for the case of ten identically and uniformly distributed random variables.  <div id=\"fig:ProbabilityInequalities_004\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:ProbabilityInequalities_004\"></div>\n",
    "\n",
    "<p>This shows the Markov and Hoeffding bounds for the case of ten identically and uniformly distributed random variables.</p>\n",
    "<img src=\"fig-probability/ProbabilityInequalities_004.png\" width=500>\n",
    "\n",
    "<!-- end figure -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.displayhook= old_displayhook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
