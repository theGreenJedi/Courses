{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absence of the probability density for the raw data means that we have to\n",
    "argue about sequences of random variables in a structured way. From basic\n",
    "calculus, recall the following convergence notation,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "x_n \\rightarrow x_o\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for the real number sequence $x_n$. This means that for any given\n",
    "$\\epsilon>0$, no matter how small, we can exhibit a $m$ such that for\n",
    "any $n>m$, we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vert x_n-x_o \\vert < \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Intuitively, this means that once we get past $m$ in the sequence, we\n",
    "get as to within $\\epsilon$ of $x_o$. This means that nothing surprising\n",
    "happens in the sequence on the long march to infinity, which gives a sense of\n",
    "uniformity to the convergence process.  When we argue about convergence for\n",
    "statistics, we want to same look-and-feel as we have here, but because we are\n",
    "now talking about random variables, we need other concepts.  There are two\n",
    "moving parts for random variables. Recall that random variables are really\n",
    "functions that map sets into the real line: $X:\\Omega \\mapsto \\mathbb{R}$.\n",
    "Thus, one part to keep track of is the behavior of the subsets of $\\Omega$\n",
    "while arguing about convergence.   The other part is the sequence of values\n",
    "that the random variable takes on the real line and how those behave in the\n",
    "convergence process.\n",
    "\n",
    "## Almost Sure Convergence\n",
    "\n",
    "The most straightforward extension into statistics of this convergence concept\n",
    "is *convergence with probability one*, which is also known as *almost sure\n",
    "convergence*, which is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:asconv\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P\\lbrace \\textnormal{for each } \\epsilon>0 \\textnormal{ there is } n_\\epsilon>0 \\textnormal{ such that for all } n>n_\\epsilon, \\: \\vert X_n-X \\vert < \\epsilon   \\rbrace = 1\n",
    "\\label{eq:asconv} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note the similarity to the prior notion of convergence for real\n",
    "numbers.  When this happens, we write this as $X_n \\overset{as}{\\to} X$.  In\n",
    "this context, almost sure convergence means that if we take any particular\n",
    "$\\omega\\in\\Omega$ and then look at the sequence of real numbers that are\n",
    "produced by each of the random variables,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(X_1(\\omega),X_2(\\omega),X_3(\\omega),\\ldots,X_n(\\omega))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " then this sequence is just a real-valued sequence in the\n",
    "sense of our convergence on the real line and converges in the same way. If we\n",
    "collect all of the $\\omega$ for which this is true and the measure of that\n",
    "collection equals one, then we have almost sure convergence of the random\n",
    "variable. Notice how the convergence idea applies to both sides of the random\n",
    "variable: the (domain) $\\Omega$ side and the (co-domain) real-valued side. \n",
    "\n",
    "An equivalent and more compact way of writing this is the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P\\left(\\omega\\in\\Omega \\colon\\lim_{n\\rightarrow\\infty} X_n(\\omega)=X(\\omega) \\right)=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** To get some feel for the mechanics of this kind of convergence\n",
    "consider the following sequence of uniformly distributed random variables on\n",
    "the unit interval, $X_n \\sim \\mathcal{U}[0,1]$. Now, consider taking\n",
    "the maximum of the set of $n$ such variables as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X_{(n)} = \\max \\lbrace X_1,\\ldots,X_n \\rbrace\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, we scan through a list of $n$ uniformly distributed\n",
    "random variables and pick out the maximum over the set. Intuitively, we should\n",
    "expect that $X_{(n)}$ should somehow converge to one. Let's see if we can make\n",
    "this happen almost surely.  We want to exhibit $m$ so that the following is\n",
    "true,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\vert 1 - X_{(n)} \\vert) < \\epsilon \\textnormal{ when } n>m\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because $X_{(n)}<1$, we can simplify this as the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "1-P(X_{(n)}<\\epsilon)=1-(1-\\epsilon)^m \\underset{m\\rightarrow\\infty}{\\longrightarrow} 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, this sequence converges almost surely. We can work this\n",
    "example out in Python using Scipy to make it concrete with the following\n",
    "code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96671783848200299"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "u=stats.uniform()\n",
    "xn = lambda i: u.rvs(i).max()\n",
    "xn(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Thus, the `xn` variable is the same as the $X_{(n)}$ random variable\n",
    "in our example. [Figure](#fig:Convergence_001) shows a plot of these random\n",
    "variables for different values of $n$ and multiple realizations of each random\n",
    "variable (multiple gray lines). The dark horizontal line is at the `0.95`\n",
    "level. For this example, suppose we are interested in the convergence of the\n",
    "random variable to within `0.05` of one so we are interested in the region\n",
    "between one and `0.95`.  Thus, in our Equation ref{eq:asconv}, $\\epsilon=0.05$.\n",
    "Now, we have to find $n_\\epsilon$ to get the almost sure convergence. From\n",
    "[Figure](#fig:Convergence_001), as soon as we get past $n>60$, we can see that\n",
    "all the realizations start to fit in the region above the `0.95` horizontal\n",
    "line.  However, there are still some cases where a particular realization will\n",
    "skip below this line. To get the probability  guarantee of the definition\n",
    "satisfied, we have to make sure that for whatever $n_\\epsilon$ we settle on,\n",
    "the probability of this kind of noncompliant behavior should be extremely\n",
    "small, say, less than 1%.  Now, we can compute the following to estimate this\n",
    "probability for $n=60$ over 1000 realizations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96099999999999997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([xn(60) > 0.95 for i in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So, the probability of having a noncompliant case beyond $n>60$ is\n",
    "pretty good, but not still what we are after (`0.99`). We can solve for the $m$\n",
    "in our analytic proof of convergence by plugging in our factors for $\\epsilon$\n",
    "and our desired probability constraint,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.7811349607\n"
     ]
    }
   ],
   "source": [
    "print np.log(1-.99)/np.log(.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, rounding this up and re-visiting the same estimate as above,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.995"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([xn(90) > 0.95 for i in range(1000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " which is the result we were looking for. The important thing to\n",
    "understand from this example is that we had to choose convergence criteria for\n",
    "*both* the values of the random variable (`0.95`) and for the probability of\n",
    "achieving that level (`0.99`) in order to compute the $m$.  Informally\n",
    "speaking, almost sure convergence means that not only will any particular $X_n$\n",
    "be close to $X$ for large $n$, but whole sequence of values will remain close\n",
    "to $X$ with high probability. \n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Convergence_001.png, width=500 frac=0.85] Almost sure convergence example for multiple realizations of the limiting sequence.   <div id=\"fig:Convergence_001\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Convergence_001\"></div>\n",
    "\n",
    "<p>Almost sure convergence example for multiple realizations of the limiting sequence.</p>\n",
    "<img src=\"fig-statistics/Convergence_001.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "## Convergence in Probability\n",
    "\n",
    "A weaker kind of convergence is *convergence in probability* which means the\n",
    "following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{P}(\\mid X_n -X\\mid > \\epsilon) \\rightarrow 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " as $n \\rightarrow \\infty$ for each $\\epsilon > 0$. \n",
    "\n",
    "This is notationally\n",
    "shown as $X_n \\overset{P}{\\to} X$.  For example, let's consider the following\n",
    "sequence of random variables where $X_n = 1/2^n$ with probability $p_n$ and\n",
    "where $X_n=c$ with probability $1-p_n$. Then, we have $X_n  \\overset{P}{\\to} 0$\n",
    "as $p_n \\rightarrow 1$.  This is allowable under this notion of convergence\n",
    "because a diminishing amount of *non-converging* behavior (namely, when\n",
    "$X_n=c$) is possible. Note that we have said nothing about *how* $p_n\n",
    "\\rightarrow 1$.\n",
    "\n",
    "**Example.** To get some sense of the mechanics of this kind of convergence,\n",
    "let $\\lbrace X_1,X_2,X_3,\\ldots \\rbrace$ be the indicators of the corresponding\n",
    "intervals,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(0,1],(0,\\tfrac{1}{2}],(\\tfrac{1}{2},1],(0,\\tfrac{1}{3}],(\\tfrac{1}{3},\\tfrac{2}{3}],(\\tfrac{2}{3},1]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In other words, just keep splitting the unit interval into equal\n",
    "chunks and enumerate those chunks with $X_i$. Because each $X_i$ is an\n",
    "indicator function, it takes only two values: zero and one.  For example,\n",
    "for $X_2=1$ if $0<x \\le 1/2$ and zero otherwise. Note that $x \\sim\n",
    "\\mathcal{U}(0,1)$. This means that $P(X_2=1)=1/2$. Now, we want to compute\n",
    "the sequence of $P(X_n>\\epsilon)$ for each $n$ for some $\\epsilon\\in (0,1)$.\n",
    "For $X_1$, we  have $P(X_1>\\epsilon)=1$ because we already chose $\\epsilon$\n",
    "in the interval covered by $X_1$. For $X_2$, we have $P(X_2>\\epsilon)=1/2$,\n",
    "for $X_3$, we have $P(X_3>\\epsilon)=1/3$, and so on.  This produces the\n",
    "following sequence:\n",
    "$(1,\\frac{1}{2},\\frac{1}{2},\\frac{1}{3},\\frac{1}{3},\\ldots)$.  The limit\n",
    "of the sequence is zero so that $X_n \\overset{P}{\\to} 0$. However, for\n",
    "every $x\\in (0,1)$, the sequence of  function values of $X_n(x)$ consists\n",
    "of infinitely many zeros and ones (remember that indicator functions can\n",
    "evaluate to either zero or one).  Thus, the set of $x$ for which the\n",
    "sequence $X_n(x)$ converges is empty because the sequence bounces \n",
    "between zero and one. This means that almost sure\n",
    "convergence fails here even though we have convergence in probability.\n",
    "The key distinction is that convergence in probability considers the convergence\n",
    "of a sequence of probabilities whereas almost sure convergence is\n",
    "concerned about the sequence of values of the random variables over\n",
    "sets of events that *fill out* the underlying probability space entirely (i.e.,\n",
    "with probability one).\n",
    "\n",
    "This is a good example so let's see if we can make it concrete with some\n",
    "Python. The following is a function to compute the different subintervals,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_interval= lambda n: np.array(zip(range(n+1),range(1,n+1)))/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we can use this function to create a Numpy\n",
    "array of intervals, as in the example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          1.        ]\n",
      " [ 0.          0.5       ]\n",
      " [ 0.5         1.        ]\n",
      " [ 0.          0.33333333]\n",
      " [ 0.33333333  0.66666667]\n",
      " [ 0.66666667  1.        ]\n",
      " [ 0.          0.25      ]\n",
      " [ 0.25        0.5       ]\n",
      " [ 0.5         0.75      ]\n",
      " [ 0.75        1.        ]]\n"
     ]
    }
   ],
   "source": [
    "intervals= np.vstack([make_interval(i) for i in range(1,5)])\n",
    "print intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The following function computes the bit string in our example,\n",
    "$\\lbrace X_1,X_2,\\ldots,X_n \\rbrace$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bits= lambda u:((intervals[:,0] < u) & (u<=intervals[:,1])).astype(int)\n",
    "bits(u.rvs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now that we have the individual bit strings, to show convergence we\n",
    "want to show that the probability of each entry goes to a limit. For example,\n",
    "using ten realizations,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 0 0 1 0 0]\n",
      " [1 1 0 1 0 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0 1 0 0]\n",
      " [1 0 1 0 0 1 0 0 1 0]\n",
      " [1 0 1 0 0 1 0 0 1 0]\n",
      " [1 1 0 0 1 0 0 1 0 0]\n",
      " [1 1 0 1 0 0 1 0 0 0]\n",
      " [1 1 0 0 1 0 0 1 0 0]\n",
      " [1 1 0 0 1 0 0 1 0 0]\n",
      " [1 1 0 1 0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print np.vstack([bits(u.rvs()) for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We want the limiting probability of a one in each column to convert\n",
    "to a limit. We can estimate this over 1000  realizations using the following\n",
    "code,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.   ,  0.493,  0.507,  0.325,  0.34 ,  0.335,  0.253,  0.24 ,\n",
       "        0.248,  0.259])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack([bits(u.rvs()) for i in range(1000)]).mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note that these entries should approach the\n",
    "$(1,\\frac{1}{2},\\frac{1}{2},\\frac{1}{3},\\frac{1}{3},\\ldots)$ sequence we found\n",
    "earlier. [Figure](#fig:Convergence_002) shows the convergence of these\n",
    "probabilities for a large number of intervals. Eventually, the probability\n",
    "shown on this graph will decrease to zero with large enough $n$. Again, note\n",
    "that the individual sequences of zeros and ones do not converge, but the\n",
    "probabilities of these sequences converge. This is the key difference between\n",
    "almost sure convergence and convergence in probability. Thus, convergence in\n",
    "probability does *not* imply  almost sure convergence. Conversely, almost sure\n",
    "convergence *does* imply convergence in probability.\n",
    "\n",
    "\n",
    "<!-- dom:FIGURE: [fig-statistics/Convergence_002.png, width=500 frac=0.85] Convergence in probability for the random variable sequence.  <div id=\"fig:Convergence_002\"></div> -->\n",
    "<!-- begin figure -->\n",
    "<div id=\"fig:Convergence_002\"></div>\n",
    "\n",
    "<p>Convergence in probability for the random variable sequence.</p>\n",
    "<img src=\"fig-statistics/Convergence_002.png\" width=500>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "The following notation should help emphasize the difference\n",
    "between almost sure convergence and convergence in probability, \n",
    "respectively,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "P\\left(\\lim_{n\\rightarrow \\infty} \\vert X_n-X\\vert < \\epsilon\\right)&=1 \\textnormal{(almost sure convergence)}  \\\\\\\n",
    "\\lim_{n\\rightarrow \\infty}  P(\\vert X_n-X\\vert < \\epsilon)&=1 \\textnormal{(convergence in probability)}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence in Distribution\n",
    "\n",
    "<!-- DasGupta -->\n",
    "<!-- p. 225 in Boos -->\n",
    "<!-- p.133 Keener, Delta Method -->\n",
    "<!-- p. 352 MMA Rose -->\n",
    "<!-- p. 314, Kobayashi -->\n",
    "<!-- p. 291 Oloffson -->\n",
    "\n",
    "So far, we have been discussing convergence in terms of\n",
    "sequences of probabilities or sequences of values taken by\n",
    "the random variable.  By contrast,  the next major kind of\n",
    "convergence is *convergence in distribution* where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lim_{n \\to \\infty}  F_n(t) = F(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for all $t$ for which $F$ is continuous and $F$ is the\n",
    "cumulative density function. For this case, convergence is only\n",
    "concerned with the cumulative density function, written as $X_n\n",
    "\\overset{d}{\\to} X$.  \n",
    "\n",
    "**Example.** To develop some intuition about this kind of convergence,\n",
    "consider a sequence of $X_n$ Bernoulli random variables. Furthermore,\n",
    "suppose these are all really just the same random variable $X$.\n",
    "Trivially, $X_n \\overset{d}{\\to} X$. Now, suppose we define $Y=1-X$,\n",
    "which means that $Y$ has the same distribution as $X$. Thus, $X_n\n",
    "\\overset{d}{\\to} Y$. By contrast, because $\\vert X_n - Y\\vert=1$ for all\n",
    "$n$, we can never have almost sure convergence or convergence in\n",
    "probability. Thus, convergence in distribution is the weakest\n",
    "of the three forms of convergence in the sense that it is implied by\n",
    "the other two, but implies neither of the two.\n",
    "\n",
    "As another striking example, we could have $Y_n \\overset{d}{\\to} Z$ where $Z\n",
    "\\sim \\mathcal{N}(0,1)$, but we could also have $Y_n \\overset{d}{\\to} -Z$.\n",
    "That is, $Y_n$ could converge in distribution to either $Z$ or $-Z$. This\n",
    "may seem ambiguous, but this kind of convergence is practically very useful\n",
    "because it allows for complicated distributions to be approximated by\n",
    "simpler distributions.  \n",
    "\n",
    "## Limit Theorems\n",
    "<div id=\"ch:stats:sec:limit\"></div>\n",
    "\n",
    "Now that we have all of these notions of convergence, we can apply them to\n",
    "different situations and see what kinds of claims we can construct from them.\n",
    "\n",
    "**Weak Law of Large Numbers.**  Let $\\lbrace X_1,X_2,\\ldots,X_n \\rbrace$ be an\n",
    "iid set of random variables with finite mean $\\mathbb{E}(X_k)=\\mu$ and finite\n",
    "variance. Let $\\overline{X}_n = \\frac{1}{n}\\sum_k X_k$. Then, we have\n",
    "$\\overline{X}_n \\overset{P}{\\to} \\mu$. This result is important because we\n",
    "frequently estimate parameters using an averaging process of some kind. This\n",
    "basically justifies this in terms of convergence in probability. Informally,\n",
    "this means that the distribution of $\\overline{X}_n$ becomes\n",
    "concentrated around $\\mu$ as $n\\rightarrow\\infty$.\n",
    "\n",
    "**Strong Law of Large Numbers.**  Let $\\lbrace X_1,X_2,\\ldots,\\rbrace$ be an\n",
    "iid set of random variables. Suppose that $\\mu=\\mathbb{E}\\vert\n",
    "X_i\\vert<\\infty$, then $\\overline{X}_n \\overset{as}{\\to} \\mu$. The reason this\n",
    "is called the strong law is that it implies the weak law because almost sure\n",
    "convergence implies convergence in probability. The so-called  Komogorov\n",
    "criterion gives the convergence of the following,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_k \\frac{\\sigma_k^2}{k^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " as a sufficient condition for concluding that the Strong Law applies\n",
    "to the sequence $ \\lbrace X_k \\rbrace$ with corresponding $\\lbrace \\sigma_k^2\n",
    "\\rbrace$.\n",
    "\n",
    "As an example, consider an infinite sequence of Bernoulli trials with $X_i=1$\n",
    "if the $i^{th}$ trial is successful. Then $\\overline{X}_n$ is the relative\n",
    "frequency of successes in $n$ trials and $\\mathbb{E}(X_i)$ is the\n",
    "probability $p$ of success on the $i^{th}$ trial. With all that established,\n",
    "the Weak Law says only that if we consider a sufficiently large and fixed\n",
    "$n$, the probability that the relative frequency will converge to $p$ is\n",
    "guaranteed. The Strong Law states that if we regard the observation of all\n",
    "the infinite $\\lbrace X_i \\rbrace$ as one performance of the experiment, the\n",
    "relative frequency of successes will almost surely converge to $p$.  The\n",
    "difference between the Strong Law and the Weak Law of large numbers is\n",
    "subtle and rarely arises in practical applications of probability theory.\n",
    "\n",
    "**Central Limit Theorem.**  Although the Weak Law of Large Numbers tells us\n",
    "that the distribution of $\\overline{X}_n$ becomes concentrated around $\\mu$, it\n",
    "does not tell us what that distribution is. The Central Limit Theorem (CLT)\n",
    "says that $\\overline{X}_n$ has a distribution that is approximately Normal\n",
    "with mean $\\mu$ and variance $\\sigma^2/n$. Amazingly, nothing is assumed\n",
    "about the distribution of $X_i$, except the existence\n",
    "of the mean and variance. The following is the Central Limit Theorem:\n",
    "Let $\\lbrace X_1,X_2,\\ldots,X_n \\rbrace$ be iid with mean $\\mu$ and\n",
    "variance $\\sigma^2$. Then,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z_n = \\frac{\\sqrt{n}(\\overline{X}_n-\\mu)}{\\sigma} \\overset{P}{\\longrightarrow} Z\\sim\\mathcal{N}(0,1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The loose interpretation of the Central Limit Theorem is that\n",
    "$\\overline{X}_n$ can be legitimately approximated by a Normal distribution.\n",
    "Because we are talking about convergence in probability here, claims\n",
    "about probability are legitimized, not claims about the random variable\n",
    "itself. Intuitively, this shows that normality arises from sums of small,\n",
    "independent disturbances of finite variance. Technically, the finite\n",
    "variance assumption is essential for normality. Although the Central Limit\n",
    "Theorem provides a powerful, general approximation, the quality of the\n",
    "approximation for a particular situation still depends on the original\n",
    "(usually unknown) distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
