{
 "metadata": {
  "name": "",
  "signature": "sha256:daa57e8117e93a89b70779a1f8469639f2c984796d6d9707e3e4f10f923627f3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This recipe along with the two following it will be centered around automatic feature selection.  I like to think of this as the feature version of parameter tuning.  In the same way that we cross validate to find an appropriately general parameter we can find an appropriately general subset of features.  This will involve several different methods.\n",
      "\n",
      "The simplest idea is univariate selection.  The other methods involve working with a combination of features.  We've seen some of these techniques when we discussed lasso regression.\n",
      "\n",
      "And added benefit to feature selection is that it can ease the burden on the data collection.  Imagine that you have built a model on a very small subset of the data.  If all goes well you might want to scale up to predict the model on the entire subset of data.  If this is the case you can ease the engineering effort of data collection at that scale."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Getting Ready"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With univariate feature selection scoring functions will come to the forefront again.  This time they will define the comparable measure by which we can eliminate features.\n",
      "\n",
      "In this recipe we'll fit a regression model with a few 10000 features, but only 1000 points.  We'll walk through the various univariate feature selection methods."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import datasets\n",
      "X, y = datasets.make_regression(1000, 10000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the data we will compare the features that are included with the various methods.  This is actually a very common situation when you're dealing in text analysis or some areas of bioinformatics."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How to do it"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we need to import the `feature_selection` module."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import feature_selection\n",
      "f, p = feature_selection.f_regression(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "f is the f-score associated with each linear model fit with just one of the features.  We can then compare these features and based on that comparison we can cull features.  `p` is also the $p$ value associated with that f value."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([  1.06271357e-03,   2.91136869e+00,   1.01886922e+00,\n",
        "         2.22483130e+00,   4.67624756e-01])"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "array([ 0.97400066,  0.08826831,  0.31303204,  0.1361235 ,  0.49424067])"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see many of the p-values are quite large.  We would rather that the p values be quite small.  So we can grab numpy out of our tool box and choose all the pvalues less than .05.  These will be the features we'll use for the analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "idx = np.arange(0, X.shape[1])\n",
      "features_to_keep = idx[p < .05]\n",
      "len(features_to_keep)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "501"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see we're actually keeping a relatively large amount of features.  Depending on the context of the model we can tighten this pvalue.  This will lessen the number of features kept.\n",
      "\n",
      "Another option using the `VarianceThreshold` object.  We've talked a bit about it, but it's important to understand that our ability to fit models is largely based on the variance created by features.  If there is no variance then our features cannot describe the variation in the depdentdent variable.  A nice feature of this, per the documentation, is that because it does not use the outcome variable it can be used for unsupervised cases.\n",
      "\n",
      "We will need to set the threshold for which we eliminate features.  In order to do that that we just take the median of the feature variances, and supply that."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var_threshold = feature_selection.VarianceThreshold(np.median(np.var(X, axis=1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "var_threshold.fit_transform(X).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "(1000, 4835)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we can see, we elimated roughly half the features, more or less what we would expect."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How it works"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In general all these methods work by fitting a basic model with a single feature.  Dependending on if we have a classification problem or a regression problem we can use the appropriate scoreing function.\n",
      "\n",
      "Let's look at a smaller problem and visualize how feature selection will elimate certain features.  We'll use the same scoring function from the first example, but just 20 features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = datasets.make_regression(10000, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, p = feature_selection.f_regression(X, y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's plot the p values of the features, we can see which feature would be eliminated and which would be kept."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib import pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f, ax = plt.subplots(figsize=(7, 5))\n",
      "\n",
      "ax.bar(np.arange(20), p, color='k')\n",
      "ax.set_title(\"Feature p values\")\n",
      "\n",
      "f.savefig(\"978-1-78398-948-5_06_05.png\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAa0AAAFCCAYAAABCRIJgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1114b8890>"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "As we can see many of the features would not be kept, but several would."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}