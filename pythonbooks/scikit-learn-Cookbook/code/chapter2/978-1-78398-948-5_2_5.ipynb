{
 "metadata": {
  "name": "",
  "signature": "sha256:28497b1e914cdf5e2c03d199de91fd1ebf826e9252f41ce352a3eff9eb6faaee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Getting Ready\n",
      "\n",
      "The LASSO method, or least absolute shrinkage and selection operator (which we'll call LASSO), is very similar to Ridge Regression and LARS.  It's similar to Ridge Regression in the sense that we're going to penelize our regression by some amount.  And it's similar to LARS in that it can be used parameter selection and typically leads to a sparse vector of coefficients."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#How to do it\n",
      "\n",
      "Let's go back to the trusty `make_regression` function and create a dataset with the same parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_regression\n",
      "reg_data, reg_target = make_regression(n_samples=200, n_features=500, n_informative=5, noise=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we need to import the LASSO object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import Lasso\n",
      "import numpy as np\n",
      "lasso = Lasso()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "LASSO contains many parameters, but the most interesting one is `alpha`.  It scales the penelization term of the LASSO method, which we'll look at in How it works.  For now, leave it 1.  As an aside, and much like Ridge Regression, if this term is 0 LASSO is equivilent to Linear Regression."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso.fit(reg_data, reg_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
        "   normalize=False, positive=False, precompute='auto', tol=0.0001,\n",
        "   warm_start=False)"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Again, let's see how many of the coefficients remained nonzero."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(lasso.coef_ != 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "9"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at what happens when we set alpha to 0."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lasso_0 = Lasso(0)\n",
      "lasso_0.fit(reg_data, reg_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:2: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
        "/Users/thauck/miniconda/envs/zues/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:479: UserWarning: Coordinate descent with alpha=0 may lead to unexpected results and is discouraged.\n",
        "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, positive)\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "Lasso(alpha=0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
        "   normalize=False, positive=False, precompute='auto', tol=0.0001,\n",
        "   warm_start=False)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(lasso_0.coef_ != 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "500"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "None of our coefficients turned out to be zero, which is what we expect.  And actually, if you run theis, you may get a warning from scikit-learn that advises you to choose LinearRegression."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#How it works"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For linear regression we were minimizing the squared error.  Here we're still going to mimize the squared error, but we'll add a penelization term which will induse the sparsity.  The equation looks like:\n",
      "\n",
      "$$ \\sum e_i + \\lambda \\|\\beta\\|_1 $$\n",
      "\n",
      "An alternate way of looking at this to minimize the residual sum of squares \n",
      "\n",
      "$ RSS(\\beta) $ such that $\\|\\beta\\|_1 < \\beta$.\n",
      "\n",
      "This constraint is what leads to the sparsity.  Lasso Regression's constraint creates a hypercube around the origin (the coefficients being the axis), meaning that the most extreme points are the corners, where many of the coefficients are 0.  Ridge Regression creates a hypersphere due to it's constraint $\\|\\beta\\|_2 < \\beta$, but this that it's very likely that coefficients will not be zero, even if they are constrained."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#There's more"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### LASSO Cross Validation\n",
      "\n",
      "Choosing the most appropriate lambda is a critical problem.  We can specify the lambda ourself, or we can use cross validation to find the best choice given the data at hand."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LassoCV\n",
      "lassocv = LassoCV()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lassocv.fit(reg_data, reg_target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "LassoCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
        "    max_iter=1000, n_alphas=100, n_jobs=1, normalize=False, positive=False,\n",
        "    precompute='auto', tol=0.0001, verbose=False)"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`lassocv` will have, as an attribute, the most appropriate lambda.  Sci-Kit Learn mostly uses alpha in it's notation, but the literature uses lambda."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lassocv.alpha_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "0.76914755793284428"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The number of coefficients can be accessed in the regular manner."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lassocv.coef_[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "array([ 0., -0.,  0., -0.,  0.])"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Letting lassocv choose the appropriate best fit has left us with 11 non zero coefficients."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.sum(lassocv.coef_ != 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "16"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### LASSO For Feature Selection\n",
      "\n",
      "LASSO can often be used for feature selection for other methods.  For example, you may run a LASSO regression to get the appropriate number of features, then use those features in another algorithm.\n",
      "\n",
      "To get the features we want, create a masking array based on the columns that aren't zero, then filter to keep the features we want."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mask = lassocv.coef_ != 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_reg_data = reg_data[:, mask]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_reg_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 48,
       "text": [
        "(200, 23)"
       ]
      }
     ],
     "prompt_number": 48
    }
   ],
   "metadata": {}
  }
 ]
}