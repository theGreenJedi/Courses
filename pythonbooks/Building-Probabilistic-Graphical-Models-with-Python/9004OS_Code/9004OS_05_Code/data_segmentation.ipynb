{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we shall see what the effect of data segmentation on parameter estimation using Maximum Likelihood. We have a small network defined in \"small_network.txt\", which has 2 random variables, X->Y connected by an arc. The parent X takes 5 values and the child Y takes 2 values.\n",
      "\n",
      "We first load the network from file and create a DiscreteBayesianNetwork."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from libpgm.graphskeleton import GraphSkeleton\n",
      "from libpgm.nodedata import NodeData\n",
      "from libpgm.discretebayesiannetwork import DiscreteBayesianNetwork\n",
      "from libpgm.tablecpdfactor import TableCPDFactor\n",
      "from libpgm.pgmlearner import PGMLearner\n",
      "\n",
      "nd = NodeData()\n",
      "skel = GraphSkeleton()\n",
      "jsonpath=\"small_network.txt\"\n",
      "nd.load(jsonpath)\n",
      "skel.load(jsonpath)\n",
      "skel.toporder()\n",
      "\n",
      "bn = DiscreteBayesianNetwork(skel, nd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 97
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We write a function that learns the parameters of the network with data sampled from it. We print out the estimated parameter value of the assignment X=3 and Y=0 after drawing 50 samples. We run the same function a few times to compare the results we get. Since sampling is random, you might get different results when you run the same."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def learn_param(num_samp=50):\n",
      "    data = bn.randomsample(num_samp)\n",
      "    # instantiate learner \n",
      "    learner = PGMLearner()\n",
      "\n",
      "    # estimate parameters from data and skeleton\n",
      "    result = learner.discrete_mle_estimateparams(skel, data)\n",
      "    numer=len([1 for m in data if m[\"X\"]=='3' and m[\"Y\"]=='0'])\n",
      "    denom=len([1 for m in data if m[\"X\"]=='3'])\n",
      "\n",
      "    print \"numerator:\",numer,\" denominator:\",denom,\" result=\",numer/float(denom)\n",
      "\n",
      "[learn_param() for _ in range(5)]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "numerator: 1  denominator: 6  result= 0.166666666667\n",
        "numerator: 2  denominator: 6  result= 0.333333333333\n",
        "numerator: 2  denominator: 10  result= 0.2\n",
        "numerator: 1  denominator: 4  result= 0.25\n",
        "numerator: 2  denominator: 12  result= 0.166666666667\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 113,
       "text": [
        "[None, None, None, None, None]"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the result varies a lot, because the number of interesting samples (X==3, Y==0) is found in low numbers in the sampled dataset. The actual value that we've set in the file is 0.2."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nd.Vdata[\"Y\"][\"cprob\"][\"['3']\"][0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 104,
       "text": [
        "0.2"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is only when we increase the number of samples that we get values that are close.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[learn_param(5000) for _ in range(3)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "numerator: 160  denominator: 720  result= 0.222222222222\n",
        "numerator:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 156  denominator: 763  result= 0.204456094364\n",
        "numerator:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 156  denominator: 757  result= 0.20607661823\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 108,
       "text": [
        "[None, None, None]"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although a small network with a single parent, the number of discrete values the parent takes on, gives rise to the data fragmentation problem. which causes poor Maximum likelihood estimates."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}