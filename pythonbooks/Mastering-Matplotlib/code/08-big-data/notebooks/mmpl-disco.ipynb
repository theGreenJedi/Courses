{
 "metadata": {
  "name": "",
  "signature": "sha256:a09507916f4c79d606e9bb2004867e9012d10618fe75aa9fba4a1b13d3acd4e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Big Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*The Expurgated Content*\n",
      "\n",
      "The following sections were removed from the notebook for the \"Big Data\" chapter due to the following:\n",
      " 1. Running low on time\n",
      " 1. Disco set up on Amazon EC2 in coordinated cluster mode being rather more involved than running Disco on a local machine.\n",
      " 1. Hadoop on EMR is covered in the main notebook, and the usefulness of a tiny section on a local machine running Hadoop seem rather limited (in fact, only the first few lines were created in this section).\n",
      " \n",
      "However, it is felt that the content may be useful for some, so it is preserved in this orphaned notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Distributed Data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Disco"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [Disco project](http://discoproject.org/) is an implementation of mapreduce which is generally much easier to set up than Hadoop. If you don't need the ecosystm of JVM libraries that has been created around Hadoop and want to do distributed jobs across a cluster in with a quick installation, then Disco may be for you. Disco can be used for a variety data mining tasks: large-scale analytics, building probabilistic models, and full-text indexing the Web, just to name a few examples.\n",
      "\n",
      "Disco supports parallel computations over large data sets, stored on an unreliable cluster of computers, as in the original framework created by Google. This makes it a perfect tool for analyzing and processing large data sets, without having to worry about difficult technicalities related to distribution such as communication protocols, load balancing, locking, job scheduling, and fault tolerance, which are handled by Disco.\n",
      "\n",
      "The Disco core is written in Erlang, a functional language that is designed for building robust, fault-tolerant, distributed applications. However, in most casese, you won't even know that Erlang is there. Users of Disco typically write jobs in Python, with all the benefits that brings.\n",
      "\n",
      "We've provided some ``Dockerfile``s for building the necessary images. Let's get started by building the base Docker image for Disco which sets up the Erlang dependency:\n",
      "\n",
      "```bash\n",
      "$ docker build -t masteringmatplotlib/erlang ./docker/erlang\n",
      "```\n",
      "\n",
      "And make sure it works:\n",
      "\n",
      "```erlang\n",
      "$ docker run -i -t masteringmatplotlib/erlang\n",
      "Erlang/OTP 17 [erts-6.2] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false]\n",
      "\n",
      "Eshell V6.2  (abort with ^G)\n",
      "1> io:format(\"Testing ...~n\").\n",
      "Testing ...\n",
      "ok\n",
      "2>\n",
      "```\n",
      "\n",
      "Now we can build the image for the disco server, which will use the one we just built:\n",
      "\n",
      "```bash\n",
      "$ docker build -t masteringmatplotlib/disco ./docker/disco\n",
      "```\n",
      "\n",
      "Here's what that ``Dockerfile`` looks like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cat ../docker/disco/Dockerfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "FROM masteringmatplotlib/erlang\r\n",
        "MAINTAINER Duncan McGreggor <oubiwann@gmail.com>\r\n",
        "ENV ORG discoproject\r\n",
        "ENV REPO disco\r\n",
        "ENV DISCO_USER disco\r\n",
        "ENV HOME /home/$DISCO_USER\r\n",
        "ENV DISCO_HOME $HOME/$REPO\r\n",
        "ENV RELEASE 0.5.4\r\n",
        "ENV PYTHONPATH /usr/lib/python3.4/site-packages:$PYTHONPATH\r\n",
        "RUN apt-get update\r\n",
        "RUN apt-get -y upgrade\r\n",
        "RUN apt-get install -y -q openssh-server\r\n",
        "RUN ln -s `which python3` `dirname $(which python3)`/python\r\n",
        "\r\n",
        "# setup SSH for root user\r\n",
        "RUN ssh-keygen -N '' -f /root/.ssh/id_dsa\r\n",
        "RUN cat /root/.ssh/id_dsa.pub >> /root/.ssh/authorized_keys\r\n",
        "RUN echo -n \"localhost \" > /root/.ssh/known_hosts\r\n",
        "RUN cat /etc/ssh/ssh_host_rsa_key.pub >> /root/.ssh/known_hosts\r\n",
        "\r\n",
        "# setup SSH for disco user\r\n",
        "RUN adduser --system $DISCO_USER --shell /bin/sh\r\n",
        "RUN mkdir /home/$DISCO_USER/.ssh\r\n",
        "RUN ssh-keygen -N '' -f /home/$DISCO_USER/.ssh/id_dsa && \\\r\n",
        "    cat /home/$DISCO_USER/.ssh/id_dsa.pub >> \\\r\n",
        "        /home/$DISCO_USER/.ssh/authorized_keys && \\\r\n",
        "    echo -n \"localhost \" > /home/$DISCO_USER/.ssh/known_hosts && \\\r\n",
        "    cat /etc/ssh/ssh_host_rsa_key.pub >> \\\r\n",
        "        /home/$DISCO_USER/.ssh/known_hosts && \\\r\n",
        "    chown $DISCO_USER -R /home/$DISCO_USER/.ssh\r\n",
        "RUN mkdir -p /usr/var/disco/data && \\\r\n",
        "    chown -R disco /usr/var/disco\r\n",
        "\r\n",
        "# install disco from git clone\r\n",
        "RUN cd $HOME && \\\r\n",
        "    git clone https://github.com/$ORG/$REPO.git && \\\r\n",
        "    cd $REPO && \\\r\n",
        "    git checkout tags/$RELEASE && \\\r\n",
        "    make && make install\r\n",
        "RUN echo \"defaultcookievalue\" > $HOME/.erlang.cookie && \\\r\n",
        "    chmod 400 $HOME/.erlang.cookie\r\n",
        "RUN chown -R $DISCO_USER $HOME\r\n",
        "\r\n",
        "RUN echo '#!/bin/sh' > $HOME/start.sh\r\n",
        "RUN echo \"/etc/init.d/ssh start\" >> $HOME/start.sh\r\n",
        "RUN echo \"su disco -c '$DISCO_HOME/bin/disco nodaemon'\" >> $HOME/start.sh\r\n",
        "RUN chmod 755 $HOME/start.sh\r\n",
        "\r\n",
        "EXPOSE 22/tcp 8989/tcp 8990/tcp 999/tcp\r\n",
        "\r\n",
        "CMD $HOME/start.sh\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once Disco starts, we're going to want to access the Disco HTTP admin interface; with Linux, this should be no problem, but on Mac OS X you will need to set up an SSH tunnel through boot2docker:\n",
      "\n",
      "```bash\n",
      "$ boot2docker ssh -L localhost:8989:localhost:8989\n",
      "                        ##        .\n",
      "                  ## ## ##       ==\n",
      "               ## ## ## ##      ===\n",
      "           /\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\\___/ ===\n",
      "      ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~\n",
      "           \\______ o          __/\n",
      "             \\    \\        __/\n",
      "              \\____\\______/\n",
      " _                 _   ____     _            _\n",
      "| |__   ___   ___ | |_|___ \\ __| | ___   ___| | _____ _ __\n",
      "| '_ \\ / _ \\ / _ \\| __| __) / _` |/ _ \\ / __| |/ / _ \\ '__|\n",
      "| |_) | (_) | (_) | |_ / __/ (_| | (_) | (__|   <  __/ |\n",
      "|_.__/ \\___/ \\___/ \\__|_____\\__,_|\\___/ \\___|_|\\_\\___|_|\n",
      "boot2docker: 1.3.0\n",
      "             master : a083df4 - Thu Oct 16 17:05:03 UTC 2014\n",
      "docker@boot2docker:~$\n",
      "```\n",
      "\n",
      "You can leave that terminal window, and work in another.\n",
      "\n",
      "\n",
      "\n",
      "Let's make sure our new image works:\n",
      "\n",
      "```bash\n",
      "$ docker run -i -t masteringmatplotlib/disco-server\n",
      "```\n",
      "\n",
      "You should see output like the following:\n",
      "\n",
      "```bash\n",
      " * Starting OpenBSD Secure Shell server sshd                                                                                                                                                                                          [ OK ]\n",
      "Erlang/OTP 17 [erts-6.2] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:true]\n",
      "\n",
      "Eshell V6.2  (abort with ^G)\n",
      "(disco_8989_master@7f54072a7f32)1> 17:04:41.152 [info] Application lager started on node disco_8989_master@7f54072a7f32\n",
      "17:04:41.267 [info] Application inets started on node disco_8989_master@7f54072a7f32\n",
      "17:04:41.268 [info] DISCO BOOTS\n",
      "17:04:41.272 [info] Disco proxy disabled\n",
      "17:04:41.276 [info] DDFS master starts\n",
      "17:04:41.283 [info] Event server starts\n",
      "17:04:41.287 [info] Disco config starts\n",
      "17:04:41.293 [info] DISCO SERVER STARTS\n",
      "17:04:41.296 [info] Fair scheduler starts\n",
      "17:04:41.297 [info] Scheduler uses fair policy\n",
      "17:04:41.301 [info] Fair scheduler: Fair policy\n",
      "17:04:41.308 [info] Config table updated\n",
      "17:04:41.456 [info] Starting node \"disco_8989_slave\" on \"localhost\" (\"localhost\")\n",
      "17:04:41.466 [info] web server (mochiweb) starts\n",
      "17:04:41.467 [info] Application disco started on node disco_8989_master@7f54072a7f32\n",
      "17:04:42.319 [info] lock_server starts on disco_8989_slave@localhost\n",
      "17:04:42.328 [info] ddfs_node initialized on disco_8989_master@7f54072a7f32 with volumes: [\"vol0\"]\n",
      "17:04:42.328 [info] ddfs_node initialized on disco_8989_slave@localhost with volumes: [\"vol0\"]\n",
      "17:04:42.331 [info] Tempgc: error listing \"/usr/var/disco/data/localhost\": {error,enoent}\n",
      "17:04:42.331 [info] Tempgc: one pass completed on disco_8989_slave@localhost\n",
      "17:04:42.336 [info] ddfs_node starts on disco_8989_master@7f54072a7f32\n",
      "17:04:42.336 [info] Node started at disco_8989_slave@localhost (reporting as disco_8989_master@7f54072a7f32) on \"localhost\"\n",
      "17:04:42.350 [info] Started ddfs_put at disco_8989_slave@localhost on port 8990\n",
      "17:04:42.351 [info] ddfs_node starts on disco_8989_slave@localhost\n",
      "17:04:42.472 [info] Node started at disco_8989_slave@localhost (reporting as disco_8989_slave@localhost) on \"localhost\"\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Disco Clusters on AWS"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Configure the ``aws`` CLI tool with your info and a default region:\n",
      "```\n",
      "$ aws configure\n",
      "AWS Access Key ID [None]: YOURAWSACCESSKEYID\n",
      "AWS Secret Access Key [None]: YOURAWSSECRETACCESSKEY\n",
      "Default region name [None]: us-west-2\n",
      "Default output format [None]: text\n",
      "```\n",
      "\n",
      "We will be using the AWS EC2 Container Service, or *ECS*. You should read the [documentation for ECS](http://docs.aws.amazon.com/AmazonECS/latest/developerguide/get-set-up-for-amazon-ecs.html) which shows how to get setup for ECS, including:\n",
      " * Creating an IAM policy for your container instances\n",
      " * Creating an IAM role for your container instances \n",
      " * Creating a Virtual Private Cloud for your container instances\n",
      "\n",
      "If you do not have these set up, ECS will not work. Be sure to follow their instructions carefully.\n",
      "\n",
      "Once set up, let's create an ECS cluster:\n",
      "\n",
      "```\n",
      "$ aws ecs create-cluster\n",
      "CLUSTER\tarn:aws:ecs:us-west-2:149557551438:cluster/default\tdefault\tACTIVE\n",
      "```\n",
      "\n",
      "Now we need to launch container instances. Go to the AWS EC2 Console, start the \"Launch Instance\" wizard, and in the \"Community APIs, search for \"amazon-ecs-optimized\" -- go ahead an launch an instance of this. Once it's running, you can execute the following command to list the container instances for the default cluster you created above:\n",
      "\n",
      "```\n",
      "$ aws ecs list-container-instances --cluster default\n",
      "```\n",
      "```\n",
      "CONTAINERINSTANCEARNS\tarn:aws:ecs:us-west-2:149557551438:container-instance/8d93c567-5cda-44b4-aad5-2bbaa6926d97\n",
      "CONTAINERINSTANCEARNS\tarn:aws:ecs:us-west-2:149557551438:container-instance/c537beae-be52-4ae3-aefd-93bdf9ef9e8e\n",
      "```\n",
      "\n",
      "```\n",
      "$ aws ecs describe-container-instances --cluster default \\\n",
      "    --container-instances 8d93c567-5cda-44b4-aad5-2bbaa6926d97\n",
      "```\n",
      "```\n",
      "CONTAINERINSTANCES\tTrue\tarn:aws:ecs:us-west-2:149557551438:container-instance/8d93c567-5cda-44b4-aad5-2bbaa6926d97\ti-8309d275\tACTIVE\n",
      "REGISTEREDRESOURCES\t0.0\t2048\t0\tCPU\tINTEGER\n",
      "REGISTEREDRESOURCES\t0.0\t7483\t0\tMEMORY\tINTEGER\n",
      "REGISTEREDRESOURCES\t0.0\t0\t0\tPORTS\tSTRINGSET\n",
      "STRINGSETVALUE\t2376\n",
      "STRINGSETVALUE\t22\n",
      "STRINGSETVALUE\t51678\n",
      "STRINGSETVALUE\t2375\n",
      "REMAININGRESOURCES\t0.0\t2048\t0\tCPU\tINTEGER\n",
      "REMAININGRESOURCES\t0.0\t7483\t0\tMEMORY\tINTEGER\n",
      "REMAININGRESOURCES\t0.0\t0\t0\tPORTS\tSTRINGSET\n",
      "STRINGSETVALUE\t2376\n",
      "STRINGSETVALUE\t22\n",
      "STRINGSETVALUE\t51678\n",
      "STRINGSETVALUE\t2375\n",
      "```\n",
      "\n",
      "Next, create a file called ``disco-manager.json`` with the following contents:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```json\n",
      "{\n",
      "    \"family\": \"disco-manager\",\n",
      "    \"containerDefinitions\": [\n",
      "        {\n",
      "            \"environment\": [],\n",
      "            \"name\": \"disco-manager\",\n",
      "            \"image\": \"masteringmatplotlib/disco\",\n",
      "            \"cpu\": 20,\n",
      "            \"memory\": 2000,\n",
      "            \"portMappings\": [\n",
      "                {\n",
      "                    \"containerPort\": 8989,\n",
      "                    \"hostPort\": 8989\n",
      "                },\n",
      "                {\n",
      "                    \"containerPort\": 8990,\n",
      "                    \"hostPort\": 8990\n",
      "                },\n",
      "                {\n",
      "                    \"containerPort\": 8999,\n",
      "                    \"hostPort\": 8999\n",
      "                }\n",
      "            ],\n",
      "            \"essential\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, create a file called ``disco-worker.json`` with the following contents:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```json\n",
      "{\n",
      "    \"family\": \"disco-worker\",\n",
      "    \"containerDefinitions\": [\n",
      "        {\n",
      "            \"name\": \"disco-worker\",\n",
      "            \"image\": \"masteringmatplotlib/disco\",\n",
      "            \"cpu\": 10,\n",
      "            \"memory\": 1000,\n",
      "            \"portMappings\": [\n",
      "                {\n",
      "                    \"containerPort\": 8989,\n",
      "                    \"hostPort\": 8989\n",
      "                },\n",
      "                {\n",
      "                    \"containerPort\": 8990,\n",
      "                    \"hostPort\": 8990\n",
      "                },\n",
      "                {\n",
      "                    \"containerPort\": 8999,\n",
      "                    \"hostPort\": 8999\n",
      "                }\n",
      "            ],\n",
      "            \"essential\": true\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then register your Disco task:\n",
      "\n",
      "```\n",
      "$ aws ecs register-task-definition --cli-input-json file://disco-manager.json\n",
      "$ aws ecs register-task-definition --cli-input-json file://disco-worker.json\n",
      "```\n",
      "\n",
      "Let's make sure they've been registered:\n",
      "\n",
      "```\n",
      "$ aws ecs list-task-definitions\n",
      "TASKDEFINITIONARNS\tarn:aws:ecs:us-west-2:149557551438:task-definition/disco-manager:1\n",
      "TASKDEFINITIONARNS\tarn:aws:ecs:us-west-2:149557551438:task-definition/disco-worker:1\n",
      "```\n",
      "\n",
      "With the container instances running, we can run our task:\n",
      "\n",
      "```\n",
      "$ aws ecs run-task --task-definition disco-manager\n",
      "```"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Local Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Download and install Hadoop for your operating system (e.g., [part 1](http://amodernstory.com/2014/09/23/installing-hadoop-on-mac-osx-yosemite/) and [part2](http://amodernstory.com/2014/09/23/hadoop-on-mac-osx-yosemite-part-2/) of a tutorial for Mac OS X and a [tutorial for Ubuntu](http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)).\n",
      "* Start the HDFS\n",
      "\n",
      "Create a directory on Hadoop file system:\n",
      "\n",
      "```\n",
      "$ hdfs dfs -mkdir /weather\n",
      "```\n",
      "\n",
      "Save some our generated CSV data to HDFS:\n",
      "\n",
      "```\n",
      "$ hdfs dfs -put data/{0,1,2}.csv /weather\n",
      "```"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}