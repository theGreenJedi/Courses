Imaginemos que te gustaría decidir qué grado de polinomio ajustar a un conjunto de datos, o qué funciones incluir para utilizar tu algoritmo de aprendizaje. O supongamos que quieres elegir el parámetro de regularización «lambda» para el algoritmo de aprendizaje. ¿Cómo lo harías? Estos se llaman problemas de selección de modelo. En nuestra discusión de cómo resolverlos, hablaremos no sólo sobre cómo dividir los datos en los conjuntos de prueba y aprendizaje, sino también sobre cómo dividir los datos en lo que llamaremos conjuntos de validación y de prueba. En este video veremos qué son estos conjuntos y cómo podemos utilizarlos para seleccionar un modelo. Ya hemos visto en muchas ocasiones el problema del sobreajuste, que nos dice que no sólo porque el algoritmo de aprendizaje se ajusta bien un conjunto de entrenamiento, la hipótesis será buena. De manera más general, esta es la razón por la cual el error del conjunto de entrenamiento no es una buena variable predictiva para saber qué tan efectiva será la hipótesis con nuevos ejemplos. Específicamente, si aplicas un conjunto de parámetros, «theta» 0, «theta» 1, «theta» 2, y sucesivamente, a tu conjunto de entrenamiento, el hecho de que la hipótesis sea efectiva en el conjunto de entrenamiento no significa mucho en cuanto a la predicción de qué tan bien se generalizará la hipótesis con ejemplos que no se han visto antes en el conjunto de entrenamiento. El principio más general es que, una vez que los parámetros fueron ajustados a un conjunto de datos, ya sea el conjunto de entrenamiento o cualquier otro, el error de la hipótesis, como se midió en el mismo conjunto de datos, tal como el error de entrenamiento, tal vez no resulta en un estimado adecuado del error de generalización real; es decir, qué tan bien se generalizará la hipótesis con los nuevos ejemplos. Ahora consideremos el problema de selección de modelo. Digamos que intentas elegir qué grado de polinomio ajustar a los datos. ¿Debes elegir una función lineal, una función cuadrática o una cúbica? ¿Qué función, hasta un polinomio del 10 orden, debes elegir? Entonces, hay un parámetro adicional en este algoritmo, que denotaré con una “d”, que indica qué grado de polinomio quieres utilizar. De manera que además de los parámetros theta, hay un parámetro más (d) que intentamos determinar utilizando el conjunto de datos. La primera opción es “d” igual a 1, que corresponde a la función lineal. Podemos elegir “d” igual a 2, “d” igual a 3, y, hasta “d” igual a 10. Queremos, entonces, ajustar este parámetro adicional, que denotaremos como parámetro “d”. Y digamos que quieres elegir un modelo, es decir, elegir un grado de polinomio o uno de estos diez modelos, y ajustarlo para obtener un estimado de qué tan bien se generalizará la hipótesis ajustada a nuevos ejemplos.
Aquí hay algo que podrías hacer: podrías tomar tu primer modelo y minimizar el error de entrenamiento para que arroje el parámetro vector «theta». Después puedes tomar tu segundo modelo, la función cuadrática, y ajustarla al conjunto de entrenamiento. Esto te dará otros parámetros vector «theta». Para distinguir entre los diferentes parámetros de vector, utilizaré el superíndice 1 y el superíndice 2 donde «theta» superíndice 1 se refiere al parámetro que obtendré ajustando este modelo a mis datos de entrenamiento y «theta» superíndice 2 se refiere al parámetro que obtendré al ajustar la función cuadrática a mis datos de entrenamiento. Cuando ajuste el modelo cúbico obtendré el parámetro «theta» 3, y así sucesivamente hasta «theta» 10. Otra cosa que podemos hacer es tomar estos parámetros y buscar el error del conjunto de prueba. Puedo calcular en mi conjunto de prueba “j prueba” de «theta» 1, “j prueba” de «theta» 2, “j prueba” de «theta» 3, y así sucesivamente. Luego tomaré cada una de mis hipótesis con sus parámetros correspondientes y mediré su desempeño en el conjunto de prueba. Para seleccionar uno de estos modelos, una de las cosas que puedo hacer es ver que modelo tiene el error del conjunto de prueba más bajo. Digamos, que para este ejemplo elegimos el polinomio de quinto orden. Esto parece ser razonable hasta el momento, pero qué pasa si quiero tomar mi hipótesis ajustada; es decir, el modelo del quinto orden, y preguntarme qué tan bien se generaliza este modelo. Para responder esto puedo analizar qué tan bien se desempeñó la hipótesis del polinomio de quinto orden en mi conjunto de prueba. El problema de esto es que no será un estimado justo de qué tan bien se generaliza mi hipótesis. La razón es que, lo que hemos hecho es ajustar este parámetro adicional “d”, que es el grado del polinomio, utilizando el conjunto de prueba “d”. En otras palabras, elegimos el valor de d que nos dio el mejor desempeño posible en el conjunto de prueba. El desempeño de mi parámetro vector «theta» cinco del conjunto de prueba probablemente dará un estimado muy optimista del error de generalización. ¿Sí? Debido a que he ajustado este parámetro “d” a mi conjunto de prueba, ya no es justo evaluar mi hipótesis en este conjunto de prueba. Esto es porque he ajustado mis parámetros al conjunto de prueba y he elegido el grado “d” del polinomio utilizando el conjunto de prueba. De modo nuestra hipótesis, probablemente, será más efectiva en este conjunto de prueba que en nuevos ejemplos que no se han visto antes. Es por esto que estamos aquí. Solo por recapitular, en la diapositiva anterior vimos que si ajustamos un grupo de parámetros, digamos «theta» 0, «theta» 1, etc., a un conjunto de entrenamiento, entonces el desempeño del modelo ajustado en el conjunto de entrenamiento no predirá qué tan bien se generaliza la hipótesis en nuevos ejemplos, porque estos parámetros se ajustarían al conjunto de entrenamiento. Y probablemente se desempeñarán bien en este conjunto de entrenamiento, aún si los parámetros no se desempeñan bien en otros ejemplos. Y en el procedimiento que acabo de describir