データセットに 何次の多項式まで含めて フィットさせたいか決めたい、としよう。 つまり学習アルゴリズムになんのフィーチャーを含めるか、という話だ。 または学習アルゴリズムの正規化パラメータ、ラムダを 選びたいとしよう。 どうやる？ これらはモデル選択問題と呼ばれている。 これをどう行うかの議論をするにあたり、 データをどうトレーニングセットと テストセットに 分割するかだけでなく、 データを、 やがて見る事になる、 トレインバリデーションと呼ばれる物と テストセットにデータをどう分割するかを扱う。 このビデオの中で 以上の事がいったいなんなのか、 そしてそれらを使ってどうモデル選択をするかを見ていく。 ここまでに何度も オーバーフィティングの問題を見てきた。 そこでは学習アルゴリズムが トレーニングセットに良くフィットしているからといって 必ずしも良い仮説だとは限らないのだった。 より一般的には、これこそが トレーニングセットの誤差が 新しいサンプルに対してどれだけ 良い予測器かの良い指標にはならない理由だった。 具体的には、 あるパラメータの集まり、 シータ0、シータ1、シータ2など、 をトレーニングセットにフィットしようと しているとしよう。 その時に仮説がトレーニングセットに対して 良いという事実は、 仮説が新たな、トレーニングセットに無い サンプルをどれだけうまく予言するかという 観点からは、 多くは 意味しない、という事。 より一般的な原則としては、 一旦パラメータをあるデータの集合に フィッティングしたら、 それがトレーニングセットであれそれ以外であれ、 その同じデータセットに対して 測った仮説の誤差というのは、 たとえばトレーニング誤差などは それは恐らく 実際の一般のケースの誤差を 推計するには良い物では無い。 つまりどれだけ仮説が 新しいサンプルに対してうまく一般化されているか、という事については。 では、モデル選択問題を検討してみよう。 データをフィットするのに、 何次の多項式を 含めるかを選ぼうとしている、としよう。 つまり、あなたは線形関数を選びたい、 二次関数、三次関数、 、、、と10乗の多項式まで。 つまり、それはまるで、一つ追加のパラメータ、 それをdで表すが、 何次の多項式まで含めるか、を表す パラメータがアルゴリズムにあるみたいな物だ。 つまりまるで、 パラメータのシータに追加して さらにもう一つ、 パラメータdという物が 追加されていて、それをデータセットを用いて決めたい、と考える。 最初の選択肢はd=1で それは線形関数となる。 d=2も、d=3も、、、と d=10までの 選択肢がある。 つまりこの、ある意味で追加のパラメータについて フィッティングしたい、という訳だ。 それをdで示していて、 具体的には、 モデルを選びたいと、 これら10個のモデルから 一つの多項式の次数を選びたいとする。 そしてそのモデルをフィッティングして そのフィッティングした仮説が どれくらいうまく、 新しいサンプルに対して一般化されているかを 評価する。 一つ、ありえる事としては、こんなのがある: 最初のモデルを取って トレーニング誤差を最小化し、 それであるパラメータベクトルが 得られる。 そして次に、二番目のモデル、二次関数を とってきて、 トレーニングセットに対してーー 別のパラメータベクトル、シータを得る事になる。 これら別々のパラメータベクトルを 区別する為に、 下付き添字1、2、、、を使っていく。 ここでシータの下付き添字1は、 このモデルをトレーニングデータに対して フィッティングして 得られたパラメータを意味するに過ぎない。 シータ下付き添字2は単に この二次関数をトレーニングデータに対して フィッティングして得られたパラメータを表すだけ、などなど。 そして三次のモデルをフィッティングする事で、パラメータ、シータ3を得て、 これをシータ10まで続ける事が出来る。 そしてここまて来た後にやれる事としては、一つには これらのパラメータに対し テストセットの誤差を見ていく。 つまり、テストセットに対して Jのtestのシータ1、 Jのtestのシータ2、 みたいに。 Jのtestのシータ3 とか。 つまり、対応する仮説を 一つずつ見ていって テストセットに対するパフォーマンスを ただ計測していくだけ。 そこから出来る事としては、一つには これらのモデルを選ぶ為に どのモデルが 一番低い テストセットの誤差となっているかを見る、 というのがある。 そしてこの例では、仮に 5次の多項式を選ぶ事になったとしよう。 ここまではリーズナブルっぽいね。 だがここで、フィットさせた仮説、 この5次のモデルに 対して、このモデルが どれだけうまく一般化されているか知りたいとする。 一つ考えられるのは、 五次の多項式の仮説がどれだけテストセットに対して うまく機能するかを見る、という事だが、、、 だがこれには問題がある。 これは我らの仮説がどれだけうまく 一般化出来ているかを見積もるには フェアなやり方じゃないって事だ。 その理由は、我らがやった事はそもそも、 この追加のパラメータdを それは多項式の次数だが、 これをフィッティングたのだった。 そしてこのdは、 テストセットを使ってフィッティングしたのだった。 ようするに、我らはdの値を テストセットに対して最も高いパフォーマンスが出るように 選んだのだった。 だから、 パラメータベクトルのシータ5の テストセットに対してのパフォーマンスは たぶん一般化の誤差を見積もるには 過剰に楽観的に なってしまうはず。 でしょ？だってパラメータdは テストセットに対してフィッティングしたんだから。 だからもはや、 仮説をテストセットに対して用いるのは フェアじゃ無くなってしまっているよ。 だってパラメータのフィッティングにテストセットを使っちゃったんだから。 多項式の次数、dを テストセットを使って選んだんだった。 だから我らの仮説は たぶんテストセットに対しての方が まだ見ぬ新しいサンプルに対してより うまく振舞ってしまうだろう。 そしてその新しいサンプルに対してこそが知りたい事だ。 前のスライドを 繰り返すと、 もしあるパラメータの集合、 例えばシータ0とかシータ1とかを、 なんらかのトレーニングセットに対して フィッティングしたら、 そのトレーニングセットにフィットさせたモデルの（そのトレーニングセットに対する）パフォーマンスは 新しいサンプルに対して どれだけうまく一般化出来た仮説と なっているかを予測するのには 使えない。
何故ならこれらのパラメータは 当然このトレーニングセットにはフィットするだろうから。 つまりそれらはトレーニングセットに対しては 良いだろうと思われる。 たとえそのパラメータが、他のサンプルには良くなくても。 そしてこのスライドで記述した手順では、