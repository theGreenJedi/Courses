このビデオでは、あなたのアルゴリズムで学習させた 仮説をどうやって評価するか？について話す。 後半のビデオで、これを用いて オーバーフィッティングとアンダーフィッティングを どう防止するかについて話す。 学習アルゴリズムのパラメータをフィットする時は トレーニングの誤差を最小にするようにパラメータを選ぼうとする。 トレーニングでの誤差をとっても小さくするのが 良い事だ、と思う人も居るかもしれない。 でも既に見たように、 ただ仮説のトレーニング誤差が低いという事だけでは、 それが良い仮説だという事は、必ずしも意味しない。 そして既に、仮説がどうオーバーフィットしてしまうか、の例も見てきた。 そしてその結果、トレーニングセットに無い新しいサンプルに対して、一般化する事にどう失敗するかを。 ではどのように仮説がオーバーフィットしている事を知る事が出来るか？ この単純な例では、仮説h(x)をプロットして 何が起きているかを見てみる事が出来る。 だが一般的には、フィーチャーが一つよりも多い問題に対しては、、、 これらのようにたくさんのフィーチャーがある問題に対しては 仮説をプロットするのが 難しかったり、時には不可能だったりする。 だから仮説評価する他の手段が必要だ。 学習の仮説を評価するスタンダードな方法は以下のようになる。 こんなデータセットがあるとする。 ここではトレーニング手本を10個しか示していないが 通常は何十、何百、または何千もの トレーニング手本がある。 仮説を評価出来ている、という事を確認する為に、 実行するのは、データを 2つの部分に分ける、という事。 最初の部分は通常のトレーニングセットで、 二番目の部分はテストセットとなる。 そしてこの分割の典型的なやり方は 全体のデータを、トレーニングセットとテストセットが だいたい70%、30%になるように分ける。 より多くのデータがトレーニングセットになり、 相対的には少ない量がテストセットになるように。 つまりデータセットがあったとすると、 データの70%だけを トレーニングセットに振り分ける。
ここで「m」は いつも通り、トレーニング手本の数。 そして残りのデータを テストセットに振り分ける。 そしてここで、mの下付き添字のtestで テストのサンプルの数を示す。 今後は一般に、このtestという下付き添字で テストセットから来たサンプルを表す事にする。 つまりx1の下付き添字 test、y1の下付き添字のtestで 最初のテストのサンプルを表し、この例だと ここにあるサンプルとなる。 最後に一つ細かい話を。 ここではまるで最初の70%を トレーニングセットに、最後の30%をテストセットにするかのように線を引いたが、 データにもし順番があるような物なら、 ランダムに70%のデータをトレーニングセットに 残りの30%をテストセットに 選んだ方が良い。 もしあなたのデータが既にランダムに並んでいたら ただ最初の70%と後ろの30%を取ればよろしい。 もしデータがランダムに並んでる訳では無ければ、 トレーニングセットをランダムにシャッフルする、または ランダムに並べ替える方が良い。 最初の70%をトレーニングセットに送り、後ろの30%を テストセットに送る前に。 ここに、学習アルゴリズムと回帰の学習の訓練を 実施する、きわめて典型的な手順を 示す。 まず、トレーニングセットからパラメータのシータを学習して、 通常のトレーニングの目的関数であるトレーニングの誤差、Jのシータを最小化する。 ここでJのシータは全データの70%を使って 定義した物。 トレーニングデータだけについて計算した物ね。 そして次に、テストの誤差を計算する。 そしてテストの誤差をJの下付き添字testで示す。 で、何をするかといえば、トレーニングセットで学習した パラメータのシータをここに入れて、 テストセットの誤差を計算する。 それは以下のようになる。 これは基本的には テストセットに対して測った 誤差の二乗の平均だ。 たぶんご想像の通りでしょう。 つまりパラメータシータを入れた仮説で テストセットの要素一つずつを予言して、 仮説の誤差を m下付き添字test 個に対して計測する。 もちろん、これは線形回帰を使っている時の テストセットの誤差の定義であって、 二乗誤差の計量を用いている場合だ。 ではもし分類問題を行なっていて、 代わりに例えばロジスティック回帰を使っていた場合はどうだろう？ その場合でも、ロジスティック回帰でのトレーニングとテストの 手続きはとても似た物だ。 まず先にトレーニングデータからパラメータを学習する、 データの最初の70%からね。 そして以下のようにテスト誤差を計算する。 これは普段ロジスティック回帰に使っているのと 同じ目的関数だ。 違いは m下付き添字test に対して 定義されている。
つまりテストセットに対して。 このテストセットの誤差、J下付き添字test の定義も 完璧にリーズナブルだが、 時々、別の代替のテストセット計量を 用いる事もある。
そちらの方が解釈が容易かも。 それは誤判別の誤差だ。 それはまた、ゼロ ワン誤判別の誤差とも呼ばれる。 ゼロ ワンはサンプルから 正しい結果が得られたか、誤った結果が得られたかを表す。 それはこういう事だ。 予言の誤差を定義しよう。 予言とはh(x)だ。 そしてラベルyをつけて、 イコール1となるのは、仮説の出力が 0.5以上の値で、なおかつ y=0の時。 または、仮説の出力値が0.5未満で y=1の時。 つまりどちらのケースでも、基本的には 仮説がサンプルを間違えてラベルづけした時となる、 しきい値を0.5として。 つまり、より1の可能性が高いと考えたが実際は0か 仮説がより0の可能性が高いと考えたが 実際は0という事。 そしてそれ以外の場合は、このエラー関数を0と定義する。 もし仮説が基本的には手本のyを正しく分類したら。 すると、テスト誤差を 誤判別の誤差計量を用いて、 それのi=1から m下付き添字test までの errのhのx i_test、y iの 和として 定義出来る。 つまり、これをずばり書き下す方法は 正確に我らの仮説がテストセットの手本を 誤判別した割合という事になる。 以上がゼロワン誤判別計量を用いた テストセットの誤判別の誤差の 定義だ。 以上が学習した仮説がどれだけ良いかを評価する 標準的なテクニックだ。 次のビデオでは、これらのアイデアを用いて、 なんのフィーチャーを含めるべきか選んだり 何次の多項式を学習アルゴリズムに含めるべきかを選んだり 学習アルゴリズムの正規化パラメータを選ぶ助けとしていきます。