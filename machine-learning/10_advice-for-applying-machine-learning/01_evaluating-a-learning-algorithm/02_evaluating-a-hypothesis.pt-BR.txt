Nesse vídeo, eu gostaria de conversar sobre como avaliar a hipótese treinada pelo seu algoritmo. Mais à frente, vamos nos basear nisso para discutir sobre como prevenir tanto problemas com sobreajuste quanto com subajuste. Quando ajustamos os parâmetros do nosso algoritmo de aprendizagem nós pensamos em como ajustar nossos parâmetros para minimizar o erro de treinamento. Pode-se pensar que conseguir um valor muito baixo para o erro de treino é algo bom, mas nós vimos que só por que a hipótese tem um erro de treino baixo, não implica necessariamente que seja uma boa hipótese. E nós já vimos o exemplo onde a hipótese faz sobreajuste, falhando em generalizar para novos exemplos fora do conjunto de treino. Então, como discernir se a hipótese está sofrendo de sobreajuste? Nesse exemplo simples, poderíamos traçar a hipótese "h(x)" e ver o que está acontecendo. Mas em geral para problemas com mais variáveis do que apenas uma, para problemas com um número grande de variáveis como esse, pode ficar difícil ou mesmo impossível traçar o formato da hipótese. Assim, precisamos de outra maneira de avaliar nossa hipótese. A forma padrão de avaliar uma hipótese treinada é a seguinte. Suponha que tenhamos um conjunto de dados como esse. Aqui eu mostrei apenas 10 exemplos de treino, mas é claro que teremos talvez dezenas, milhares ou centenas de milhares de exemplos de treino. Para ter certeza de que poderemos avaliar nossa hipótese, o que faremos é dividir os dados em duas partes. A primeira parte será nosso conjunto de treinamento e a segunda parte será nosso conjunto de teste. Uma divisão bem comum é dividir todos os dados que temos entre treino e teste seguindo uma proporção de mais ou menos 70% e 30%, Com mais dados indo para o conjunto de treino e relativamente menos para o conjunto de teste. E agora, se tivermos um conjunto de dados, nós iremos utilizar apenas 70% dos dados como conjunto de treino onde "m", como antes, é o nosso número de exemplos de treino e o resto dos dados será atribuído ao conjunto de teste. Aqui, eu vou usar a notação "m_test" para me referir ao número de exemplos de teste. E em geral, esse "_test" irá indicar exemplos provindos do conjunto de treino, de forma que "(x₁_test, y₁_test)" é meu primeiro exemplo de teste, que neste caso seria esse exemplo aqui. Um último detalhe, apesar de eu dividir a tabela com os primeiros 70% no conjunto de treino e os últimos 30% no conjunto de teste, caso houvesse algum tipo de ordenação nos dados. seria melhor escolher aleatoriamente os 70% dos dados que vão para o conjunto de treino e os 30% para o de teste. Assim, se os seus dados fossem ordenados aleatoriamente, você poderia simplesmente pegar os primeiros 70% e os últimos 30%, mas se os seus dados não estiverem ordenados aleatoriamente seria melhor misturá-los, reordená-los antes de enviar os primeiros 70% para o conjunto de treino e os 30% finais para o de teste. Aqui está um procedimento bem típico para treinar e testar o algoritmo de aprendizagem, talvez regressão linear. Primeiro, você aprende os parâmetros "θ" do conjunto de treino, minimizando a função de custo "J(θ)", onde "J(θ)" foi definida usando aqueles 70% dos dados que você tem. usamos apenas o conjunto de treino. E então você iria calcular o erro no conjunto de teste. Vou indicar o erro de teste como "J_test". O que fazemos é pegar o parâmetro "θ" que você aprendeu através do conjunto de treino, e colocá-lo aqui e calcular o erro no conjunto de teste, que vou escrever aqui. Isso é basicamente o erro médio quadrático medido no seu conjunto de teste, minha hipótese ajustada, o modelo e usaremos todos os exemplos de teste na hipótese com parâmetro "θ" e mediremos o erro quadrático que sua hipótese gera para os exemplos "m_test". Essa é a definição do erro de teste, se você estiver usando regressão linear e a métrica do erro quadrático. E se estivéssemos resolvendo um problema de classificação e usando regressão logística? Nesse caso, o procedimento para treinamento e teste é bem parecido. Primeiramente, geramos os parâmetros a partir do conjunto de treinamento, os primeiros 70% dos dados. E calculamos o erro de teste da seguinte maneira. É a mesma função objetivo que sempre usamos para regressão logística, exceto que agora nós a definimos usando nossos "m_test" exemplos de teste. Enquanto essa definição de erro de teste "J_test" é perfeitamente razoável, às vezes existe uma métrica alternativa mais fácil de interpretar, o erro de classificações incorretas. Chamamos também de erro de classificação "0/1", onde 0 indica que você acertou no exemplo e 1 indica que você errou. O que eu quero dizer é Vamos definir o erro de uma estimativa "h(x)", dada a classificação correta "y" como igual a 1, se minha hipótese retornar um valor maior ou igual a "0.5" e "y = 0", ou se minha hipótese retornar um valor menor ou igual a "0.5" e "y = 1", ambos esses casos correspondem a sua hipótese ter rotulado o exemplo incorretamente, assumindo que você separou no "0.5". Ou pensou que o exemplo era 1 quando era 0, ou achou que era mais provável que fosse 0 quando na realidade era 1. E, caso contrário, essa função de erro será 0 se nossa hipótese classificar o exemplo "y" corretamente. Nós podemos então definir o erro de teste usando erro de classificações incorretas como "1 / m_test" vezes a somatória, de "i = 1" até "i  = m_test", de "err(h(x_test⁽ⁱ⁾), y⁽ⁱ⁾)". de "err(h(x_test⁽ⁱ⁾), y⁽ⁱ⁾)". E essa é a minha forma de escrever a fração exata de exemplos de teste que foram classificados de forma errada pela minha hipótese. E essa é a definição do erro de teste usando o erro de classificações incorretas, com a métrica de erro de classificação "0/1". Essa é a maneira padrão para avaliar o quão boa é uma hipótese treinada. No próximo vídeo, nós adaptaremos essas ideias para nos ajudar a fazer coisas como escolher quais variáveis, como o grau do polinômio, que devemos usar no nosso algoritmo de aprendizagem. ou escolher o parâmetro de regularização a ser usado.
Tradução: Eduardo Bonet | Revisão: Marcel de Sena Dall'Agnol