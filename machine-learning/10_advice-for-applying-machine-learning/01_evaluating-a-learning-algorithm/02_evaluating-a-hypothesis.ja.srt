1
00:00:00,146 --> 00:00:02,515
このビデオでは、あなたのアルゴリズムで学習させた

2
00:00:02,523 --> 00:00:06,662
仮説をどうやって評価するか？について話す。

3
00:00:06,685 --> 00:00:09,200
後半のビデオで、これを用いて

4
00:00:09,231 --> 00:00:11,846
オーバーフィッティングとアンダーフィッティングを

5
00:00:11,869 --> 00:00:14,908
どう防止するかについて話す。

6
00:00:15,615 --> 00:00:19,023
学習アルゴリズムのパラメータをフィットする時は

7
00:00:19,038 --> 00:00:23,154
トレーニングの誤差を最小にするようにパラメータを選ぼうとする。

8
00:00:23,169 --> 00:00:26,077
トレーニングでの誤差をとっても小さくするのが

9
00:00:26,100 --> 00:00:28,108
良い事だ、と思う人も居るかもしれない。

10
00:00:28,108 --> 00:00:29,562
でも既に見たように、

11
00:00:29,562 --> 00:00:32,400
ただ仮説のトレーニング誤差が低いという事だけでは、

12
00:00:32,400 --> 00:00:35,254
それが良い仮説だという事は、必ずしも意味しない。

13
00:00:35,254 --> 00:00:40,223
そして既に、仮説がどうオーバーフィットしてしまうか、の例も見てきた。

14
00:00:40,415 --> 00:00:45,785
そしてその結果、トレーニングセットに無い新しいサンプルに対して、一般化する事にどう失敗するかを。

15
00:00:45,962 --> 00:00:50,000
ではどのように仮説がオーバーフィットしている事を知る事が出来るか？

16
00:00:50,015 --> 00:00:54,346
この単純な例では、仮説h(x)をプロットして

17
00:00:54,365 --> 00:00:56,338
何が起きているかを見てみる事が出来る。

18
00:00:56,346 --> 00:01:00,538
だが一般的には、フィーチャーが一つよりも多い問題に対しては、、、

19
00:01:00,554 --> 00:01:03,531
これらのようにたくさんのフィーチャーがある問題に対しては

20
00:01:03,546 --> 00:01:06,692
仮説をプロットするのが

21
00:01:06,708 --> 00:01:09,515
難しかったり、時には不可能だったりする。

22
00:01:09,531 --> 00:01:13,046
だから仮説評価する他の手段が必要だ。

23
00:01:13,062 --> 00:01:17,315
学習の仮説を評価するスタンダードな方法は以下のようになる。

24
00:01:17,331 --> 00:01:19,308
こんなデータセットがあるとする。

25
00:01:19,323 --> 00:01:21,977
ここではトレーニング手本を10個しか示していないが

26
00:01:21,992 --> 00:01:23,969
通常は何十、何百、または何千もの

27
00:01:23,985 --> 00:01:27,254
トレーニング手本がある。

28
00:01:27,269 --> 00:01:30,246
仮説を評価出来ている、という事を確認する為に、

29
00:01:30,262 --> 00:01:32,808
実行するのは、データを

30
00:01:32,823 --> 00:01:35,554
2つの部分に分ける、という事。

31
00:01:35,569 --> 00:01:40,723
最初の部分は通常のトレーニングセットで、

32
00:01:42,638 --> 00:01:47,446
二番目の部分はテストセットとなる。

33
00:01:47,462 --> 00:01:50,398
そしてこの分割の典型的なやり方は

34
00:01:50,413 --> 00:01:53,482
全体のデータを、トレーニングセットとテストセットが

35
00:01:53,498 --> 00:01:57,936
だいたい70%、30%になるように分ける。

36
00:01:57,952 --> 00:02:00,052
より多くのデータがトレーニングセットになり、

37
00:02:00,067 --> 00:02:02,367
相対的には少ない量がテストセットになるように。

38
00:02:02,382 --> 00:02:05,782
つまりデータセットがあったとすると、

39
00:02:05,790 --> 00:02:08,459
データの70%だけを

40
00:02:08,475 --> 00:02:11,529
トレーニングセットに振り分ける。
ここで「m」は

41
00:02:11,544 --> 00:02:14,336
いつも通り、トレーニング手本の数。

42
00:02:14,352 --> 00:02:16,913
そして残りのデータを

43
00:02:16,929 --> 00:02:19,310
テストセットに振り分ける。

44
00:02:19,325 --> 00:02:23,410
そしてここで、mの下付き添字のtestで

45
00:02:23,425 --> 00:02:27,187
テストのサンプルの数を示す。

46
00:02:27,202 --> 00:02:32,225
今後は一般に、このtestという下付き添字で

47
00:02:32,241 --> 00:02:34,987
テストセットから来たサンプルを表す事にする。

48
00:02:35,002 --> 00:02:40,810
つまりx1の下付き添字 test、y1の下付き添字のtestで

49
00:02:40,825 --> 00:02:43,648
最初のテストのサンプルを表し、この例だと

50
00:02:43,664 --> 00:02:45,656
ここにあるサンプルとなる。

51
00:02:45,671 --> 00:02:47,495
最後に一つ細かい話を。

52
00:02:47,510 --> 00:02:50,795
ここではまるで最初の70%を

53
00:02:50,810 --> 00:02:54,479
トレーニングセットに、最後の30%をテストセットにするかのように線を引いたが、

54
00:02:54,495 --> 00:02:57,518
データにもし順番があるような物なら、

55
00:02:57,533 --> 00:03:01,048
ランダムに70%のデータをトレーニングセットに

56
00:03:01,048 --> 00:03:02,948
残りの30%をテストセットに

57
00:03:02,964 --> 00:03:05,556
選んだ方が良い。

58
00:03:05,571 --> 00:03:08,579
もしあなたのデータが既にランダムに並んでいたら

59
00:03:08,595 --> 00:03:12,110
ただ最初の70%と後ろの30%を取ればよろしい。

60
00:03:12,125 --> 00:03:14,718
もしデータがランダムに並んでる訳では無ければ、

61
00:03:14,733 --> 00:03:16,756
トレーニングセットをランダムにシャッフルする、または

62
00:03:16,771 --> 00:03:19,718
ランダムに並べ替える方が良い。

63
00:03:19,733 --> 00:03:23,310
最初の70%をトレーニングセットに送り、後ろの30%を

64
00:03:23,325 --> 00:03:26,669
テストセットに送る前に。

65
00:03:27,054 --> 00:03:30,169
ここに、学習アルゴリズムと回帰の学習の訓練を

66
00:03:30,185 --> 00:03:32,008
実施する、きわめて典型的な手順を

67
00:03:32,023 --> 00:03:34,492
示す。

68
00:03:34,508 --> 00:03:38,115
まず、トレーニングセットからパラメータのシータを学習して、

69
00:03:38,131 --> 00:03:41,798
通常のトレーニングの目的関数であるトレーニングの誤差、Jのシータを最小化する。

70
00:03:41,813 --> 00:03:44,713
ここでJのシータは全データの70%を使って

71
00:03:44,729 --> 00:03:47,059
定義した物。

72
00:03:47,075 --> 00:03:49,759
トレーニングデータだけについて計算した物ね。

73
00:03:49,882 --> 00:03:52,167
そして次に、テストの誤差を計算する。

74
00:03:52,182 --> 00:03:56,298
そしてテストの誤差をJの下付き添字testで示す。

75
00:03:56,313 --> 00:03:59,229
で、何をするかといえば、トレーニングセットで学習した

76
00:03:59,259 --> 00:04:02,190
パラメータのシータをここに入れて、

77
00:04:02,205 --> 00:04:04,875
テストセットの誤差を計算する。

78
00:04:04,890 --> 00:04:08,529
それは以下のようになる。

79
00:04:08,698 --> 00:04:11,275
これは基本的には

80
00:04:11,290 --> 00:04:15,244
テストセットに対して測った

81
00:04:15,269 --> 00:04:18,154
誤差の二乗の平均だ。

82
00:04:18,169 --> 00:04:19,915
たぶんご想像の通りでしょう。

83
00:04:19,931 --> 00:04:23,415
つまりパラメータシータを入れた仮説で

84
00:04:23,431 --> 00:04:28,008
テストセットの要素一つずつを予言して、

85
00:04:28,023 --> 00:04:33,338
仮説の誤差を m下付き添字test 個に対して計測する。

86
00:04:33,354 --> 00:04:37,054
もちろん、これは線形回帰を使っている時の

87
00:04:37,069 --> 00:04:40,815
テストセットの誤差の定義であって、

88
00:04:40,831 --> 00:04:44,362
二乗誤差の計量を用いている場合だ。

89
00:04:44,377 --> 00:04:47,477
ではもし分類問題を行なっていて、

90
00:04:47,492 --> 00:04:50,654
代わりに例えばロジスティック回帰を使っていた場合はどうだろう？

91
00:04:50,669 --> 00:04:53,877
その場合でも、ロジスティック回帰でのトレーニングとテストの

92
00:04:53,892 --> 00:04:57,085
手続きはとても似た物だ。

93
00:04:57,100 --> 00:04:59,985
まず先にトレーニングデータからパラメータを学習する、

94
00:05:00,000 --> 00:05:02,331
データの最初の70%からね。

95
00:05:02,346 --> 00:05:05,115
そして以下のようにテスト誤差を計算する。

96
00:05:05,131 --> 00:05:07,015
これは普段ロジスティック回帰に使っているのと

97
00:05:07,031 --> 00:05:09,592
同じ目的関数だ。

98
00:05:09,608 --> 00:05:11,569
違いは m下付き添字test に対して

99
00:05:11,585 --> 00:05:15,115
定義されている。
つまりテストセットに対して。

100
00:05:15,131 --> 00:05:17,600
このテストセットの誤差、J下付き添字test の定義も

101
00:05:17,631 --> 00:05:20,238
完璧にリーズナブルだが、

102
00:05:20,254 --> 00:05:22,231
時々、別の代替のテストセット計量を

103
00:05:22,246 --> 00:05:25,469
用いる事もある。
そちらの方が解釈が容易かも。

104
00:05:25,485 --> 00:05:27,877
それは誤判別の誤差だ。

105
00:05:27,892 --> 00:05:30,792
それはまた、ゼロ ワン誤判別の誤差とも呼ばれる。

106
00:05:30,808 --> 00:05:32,692
ゼロ ワンはサンプルから

107
00:05:32,708 --> 00:05:36,146
正しい結果が得られたか、誤った結果が得られたかを表す。

108
00:05:36,162 --> 00:05:37,910
それはこういう事だ。

109
00:05:37,925 --> 00:05:41,795
予言の誤差を定義しよう。

110
00:05:41,825 --> 00:05:44,202
予言とはh(x)だ。

111
00:05:44,218 --> 00:05:47,518
そしてラベルyをつけて、

112
00:05:47,533 --> 00:05:51,848
イコール1となるのは、仮説の出力が

113
00:05:51,864 --> 00:05:54,633
0.5以上の値で、なおかつ

114
00:05:54,641 --> 00:05:57,510
y=0の時。

115
00:05:57,525 --> 00:06:03,718
または、仮説の出力値が0.5未満で

116
00:06:03,733 --> 00:06:05,402
y=1の時。

117
00:06:05,418 --> 00:06:08,118
つまりどちらのケースでも、基本的には

118
00:06:08,133 --> 00:06:11,833
仮説がサンプルを間違えてラベルづけした時となる、

119
00:06:11,833 --> 00:06:14,518
しきい値を0.5として。

120
00:06:14,533 --> 00:06:18,171
つまり、より1の可能性が高いと考えたが実際は0か

121
00:06:18,187 --> 00:06:20,733
仮説がより0の可能性が高いと考えたが

122
00:06:20,748 --> 00:06:23,556
実際は0という事。

123
00:06:23,571 --> 00:06:28,471
そしてそれ以外の場合は、このエラー関数を0と定義する。

124
00:06:28,487 --> 00:06:34,841
もし仮説が基本的には手本のyを正しく分類したら。

125
00:06:34,864 --> 00:06:38,841
すると、テスト誤差を

126
00:06:38,856 --> 00:06:42,371
誤判別の誤差計量を用いて、

127
00:06:42,387 --> 00:06:46,779
それのi=1から m下付き添字test までの

128
00:06:46,795 --> 00:06:49,941
errのhのx i_test、y iの

129
00:06:49,956 --> 00:06:55,164
和として

130
00:06:55,179 --> 00:06:57,971
定義出来る。

131
00:06:57,987 --> 00:07:02,010
つまり、これをずばり書き下す方法は

132
00:07:02,025 --> 00:07:05,587
正確に我らの仮説がテストセットの手本を

133
00:07:05,602 --> 00:07:08,864
誤判別した割合という事になる。

134
00:07:08,871 --> 00:07:10,602
以上がゼロワン誤判別計量を用いた

135
00:07:10,618 --> 00:07:13,687
テストセットの誤判別の誤差の

136
00:07:13,718 --> 00:07:16,948
定義だ。

137
00:07:16,971 --> 00:07:19,995
以上が学習した仮説がどれだけ良いかを評価する

138
00:07:20,010 --> 00:07:22,833
標準的なテクニックだ。

139
00:07:22,848 --> 00:07:25,579
次のビデオでは、これらのアイデアを用いて、

140
00:07:25,595 --> 00:07:28,525
なんのフィーチャーを含めるべきか選んだり

141
00:07:28,541 --> 00:07:31,641
何次の多項式を学習アルゴリズムに含めるべきかを選んだり

142
00:07:31,656 --> 00:07:34,964
学習アルゴリズムの正規化パラメータを選ぶ助けとしていきます。