You've seen how regularization
can help prevent over-fitting. But how does it affect the bias and
variances of a learning algorithm? In this video I'd like to go
deeper into the issue of bias and variances and
talk about how it interacts with and is affected by the regularization
of your learning algorithm. Suppose we're fitting a high auto
polynomial, like that showed here, but to prevent over fitting we need to use
regularization, like that shown here. So we have this regularization term to try
to keep the values of the prem to small. And as usual, the regularizations comes
from J = 1 to m, rather than j = 0 to m. Let's consider three cases. The first is the case of the very large
value of the regularization parameter lambda, such as if lambda
were equal to 10,000. Some huge value. In this case, all of these parameters,
theta 1, theta 2, theta 3, and so on would be heavily penalized and so we end up with most of these
parameter values being closer to zero. And the hypothesis will be roughly h of x, just equal or
approximately equal to theta zero. So we end up with a hypothesis that
more or less looks like that, more or less a flat, constant straight line. And so this hypothesis has high bias and
it badly under fits this data set, so the horizontal straight line is just
not a very good model for this data set. At the other extreme is if we have
a very small value of lambda, such as if lambda were equal to zero. In that case, given that we're
fitting a high order polynomial, this is a usual over-fitting setting. In that case, given that we're fitting
a high-order polynomial, basically, without regularization or
with very minimal regularization, we end up with our usual high-variance,
over fitting setting. This is basically if lambda is equal
to zero, we're just fitting with our regularization, so
that over fits the hypothesis. And it's only if we have some
intermediate value of longer that is neither too large nor
too small that we end up with parameters data that give us
a reasonable fit to this data. So, how can we automatically choose a good
value for the regularization parameter? Just to reiterate, here's our model, and
here's our learning algorithm's objective. For the setting where we're
using regularization, let me define J train(theta)
to be something different, to be the optimization objective,
but without the regularization term. Previously, in an earlier video, when we
were not using regularization I define J train of data to be the same as J of theta
as the cause function but when we're using regularization when the six well under
term we're going to define J train my training set to be just my sum of squared
errors on the training set or my average squared error on the training set without
taking into account that regularization. And similarly I'm then also going to
define the cross validation sets error and to test that error as before to be the
average sum of squared errors on the cross validation in the test sets so just to
summarize my definitions of J train J CU and J test are just the average square
there one half of the other square record on the training validation of the test set
without the extra regularization term. So, this is how we can automatically
choose the regularization parameter lambda. So what I usually do is maybe have some
range of values of lambda I want to try out. So I might be considering not using
regularization or here are a few values I might try lambda considering lambda
= 0.01, 0.02, 0.04, and so on. And I usually set these up in multiples
of two, until some maybe larger value if I were to do these in multiples
of 2 I'd end up with a 10.24. It's 10 exactly, but this is close enough. And the three to four decimal places
won't effect your result that much. So, this gives me maybe
12 different models. And I'm trying to select a month
corresponding to 12 different values of the regularization of
the parameter lambda. And of course you can also go
to values less than 0.01 or values larger than 10 but I've just
truncated it here for convenience. Given the issue of these 12 models,
what we can do is then the following, we can take this first model with
lambda equals zero and minimize my cost function J of data and this will
give me some parameter of active data. And similar to the earlier video, let me just denote this as
theta super script one. And then I can take my second
model with lambda set to 0.01 and minimize my cost function now using
lambda equals 0.01 of course. To get some different
parameter vector theta. Let me denote that theta(2). And for that I end up with theta(3). So if part for my third model. And so on until for
my final model with lambda set to 10 or 10.24, I end up with this theta(12). Next, I can talk all of these hypotheses,
all of these parameters and use my cross validation
set to validate them so I can look at my first model,
my second model, fit to these different values of
the regularization parameter, and evaluate them with my cross validation set
based in measure the average square error of each of these square vector parameters
theta on my cross validation sets. And I would then pick whichever one of
these 12 models gives me the lowest error on the trans validation set. And let's say, for the sake of this
example, that I end up picking theta 5, the 5th order polynomial, because that
has the lowest cause validation error. Having done that, finally what I would do
if I wanted to report each test set error, is to take the parameter theta
5 that I've selected, and look at how well it does on my test set. So once again, here is as if
we've fit this parameter, theta, to my cross-validation set,
which is why I'm setting aside a separate test set that I'm going to
use to get a better estimate of how well my parameter vector, theta, will
generalize to previously unseen examples. So that's model selection
applied to selecting the regularization parameter lambda. The last thing I'd like to do in this
video is get a better understanding of how cross validation and training error vary as we vary
the regularization parameter lambda. And so just a reminder right,
that was our original cost on j of theta. But for this purpose we're going to define training error without using
a regularization parameter, and cross validation error without
using the regularization parameter. And what I'd like to do is plot
this Jtrain and plot this Jcv, meaning just how well does my
hypothesis do on the training set and how does my hypothesis do when
it cross validation sets. As I vary my regularization
parameter lambda. So as we saw earlier if lambda is small
then we're not using much regularization and we run a larger risk of
over fitting whereas if lambda is large that is if we were on the right
part of this horizontal axis then, with a large value of lambda, we run the
higher risk of having a biased problem, so if you plot J train and J cv, what you
find is that, for small values of lambda, you can fit the trading set relatively
way cuz you're not regularizing. So, for small values of lambda, the
regularization term basically goes away, and you're just minimizing
pretty much just gray arrows. So when lambda is small,
you end up with a small value for Jtrain, whereas if lambda is large,
then you have a high bias problem, and you might not feel your training that
well, so you end up the value up there. So Jtrain of theta will tend to
increase when lambda increases, because a large value of lambda
corresponds to high bias where you might not even fit your
trainings that well, whereas a small value of
lambda corresponds to, if you can really fit a very high degree
polynomial to your data, let's say. After the cost validation error we
end up with a figure like this, where over here on the right,
if we have a large value of lambda, we may end up under fitting,
and so this is the bias regime. And so
the cross validation error will be high. Let me just leave all of that to this
Jcv (theta) because so, with high bias, we won't be fitting, we won't be
doing well in cross validation sets, whereas here on the left,
this is the high variance regime, where we have two smaller value with longer,
then we may be over fitting the data. And so by over fitting the data, then the
cross validation error will also be high. And so, this is what the cross validation
error and what the trading error may look like on a trading stance as we
vary the regularization parameter lambda. And so once again, it will often be some
intermediate value of lambda that is just right or that works best In terms of
having a small cross validation error or a small test theta. And whereas the curves I've drawn
here are somewhat cartoonish and somewhat idealized so on the real data
set the curves you get may end up looking a little bit more messy and
just a little bit more noisy then this. For some data sets you will really
see these for sorts of trends and by looking at a plot of the hold-out cross
validation error you can either manual, automatically try to select a point that
minimizes the cross validation error and select the value of lambda corresponding
to low cross validation error. When I'm trying to pick the regularization
parameter lambda for learning algorithm, often I find that plotting a figure like
this one shown here helps me understand better what's going on and helps me verify
that I am indeed picking a good value for the regularization parameter monitor. So hopefully that gives you more
insight into regularization and it's effects on the bias and
variance of a learning algorithm. By now you've seen bias and variance
from a lot of different perspectives. And what we like to do in the next video
is take all the insights we've gone through and build on them to put together
a diagnostic that's called learning curves, which is a tool that I often use
to diagnose if the learning algorithm may be suffering from a bias problem or a
variance problem, or a little bit of both.