1
00:00:00,430 --> 00:00:04,460
अपने देखा है कि कैसे रेगुलराइज़ेशन सहायता कर सकता है ओवर-फ़िटिंग से बचने में.

2
00:00:04,460 --> 00:00:08,680
लेकिन यह कैसे प्रभावित करता है बाइयस और वेरीयन्स को एक लर्निंग अल्गोरिद्म के?

3
00:00:08,680 --> 00:00:12,308
इस वीडियो में मैं जाना चाहूँगा गहराई में इस बाइयस और

4
00:00:12,308 --> 00:00:15,358
वेरीयन्स के मुद्दे में और बताऊँगा कि यह कैसे प्रभावित करता है और

5
00:00:15,358 --> 00:00:19,297
कैसे प्रभावित होता है रेगुलराइज़ेशन से आपके लर्निंग अल्गोरिद्म के.

6
00:00:22,154 --> 00:00:26,364
मान लो हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल, जैसे दिखाया है यहाँ, लेकिन

7
00:00:26,364 --> 00:00:30,730
ओवरफ़िटिंग से बचने के लिए हमें आवश्यकता है इस्तेमाल करने की रेगुलराइज़ेशन, जैसे दिखाया है यहाँ.

8
00:00:30,730 --> 00:00:36,270
तो हमारे पास है यह रेगुलराइज़ेशन टर्म रखने के लिए पेरमिटर्स की वैल्यूज़ को कम.

9
00:00:36,270 --> 00:00:41,467
और हमेशा की तरह, रेगुलराइज़ेशन आता है j=1 से m, बजाय j =0 से m तक से.

10
00:00:41,467 --> 00:00:44,281
चलो लेते हैं तीन केस.

11
00:00:44,281 --> 00:00:48,973
पहला केस है बहुत बड़ी वैल्यू रेगुलराइज़ेशन पेरामिटर

12
00:00:48,973 --> 00:00:52,430
लैम्डा की, जैसे कि लैम्डा होता बराबर 10,000 के.

13
00:00:52,430 --> 00:00:53,340
कुछ बहुत बड़ी वैल्यू.

14
00:00:54,450 --> 00:00:58,660
इस केस में, सारे ये पेरमिटर्स, थीटा 1, थीटा 2, थीटा 3 तथा इसी प्रकार

15
00:00:58,660 --> 00:01:00,970
आगे होंगे बहुत अधिक प्रभावित होंगे और

16
00:01:00,970 --> 00:01:06,360
इसलिए हमें मिलेंगे इनमें से बहुत से पेरमिटर्स ज़ीरो के नज़दीक.

17
00:01:06,360 --> 00:01:08,960
और हायपॉथिसस होगी लगभग h ऑफ़ x,

18
00:01:08,960 --> 00:01:11,860
सिर्फ़ बराबर या लगभग बराबर थीटा ज़ीरो के.

19
00:01:11,860 --> 00:01:15,840
तब हमें मिलती है एक हायपॉथिसस जो लगभग ऐसी दिखती है,

20
00:01:15,840 --> 00:01:18,720
जैसे एक समतल, कॉन्स्टंट सीधी लाइन.

21
00:01:18,720 --> 00:01:23,690
और इसलिए इस हायपॉथिसस में हाई बाइयस और यह बुरी तरह से अंडरफ़िट होती है इस डेटा सेट को,

22
00:01:23,690 --> 00:01:28,440
तो हॉरिज़ॉंटल सीधी लाइन नहीं है एक अच्छा माडल इस डेटा सेट के लिए.

23
00:01:28,440 --> 00:01:32,690
और दूसरा छोर है कि यदि हमारे पास है एक बहुत छोटी वैल्यू लैम्डा की,

24
00:01:32,690 --> 00:01:36,240
जैसे कि लैम्डा होता बराबर ज़ीरो के.

25
00:01:36,240 --> 00:01:39,750
उस केस में, दिया होने पर कि हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल,

26
00:01:39,750 --> 00:01:43,320
यह है एक सामान्य ओवर-फ़िटिंग की सेटिंग.

27
00:01:43,320 --> 00:01:47,150
उस केस में, दिया होने पर कि हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल, मूल रूप से,

28
00:01:47,150 --> 00:01:50,630
बिना रेगुलराइज़ेशन के या बहुत कम रेगुलराइज़ेशन से,

29
00:01:50,630 --> 00:01:53,570
हमें मिलता हमारा हमेशा की तरह हाई-वेरीयन्स, ओवर-फ़िटिंग सेटिंग.

30
00:01:53,570 --> 00:01:57,304
यह है मूलत: यदि हमारा लैम्डा है बराबर ज़ीरो, हम सिर्फ़ फ़िट कर रहे हैं बिना

31
00:01:57,304 --> 00:02:01,200
रेगुलराइज़ेशन के, तो वह ओवर-फ़िट होता है हायपॉथिसस को.

32
00:02:01,200 --> 00:02:05,030
और यह केवल तब जब हमारे पास कुछ मध्यम स्तर की वैल्यू लैम्डा की जो है

33
00:02:05,030 --> 00:02:08,660
न ज़्यादा बड़ी न ज़्यादा छोटी कि हमें मिलते हैं

34
00:02:08,660 --> 00:02:13,470
पेरामिटर थीटा जो देते हैं हमें एक उचित फ़िट इस डेटा को.

35
00:02:13,470 --> 00:02:18,660
तो, कैसे हम अपने आप चुन सकते हैं एक उचित वेल्युु रेगुलराइज़ेशन पेरामिटर की?

36
00:02:19,660 --> 00:02:24,250
सिर्फ़ दोहराने के लिए, यहाँ है हमारा मॉडल, और यहाँ है हमारा लर्निंग अल्गोरिद्म का अब्जेक्टिव.

37
00:02:24,250 --> 00:02:27,080
उस सेटिंग में जब हम इस्तेमाल कर रहे हैं रेगुलराइज़ेशन,

38
00:02:27,080 --> 00:02:30,990
मैं परिभाषित करता हूँ J ट्रेन (थीटा) कुछ अलग ढंग से,

39
00:02:30,990 --> 00:02:36,110
कि वह हमारा ऑप्टिमायज़ेशन अब्जेक्टिव हो, लेकिन बिना रेगुलराइज़ेशन टर्म के.

40
00:02:36,110 --> 00:02:41,630
पहले, एक पिछले वीडियो में, जब हम नहीं इस्तेमाल कर रहे थे रेगुलराइज़ेशन तब मैंने परिभाषित किया था J

41
00:02:41,630 --> 00:02:47,650
ट्रेन ऑफ़ थीटा समान J ऑफ़ थीटा के जो था कॉस्ट फ़ंक्शन लेकिन जब हम ले रहे हैं

42
00:02:47,650 --> 00:02:52,180
रेगुलराइज़ेशन हम परिभाषित करेंगे J ट्रेन मेरे

43
00:02:52,180 --> 00:02:57,040
ट्रेनिंग सेट का सिर्फ़ सम स्क्वेर्ड एरर का ट्रेनिंग सेट पर या मेरा औसत

44
00:02:57,040 --> 00:03:01,610
स्क्वेर्ड एरर ट्रेनिंग सेट पर बिना ध्यान दिए रेगुलराइज़ेशन पर.

45
00:03:01,610 --> 00:03:07,077
और इसी तरह मैं तब परिभाषित करूँगा क्रॉस वैलिडेशन सेट एरर और

46
00:03:07,077 --> 00:03:12,380
टेस्ट सेट एरर पहले जैसे औसत सम स्क्वेर्ड एरर का क्रॉस

47
00:03:12,380 --> 00:03:17,600
वैलिडेशन सेट और टेस्ट सेट पर इसलिए सारांश में मेरी परिभाषाएँ J ट्रेन J CV

48
00:03:17,600 --> 00:03:22,904
और J टेस्ट की हैं सिर्फ़ औसत स्क्वेर्ड एरर क्रॉस-वैलिडेशन सेट पर और दूसरी है स्क्वेर्ड एरर

49
00:03:22,904 --> 00:03:28,644
टेस्ट सेट पर बिना अतिरिक्त रेगुलराइज़ेशन टर्म के.

50
00:03:28,644 --> 00:03:32,981
तो यह है कि कैसे हम चुन सकते हैं रेगुलराइज़ेशन

51
00:03:32,981 --> 00:03:34,356
पैरामीटर लैम्डा.

52
00:03:34,356 --> 00:03:38,700
तो मैं अक्सर क्या करता हूँ कि लेता हूँ एक रेंज लैम्डा की वैल्यूज़ की जो मैं चाहता हूँ

53
00:03:38,700 --> 00:03:39,360
जाँचना.

54
00:03:39,360 --> 00:03:43,555
तो मैं शायद नहीं सोच रहा हूँ इस्तेमाल करने की रेगुलराइज़ेशन या ये हैं कुछ वैल्यूज़ जिन पर मैं

55
00:03:43,555 --> 00:03:48,360
शायद विचार करूँ लैम्डा = 0.01, 0.02, 0.04 और इसी प्रकार आगे.

56
00:03:48,360 --> 00:03:54,395
और आमतौर पर मैं इनको सेट करता हूँ दो के गुणकों / मल्टिपल्स में, कुछ शायद बड़ी वैल्यू तक

57
00:03:54,395 --> 00:03:59,784
यदि मुझे करना होता 2 के गुणकों/ मल्टिपल्स में मैं समाप्त करता हूँ 10.24 पर.

58
00:03:59,784 --> 00:04:02,663
यह नहीं हैं पूरा 10, लेकिन काफ़ी नज़दीक है.

59
00:04:02,663 --> 00:04:07,880
और तीन से चार दशमलव स्थान आपके परिणाम को उतना प्रभावित नहीं करेंगे.

60
00:04:07,880 --> 00:04:11,920
तो, वह देता है मुझे शायद 12 भिन्न मॉडल.

61
00:04:11,920 --> 00:04:16,040
और मैं चुनना चाह रहा हूँ एक मॉडल 12 भिन्न वैल्यूज़ में से

62
00:04:16,040 --> 00:04:19,000
रेगुलराइज़ेशन पेरामिटर लैम्डा की.

63
00:04:19,000 --> 00:04:22,820
निश्चय ही आप ले सकते हैं एक वैल्यू 0.01 से कम या

64
00:04:22,820 --> 00:04:27,450
10 से बड़ी वैल्यूज़ लेकिन मैंने सुविधा के लिए इतना ही लिया है.

65
00:04:27,450 --> 00:04:31,830
दिए होने पर ये 12 मॉडल, मैं तब क्या कर सकता हूँ है निम्नलिखित,

66
00:04:31,830 --> 00:04:36,980
हम ले सकते हैं पहला मॉडल जिसमें लैम्डा है ज़ीरो और मिनमायज़ करते हैं

67
00:04:36,980 --> 00:04:41,880
मेरा कॉस्ट फ़ंक्शन J ऑफ़ थीटा और यह देगा मुझे कुछ पेरामिटर थीटा वेक्टर के.

68
00:04:41,880 --> 00:04:43,200
और पिछले वीडियो के समान,

69
00:04:43,200 --> 00:04:47,900
मैं इसे डिनोट करता हूँ थीटा सूपरस्क्रिप्ट एक से.

70
00:04:49,570 --> 00:04:54,184
और फिर मैं ले सकता हूँ मेरा दूसरा मॉडल जिसमें लैम्डा है 0.01 और

71
00:04:54,184 --> 00:04:59,328
मिनमायज़ करता हूँ मेरा कॉस्ट फ़ंक्शन अब लेते हुए लैम्डा बराबर 0.01 ही.

72
00:04:59,328 --> 00:05:01,532
मिलेगा कुछ भिन्न पेरामिटर वेक्टर थीटा.

73
00:05:01,532 --> 00:05:03,227
उसे मैं डिनोट करता हूँ थीटा (2) से.

74
00:05:03,227 --> 00:05:05,295
और फिर मुझे मिलता है थीटा(3).

75
00:05:05,295 --> 00:05:07,476
जो है मेरे तीसरे मॉडल के लिए.

76
00:05:07,476 --> 00:05:13,249
और इसी प्रकार मेरे अंतिम मॉडल तक जिसमें लैम्डा है 10 या

77
00:05:13,249 --> 00:05:18,250
10.24, और मुझे मिलता है यह थीटा (12).

78
00:05:18,250 --> 00:05:22,920
आगे, में ले सकता हूँ मेरी सारी हायपॉथिसस, सारे ये पेरमिटर्स और

79
00:05:22,920 --> 00:05:26,090
इस्तेमाल करता हूँ मेरा क्रॉस वैलिडेशन सेट इन्हें वैलिडेट/ सत्यापित करने के लिए अत:

80
00:05:26,090 --> 00:05:30,380
मैंने लिया मेरा पहला मॉडल, मेरा दूसरा मॉडल,

81
00:05:30,380 --> 00:05:34,490
जो फ़िट किए गए हैं इन भिन्न वैल्यूज़ से रेगुलराइज़ेशन पेरामिटर की, और

82
00:05:34,490 --> 00:05:39,660
जाँचता हूँ उन्हें मेरे क्रॉस वैलिडेशन सेट से जो निर्भर करता है औसत स्क्वेर्ड एरर पर

83
00:05:39,660 --> 00:05:44,650
प्रत्येक इन स्क्वेर्ड वेक्टर पेरमिटर्स थीटा की मेरे क्रॉस वैलिडेशन सेट्स पर.

84
00:05:44,650 --> 00:05:49,910
और फिर मैं लूँगा वह जो इन 12 मॉडल में से देता हैं मुझे सबसे कम एरर

85
00:05:49,910 --> 00:05:52,070
क्रॉस-वैलिडेशन सेट पर.

86
00:05:52,070 --> 00:05:57,200
और मान लो, इस उदाहरण के लिए, कि मैं चुनता हूँ थीटा 5,

87
00:05:57,200 --> 00:06:03,060
वह 5थ डिग्री पालिनोमीयल था क्योंकि उसकी सबसे कम क्रॉस वैलिडेशन एरर थी.

88
00:06:03,060 --> 00:06:08,450
वैसा कर लेने के बाद, अंत में मैं करूँगा यदि मुझे रिपोर्ट करना है प्रत्येक टेस्ट सेट की एरर,

89
00:06:08,450 --> 00:06:13,350
कि लेता पेरामिटर थीटा 5 जो मैंने चुना है, और

90
00:06:13,350 --> 00:06:15,720
देखता कि कितना सही यह करता है मेरे टेस्ट सेट पर.

91
00:06:15,720 --> 00:06:19,738
तो एक बार फिर, यहाँ है ऐसे जैसे हमने फ़िट किया है यह पेरामिटर, थीटा,

92
00:06:19,738 --> 00:06:24,880
मेरे क्रॉस-वैलिडेशन सेट को, यही वजह है मैं अलग रखता हूँ

93
00:06:24,880 --> 00:06:29,770
एक अलग टेस्ट सेट जिसका में इस्तेमाल करूँगा पाने के लिए एक बेहतर अनुमान कि कितना

94
00:06:29,770 --> 00:06:35,140
सही मेरा पेरामिटर वेक्टर, थीटा, जनरलाइज करेगा पहले से अनदेखे इग्ज़ाम्पल्ज़ पर.

95
00:06:35,140 --> 00:06:38,280
तो वह है मॉडल सिलेक्शन जो अप्लाई किया है चुनने के लिए

96
00:06:38,280 --> 00:06:40,990
रेगुलराइज़ेशन पेरामिटर लैम्डा.

97
00:06:40,990 --> 00:06:45,430
अंतिम काम जो मैं करना चाहता हूँ इस वीडियो में है कि पाना एक बेहतर समझ

98
00:06:45,430 --> 00:06:47,830
कि कैसे क्रॉस वैलिडेशन और

99
00:06:47,830 --> 00:06:52,750
ट्रेनिंग एरर घटते बढ़ते हैं रेगुलराइज़ेशन पेरामिटर लैम्डा के साथ.

100
00:06:52,750 --> 00:06:56,870
और सिर्फ़ एक याद दिलाने के लिए, वह था हमारा प्रारम्भिक कॉस्ट फ़ंक्शन J ऑफ़ थीटा.

101
00:06:56,870 --> 00:06:59,480
लेकिन इस हेतु हम परिभाषित करेंगे

102
00:06:59,480 --> 00:07:02,940
ट्रेनिंग एरर बिना इस्तेमाल किए रेगुलराइज़ेशन पेरामिटर, और

103
00:07:02,940 --> 00:07:06,340
क्रॉस वैलिडेशन एरर बिना इस्तेमाल किए रेगुलराइज़ेशन पेरामिटर.

104
00:07:07,560 --> 00:07:12,790
और मैं करना चाहता हूँ कि प्लॉट करूँ इस J ट्रेन को और प्लॉट करूँ इस Jcv को,

105
00:07:12,790 --> 00:07:18,590
देखने के लिए कैसे काम करती है मेरी हायपाथिसस ट्रेनिंग सेट पर और

106
00:07:18,590 --> 00:07:22,000
कितनी सही मेरी हायपॉथिसस करती हैं क्रॉस वैलिडेशन सेट पर.

107
00:07:22,000 --> 00:07:25,140
जैसे मैं बढ़ाता या कम करता हूँ रेगुलराइज़ेशन पेरामिटर लैम्डा.

108
00:07:27,820 --> 00:07:34,680
तो जैसे कि हमने पहले देखा था कि यदि लैम्डा बहुत छोटा है तब हम ज़्यादा रेगुलराइज़ नहीं कर रहे

109
00:07:35,680 --> 00:07:40,740
और अधिक सम्भावना है ओवर फ़िटिंग की जबकि यदि लैम्डा

110
00:07:40,740 --> 00:07:46,910
है बड़ा तब हम हैं जैसे दाईं तरफ़ इस हॉरिज़ॉंटल ऐक्सिस के तब,

111
00:07:46,910 --> 00:07:52,500
लैम्डा की बड़ी वैल्यूज़ अधिक सम्भावना हो जाती है एक बाइयस्ड समस्या की, तो

112
00:07:52,500 --> 00:07:59,720
यदि आप प्लॉट करते हैं J ट्रेन और J cv, आपको क्या मिलता है कि, लैम्डा की छोटी वैल्यूज़ के लिए,

113
00:07:59,720 --> 00:08:04,570
आप फ़िट कर सकते हैं अपेक्षाकृत आसानी से क्योंकि आप रेगुलराइज़ नहीं कर रहे हैं.

114
00:08:04,570 --> 00:08:08,754
अत:, लैम्डा की छोटी वैल्यूज़ के लिए, रेगुलराइज़ेशन टर्म वास्तव में न के बराबर है,

115
00:08:08,754 --> 00:08:11,844
और आप मिनमायज़ कर रहे हैं ये ग्रे ऐरोज़.

116
00:08:11,844 --> 00:08:15,489
तो जब लैम्ब्डा छोटा है, आपको मिलती है Jट्रेन की कम वैल्यू,

117
00:08:15,489 --> 00:08:19,201
जबकि यदि लैम्ब्डा बड़ा है तो आपको मिलती है एक हाई बाइयस की समस्या, और

118
00:08:19,201 --> 00:08:23,475
आप शायद न फ़िट करें आपके ट्रेनिंग सेट को उचित रूप से, आपको मिलती है एक वैल्यू ऊपर यहाँ.

119
00:08:23,475 --> 00:08:28,254
तो Jट्रेन बढ़ता है जब लैम्डा बढ़ता है,

120
00:08:28,254 --> 00:08:33,119
क्योंकि लैम्डा की एक बड़ी वैल्यू से मिलता है हाई बाइयस जहाँ आप

121
00:08:33,119 --> 00:08:36,481
शायद फ़िट ही न करें आपके ट्रेनिंग सेट को सही ढंग से,

122
00:08:36,481 --> 00:08:40,108
जबकि लैम्डा की एक छोटी वैल्यू से मिलता है,

123
00:08:40,108 --> 00:08:46,270
यदि फ़िट कर पाते हैं एक बहुत बड़ी डिग्री का पालिनोमीयल आपके डेटा को, मान लो.

124
00:08:46,270 --> 00:08:50,370
क्रॉस वैलिडेशन एरर के बाद हमें मिलता है एक चित्र इस तरह का,

125
00:08:51,470 --> 00:08:56,450
जहाँ यहाँ दाईं तरफ़, यदि हमारे पास है एक बड़ी वैल्यू लैम्डा की,

126
00:08:56,450 --> 00:09:03,080
हम शायद अंडर फ़िटिंग करेंगे और इसलिए यह बाइयस का क्षेत्र है.

127
00:09:03,080 --> 00:09:07,518
और इसलिए क्रॉस वैलिडेशन एरर भी ज़्यादा होगी.

128
00:09:07,518 --> 00:09:12,666
मैं छोड़ देता हूँ यह सब J cv(थीटा) पर क्योंकि हाई बाइयस में,

129
00:09:12,666 --> 00:09:17,424
हम फ़िट नही करेंगे, हम अच्छा नहीं करेंगे क्रॉस वैलिडेशन सेट्स पर,

130
00:09:17,424 --> 00:09:21,948
जबकि यहाँ बाईं तरफ, यह है हाई वेरीयन्स का क्षेत्र, जहाँ हमारे

131
00:09:21,948 --> 00:09:26,802
पास हैं बहुत कम वैल्यूज़ लैम्डा की तब हम शायद ओवर फ़िट करेंगे डेटा को.

132
00:09:26,802 --> 00:09:32,690
और इसलिए ओवर फ़िट करने से डेटा को, क्रॉस वैलिडेशन एरर भी ज़्यादा होगी.

133
00:09:32,690 --> 00:09:38,670
और इसलिए, यह है जो क्रॉस वैलिडेशन एरर है और यह है जो ट्रेनिंग एरर है

134
00:09:38,670 --> 00:09:43,790
जो दिखती है ट्रेनिंग सेट पर जैसे हम बढ़ाते या कम करते हैं रेगुलराइज़ेशन पेरामिटर लैम्डा.

135
00:09:43,790 --> 00:09:50,180
और एक बार फिर, यह अक्सर कुछ मध्यम स्तर की वैल्यू लैम्डा की होगी जो होगी

136
00:09:50,180 --> 00:09:54,850
उचित या जो सबसे अच्छा काम करेगी पाने के लिए एक कम क्रॉस वैलिडेशन एरर या

137
00:09:54,850 --> 00:09:56,540
एक कम टेस्ट सेट एरर.

138
00:09:56,540 --> 00:09:59,970
और जबकि जो कर्व मैंने बनाए है यहाँ वह थोड़े कार्टून जैसे हैं और

139
00:09:59,970 --> 00:10:04,730
और कुछ हद तक सम्पूर्ण है अत: वास्तविक डेटा पर जो कर्व आपको मिलेंगे वे

140
00:10:04,730 --> 00:10:08,350
दिखेंगे थोड़े टेड़े मेड़ें, सिर्फ़ थोड़े टेड़े मेड़ें इस से.

141
00:10:08,350 --> 00:10:13,230
कुछ डेटा सेट्स में आप वाक़ई देख पाएँगे इस तरह के ट्रेंड और

142
00:10:13,230 --> 00:10:19,240
देखने से प्लॉट क्रॉस-वैलिडेशन एरर का आप कर सकते हैं या तो हाथ से

143
00:10:19,240 --> 00:10:25,610
या प्रोग्राम से, चुनाव एक पोईँट का जो मिनमायज़ करता है क्रॉस वैलिडेशन एरर को और

144
00:10:25,610 --> 00:10:30,380
चुन सकते हैं लैम्डा की वैल्यू जो जुड़ी है न्यूनतम क्रॉस वैलिडेशन एरर से.

145
00:10:30,380 --> 00:10:35,590
जब मैं प्रयास कर रहा हूँ चुनने का रेगुलराइज़ेशन पेरामिटर लैम्डा लर्निंग अल्गोरिद्म के लिए,

146
00:10:35,590 --> 00:10:40,640
अक्सर मैं पाता हूँ कि प्लॉट करने से एक चित्र इस तरह का, जैसा दिखाया है यहाँ सहायता करता है मुझे समझने में

147
00:10:40,640 --> 00:10:45,760
कि क्या चल रहा है और सहायता करता है मुझे सत्यापित करने में कि मैंने वाक़ई में ली है एक सही वैल्यू

148
00:10:45,760 --> 00:10:48,350
रेगुलराइज़ेशन पेरामिटर लैम्डा की.

149
00:10:48,350 --> 00:10:52,630
तो उम्मीद है उससे आपको मिली होगी एक ज़्यादा समझ रेगुलराइज़ेशन की और

150
00:10:52,630 --> 00:10:56,810
उसके प्रभाव बाइयस और वेरीयन्स पर एक लर्निंग अल्गोरिद्म के.

151
00:10:56,810 --> 00:11:01,290
अब तक आपने देखा बाइयस और वेरीयन्स विभिन्न दृष्टिकोणों से.

152
00:11:01,290 --> 00:11:05,540
और हम क्या करना चाहते हैं अगले वीडियो में कि लें यह सब समझ जो हमने

153
00:11:05,540 --> 00:11:10,890
सीखा है और बनाए उससे एक डाइयग्नास्टिक जिसे कहते हैं लर्निंग

154
00:11:10,890 --> 00:11:16,170
कर्व्ज़, जो है एक टूल जो मैं अक्सर इस्तेमाल करता हूँ पहचानने के लिए कि क्या लर्निंग अल्गोरिद्म में

155
00:11:16,170 --> 00:11:19,900
हाई बाइयस की समस्या है या एक वेरीयन्स की समस्या है या थोड़ा बहुत दोनो ही.