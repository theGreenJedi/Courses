1
00:00:00,090 --> 00:00:02,040
本节课我们介绍学习曲线

2
00:00:03,310 --> 00:00:05,850
绘制学习曲线非常有用

3
00:00:06,710 --> 00:00:08,170
也许你想检查你的学习算法

4
00:00:08,430 --> 00:00:09,590
运行是否一切正常

5
00:00:10,400 --> 00:00:12,730
或者你希望改进算法的表现或效果

6
00:00:13,950 --> 00:00:15,200
那么学习曲线

7
00:00:15,310 --> 00:00:16,410
就是一种很好的工具

8
00:00:16,820 --> 00:00:17,920
我经常使用学习曲线

9
00:00:18,290 --> 00:00:20,030
来判断某一个学习算法

10
00:00:20,180 --> 00:00:23,220
是否处于偏差 方差问题 或是二者皆有

11
00:00:27,170 --> 00:00:28,070
下面我们就来介绍学习曲线

12
00:00:28,830 --> 00:00:30,550
为了绘制一条学习曲线

13
00:00:30,700 --> 00:00:31,760
我通常先绘制出Jtrain

14
00:00:32,210 --> 00:00:33,950
也就是训练集数据的

15
00:00:35,030 --> 00:00:36,050
平均误差平方和

16
00:00:36,440 --> 00:00:39,090
或者Jcv 也即交叉验证集数据的

17
00:00:39,340 --> 00:00:41,130
平均误差平方和

18
00:00:41,590 --> 00:00:42,900
我要将其绘制成一个

19
00:00:43,140 --> 00:00:44,160
关于参数m的函数

20
00:00:44,500 --> 00:00:46,380
也就是一个关于训练集

21
00:00:47,230 --> 00:00:51,260
样本总数的函数

22
00:00:51,950 --> 00:00:53,420
所以m一般都是一个常数 比如m等于100

23
00:00:53,650 --> 00:00:55,220
表示100组训练样本

24
00:00:55,330 --> 00:00:57,670
但我要自己取一些m的值

25
00:00:57,860 --> 00:00:59,280
也就是说我要自行对m的取值

26
00:00:59,500 --> 00:01:01,460
做一点限制

27
00:01:01,840 --> 00:01:03,440
比如说我取10 20或者

28
00:01:03,660 --> 00:01:06,040
30 40组训练集

29
00:01:06,170 --> 00:01:07,610
然后绘出训练集误差

30
00:01:07,740 --> 00:01:09,640
以及交叉验证集误差

31
00:01:10,040 --> 00:01:12,260
好的 那么我们来看看

32
00:01:12,620 --> 00:01:14,090
这条曲线绘制出来是什么样子

33
00:01:14,270 --> 00:01:15,530
假设我只有一组训练样本

34
00:01:15,730 --> 00:01:17,210
也即m=1

35
00:01:17,390 --> 00:01:18,450
正如第一幅图中所示

36
00:01:18,860 --> 00:01:19,970
并且假设使用二次函数来拟合模型

37
00:01:22,470 --> 00:01:24,490
那么由于我只有一个训练样本

38
00:01:25,040 --> 00:01:26,100
拟合的结果很明显会很好

39
00:01:26,650 --> 00:01:28,590
是吧 用二次函数来拟合

40
00:01:28,760 --> 00:01:30,000
对这一个训练样本拟合

41
00:01:30,150 --> 00:01:32,240
其误差一定为0

42
00:01:32,570 --> 00:01:34,170
如果有两组训练样本 二次函数也能很好地拟合

43
00:01:37,050 --> 00:01:38,550
即使是使用正则化

44
00:01:38,750 --> 00:01:40,220
拟合的结果也会很好

45
00:01:41,080 --> 00:01:41,970
而如果不使用正则化的话

46
00:01:42,030 --> 00:01:45,200
那么拟合效果绝对棒极了

47
00:01:45,440 --> 00:01:46,400
如果我用三组训练样本的话

48
00:01:47,260 --> 00:01:48,380
好吧 看起来依然能很好地

49
00:01:48,660 --> 00:01:51,320
用二次函数拟合

50
00:01:51,550 --> 00:01:52,590
也就是说 当m等于1 m=2 或m=3时

51
00:01:54,850 --> 00:01:56,770
对训练集数据进行预测

52
00:01:57,350 --> 00:01:58,870
得到的训练集误差

53
00:01:59,110 --> 00:02:01,180
都将等于0

54
00:02:01,220 --> 00:02:02,760
这里假设我不使用正则化

55
00:02:03,150 --> 00:02:04,290
当然如果使用正则化

56
00:02:04,560 --> 00:02:06,400
那么误差就稍大于0

57
00:02:06,500 --> 00:02:07,350
顺便提醒一下

58
00:02:07,740 --> 00:02:08,980
如果我的训练集样本很大

59
00:02:09,940 --> 00:02:11,040
而我要人为地限制训练集

60
00:02:11,120 --> 00:02:13,080
样本的容量

61
00:02:13,830 --> 00:02:14,770
比如说这里

62
00:02:15,110 --> 00:02:16,720
我将m值设为3

63
00:02:17,040 --> 00:02:18,290
然后我仅用这三组样本进行训练

64
00:02:19,270 --> 00:02:21,030
然后对应到这个图中

65
00:02:21,110 --> 00:02:22,430
我只看对这三组训练样本

66
00:02:22,830 --> 00:02:24,450
进行预测得到的训练误差

67
00:02:24,550 --> 00:02:25,580
也是和我模型拟合的三组样本

68
00:02:27,150 --> 00:02:28,130
所以即使我有100组训练样本

69
00:02:28,290 --> 00:02:31,160
而我还是想绘制

70
00:02:31,430 --> 00:02:32,620
当m等于3时的训练误差

71
00:02:34,270 --> 00:02:35,200
那么我要关注的仍然是

72
00:02:35,340 --> 00:02:36,660
对这三组训练样本进行预测的误差

73
00:02:36,750 --> 00:02:39,870
同样 这三组样本也是我们用来拟合模型的三组样本

74
00:02:41,290 --> 00:02:42,900
所有其他的样本

75
00:02:43,010 --> 00:02:44,940
我都在训练过程中选择性忽略了

76
00:02:45,140 --> 00:02:46,750
好的 总结一下

77
00:02:46,960 --> 00:02:48,460
我们现在已经看到

78
00:02:48,820 --> 00:02:50,560
当训练样本容量m很小的时候

79
00:02:50,630 --> 00:02:52,630
训练误差也会很小

80
00:02:52,960 --> 00:02:53,900
因为很显然

81
00:02:53,930 --> 00:02:55,150
如果我们训练集很小

82
00:02:55,350 --> 00:02:56,790
那么很容易就能把

83
00:02:56,900 --> 00:02:58,080
训练集拟合到很好

84
00:02:58,720 --> 00:02:59,490
甚至拟合得天衣无缝

85
00:02:59,790 --> 00:03:02,970
现在我们来看

86
00:03:03,190 --> 00:03:04,460
当m等于4的时候

87
00:03:04,680 --> 00:03:06,800
好吧 二次函数似乎也能

88
00:03:06,920 --> 00:03:07,900
对数据拟合得很好

89
00:03:08,100 --> 00:03:09,680
那我们再看

90
00:03:09,790 --> 00:03:11,350
当m等于5的情况

91
00:03:11,460 --> 00:03:13,830
这时候再用二次函数来拟合

92
00:03:14,090 --> 00:03:15,940
好像效果有下降但还是差强人意

93
00:03:16,980 --> 00:03:18,460
而当我的训练集越来越大的时候

94
00:03:18,620 --> 00:03:19,860
你不难发现 要保证使用二次函数

95
00:03:20,060 --> 00:03:21,820
的拟合效果依然很好

96
00:03:21,960 --> 00:03:25,460
就显得越来越困难了

97
00:03:25,840 --> 00:03:27,300
因此 事实上随着训练集容量的增大

98
00:03:27,690 --> 00:03:28,770
我们不难发现

99
00:03:29,300 --> 00:03:30,960
我们的平均训练误差

100
00:03:31,310 --> 00:03:33,080
是逐渐增大的

101
00:03:33,500 --> 00:03:34,650
因此如果你画出这条曲线

102
00:03:35,220 --> 00:03:36,860
你就会发现

103
00:03:37,130 --> 00:03:38,520
训练集误差 也就是

104
00:03:38,940 --> 00:03:40,660
对假设进行预测的误差平均值

105
00:03:41,300 --> 00:03:44,730
随着m的增大而增大

106
00:03:45,020 --> 00:03:46,200
再重复一遍对这一问题的理解

107
00:03:46,500 --> 00:03:48,070
当训练样本很少的时候

108
00:03:48,350 --> 00:03:49,420
对每一个训练样本

109
00:03:49,790 --> 00:03:51,350
都能很容易地拟合到很好

110
00:03:51,610 --> 00:03:52,840
所以训练误差将会很小

111
00:03:52,940 --> 00:03:54,540
而反过来

112
00:03:54,710 --> 00:03:56,100
当m的值逐渐增大

113
00:03:56,460 --> 00:03:57,900
那么想对每一个训练样本都拟合到很好

114
00:03:58,220 --> 00:03:59,900
就显得愈发的困难了

115
00:04:00,430 --> 00:04:01,830
因此训练集误差就会越来越大

116
00:04:02,370 --> 00:04:05,840
那么交叉验证集误差的情况如何呢

117
00:04:06,720 --> 00:04:08,460
好的 交叉验证集误差

118
00:04:08,590 --> 00:04:10,100
是对完全陌生的交叉验证集数据

119
00:04:10,350 --> 00:04:12,660
进行预测得到的误差

120
00:04:12,880 --> 00:04:14,600
那么我们知道

121
00:04:14,720 --> 00:04:15,900
当训练集很小的时候

122
00:04:16,080 --> 00:04:16,890
泛化程度不会很好

123
00:04:17,020 --> 00:04:19,610
意思是不能很好地适应新样本

124
00:04:19,850 --> 00:04:21,220
因此这个假设

125
00:04:21,620 --> 00:04:22,720
就不是一个理想的假设

126
00:04:23,020 --> 00:04:23,970
只有当我使用

127
00:04:24,050 --> 00:04:25,270
一个更大的训练集时

128
00:04:25,500 --> 00:04:26,380
我才有可能

129
00:04:26,890 --> 00:04:28,100
得到一个能够更好拟合数据的

130
00:04:28,480 --> 00:04:30,810
可能的假设

131
00:04:31,380 --> 00:04:32,050
因此 你的验证集误差和

132
00:04:32,260 --> 00:04:35,650
测试集误差

133
00:04:35,890 --> 00:04:37,160
都会随着训练集样本容量m的增加

134
00:04:37,470 --> 00:04:39,150
而减小 因为你使用的数据越多

135
00:04:39,250 --> 00:04:40,700
你越能获得更好地泛化表现

136
00:04:40,990 --> 00:04:43,410
或者说对新样本的适应能力更强

137
00:04:44,010 --> 00:04:46,730
因此 数据越多 越能拟合出合适的假设

138
00:04:47,560 --> 00:04:48,560
所以 如果你把Jtrain和Jcv绘制出来

139
00:04:49,420 --> 00:04:51,670
就应该得到这样的曲线

140
00:04:52,490 --> 00:04:53,550
现在我们来看看

141
00:04:53,770 --> 00:04:54,940
当处于高偏差或者高方差的情况时

142
00:04:55,360 --> 00:04:56,550
这些学习曲线

143
00:04:56,930 --> 00:04:58,210
又会变成什么样子

144
00:04:58,920 --> 00:05:00,530
假如你的假设处于高偏差问题

145
00:05:00,830 --> 00:05:02,150
为了更清楚地解释这个问题

146
00:05:02,370 --> 00:05:03,780
我要用一个简单的例子来说明

147
00:05:03,940 --> 00:05:05,250
也就是用一条直线

148
00:05:05,440 --> 00:05:06,500
来拟合数据的例子

149
00:05:06,770 --> 00:05:08,240
很显然一条直线不能很好地拟合数据

150
00:05:09,540 --> 00:05:12,330
所以最后得到的假设很有可能是这样的

151
00:05:13,910 --> 00:05:15,450
现在我们来想一想

152
00:05:15,750 --> 00:05:16,840
如果我们增大训练集样本容量

153
00:05:17,470 --> 00:05:18,880
会发生什么情况呢

154
00:05:19,160 --> 00:05:20,480
所以现在不像画出的这样

155
00:05:20,590 --> 00:05:22,400
只有这五组样本了

156
00:05:22,570 --> 00:05:24,080
我们有了更多的训练样本

157
00:05:25,280 --> 00:05:27,230
那么如果你用一条直线来拟合

158
00:05:27,980 --> 00:05:29,700
不难发现

159
00:05:30,040 --> 00:05:31,360
还是会得到类似的一条直线假设

160
00:05:31,690 --> 00:05:32,940
我的意思是

161
00:05:33,530 --> 00:05:35,110
刚才的情况用一条直线不能很好地拟合

162
00:05:35,270 --> 00:05:37,320
而现在把样本容量扩大了

163
00:05:37,890 --> 00:05:39,460
这条直线也基本不会变化太大

164
00:05:40,230 --> 00:05:41,400
因为这条直线是对这组数据

165
00:05:41,840 --> 00:05:42,770
最可能也是最接近的拟合

166
00:05:42,890 --> 00:05:44,160
但一条直线再怎么接近

167
00:05:44,320 --> 00:05:45,630
也不可能对这组数据进行很好的拟合

168
00:05:45,870 --> 00:05:47,420
所以 如果你绘出交叉验证集误差

169
00:05:49,260 --> 00:05:50,170
应该是这样子的

170
00:05:51,320 --> 00:05:54,470
最左端表示训练集样本容量很小 比如说只有一组样本

171
00:05:55,410 --> 00:05:57,710
那么表现当然很不好

172
00:05:58,550 --> 00:05:59,470
而随着你增大训练集样本数

173
00:05:59,660 --> 00:06:00,760
当达到某一个容量值的时候

174
00:06:00,940 --> 00:06:02,350
你就会找到那条最有可能

175
00:06:02,810 --> 00:06:04,010
拟合数据的那条直线

176
00:06:04,200 --> 00:06:05,400
并且此时即便

177
00:06:05,490 --> 00:06:06,260
你继续增大训练集的

178
00:06:06,480 --> 00:06:07,790
样本容量

179
00:06:07,970 --> 00:06:09,170
即使你不断增大m的值

180
00:06:10,010 --> 00:06:12,040
你基本上还是会得到的一条差不多的直线

181
00:06:12,370 --> 00:06:14,190
因此 交叉验证集误差

182
00:06:14,480 --> 00:06:15,420
我把它标在这里

183
00:06:15,650 --> 00:06:17,040
或者测试集误差

184
00:06:17,140 --> 00:06:18,660
将会很快变为水平而不再变化

185
00:06:18,990 --> 00:06:20,480
只要训练集样本容量值达到

186
00:06:20,910 --> 00:06:22,920
或超过了那个特定的数值

187
00:06:23,270 --> 00:06:24,700
交叉验证集误差和测试集误差就趋于不变

188
00:06:25,130 --> 00:06:27,480
这样你会得到最能拟合数据的那条直线

189
00:06:28,390 --> 00:06:29,540
那么训练误差又如何呢

190
00:06:30,120 --> 00:06:33,050
同样 训练误差一开始也是很小的

191
00:06:34,620 --> 00:06:36,280
而在高偏差的情形中

192
00:06:36,760 --> 00:06:38,080
你会发现训练集误差

193
00:06:38,210 --> 00:06:40,770
会逐渐增大

194
00:06:41,000 --> 00:06:42,510
一直趋于接近

195
00:06:42,830 --> 00:06:44,700
交叉验证集误差

196
00:06:44,810 --> 00:06:46,370
这是因为你的参数很少

197
00:06:46,590 --> 00:06:48,070
但当m很大的时候 数据太多

198
00:06:48,900 --> 00:06:49,840
此时训练集和交叉验证集的

199
00:06:50,220 --> 00:06:52,500
预测效果将会非常接近

200
00:06:53,800 --> 00:06:54,750
这就是当你的学习算法处于

201
00:06:54,870 --> 00:06:56,460
高偏差情形时

202
00:06:56,770 --> 00:06:58,850
学习曲线的大致走向

203
00:07:00,220 --> 00:07:01,470
最后补充一点

204
00:07:01,630 --> 00:07:03,260
高偏差的情形

205
00:07:03,450 --> 00:07:04,930
反映出的问题是

206
00:07:05,580 --> 00:07:07,350
交叉验证集和训练集

207
00:07:07,420 --> 00:07:09,130
误差都很大

208
00:07:09,560 --> 00:07:10,440
也就是说 你最终会得到一个

209
00:07:10,650 --> 00:07:12,040
值比较大Jcv

210
00:07:12,280 --> 00:07:14,250
和Jtrain

211
00:07:15,370 --> 00:07:16,820
这也得出一个很有意思的结论

212
00:07:17,120 --> 00:07:18,520
那就是

213
00:07:18,800 --> 00:07:19,990
如果一个学习算法

214
00:07:20,360 --> 00:07:22,250
有很大的偏差

215
00:07:22,390 --> 00:07:23,430
那么当我们选用更多的训练样本时

216
00:07:24,060 --> 00:07:25,100
也就是在这幅图中

217
00:07:25,210 --> 00:07:26,600
随着我们增大横坐标

218
00:07:26,740 --> 00:07:27,880
我们发现交叉验证集误差的值

219
00:07:28,220 --> 00:07:29,430
不会表现出明显的下降

220
00:07:29,740 --> 00:07:31,020
实际上是变为水平了

221
00:07:31,560 --> 00:07:32,820
所以如果学习算法

222
00:07:32,950 --> 00:07:35,020
正处于高偏差的情形

223
00:07:36,640 --> 00:07:38,200
那么选用更多的训练集数据

224
00:07:38,370 --> 00:07:39,710
对于改善算法表现无益

225
00:07:40,190 --> 00:07:41,580
正如我们右边的

226
00:07:41,760 --> 00:07:43,120
这两幅图所体现的

227
00:07:43,210 --> 00:07:45,670
这里我们只有五组训练样本

228
00:07:46,060 --> 00:07:47,970
然后我们找到这条直线来拟合

229
00:07:48,550 --> 00:07:49,270
然后我们增加了更多的训练样本

230
00:07:49,540 --> 00:07:50,730
但我们仍然得到几乎一样的

231
00:07:51,040 --> 00:07:52,710
一条直线

232
00:07:53,200 --> 00:07:54,290
因此如果学习算法

233
00:07:54,440 --> 00:07:57,090
处于高偏差时

234
00:07:57,650 --> 00:07:59,060
给我再多的训练数据也于事无补

235
00:07:59,830 --> 00:08:01,290
交叉验证集误差或测试集误差

236
00:08:01,890 --> 00:08:02,890
也不会降低多少

237
00:08:03,730 --> 00:08:04,950
所以 能够看清你的算法正处于

238
00:08:05,250 --> 00:08:06,600
高偏差的情形

239
00:08:06,780 --> 00:08:07,620
是一件很有意义的事情

240
00:08:08,100 --> 00:08:09,500
因为这样可以让你避免

241
00:08:09,640 --> 00:08:11,140
把时间浪费在

242
00:08:11,290 --> 00:08:12,520
想收集更多的训练样本

243
00:08:12,920 --> 00:08:15,440
因为再多的数据也是无意义的

244
00:08:16,200 --> 00:08:17,070
接下来我们再来看看

245
00:08:17,140 --> 00:08:18,530
当学习算法正处于高方差的时候

246
00:08:19,470 --> 00:08:20,340
学习曲线应该是什么样子的

247
00:08:21,590 --> 00:08:22,880
首先我们来看

248
00:08:23,550 --> 00:08:24,260
训练集误差

249
00:08:25,120 --> 00:08:26,350
如果你的训练集样本容量很小

250
00:08:26,680 --> 00:08:28,730
比如像图中所示情形

251
00:08:29,130 --> 00:08:30,720
只有五组训练样本

252
00:08:31,150 --> 00:08:32,170
如果我们用很高阶次的

253
00:08:32,200 --> 00:08:33,050
多项式来拟合

254
00:08:34,380 --> 00:08:36,530
比如这里我用了100次的多项式函数

255
00:08:37,090 --> 00:08:38,750
当然不会有人这么用的 这里只是演示

256
00:08:39,920 --> 00:08:41,460
并且假设我们使用

257
00:08:41,550 --> 00:08:43,160
一个很小的lambda值

258
00:08:43,800 --> 00:08:44,920
可能不等于0

259
00:08:45,070 --> 00:08:46,830
但足够小的lambda

260
00:08:47,040 --> 00:08:47,980
那么很显然 我们会对这组数据

261
00:08:48,190 --> 00:08:50,590
拟合得非常非常好

262
00:08:50,860 --> 00:08:53,390
因此这个假设函数对数据过拟合

263
00:08:54,380 --> 00:08:55,640
所以 如果训练集

264
00:08:55,990 --> 00:08:57,820
样本容量很小时

265
00:08:58,320 --> 00:08:59,530
训练集误差Jtrain

266
00:09:00,030 --> 00:09:01,810
将会很小

267
00:09:03,130 --> 00:09:04,330
随着训练集样本容量的增加

268
00:09:04,940 --> 00:09:05,870
可能这个假设函数仍然会

269
00:09:06,000 --> 00:09:07,160
对数据或多或少

270
00:09:07,330 --> 00:09:08,810
有一点过拟合

271
00:09:09,780 --> 00:09:11,880
但很明显此时要对数据很好地拟合

272
00:09:12,020 --> 00:09:12,970
显得更加困难和吃力了

273
00:09:13,940 --> 00:09:15,140
所以 随着训练集样本容量的增大

274
00:09:15,350 --> 00:09:16,810
我们会发现Jtrain的值

275
00:09:16,960 --> 00:09:19,390
会随之增大

276
00:09:19,840 --> 00:09:21,040
因为当训练样本越多的时候

277
00:09:21,260 --> 00:09:22,720
我们就越难跟训练集数据拟合得很好

278
00:09:22,890 --> 00:09:25,700
但总的来说训练集误差还是很小

279
00:09:26,530 --> 00:09:28,600
交叉验证集误差又如何呢

280
00:09:29,220 --> 00:09:30,590
好的 在高方差的情形中

281
00:09:31,040 --> 00:09:32,760
假设函数对数据过拟合

282
00:09:32,980 --> 00:09:34,190
因此交叉验证集误差

283
00:09:34,290 --> 00:09:35,680
将会一直都很大

284
00:09:36,120 --> 00:09:37,650
即便我们选择一个

285
00:09:37,750 --> 00:09:38,930
比较合适恰当的

286
00:09:39,260 --> 00:09:40,520
训练集样本数

287
00:09:41,170 --> 00:09:42,950
因此交叉验证集误差

288
00:09:43,730 --> 00:09:45,520
画出来差不多是这样的

289
00:09:45,660 --> 00:09:47,720
所以算法处于高方差情形

290
00:09:47,830 --> 00:09:49,200
最明显的一个特点是

291
00:09:50,210 --> 00:09:51,490
在训练集误差

292
00:09:51,720 --> 00:09:54,010
和交叉验证集误差之间

293
00:09:54,340 --> 00:09:56,440
有一段很大的差距

294
00:09:57,440 --> 00:09:58,180
而这个曲线图也反映出

295
00:09:58,720 --> 00:10:00,170
如果我们要考虑增大训练集的样本数

296
00:10:00,440 --> 00:10:01,810
也就是在这幅图中

297
00:10:02,110 --> 00:10:03,660
向右延伸曲线

298
00:10:03,790 --> 00:10:05,220
我们大致可以看出

299
00:10:05,330 --> 00:10:06,830
这两条学习曲线

300
00:10:07,030 --> 00:10:08,120
蓝色和红色的两条曲线

301
00:10:08,480 --> 00:10:10,480
正在相互靠近

302
00:10:11,420 --> 00:10:12,360
因此 如果我们将曲线

303
00:10:12,520 --> 00:10:13,840
向右延伸出去

304
00:10:13,980 --> 00:10:21,230
那么似乎

305
00:10:21,360 --> 00:10:23,000
训练集误差很可能会

306
00:10:23,170 --> 00:10:24,120
逐渐增大

307
00:10:24,270 --> 00:10:25,740
而交叉验证集误差

308
00:10:27,130 --> 00:10:29,040
则会持续下降

309
00:10:30,000 --> 00:10:32,340
当然我们最关心的还是交叉验证集误差

310
00:10:33,010 --> 00:10:35,150
或者测试集误差 对吧

311
00:10:35,300 --> 00:10:36,460
所以从这幅图中

312
00:10:36,730 --> 00:10:37,850
我们基本可以预测

313
00:10:38,230 --> 00:10:39,420
如果继续增大训练样本的数量

314
00:10:39,820 --> 00:10:40,930
将曲线向右延伸

315
00:10:41,050 --> 00:10:42,650
交叉验证集误差将会

316
00:10:43,290 --> 00:10:44,610
逐渐下降

317
00:10:45,120 --> 00:10:46,090
所以 在高方差的情形中

318
00:10:46,330 --> 00:10:47,980
使用更多的训练集数据

319
00:10:48,180 --> 00:10:49,550
对改进算法的表现

320
00:10:50,170 --> 00:10:51,240
事实上是有效果的

321
00:10:51,520 --> 00:10:52,810
这同样也体现出

322
00:10:53,060 --> 00:10:54,180
知道你的算法正处于

323
00:10:54,330 --> 00:10:55,830
高方差的情形

324
00:10:56,150 --> 00:10:57,460
也是非常有意义的

325
00:10:57,810 --> 00:10:59,150
因为它能告诉你

326
00:10:59,220 --> 00:11:00,100
是否有必要花时间

327
00:11:00,680 --> 00:11:02,430
来增加更多的训练集数据

328
00:11:03,700 --> 00:11:04,920
好的 在前一页和这一页幻灯片中

329
00:11:05,330 --> 00:11:06,450
我画出的学习曲线

330
00:11:06,970 --> 00:11:08,510
都是相当理想化的曲线

331
00:11:08,900 --> 00:11:10,050
针对一个实际的学习算法

332
00:11:10,170 --> 00:11:11,970
如果你画出学习曲线的话

333
00:11:12,500 --> 00:11:13,910
你会看到基本类似的结果

334
00:11:14,560 --> 00:11:15,900
就像我在这里画的一样

335
00:11:16,600 --> 00:11:17,730
虽然如此

336
00:11:18,150 --> 00:11:19,160
有时候你也会看到

337
00:11:19,230 --> 00:11:20,820
带有一点噪声或干扰的曲线

338
00:11:21,090 --> 00:11:22,440
但总的来说

339
00:11:22,620 --> 00:11:23,850
像这样画出学习曲线

340
00:11:24,120 --> 00:11:25,460
确实能帮助你

341
00:11:25,570 --> 00:11:26,650
看清你的学习算法

342
00:11:26,950 --> 00:11:29,080
是否处于高偏差 高方差 或二者皆有的情形

343
00:11:29,170 --> 00:11:31,030
所以当我打算

344
00:11:31,200 --> 00:11:32,700
改进一个学习算法

345
00:11:32,760 --> 00:11:34,060
的表现时

346
00:11:34,260 --> 00:11:35,720
我通常会进行的一项工作

347
00:11:35,960 --> 00:11:37,440
就是画出这些学习曲线

348
00:11:37,970 --> 00:11:39,460
一般来讲 这项工作会让你

349
00:11:39,490 --> 00:11:41,710
更轻松地看出偏差或方差的问题

350
00:11:44,280 --> 00:11:45,180
在下一节视频中

351
00:11:45,420 --> 00:11:46,440
我们将介绍如何判断

352
00:11:46,650 --> 00:11:48,370
是否应采取具体的某个行为

353
00:11:48,450 --> 00:11:49,580
来改进学习算法的表现

354
00:11:50,260 --> 00:11:53,250
【教育无边界字幕组】翻译：所罗门捷列夫 校对：竹二个 审核：Naplessss