Neste vídeo, gostaria de lhe falar sobre curvas de aprendizagem. Curvas de aprendizado normalmente são muito úteis de se traçar quer você queira verificar se seu algoritmo está funcionando direito ou se você quiser melhorar a performance do algoritmo. Curvas de aprendizagem são uma ferramenta que eu uso bastante para tentar diagnosticar se um algoritmo de aprendizagem particular está sofrendo de problemas de bias, variância ou dos dois. Aqui está uma curva de aprendizagem. Para traçar uma curva de aprendizagem, o que eu geralmente faço é traçar "J_train", o qual é o erro quadrático médio do meu conjunto de treinamento, ou "J_cv", o erro quadrático médio no meu conjunto de validação cruzada. Eu vou traçar o erro como uma função de "m", ou seja, como uma função do número de exemplos de treinamento que eu tenho. Assim, "m" geralmente é uma constante, digamos, 100 exemplos de treinamento. 
Mas o que eu vou fazer é reduzir artificialmente o tamanho do meu conjunto de treinamento. Assim, eu me limito propositalmente a usar somente, digamos, 10, 20, 30 ou 40 exemplos de treinamento, e traço o erro no conjunto de treinamento e no conjunto de validação cruzada para esses pequenos conjuntos de treinamento. Vamos ver qual é a cara desses gráficos. Suponha que eu tenho somente um exemplo de treinamento, como este mostrado acima e à esquerda. Digamos que eu estou ajustando uma função quadrática. Bom, como só tenho um exemplo de treinamento, vou conseguir ajustá-lo perfeitamente, certo? Somente ajustar a função quadrática. Assim, vou ter erro 0 com um exemplo de treinamento. Se eu tiver dois exemplos de treinamento, a minha função quadrática também consegue se ajustar muito bem. Mesmo que eu esteja usando regularização, provavelmente vou conseguir ajustar a função muito bem. E se eu não estiver usando regularização, vou conseguir ajustar perfeitamente, e se eu tiver três exemplos de treinamento, também consigo ajustar uma função quadrática perfeitamente, ou seja, se "m" for igual a 1, 2 ou 3, meu erro de treinamento no conjunto de treinamento será 0, assumindo que eu não usei regularização, ou pode ser um pouco maior que 0 se eu usar regularização. Aliás, se eu tiver um conjunto de treinamento grande e restringir artificialmente o seu tamanho para traçar "J_train", se eu colocar "m" igual a 3 aqui, e treinar em apenas 3 exemplos, então neste gráfico vou medir meu erro de treinamento somente nos três exemplos aos quais ajustei meus dados. Assim, mesmo que eu tiver, digamos, 100 exemplos mas quero traçar meu erro de treinamento para "m" = 3, o que vou fazer é medir o erro de treinamento somente com os 3 exemplos aos quais ajustei minha hipótese e não em todos os outros exemplos que eu propositalmente omiti do processo de treinamento. 
Resumindo o que vimos: se o conjunto de treino é pequeno, certamente o erro de treinamento será pequeno também. Afinal, como temos um conjunto de treinamento pequeno, será bem fácil se ajustar ao seu conjunto de treinamento muito bem, talvez até perfeitamente. Digamos, agora que "m" = 4.
Agora uma função quadrática não consegue mais se ajustar perfeitamente do conjunto de treinamento e, se "m" = 5, talvez uma função quadrática vai se ajustar mais ou menos aos pontos, e à medida que o conjunto de treinamento aumenta, fica cada vez mais difícil garantir que eu consiga encontrar uma função quadrática que se ajusta a todos os meus exemplos perfeitamente. Assim, à medida que o conjunto de treinamento aumenta, você vai ver que o erro de treinamento médio aumenta e, se você traçar esse gráfico, vai ver que o erro de treinamento, o erro médio da sua hipótese aumenta à medida que "m" cresce e, repetindo, a intuição é que quando "m" é pequeno, quando temos poucos exemplos de treinamento, é bem fácil ajustar todos os exemplos do seu conjunto de treinamento perfeitamente e seu erro será pequeno, mas quando "m" for maior, se torna mais difícil de ajustar perfeitamente todos os exemplos e o erro de treinamento do seu conjunto se torna maior. E o que acontece com o erro de validação cruzada? Bom, a validação cruzada é calculada nesse conjunto que eu nunca vi, ou seja, quando eu tenho um conjunto de treinamento pequeno, minha hipótese não generaliza bem, ela não vai se dar muito bem nesse conjunto. Minha hipótese aqui não parece ser muito boa e somente quando eu obtenho um conjunto de treinamento maior que começo a encontrar hipóteses que talvez se ajustem aos dados um pouco melhor. Assim, o erro de validação cruzada e do seu conjunto de teste vão tender a diminuir à medida que seu conjunto de treinamento aumentar, porque, quanto mais dados você tiver, melhor fica a generalização para novos exemplos. Assim, quanto mais dados você tiver, melhor o ajuste de sua hipótese. Se você traçar "J_train" e "J_cv", é esse tipo de coisa que você vai encontrar. Agora vamos ver com que as curvas de aprendizagem se parecem se tivermos problemas de alto bias ou alta variância. Suponha que sua hipótese tenha um alto bias e, para explicar isso, vou usar como exemplo o ajuste de uma reta a dados que não podem ser bem aproximados por uma reta. Assim, acabamos com uma hipótese que tem essa cara. Agora vamos pensar o que iria acontecer se aumentássemos o tamanho do conjunto de treinamento. Se, em vez de cinco exemplos, como o que desenhei aqui, imagine que temos muito mais exemplos de treinamento. O que acontece quando ajustamos uma linha reta a isso? O que encontramos é basicamente a mesma reta. Claro, se uma linha reta simplesmente não consegue se ajustar a esses dados, conseguir um monte de dados a mais não vai fazer a reta mudar muito. Esta é a melhor reta possível para aproximar esses dados, mas uma linha reta simplesmente não se ajusta bem ao conjunto de dados. Assim, se traçarmos o erro de validação cruzada, ele se parecerá com isso. Aqui à esquerda, se você tiver um conjunto de treinamento muito pequeno, talvez apenas um exemplo, a hipótese não será boa. Mas quando chegamos em um certo número de exemplos de treinamento, você já quase encontrou a melhor reta possível, e mesmo que você consiga um conjunto de treinamento muito maior, um valor de "m" muito maior, você vai encontrar basicamenta a mesma reta, e, assim, o erro de validação cruzada - deixe-me escrever isso - ou erro de conjunto de teste, chegará em um platô, ficará reto, muito rapidamente, assim que você passar de um certo número de exemplos de treinamento, isso permite encontrar uma linha reta muito próxima da melhor possível. E quanto ao erro de treinamento? Novamente, ele será pequeno. O que você encontra em casos com alto bias é um erro de treinamento próximo do erro de validação cruzada, porque você tem poucos parâmetros e muitos dados, pelo menos quando "m" é grande. O desempenho no conjunto de treinamento e no de validação cruzada serão bem similares. É assim que as suas curvas de aprendizagem vão ficar se você tiver um algoritmo com alto bias. Assim, o problema com alto bias é refletido no fato de que tanto o erro de validação cruzada e o erro de treinamento são altos e você obtém um valor relativamente alto tanto para "J_cv" quanto para "J_train". Isso também implica em algo bem interessante: se um algoritmo de aprendizagem tem alto bias, à medida que conseguimos mais exemplos de treinamento, quando nos movemos para a direita deste gráfico, notaremos que o erro de validação cruzada não está diminuindo muito, ele fica basicamente reto, portanto, quando algoritmos de aprendizagem sofrem de bias alto, conseguir mais dados de treinamento não vai ajudar muito, e, como no exemplo do gráfico à direita, aqui tínhamos só cinco exemplos de treinamento e ajustamos uma reta a eles. E quando conseguirmos mais um monte de dados de treinamento, ainda encontramos mais ou menos a mesma reta. Assim, se o algoritmo de aprendizagem tiver bias alto e, se você conseguir muito mais dados, isso não te ajudará a conseguir um erro de validação cruzada ou de teste muito menores. Saber se seu algoritmo de aprendizagem sofre de alto bias parece ser algo útil de se saber porque pode prevenir que você gaste muito tempo coletando mais dados de treinamento, os quais não lhe ajudariam muito. Agora, vamos olhar para o caso de um algoritmo que tenha alta variância. Vamos dar uma olhada no erro de treinamento quando você tem um conjunto de treinamento muito pequeno, como um com 5 exemplos, mostrado no gráfico à direita. Se ajustarmos um polinômio de ordem bem alta, e escolhi um polinômio de ordem 100, que ninguém usa na verdade, mas apenas para ilustração. Se usarmos um valor de lambda pequeno, não zero, mas razoavelmente pequeno, vamos conseguir aproximar os dados muito bem com uma função que se ajusta aos dados com sobreajuste. Assim, se o conjunto de treinamento é pequeno, nosso erro de treinamento, "J_train(θ)" será pequeno. À medida que o tamanho do conjunto de treinamento aumentar um pouco, podemos ainda estar sobreajustando esses dados um pouco mas também se torna um pouco mais difícil de se ajustar os dados perfeitamente. Assim, à medida que o tamanho do conjunto de treinamento aumenta, "J_train" aumenta, porque é mais difícil se ajustar ao conjunto de treinamento perfeitamente quando temos mais exemplos, mas este erro ainda será bem pequeno. E quanto ao erro de validação cruzada? Em uma situação de alta variância, a hipótese sobreajusta, e o erro de validação cruzada continuará alto, mesmo que consigamos um número moderado de exemplos de treinamento. O erro de validação cruzada será mais ou menos isso. Um indicativo de que temos um problema de alta variância é a presença desse espaço grande entre o erro de treinamento e o de validação cruzada. Olhando para este gráfico, se pensarmos em adicionar mais dados de treinamento, ou seja, extrapolar a curva para a direita, podemos perceber que as duas curvas, a curva azul e a magenta estão convergindo uma para a outra. Assim, se fôssemos extrapolar a curva para a direita, parece provável que o erro de treinamento continuaria subindo e o erro de validação cruzada continuaria descendo. E o que nós nos importamos de verdade é com o erro de validação cruzada ou o erro de teste, certo? Neste tipo de gráfico podemos perceber que, se continuarmos adicionando exemplos de treinamento e extrapolarmos para a direita, nosso erro de validação cruzada continuará diminuindo. Assim, no caso de alta variância, conseguir mais dados de treinamento irá, de fato, ajudar. Novamente, esta parece ser uma coisa boa para lembrar se seu algoritmo de aprendizagem sofre de um problema de alta variância, pois isso mostra, por exemplo, que pode valer à pena procurar mais dados de treinamento. Porém, no slide anterior e neste slide, eu desenhei curvas bem idealizadas. Se você traçar essas curvas para um algoritmo de verdade, às vezes você verá curvas bem parecidas com as que eu desenhei aqui. Às vezes vemos curvas que têm mais ruído e são mais complicadas que esta. Mas traçar curvas de aprendizagem como estas pode, muitas vezes, lhe ajudar a entender se seu algoritmo de aprendizagem sofre de bias, variância, ou até de um pouco de cada. Assim, quando eu estou tentando melhorar o desempenho de um algoritmo de aprendizagem, uma coisa que quase sempre faço é traçar essas curvas de aprendizagem e, normalmente, isso lhe dará uma noção se existe um problema de bias ou variância. No próximo vídeo veremos como isso pode ajudar a sugerir ações específicas para se tomar ou evitar, para você tentar melhorar o desempenho do seu algoritmo de aprendizagem.
Tradução: Marcel Dall'Agnol | Revisão: Caio H. K. Miyashiro