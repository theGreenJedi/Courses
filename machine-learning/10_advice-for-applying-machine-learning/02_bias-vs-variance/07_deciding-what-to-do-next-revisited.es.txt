Ya hemos hablado de cómo evaluar los algoritmos de aprendizaje, de la selección de modelos y hemos hablado mucho acerca de oscilación y varianza. Ahora, ¿cómo nos ayuda esto a entender cuáles son las vías más fructíferas que podemos seguir para mejorar el desempeño de un algoritmo de aprendizaje? Tomemos de nuevo nuestro ejemplo original y busquemos el resultado. Aquí tenemos nuestro ejemplo anterior en el que ajustamos una regresión lineal regularizada y nos encontramos con que no funciona tan bien como esperábamos. Dijimos que teníamos un menú de opciones. ¿Hay alguna manera de saber cuál de estas opciones será efectiva? Lo primero que intentamos fue obtener más ejemplos de entrenamiento. Esto resultó bien para corregir la varianza alta. Pero, por el contrario, si tienes un problema de alta oscilación y no de varianza alta, entonces, como vimos en el video anterior, obtener más ejemplos de entrenamiento no será de mucha ayuda. Esta primera opción, entonces, sólo es útil si trazas las curvas de aprendizaje y te das cuenta de que tienes un problema, aunque sea pequeño, de varianza alta; es decir, que el error de validación cruzada es mucho más grande que el error del conjunto de entrenamiento. Y ¿qué pasa si intentamos con un conjunto de funciones más pequeño? Cuando intentamos con un conjunto de funciones más pequeño, de nuevo, podemos corregir la varianza alta. En otras palabras, si te das cuenta, al ver las curvas de aprendizaje o algún otro indicador que hayas usado, que tienes un problema de oscilación alto, por favor, no pierdas tu tiempo intentando seleccionar cuidadosamente un conjunto de funciones más pequeño, porque si tienes un problema de alta oscilación, utilizar menos funciones no ayudará. Por el contrario, si ves las curvas de aprendizaje o algún otro indicador y te das cuenta que tienes un problema de varianza alta, entonces, intenta seleccionar un conjunto de funciones más pequeño. En este caso sí valdría la pena invertir tu tiempo. ¿Qué pasa si añadimos variables adicionales? Pensamos que añadir variables es una solución para corregir problemas de alta oscilación . Si añades variables adicionales es, generalmente, porque tu hipótesis actual es muy simple. Por lo tanto, intentamos añadir funciones para que nuestra hipótesis sea más capaz de ajustar el conjunto de entrenamiento. Igualmente, añadir funciones de polinomios es otra manera de añadir funciones y otra manera de corregir el problema de alta oscilación . Por el contrario, si las curvas de aprendizaje te muestran que tienes un problema de varianza alta, entonces, quizá esto no sea un buen uso para tu tiempo. Y finalmente, la disminución y el aumento de «lambda». Intentar esto es fácil y rápido. Es menos probable que esto represente tiempo perdido. Unos meses de tu vida. Disminuir «lambda», como ya lo sabes, corrige el alta oscilación . Si todavía no tienes claro esto, te recomiendo que pongas pausa al video y pienses y te convenzas de que disminuir «lambda» corrige el alta oscilación , mientras que incrementar «lambda» corrige la varianza alta. Si no estás seguro de por qué pasa esto, pon pausa al video y asegúrate de convencerte de que así es o mira las curvas que trazamos al final del video anterior y asegúrate de entender por qué resultan así. Finalmente, veamos todo lo que hemos aprendido y relacionémoslo con las redes neuronales. Aquí tenemos evidencias prácticas de cómo elegir generalmente la arquitectura o el patrón de conectividad de las redes neuronales que utilizamos. Si estás ajustando una red neuronal, una opción sería ajustar, digamos, una red neuronal pequeña con relativamente pocas unidades ocultas. Quizá sólo una unidad oculta. Si estás ajustando una una red neuronal, una opción sería ajustar una red neuronal relativamente pequeña con pocas o quizá sólo una capa oculta y tal vez un número relativamente bajo de unidades ocultas. Una red como esta tendría relativamente pocos parámetros y sería menos propensa al subajuste. La ventaja principal de estas pequeñas redes neuronales es que su cálculo computacional es más barato. Una alternativa sería ajustar una red neuronal relativamente grande con, ya sea, más unidades ocultas, hay muchas unidades ocultas aquí, o más capas ocultas. Estas redes neuronales tienden a tener más parámetros y, por lo tanto, son más propensas al sobreajuste. Una desventaja, que generalmente no es grave, pero es algo en lo que debemos pensar, es que si tienes un número alto de neuronas en tu red, puede resultar más caro calcularlas. Aunque esto no es un gran problema, dentro de lo razonable. El problema potencial más importante de las redes neuronales muy grandes es que pueden ser más propensas al sobreajuste y resulta que si aplicas una red neuronal, usar una red neuronal grande es mejor. Generalmente, entre más grande, mejor, pero si se sobreajusta, puedes utilizar la regularización para corregir el sobreajuste. Con redes neuronales más grandes, utilizar la regularización para corregir el sobreajuste es, a veces, más efectivo que utilizar una red neuronal más pequeña. La desventaja principal es que puede ser más caro calcularlas. Finalmente, una de las otras decisiones es, digamos el número de capas ocultas que quieres tener ¿Cierto? Es decir, si quieres tener una capa oculta o quieres tener tres capas ocultas como mostramos aquí, o dos capas ocultas. Usualmente, como creo que mencioné en el video anterior, utilizar una sola capa oculta es un valor por defecto razonable, pero si quieres elegir el número de capas ocultas, otra cosa que puedes intentar es conseguir una división de conjuntos de entrenamiento, validación cruzada y de prueba e intentar entrenar redes neuronales con una capa oculta o dos capas ocultas o tres capas ocultas y ver cuál de esas redes neuronales se desempeña mejor en el conjunto de validación cruzada. Tomas tus tres redes neuronales con una, dos y tres capas ocultas y calcula el error de validación cruzada o el “Jcv” en todas ellas y utiliza esto para seleccionar la red neuronal que crees que sea la mejor. Eso es todo sobre la oscilación y la varianza, sobre los métodos, como las curvas de aprendizaje, para diagnosticar esos problemas, y sobre el impacto de esto en la determinación de si un método es útil o no para mejorar el desempeño de un algoritmo de aprendizaje. Si has entendido el contenido de estos últimos videos y si lo aplicas, eres, ahora, más efectivo para hacer que tus algoritmos de aprendizaje funcionen en problemas. Una gran parte o quizá la mayoría de los desarrolladores de aprendizaje automático aquí en Silicon Valley realizan estas actividades como su trabajo de tiempo completo. Espero que estos consejos sobre la oscilación, la varianza, las curvas de aprendizaje y el diagnóstico te ayuden a aplicar algoritmos de aprendizaje más efectivamente y a hacerlos funcionar mucho mejor.