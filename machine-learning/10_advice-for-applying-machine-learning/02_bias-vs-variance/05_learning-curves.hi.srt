1
00:00:00,090 --> 00:00:02,040
इस वीडियो में मैं आप को बताना चाहता हूँ लर्निंग कर्व्ज़ के बारे में.

2
00:00:03,310 --> 00:00:05,850
लर्निंग कर्व्ज़ है अक्सर एक उपयोगी चीज़ प्लॉट करने के लिए.

3
00:00:06,710 --> 00:00:08,170
यदि आप चाहते हैं चेक करना

4
00:00:08,430 --> 00:00:09,590
कि आपका अल्गोरिद्म सही काम कर रहा हैं,

5
00:00:10,400 --> 00:00:12,730
या यदि आप चाहते हैं लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स को बेहतर करना.

6
00:00:13,950 --> 00:00:15,200
और लर्निंग कर्व है एक

7
00:00:15,310 --> 00:00:16,410
टूल जो मैं वास्तव में इस्तेमाल करता हूँ

8
00:00:16,820 --> 00:00:17,920
अक्सर कोशिश में

9
00:00:18,290 --> 00:00:20,030
पहचानने के लिए कि क्या लर्निंग अल्गोरिद्म में शायद

10
00:00:20,180 --> 00:00:23,220
बाइयस, या वेरीयन्स या थोड़ा बहुत दोनो की समस्या है.

11
00:00:27,170 --> 00:00:28,070
यहाँ है कि लर्निंग कर्व क्या है.

12
00:00:28,830 --> 00:00:30,550
प्लॉट करने के लिए एक लर्निंग कर्व, क्या

13
00:00:30,700 --> 00:00:31,760
मैं अक्सर करता हूँ कि प्लॉट करता हूँ

14
00:00:32,210 --> 00:00:33,950
J ट्रेन जो है, मान लो,

15
00:00:35,030 --> 00:00:36,050
औसत स्क्वेर्ड एरर मेरे ट्रेनिंग

16
00:00:36,440 --> 00:00:39,090
सेट पर या J cv जो है

17
00:00:39,340 --> 00:00:41,130
औसत स्क्वेर्ड एरर मेरे क्रॉस वैलिडेशन सेट पर.

18
00:00:41,590 --> 00:00:42,900
और मैं करूँगा प्लॉट

19
00:00:43,140 --> 00:00:44,160
उसे एक फ़ंक्शन की तरह

20
00:00:44,500 --> 00:00:46,380
m के, अर्थात् एक फ़ंक्शन

21
00:00:47,230 --> 00:00:51,260
ट्रेनिंग इग्ज़ाम्पल्ज़ की संख्या की तरह जो मेरे पास हैं.

22
00:00:51,950 --> 00:00:53,420
और इसलिए m है आमतौर पर एक स्थिरांक/ कॉन्स्टंट जैसे शायद मेरे पास हैं सिर्फ़, आप जानते हैं, एक 100

23
00:00:53,650 --> 00:00:55,220
ट्रेनिंग इग्ज़ाम्पल लेकिन मैं क्या

24
00:00:55,330 --> 00:00:57,670
करूँगा कि कृत्रिम रूप से

25
00:00:57,860 --> 00:00:59,280
इस्तेमाल करूँगा कुछ ही ट्रेनिंग सेट इग्ज़ाम्पल्ज़. तो, मैं

26
00:00:59,500 --> 00:01:01,460
स्वेच्छा से, सीमित करूँगा इस्तेमाल करने के लिए केवल,

27
00:01:01,840 --> 00:01:03,440
मान लो, 10 या 20 या

28
00:01:03,660 --> 00:01:06,040
30 या 40 ट्रेनिंग इग्ज़ाम्पलज़ और

29
00:01:06,170 --> 00:01:07,610
प्लॉट करूँगा कि क्या है ट्रेनिंग एरर और

30
00:01:07,740 --> 00:01:09,640
क्या है क्रॉस वैलिडेशन इस

31
00:01:10,040 --> 00:01:12,260
सबसे छोटे ट्रेनिंग सेट के लिए. तो

32
00:01:12,620 --> 00:01:14,090
चलो देखते हैं कैसे ये प्लॉट्स शायद दिखें.

33
00:01:14,270 --> 00:01:15,530
मान लो मेरे पास है केवल

34
00:01:15,730 --> 00:01:17,210
एक ट्रेनिंग इग्ज़ाम्पल उस तरह का

35
00:01:17,390 --> 00:01:18,450
दिखाया हुआ इस पहले उदाहरण में

36
00:01:18,860 --> 00:01:19,970
यहाँ और मान लो मैं फ़िट कर रहा हूँ एक क्वाड्रैटिक फ़ंक्शन. ठीक है, मेरे

37
00:01:22,470 --> 00:01:24,490
पास है केवल एक ट्रेनिंग इग्ज़ाम्पल. मैं

38
00:01:25,040 --> 00:01:26,100
कर पाऊँगा फ़िट इसे पूरी तरह से

39
00:01:26,650 --> 00:01:28,590
ठीक है? आप जानते हैं, सिर्फ़ फ़िट करें क्वाड्रैटिक फ़ंक्शन. मुझे

40
00:01:28,760 --> 00:01:30,000
मिलेगी 0

41
00:01:30,150 --> 00:01:32,240
एरर एक ट्रेनिंग इग्ज़ाम्पल पर. यदि मेरे

42
00:01:32,570 --> 00:01:34,170
पास हैं दो ट्रेनिंग इग्ज़ाम्पल्ज़. ठीक है क्वाड्रैटिक फ़ंक्शन फ़िट हो सकता है उसे भी अच्छे से. अत:,

43
00:01:37,050 --> 00:01:38,550
यदि मैं इस्तेमाल कर रहा हूँ रेगुलराइज़ेशन भी,

44
00:01:38,750 --> 00:01:40,220
मैं शायद फ़िट कर सकता हूँ उसे भी अच्छी तरह.

45
00:01:41,080 --> 00:01:41,970
और यदि मैं इस्तेमाल नहीं कर रहा हूँ रेगुलराइज़ेशन,

46
00:01:42,030 --> 00:01:45,200
मैं तब भी फ़िट करता हूँ इसे पूरी तरह से और

47
00:01:45,440 --> 00:01:46,400
यदि मेरे पास हैं तीन ट्रेनिंग इग्ज़ाम्पल्ज़

48
00:01:47,260 --> 00:01:48,380
फिर से, हाँ, मैं फ़िट कर सकता हूँ एक क्वाड्रैटिक

49
00:01:48,660 --> 00:01:51,320
फ़ंक्शन पूरी तरह से तो यदि

50
00:01:51,550 --> 00:01:52,590
m बराबर है 1 या m बराबर है 2 या m बराबर है 3,

51
00:01:54,850 --> 00:01:56,770
मेरी ट्रेनिंग एरर

52
00:01:57,350 --> 00:01:58,870
मेरे ट्रेनिंग सेट पर होगी

53
00:01:59,110 --> 00:02:01,180
0 मान कर कि मैं

54
00:02:01,220 --> 00:02:02,760
नहीं इस्तेमाल कर रहा रेगुलराइज़ेशन या यह होगी

55
00:02:03,150 --> 00:02:04,290
थोड़ी ज़्यादा 0 से यदि

56
00:02:04,560 --> 00:02:06,400
मैं इस्तेमाल कर रहा हूँ रेगुलराइज़ेशन और

57
00:02:06,500 --> 00:02:07,350
वैसे तो यदि मेरे पास है

58
00:02:07,740 --> 00:02:08,980
एक बड़ा ट्रेनिंग सेट और मैं कृत्रिम रूप से

59
00:02:09,940 --> 00:02:11,040
सीमित साइज़ मेरे

60
00:02:11,120 --> 00:02:13,080
ट्रेनिंग सेट का पाने के लिए J ट्रेन.

61
00:02:13,830 --> 00:02:14,770
यहाँ यदि मैं सेट करता हूँ

62
00:02:15,110 --> 00:02:16,720
m बराबर 3, मान लो, और मैं

63
00:02:17,040 --> 00:02:18,290
ट्रेन करता हूँ केवल तीन इग्ज़ाम्पल्ज़ पर,

64
00:02:19,270 --> 00:02:21,030
तब, इस चित्र के लिए मैं

65
00:02:21,110 --> 00:02:22,430
मापूँगा मेरी ट्रेनिंग एरर

66
00:02:22,830 --> 00:02:24,450
केवल तीन इग्ज़ाम्पल्ज़ पर जो

67
00:02:24,550 --> 00:02:25,580
वास्तव में फ़िट करते हैं डेटा को भी

68
00:02:27,150 --> 00:02:28,130
और इसलिए यद्यपि मेरे पास है

69
00:02:28,290 --> 00:02:31,160
एक 100 इग्ज़ाम्पल्ज़, लेकिन यदि मैं प्लॉट करना चाहता हूँ क्या मेरी

70
00:02:31,430 --> 00:02:32,620
ट्रेनिंग एरर है तब m है 3. मैं क्या करूँगा कि

71
00:02:34,270 --> 00:02:35,200
मापूँगा

72
00:02:35,340 --> 00:02:36,660
ट्रेनिंग एरर

73
00:02:36,750 --> 00:02:39,870
तीन इग्ज़ाम्पल्ज़ पर जो मैंने वास्तव में फ़िट किये हैं हायपॉथिसस में.

74
00:02:41,290 --> 00:02:42,900
और न कि बाक़ी के इग्ज़ाम्पल्ज़ जो मैंने

75
00:02:43,010 --> 00:02:44,940
जानबूझ कर छोड़े हैं ट्रेनिंग

76
00:02:45,140 --> 00:02:46,750
प्रक्रिया से. तो सारांश में हमने क्या

77
00:02:46,960 --> 00:02:48,460
देखा है कि यदि ट्रेनिंग सेट का

78
00:02:48,820 --> 00:02:50,560
साइज़ छोटा है तब

79
00:02:50,630 --> 00:02:52,630
ट्रेनिंग एरर भी होगी कम ही.

80
00:02:52,960 --> 00:02:53,900
क्योंकि आप जानते हैं, हमारे पास है एक

81
00:02:53,930 --> 00:02:55,150
छोटा ट्रेनिंग सेट जो

82
00:02:55,350 --> 00:02:56,790
है बहुत आसान

83
00:02:56,900 --> 00:02:58,080
फ़िट करना ट्रेनिंग सेट को

84
00:02:58,720 --> 00:02:59,490
बहुत अच्छे से यहाँ तक कि

85
00:02:59,790 --> 00:03:02,970
बिल्कुल सही, अब, मान लो

86
00:03:03,190 --> 00:03:04,460
हमारे पास है m बराबर 4 उदाहरण के लिए. ठीक है तब

87
00:03:04,680 --> 00:03:06,800
एक क्वाड्रैटिक फ़ंक्शन किया जा सकता है

88
00:03:06,920 --> 00:03:07,900
शायद फ़िट इस डेटा सेट को

89
00:03:08,100 --> 00:03:09,680
पूरी तरह से और अगर मेरे

90
00:03:09,790 --> 00:03:11,350
पास है m बराबर 5 तब आप

91
00:03:11,460 --> 00:03:13,830
जानते हैं, शायद क्वाड्रैटिक फ़ंक्शन अभी भी फ़िट हो सकता है वहाँ इसलिए

92
00:03:14,090 --> 00:03:15,940
तो, जैसे मेरा ट्रेनिंग सेट बड़ा होता जाता हैं,

93
00:03:16,980 --> 00:03:18,460
यह कठिनतर होता जाता है

94
00:03:18,620 --> 00:03:19,860
सुनिश्चित करना कि मैं

95
00:03:20,060 --> 00:03:21,820
ढूँढ सकता हूँ एक क्वाड्रैटिक फ़ंक्शन जो जाता है

96
00:03:21,960 --> 00:03:25,460
मेरे सारे इग्ज़ाम्पल्ज़ से पूरी तरह. तो

97
00:03:25,840 --> 00:03:27,300
वास्तव में जैसे ट्रेनिंग सेट का साइज़

98
00:03:27,690 --> 00:03:28,770
बढ़ता है आपको क्या मिलता है

99
00:03:29,300 --> 00:03:30,960
कि मेरी औसत ट्रेनिंग एरर

100
00:03:31,310 --> 00:03:33,080
वास्तव में बढ़ती है और इसलिए यदि आप प्लॉट करते हैं

101
00:03:33,500 --> 00:03:34,650
यह चित्र, आपको क्या मिलता है

102
00:03:35,220 --> 00:03:36,860
कि ट्रेनिंग सेट

103
00:03:37,130 --> 00:03:38,520
एरर जो है औसत

104
00:03:38,940 --> 00:03:40,660
एरर आपकी हायपॉथिसस पर बढ़ती है

105
00:03:41,300 --> 00:03:44,730
जैसे m बढ़ता है और सिर्फ़ दोहराने के लिए कि अनुभव क्या है कि जब

106
00:03:45,020 --> 00:03:46,200
m है छोटा जब आपके पास हैं बहुत

107
00:03:46,500 --> 00:03:48,070
कम ट्रेनिंग इग्ज़ाम्पल. यह होता है बहुत

108
00:03:48,350 --> 00:03:49,420
आसान फ़िट करना प्रत्येक

109
00:03:49,790 --> 00:03:51,350
आपके एक ट्रेनिंग इग्ज़ाम्पल को पूरी तरह से और

110
00:03:51,610 --> 00:03:52,840
इसलिए आपकी ट्रेनिंग एरर होगी

111
00:03:52,940 --> 00:03:54,540
कम जबकि

112
00:03:54,710 --> 00:03:56,100
जब m है बड़ा तब यह हो जाता है

113
00:03:56,460 --> 00:03:57,900
कठिन सारे ट्रेनिंग

114
00:03:58,220 --> 00:03:59,900
इग्ज़ाम्पल्ज़ का पूरी तरह फ़िट होना और इसलिए

115
00:04:00,430 --> 00:04:01,830
आपकी ट्रेनिंग सेट एरर हो जाती है

116
00:04:02,370 --> 00:04:05,840
अधिक अब, तो क्रॉस वैलिडेशन एरर कितनी है.

117
00:04:06,720 --> 00:04:08,460
ठीक है, क्रॉस वैलिडेशन है

118
00:04:08,590 --> 00:04:10,100
मेरी एरर इस क्रॉस

119
00:04:10,350 --> 00:04:12,660
वैलिडेशन सेट पर जो मैंने नहीं देखा है और

120
00:04:12,880 --> 00:04:14,600
इसलिए, आप जानते हैं, जब मेरे पास है

121
00:04:14,720 --> 00:04:15,900
एक बहुत छोटा ट्रेनिंग सेट, मैं

122
00:04:16,080 --> 00:04:16,890
नहीं कर पाऊँगा जनरलाइज़ सही ढंग से, तो

123
00:04:17,020 --> 00:04:19,610
अच्छा नहीं कर पाऊँगा उस पर.

124
00:04:19,850 --> 00:04:21,220
तो, ठीक है, यह हायपॉथिसस यहाँ नहीं

125
00:04:21,620 --> 00:04:22,720
दिखती एक संतोषजनक, और

126
00:04:23,020 --> 00:04:23,970
केवल तब जब मुझे मिलता है

127
00:04:24,050 --> 00:04:25,270
एक बड़ा ट्रेनिंग सेट जिसमें,

128
00:04:25,500 --> 00:04:26,380
आप जानते हैं, मुझे मिलने लगी है

129
00:04:26,890 --> 00:04:28,100
हायपॉथिसस जो शायद फ़िट होती है

130
00:04:28,480 --> 00:04:30,810
डेटा को थोड़ा बेहतर.

131
00:04:31,380 --> 00:04:32,050
तो आपकी क्रॉस-वैलिडेशन एरर और

132
00:04:32,260 --> 00:04:35,650
आपकी टेस्ट सेट एरर होने लगती है

133
00:04:35,890 --> 00:04:37,160
कम जैसे आपका ट्रेनिंग

134
00:04:37,470 --> 00:04:39,150
सेट का साइज़ बढ़ता है क्योंकि

135
00:04:39,250 --> 00:04:40,700
जितना अधिक डेटा आपके पास होगा, उतना बेहतर

136
00:04:40,990 --> 00:04:43,410
आप जनरलाइज़ कर पाएँगे नए इग्ज़ाम्पल्स पर.

137
00:04:44,010 --> 00:04:46,730
तो, जितना अधिक डेटा होगा आपके पास, उतना बेहतर आप फ़िट कर पाएँगे हायपॉथिसस.

138
00:04:47,560 --> 00:04:48,560
तो यदि आप प्लॉट करते हैं J ट्रेन,

139
00:04:49,420 --> 00:04:51,670
और Jcv यह है जिस प्रकार की चीज़ आपको मिलती है.

140
00:04:52,490 --> 00:04:53,550
अब, चलो देखते हैं कि कैसे

141
00:04:53,770 --> 00:04:54,940
दिखते हैं शायद लर्निंग कर्व्स

142
00:04:55,360 --> 00:04:56,550
हमारे पास या तो हाई

143
00:04:56,930 --> 00:04:58,210
बाइयस या हाई वेरीयन्स की समस्या है.

144
00:04:58,920 --> 00:05:00,530
मान लो आपकी हायपॉथिसस में है हाई

145
00:05:00,830 --> 00:05:02,150
बाइयस और इसे समझाने के लिए

146
00:05:02,370 --> 00:05:03,780
मैं करूँगा इस्तेमाल एक, सेट करुँगा एक

147
00:05:03,940 --> 00:05:05,250
उदाहरण, फ़िट करने का एक सीधी

148
00:05:05,440 --> 00:05:06,500
लाइन डेटा को जो, आप

149
00:05:06,770 --> 00:05:08,240
जानते है, नहीं हो वास्तव में एक सीधी लाइन में फ़िट.

150
00:05:09,540 --> 00:05:12,330
तब हमें मिलती है एक हायपॉथिसस जो शायद ऐसी दिखती है.

151
00:05:13,910 --> 00:05:15,450
चलो अब सोचते हैं कि क्या

152
00:05:15,750 --> 00:05:16,840
होगा यदि हमें बढ़ाना होता

153
00:05:17,470 --> 00:05:18,880
ट्रेनिंग सेट का साइज़. तो यदि

154
00:05:19,160 --> 00:05:20,480
बजाय पाँच इग्ज़ाम्पल्ज़ के जैसे

155
00:05:20,590 --> 00:05:22,400
मैंने बनाए हैं वहाँ, कल्पना करो कि

156
00:05:22,570 --> 00:05:24,080
हमारे पास हैं बहुत अधिक ट्रेनिंग इग्ज़ाम्पल्ज़.

157
00:05:25,280 --> 00:05:27,230
ठीक है क्या होता है, यदि आप फ़िट करते हैं एक सीधी लाइन इसमें.

158
00:05:27,980 --> 00:05:29,700
आप क्या पाते है, कि आपको

159
00:05:30,040 --> 00:05:31,360
मिलती है, आप जानते हैं, वही सीधी लाइन.

160
00:05:31,690 --> 00:05:32,940
मेरा मतलब है एक सीधी लाइन जो

161
00:05:33,530 --> 00:05:35,110
सिर्फ़ फ़िट नहीं हो सकती इस

162
00:05:35,270 --> 00:05:37,320
डेटा में और लेने से और अधिक डेटा, ठीक है,

163
00:05:37,890 --> 00:05:39,460
सीधी लाइन नहीं बदलेगी उतना.

164
00:05:40,230 --> 00:05:41,400
यह सबसे अधिक संभव सीधी लाइन है

165
00:05:41,840 --> 00:05:42,770
जो फ़िट होती है इस डेटा को, लेकिन

166
00:05:42,890 --> 00:05:44,160
सीधी लाइन फ़िट हो ही नहीं सकती इस

167
00:05:44,320 --> 00:05:45,630
डेटा सेट को उतने अच्छे से. तो,

168
00:05:45,870 --> 00:05:47,420
यदि आप प्लॉट करते हैं क्रॉस-वैलिडेशन एरर,

169
00:05:49,260 --> 00:05:50,170
वह ऐसी दिखेगी.

170
00:05:51,320 --> 00:05:54,470
बाईं तरफ़ है, यदि आपके पास है एक बहुत छोटा ट्रेनिंग सेट का साइज़ जैसे आप जानते है,

171
00:05:55,410 --> 00:05:57,710
शायद सिर्फ़ एक ट्रेनिंग इग्ज़ाम्पल और यह नही करेगा सही काम.

172
00:05:58,550 --> 00:05:59,470
लेकिन जब तक पहुँचते हैं

173
00:05:59,660 --> 00:06:00,760
एक निश्चित संख्या पर ट्रेनिंग

174
00:06:00,940 --> 00:06:02,350
इग्ज़ाम्पल्ज़ की, आपने लगभग

175
00:06:02,810 --> 00:06:04,010
फ़िट कर ली है सबसे अधिक सम्भव सीधी

176
00:06:04,200 --> 00:06:05,400
लाइन, और भले ही

177
00:06:05,490 --> 00:06:06,260
आपको मिले एक अधिक

178
00:06:06,480 --> 00:06:07,790
बड़ा ट्रेनिंग सेट साइज़, एक

179
00:06:07,970 --> 00:06:09,170
बहुत अधिक वैल्यू m की,

180
00:06:10,010 --> 00:06:12,040
आप जानते हैं, आपको मूल रूप से मिलेगी वही सीधी लाइन.

181
00:06:12,370 --> 00:06:14,190
और इसलिए, क्रॉस वैलिडेशन एरर

182
00:06:14,480 --> 00:06:15,420
- चलो मैं लेबल करता हूँ उसे-

183
00:06:15,650 --> 00:06:17,040
या टेस्ट सेट एरर

184
00:06:17,140 --> 00:06:18,660
समतल हो जाएगी

185
00:06:18,990 --> 00:06:20,480
बहुत जल्द, एक बार आप पहुँच जाते हैं

186
00:06:20,910 --> 00:06:22,920
एक निश्चित संख्या से परे

187
00:06:23,270 --> 00:06:24,700
ट्रेनिंग इग्ज़ाम्पल्ज़ की, जब तक कि आप

188
00:06:25,130 --> 00:06:27,480
फ़िट नहीं कर पाते सबसे अधिक संभव सीधी लाइन.

189
00:06:28,390 --> 00:06:29,540
और मेरी ट्रेनिंग एरर का क्या?

190
00:06:30,120 --> 00:06:33,050
ठीक है, ट्रेनिंग एरर फिर से कम होगी.

191
00:06:34,620 --> 00:06:36,280
और आप क्या पाते हैं कि

192
00:06:36,760 --> 00:06:38,080
हाई बाइयस के केस में

193
00:06:38,210 --> 00:06:40,770
ट्रेनिंग एरर होगी

194
00:06:41,000 --> 00:06:42,510
लगभग बराबर क्रॉस

195
00:06:42,830 --> 00:06:44,700
वैलिडेशन एरर के, क्योंकि आपके

196
00:06:44,810 --> 00:06:46,370
पास हैं इतने कम पेरामीटरज़ और इतना

197
00:06:46,590 --> 00:06:48,070
अधिक डेटा, कम से कम तब जब m बड़ा है.

198
00:06:48,900 --> 00:06:49,840
पर्फ़ॉर्मन्स ट्रेनिंग

199
00:06:50,220 --> 00:06:52,500
सेट पर और क्रॉस वैलिडेशन सेट पर होगी समान.

200
00:06:53,800 --> 00:06:54,750
और इसलिए, यह है जो आपके

201
00:06:54,870 --> 00:06:56,460
लर्निंग कर्व्स दिखेंगे,

202
00:06:56,770 --> 00:06:58,850
यदि आपके पास है एक अल्गोरिद्म जिसमें हाई बाइयस है.

203
00:07:00,220 --> 00:07:01,470
और अंत में, समस्या

204
00:07:01,630 --> 00:07:03,260
हाई बाइयस की दिखती है

205
00:07:03,450 --> 00:07:04,930
इस बात से कि दोनो

206
00:07:05,580 --> 00:07:07,350
क्रॉस-वैलिडेशन एरर और

207
00:07:07,420 --> 00:07:09,130
ट्रेनिंग एरर बहुत अधिक हैं,

208
00:07:09,560 --> 00:07:10,440
और इसलिए आपको मिलती है

209
00:07:10,650 --> 00:07:12,040
एक अपेक्षाकृत अधिक वैल्यू

210
00:07:12,280 --> 00:07:14,250
दोनो J cv और J ट्रेन की.

211
00:07:15,370 --> 00:07:16,820
यह सूचित भी करता है कुछ बहुत

212
00:07:17,120 --> 00:07:18,520
दिलचस्प, जो है कि,

213
00:07:18,800 --> 00:07:19,990
यदि एक अल्गोरिद्म में हाई

214
00:07:20,360 --> 00:07:22,250
बाइयस है, जैसे हमें

215
00:07:22,390 --> 00:07:23,430
मिलते हैं और अधिक ट्रेनिंग इग्ज़ाम्पल्ज़,

216
00:07:24,060 --> 00:07:25,100
अर्थात्, जैसे हम बढ़ते हैं

217
00:07:25,210 --> 00:07:26,600
दाईं तरफ़ इस चित्र में, हम करेंगे

218
00:07:26,740 --> 00:07:27,880
नोटिस कि क्रॉस

219
00:07:28,220 --> 00:07:29,430
वैलिडेशन एरर नहीं हो रही है

220
00:07:29,740 --> 00:07:31,020
कम, यह मूलरूप से समतल

221
00:07:31,560 --> 00:07:32,820
हो गई है, और इसलिए यदि

222
00:07:32,950 --> 00:07:35,020
लर्निंग अल्गोरिद्म में हाई बाइयस की समस्या है.

223
00:07:36,640 --> 00:07:38,200
लेना और अधिक ट्रेनिंग डेटा

224
00:07:38,370 --> 00:07:39,710
ख़ुद से वास्तव में सहायता नहीं करेगा

225
00:07:40,190 --> 00:07:41,580
उतनी, और जैसे कि हमारा चित्र

226
00:07:41,760 --> 00:07:43,120
के उदाहरण में चित्र

227
00:07:43,210 --> 00:07:45,670
में दाईं तरफ़, यहाँ हमारे पास थे केवल पाँच ट्रेनिंग

228
00:07:46,060 --> 00:07:47,970
इग्ज़ाम्पल्ज़, और हमने फ़िट की कोई सीधी लाइन.

229
00:07:48,550 --> 00:07:49,270
और जब हमारे पास थे बहुत

230
00:07:49,540 --> 00:07:50,730
अधिक ट्रेनिंग डेटा, हमें अभी भी

231
00:07:51,040 --> 00:07:52,710
मिलती है लगभग वही सीधी लाइन.

232
00:07:53,200 --> 00:07:54,290
और इसलिए यदि लर्निंग अल्गोरिद्म में

233
00:07:54,440 --> 00:07:57,090
है हाई बाइयस, उसे देना और अधिक ट्रेनिंग डेटा,

234
00:07:57,650 --> 00:07:59,060
वह वास्तव में सहायता नहीं करता आपकी

235
00:07:59,830 --> 00:08:01,290
कम करने में क्रॉस वैलिडेशन

236
00:08:01,890 --> 00:08:02,890
एरर या टेस्ट सेट एरर.

237
00:08:03,730 --> 00:08:04,950
तो जान लेने से कि क्या आपके लर्निंग

238
00:08:05,250 --> 00:08:06,600
अल्गोरिद्म में समस्या है हाई

239
00:08:06,780 --> 00:08:07,620
बाइयस की प्रतीत होती है एक उपयोगी

240
00:08:08,100 --> 00:08:09,500
चीज़ जानने के लिए क्योंकि यह

241
00:08:09,640 --> 00:08:11,140
बचा सकती है आपको व्यर्थ करने से

242
00:08:11,290 --> 00:08:12,520
बहुत सा समय इकट्ठा करने में और अधिक ट्रेनिंग

243
00:08:12,920 --> 00:08:15,440
डेटा जबकि वह उपयोगी हो ही नहीं सकता.

244
00:08:16,200 --> 00:08:17,070
आगे, चलिए देखते हैं

245
00:08:17,140 --> 00:08:18,530
सेटिंग एक लर्निंग अल्गोरिद्म की

246
00:08:19,470 --> 00:08:20,340
जिसमें हो सकता हाई वेरीयन्स.

247
00:08:21,590 --> 00:08:22,880
चलो सिर्फ़ देखते हैं

248
00:08:23,550 --> 00:08:24,260
ट्रेनिंग एरर पर लगभग वहाँ जहाँ

249
00:08:25,120 --> 00:08:26,350
आपके पास है एक बहुत छोटा ट्रेनिंग

250
00:08:26,680 --> 00:08:28,730
सेट जैसे पाँच ट्रेनिंग इग्ज़ाम्पल्ज़ दिखाए हैं

251
00:08:29,130 --> 00:08:30,720
चित्र में दाईं तरफ़ और

252
00:08:31,150 --> 00:08:32,170
यदि हम फ़िट कर रहे हैं मान लो एक

253
00:08:32,200 --> 00:08:33,050
बहुत बड़ी डिग्री का पालिनोमीयल,

254
00:08:34,380 --> 00:08:36,530
और मैंने लिखा है एक सौ डिग्री का पालिनोमीयल जो

255
00:08:37,090 --> 00:08:38,750
वास्तव में कोई इस्तेमाल नही करता, लेकिन सिर्फ़ उदाहरण ले लिए.

256
00:08:39,920 --> 00:08:41,460
अब यदि हम इस्तेमाल कर रहे हैं एक

257
00:08:41,550 --> 00:08:43,160
काफ़ी छोटी वैल्यू लैम्डा की,

258
00:08:43,800 --> 00:08:44,920
शायद ज़ीरो नही, लेकिन एक काफ़ी

259
00:08:45,070 --> 00:08:46,830
काफ़ी छोटी वैल्यू लैम्डा की, तब

260
00:08:47,040 --> 00:08:47,980
हमें मिलेगा, आप जानते हैं,

261
00:08:48,190 --> 00:08:50,590
यह डेटा फ़िट होते हुए बहुत अच्छे से

262
00:08:50,860 --> 00:08:53,390
एक फ़ंक्शन से जो ओवरफ़िट होता है इसे.

263
00:08:54,380 --> 00:08:55,640
तो, यदि ट्रेनिंग

264
00:08:55,990 --> 00:08:57,820
सेट का साइज़ है छोटा, हमारी ट्रेनिंग

265
00:08:58,320 --> 00:08:59,530
एरर, जो है, J ट्रेन

266
00:09:00,030 --> 00:09:01,810
ऑफ़ थीटा होगी कम.

267
00:09:03,130 --> 00:09:04,330
और जैसे यह ट्रेनिंग सेट का साइज़ बढ़ता है

268
00:09:04,940 --> 00:09:05,870
थोड़ा, आप जानते हैं, हम शायद

269
00:09:06,000 --> 00:09:07,160
अभी भी ओवरफ़िट कर रहे हैं इस

270
00:09:07,330 --> 00:09:08,810
डेटा को थोड़ा बहुत लेकिन

271
00:09:09,780 --> 00:09:11,880
यह होता जाता है थोड़ा कठिन भी

272
00:09:12,020 --> 00:09:12,970
फ़िट करना इस डेटा सेट को पूरी तरह से,

273
00:09:13,940 --> 00:09:15,140
और इसलिए, जैसे ट्रेनिंग सेट का साइज़

274
00:09:15,350 --> 00:09:16,810
बढ़ता है, हम पाएँगे कि

275
00:09:16,960 --> 00:09:19,390
J ट्रेन बढ़ता है, क्योंकि

276
00:09:19,840 --> 00:09:21,040
यह है सिर्फ़ थोड़ा कठिन फ़िट करना

277
00:09:21,260 --> 00:09:22,720
ट्रेनिंग सेट पूरी तरह से जब हमारे पास हैं

278
00:09:22,890 --> 00:09:25,700
अधिक इग्ज़ाम्पल्ज़, लेकिन ट्रेनिंग सेट एरर अभी भी काफ़ी कम होगी.

279
00:09:26,530 --> 00:09:28,600
अब, तो क्रॉस वैलिडेशन एरर कितनी है?

280
00:09:29,220 --> 00:09:30,590
ठीक है, हाई वेरीयन्स की

281
00:09:31,040 --> 00:09:32,760
सेटिंग में, एक हायपॉथिसस है

282
00:09:32,980 --> 00:09:34,190
ओवरफ़िटिंग और इसलिए

283
00:09:34,290 --> 00:09:35,680
क्रॉस-वैलिडेशन एरर रहेगी

284
00:09:36,120 --> 00:09:37,650
ज़्यादा, जब हमारे पास हैं

285
00:09:37,750 --> 00:09:38,930
आप जानते हैं, एक मध्यम संख्या भी

286
00:09:39,260 --> 00:09:40,520
ट्रेनिंग इग्ज़ाम्पल्ज़ की और, इसलिए

287
00:09:41,170 --> 00:09:42,950
शायद, क्रॉस वैलिडेशन

288
00:09:43,730 --> 00:09:45,520
एरर शायद वैसे दिखेगी.

289
00:09:45,660 --> 00:09:47,720
और सांकेतिक लक्षण कि हमारे

290
00:09:47,830 --> 00:09:49,200
पास हाई वेरीयन्स की समस्या है,

291
00:09:50,210 --> 00:09:51,490
है कि वहाँ है

292
00:09:51,720 --> 00:09:54,010
यह बड़ा अंतर बीच में

293
00:09:54,340 --> 00:09:56,440
ट्रेनिंग एरर और क्रॉस वैलिडेशन एरर के.

294
00:09:57,440 --> 00:09:58,180
और देखने से इस चित्र को,

295
00:09:58,720 --> 00:10:00,170
यदि हम सोचते हैं लेने से और

296
00:10:00,440 --> 00:10:01,810
ट्रेनिंग डेटा, अर्थात्, लेने से

297
00:10:02,110 --> 00:10:03,660
इस चित्र को और विस्तार करने से इसका

298
00:10:03,790 --> 00:10:05,220
दाईं तरफ, हम एक प्रकार से

299
00:10:05,330 --> 00:10:06,830
बता सकते हैं कि, आप जानते हैं

300
00:10:07,030 --> 00:10:08,120
दो कर्व्ज़, नीला कर्व

301
00:10:08,480 --> 00:10:10,480
और मजेंटा कर्व, एक दूसरे के नज़दीक पहुँच रहे हैं.

302
00:10:11,420 --> 00:10:12,360
और इसलिए, यदि हमें करना होता

303
00:10:12,520 --> 00:10:13,840
विस्तार इस चित्र का

304
00:10:13,980 --> 00:10:21,230
दाईं तरफ़, तब ऐसा

305
00:10:21,360 --> 00:10:23,000
लगता सम्भावित कि

306
00:10:23,170 --> 00:10:24,120
ट्रेनिंग एरर बढ़ती

307
00:10:24,270 --> 00:10:25,740
जाएगी और

308
00:10:27,130 --> 00:10:29,040
क्रॉस-वैलिडेशन एरर कम होती जाएगी.

309
00:10:30,000 --> 00:10:32,340
जिस चीज़ की हम वास्तव में परवाह करते हैं वह है क्रॉस वैलिडेशन एरर

310
00:10:33,010 --> 00:10:35,150
या टेस्ट सेट एरर, ठीक है?

311
00:10:35,300 --> 00:10:36,460
तो इस तरह

312
00:10:36,730 --> 00:10:37,850
के चित्र में, हम बता सकते हैं कि

313
00:10:38,230 --> 00:10:39,420
यदि हम जोड़ते रहेंगे ट्रेनिंग

314
00:10:39,820 --> 00:10:40,930
इग्ज़ाम्पल्ज़ और विस्तार करते रहेंगे

315
00:10:41,050 --> 00:10:42,650
दाईं तरफ, ठीक है, हमारी क्रॉस वैलिडेशन

316
00:10:43,290 --> 00:10:44,610
एरर कम होती रहेगी.

317
00:10:45,120 --> 00:10:46,090
और, इसलिए, हाई

318
00:10:46,330 --> 00:10:47,980
वेरीयन्स की सेटिंग में, लेना और

319
00:10:48,180 --> 00:10:49,550
ट्रेनिंग डेटा है, वास्तव में,

320
00:10:50,170 --> 00:10:51,240
सम्भावित कि सहायता करेगा.

321
00:10:51,520 --> 00:10:52,810
और इसलिए फिर से, यह लगता है एक

322
00:10:53,060 --> 00:10:54,180
उपयोगी चीज़ जानने के लिए यदि आपके

323
00:10:54,330 --> 00:10:55,830
लर्निंग अल्गोरिद्म में समस्या है

324
00:10:56,150 --> 00:10:57,460
हाई वेरीयन्स की, क्योंकि

325
00:10:57,810 --> 00:10:59,150
वह बताता है आपको, उदाहरण के लिए, कि यह

326
00:10:59,220 --> 00:11:00,100
होगा शायद आपके प्रयास योग्य

327
00:11:00,680 --> 00:11:02,430
देखना कि क्या आपको मिल सकता है और अधिक ट्रेनिंग डेटा.

328
00:11:03,700 --> 00:11:04,920
अब पिछली स्लाइड पर

329
00:11:05,330 --> 00:11:06,450
और इस स्लाइड पर, मैंने बनाए हैं उचित

330
00:11:06,970 --> 00:11:08,510
साफ़ सटीक कर्व.

331
00:11:08,900 --> 00:11:10,050
यदि आप प्लॉट करते हैं कर्व

332
00:11:10,170 --> 00:11:11,970
एक वास्तविक लर्निंग अल्गोरिद्म के लिए, कभी-कभी

333
00:11:12,500 --> 00:11:13,910
आप वाक़ई देखेंगे, आप जानते हैं, काफ़ी

334
00:11:14,560 --> 00:11:15,900
कुछ ऐसे कर्व, जैसे मैंने बनाए हैं यहाँ.

335
00:11:16,600 --> 00:11:17,730
हालांकि, कभी कभी देखेंगे कर्व

336
00:11:18,150 --> 00:11:19,160
थोड़े बहुत इधर-उधर और

337
00:11:19,230 --> 00:11:20,820
थोड़े बहुत टेड़े मेड़ें इससे.

338
00:11:21,090 --> 00:11:22,440
लेकिन प्लॉट करने से लर्निंग कर्व्स, इस

339
00:11:22,620 --> 00:11:23,850
तरह के अक्सर बताते हैं

340
00:11:24,120 --> 00:11:25,460
आपको, अक्सर मदद कर सकते हैं आपकी

341
00:11:25,570 --> 00:11:26,650
समझने में कि आपके लर्निंग अल्गोरिद्म में

342
00:11:26,950 --> 00:11:29,080
बाइयस, या वेरीयन्स या थोड़ा बहुत दोनो की समस्या है.

343
00:11:29,170 --> 00:11:31,030
तो जब मैं 

344
00:11:31,200 --> 00:11:32,700
प्रयास करता हूँ पर्फ़ॉर्मन्स को बेहतर करने की

345
00:11:32,760 --> 00:11:34,060
एक लर्निंग अल्गोरिद्म की, एक काम

346
00:11:34,260 --> 00:11:35,720
मैं लगभग हमेशा करता हूँ कि

347
00:11:35,960 --> 00:11:37,440
प्लॉट करता हूँ ये लर्निंग

348
00:11:37,970 --> 00:11:39,460
कर्व्ज़, और बहुधा यह

349
00:11:39,490 --> 00:11:41,710
देता है आपको एक बेहतर समझ कि क्या वहाँ एक बाइयस या वेरीयन्स की समस्या है.

350
00:11:44,280 --> 00:11:45,180
और अगले वीडियो में,

351
00:11:45,420 --> 00:11:46,440
हम देखेंगे कि कैसे यह कर सकता है

352
00:11:46,650 --> 00:11:48,370
मदद सुझाने में विशेष काम जो

353
00:11:48,450 --> 00:11:49,580
किए जा सकते हैं, या नहीं किए जा सकते,

354
00:11:50,260 --> 00:11:53,250
आपके लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स को बेहतर करने के लिए.