1
00:00:00,260 --> 00:00:01,340
ここまで、学習アルゴリズムをどう評価するか、

2
00:00:01,960 --> 00:00:03,360
モデルの選択、バイアスとバリアンスについて

3
00:00:04,150 --> 00:00:06,490
議論してきた。

4
00:00:06,970 --> 00:00:08,110
ではこれらがどのように

5
00:00:08,330 --> 00:00:09,730
学習アルゴリズムのパフォーマンスを改善しようとする時に

6
00:00:10,340 --> 00:00:11,710
何が潜在的に実りが多そうで

7
00:00:11,950 --> 00:00:13,980
何はそんなに良く無さそうかを見分けるのに役立たせる事が出来るだろうか？

8
00:00:15,480 --> 00:00:16,660
もとの動機づけ目的の例に戻り、

9
00:00:16,940 --> 00:00:18,890
その結果を見てみよう。

10
00:00:21,030 --> 00:00:22,570
さて、これは前に見た例で

11
00:00:23,000 --> 00:00:24,120
正規化した線形回帰をフィッティングして

12
00:00:24,720 --> 00:00:27,640
期待通りには振る舞ってない、と判明した物だ。

13
00:00:28,300 --> 00:00:30,080
そしてこんな選択肢のメニューがあると言った。

14
00:00:30,910 --> 00:00:32,430
ではこの選択肢のうちどれが

15
00:00:32,590 --> 00:00:34,530
実り多そうな選択肢かを見分けるは方法が無いものか？

16
00:00:35,480 --> 00:00:36,490
これらのうちの最初の物は

17
00:00:36,660 --> 00:00:38,770
もっと多くのトレーニング手本を得る、という物だった。

18
00:00:39,550 --> 00:00:40,700
これは高バリアンスを直すのに

19
00:00:40,880 --> 00:00:42,890
良い物だった。

20
00:00:45,320 --> 00:00:46,610
そして例えば、

21
00:00:47,150 --> 00:00:48,550
代わりに高バイアスの問題にかかっていて、

22
00:00:48,680 --> 00:00:50,530
そしてバリアンスの問題は無ければ、

23
00:00:50,830 --> 00:00:52,000
その場合は前のビデオで見たように

24
00:00:52,500 --> 00:00:53,560
トレーニング手本の数を増やす事は

25
00:00:54,640 --> 00:00:56,380
そんなに役には立たないと思われる。

26
00:00:57,360 --> 00:00:58,320
つまり、この最初の選択肢が有用なのは

27
00:00:58,780 --> 00:01:00,230
学習曲線を

28
00:01:00,580 --> 00:01:01,620
プロットしてみて

29
00:01:01,720 --> 00:01:02,820
最低でもちょっとはバリアンスがある、と

30
00:01:02,860 --> 00:01:03,970
確認出来た場合のとにだけだ。

31
00:01:04,170 --> 00:01:06,530
つまり、クロスバリデーション誤差は

32
00:01:06,680 --> 00:01:08,800
トレーニングセット誤差よりもかなり大きい時という事。

33
00:01:08,910 --> 00:01:10,400
フィーチャーの数を減らして試す、という件についてはどうだろう？

34
00:01:10,940 --> 00:01:11,920
フィーチャーの数を減らして試す、というのは

35
00:01:12,350 --> 00:01:13,570
これまた高バリアンスを

36
00:01:13,970 --> 00:01:16,060
治す為の物だ。

37
00:01:17,100 --> 00:01:18,080
言い換えると、学習曲線なり

38
00:01:18,420 --> 00:01:19,440
それ以外の何かしらなりで

39
00:01:19,820 --> 00:01:20,830
高バイアスの問題だ、と

40
00:01:21,190 --> 00:01:22,110
分かったなら、

41
00:01:22,370 --> 00:01:23,460
おお、どうか

42
00:01:23,670 --> 00:01:25,000
より少ない数のフィーチャーにする為に何を残すか、を

43
00:01:25,540 --> 00:01:27,250
慎重に選びだす事に

44
00:01:27,450 --> 00:01:29,130
時間を使わないでください。

45
00:01:29,330 --> 00:01:31,190
何故ならもし高バイアスの問題にかかっているなら

46
00:01:32,060 --> 00:01:33,220
より少ない数のフィーチャーを使うというのは、きっと役には立たないから。

47
00:01:33,890 --> 00:01:35,270
一方で、

48
00:01:35,490 --> 00:01:36,730
学習曲線なりなんなりを見て

49
00:01:36,900 --> 00:01:38,020
高バリアンスの問題を

50
00:01:38,360 --> 00:01:39,780
見つけた時には、

51
00:01:40,320 --> 00:01:41,730
まさにフィーチャーを減らす事を

52
00:01:42,160 --> 00:01:43,180
試してみるべきだ。

53
00:01:43,440 --> 00:01:45,380
それはきっとあなたの時間の、とても有効な使い方に違いない。

54
00:01:45,790 --> 00:01:47,120
フィーチャーを追加する、という選択肢はどうだろうか？

55
00:01:47,710 --> 00:01:49,640
フィーチャーを追加する、というのは

56
00:01:50,170 --> 00:01:51,380
必ずという訳では無いにせよ普通は

57
00:01:51,490 --> 00:01:53,020
高バイアスの問題の

58
00:01:54,070 --> 00:01:56,920
修正方法だとみなしている。

59
00:01:57,600 --> 00:01:58,700
つまりフィーチャーを追加する時は

60
00:01:58,980 --> 00:02:00,640
普通は現在の仮説が

61
00:02:01,750 --> 00:02:03,150
あまりにもシンプル過ぎる為、

62
00:02:03,280 --> 00:02:04,280
さらなるフィーチャーを追加する事で

63
00:02:04,540 --> 00:02:06,520
仮説がトレーニングセットに

64
00:02:06,730 --> 00:02:08,540
もっと良くフィットするように

65
00:02:09,060 --> 00:02:10,800
しようとしたいからだ。

66
00:02:11,420 --> 00:02:13,460
同様に、多項式のフィーチャーを追加するのも、

67
00:02:13,770 --> 00:02:14,930
これはフィーチャーを追加する

68
00:02:15,140 --> 00:02:16,420
もう一つの方法で、つまり

69
00:02:16,570 --> 00:02:18,220
高バイアスの問題を修正する

70
00:02:18,430 --> 00:02:19,950
もう一つの方法がある訳だ。

71
00:02:21,020 --> 00:02:22,820
そして例えば、

72
00:02:23,210 --> 00:02:24,350
学習曲線が

73
00:02:24,570 --> 00:02:25,410
高バリアンスの問題を示していたら、

74
00:02:25,520 --> 00:02:27,190
その時はこの場合も

75
00:02:27,320 --> 00:02:29,360
あなたの時間の有効な使い方とはならないだろう。

76
00:02:30,640 --> 00:02:32,690
そして最後にラムダを増減させる。

77
00:02:33,200 --> 00:02:34,090
これは手早く、簡単に試す事が出来る。

78
00:02:34,470 --> 00:02:36,000
これらはあなたの人生の何ヶ月とかを

79
00:02:36,140 --> 00:02:38,190
無駄にする可能性は、まぁ無いだろう。

80
00:02:39,070 --> 00:02:41,530
だがラムダを現象させるのは

81
00:02:41,650 --> 00:02:43,400
高バイアスを修正するという事を、既に知っている。

82
00:02:45,360 --> 00:02:46,340
もしこれが当然、と感じられなければ、

83
00:02:46,500 --> 00:02:47,340
ひとまずビデオを止めて、

84
00:02:47,810 --> 00:02:50,350
ラムダを減らすのが高バイアスの問題を修正するのに有効で、

85
00:02:50,990 --> 00:02:52,790
ラムダを増加させるのか高バリアンスの問題を修正する事を

86
00:02:53,620 --> 00:02:55,030
しっかり納得出来るまで

87
00:02:55,590 --> 00:02:57,480
よく考えてみて欲しい。

88
00:02:59,870 --> 00:03:00,930
どうしてそうなるのか

89
00:03:01,270 --> 00:03:02,470
いまいち自信が持てなければ、

90
00:03:02,650 --> 00:03:04,130
ビデオを止めてそうだと納得出来るまで

91
00:03:04,150 --> 00:03:05,820
確認して欲しい。

92
00:03:06,580 --> 00:03:07,320
または前回のビデオの最後で

93
00:03:07,800 --> 00:03:09,040
プロットしたカーブを見て

94
00:03:09,190 --> 00:03:10,590
そしてこれらが

95
00:03:10,720 --> 00:03:11,650
そうだと言う事を

96
00:03:12,170 --> 00:03:13,670
しっかりと納得してくれ。

97
00:03:15,080 --> 00:03:16,120
最後に、学んだ事全てを

98
00:03:16,440 --> 00:03:17,840
あわせて、それをニューラルネットワークに

99
00:03:18,400 --> 00:03:19,980
関連づけると、

100
00:03:20,130 --> 00:03:21,190
ニューラルネットワークの

101
00:03:21,720 --> 00:03:22,720
接続のパターンを、

102
00:03:23,520 --> 00:03:25,060
普段どうやって選んでいるかの

103
00:03:25,530 --> 00:03:28,660
実践的なアドバイスをしておく。

104
00:03:30,070 --> 00:03:31,190
ニューラルネットワークをフィッティングする時は

105
00:03:31,410 --> 00:03:33,160
考えられる一つの選択肢としては

106
00:03:33,400 --> 00:03:34,680
とても小さなニューラルネット、

107
00:03:34,840 --> 00:03:36,540
相対的に少しの隠れユニットしか無いような、

108
00:03:37,530 --> 00:03:38,670
例えば一つの隠れユニットだけのような物に

109
00:03:38,930 --> 00:03:40,430
フィッティングする、というのが考えられる、、、

110
00:03:40,890 --> 00:03:42,670
ニューラルネットワークをフィッティングする時には

111
00:03:42,800 --> 00:03:44,440
考えられる一つの選択肢としては

112
00:03:44,920 --> 00:03:46,500
相対的に小さめのニューラルネットワーク、

113
00:03:48,030 --> 00:03:49,630
相対的に少しだけの、たとえば隠れレイヤが一つだけの物で

114
00:03:49,980 --> 00:03:51,760
隠れユニットの数も相対的に少しだけのネットワークに

115
00:03:52,070 --> 00:03:53,370
フィッティングするというのが

116
00:03:53,750 --> 00:03:55,160
考えられる。

117
00:03:55,570 --> 00:03:56,580
さて、このようなネットワークは

118
00:03:57,050 --> 00:03:59,170
相対的にはより少しのパラメータしか持たず、アンダーフィッティングしがちだ。

119
00:04:00,450 --> 00:04:01,850
これらの小さなネットワークの主な利点は

120
00:04:02,260 --> 00:04:04,760
計算量がより安上がり、という所。

121
00:04:05,820 --> 00:04:06,910
これに対して代替的な方法としては

122
00:04:07,010 --> 00:04:08,470
相対的により大きな

123
00:04:08,900 --> 00:04:10,790
ニューラルネットワークを用いる事で、

124
00:04:10,970 --> 00:04:12,370
もっと多くの隠れユニットなり、

125
00:04:12,560 --> 00:04:14,940
この場合は一層内にたくさんの隠れユニットがあるとか。またはもっと多くの隠れレイヤがあるような物も考えられる。

126
00:04:16,200 --> 00:04:17,800
するとこれらのネットワークはより多くのパラメータを持つ傾向にあり、

127
00:04:18,010 --> 00:04:20,870
ゆえによりオーバーフィッティングしやすい。

128
00:04:22,410 --> 00:04:24,010
一つの欠点としては

129
00:04:24,050 --> 00:04:25,160
しばしばそれほど大きな問題とはならないが

130
00:04:25,250 --> 00:04:26,440
たまに考える必要がある事としては、

131
00:04:27,000 --> 00:04:28,450
ネットワークに多くのニューロンがあると、

132
00:04:28,960 --> 00:04:30,040
より計算量的に

133
00:04:30,230 --> 00:04:31,920
高くつく場合がある、という事。

134
00:04:33,070 --> 00:04:35,790
だが様々な理由から、これはたいていの場合は大きな問題とはならない。

135
00:04:36,840 --> 00:04:38,420
これらより大きなニューラルネットワークの

136
00:04:38,540 --> 00:04:39,710
主要な問題点たりえるのは、よりオーバーフィッティングしやすい、という物で、

137
00:04:39,980 --> 00:04:44,120
ニューラルネットワークを用いてみると、

138
00:04:44,700 --> 00:04:46,900
ネットワークは大きければ大きいほど

139
00:04:47,240 --> 00:04:48,900
良い事が分かる。

140
00:04:50,610 --> 00:04:51,700
だがもしオーバーフィットしてしまったら

141
00:04:51,890 --> 00:04:53,800
正規化を用いてオーバーフィッティングの問題に対処出来る、

142
00:04:54,230 --> 00:04:56,510
通常は大きめのネットワークに

143
00:04:56,910 --> 00:04:58,480
正規化を適用して

144
00:04:58,720 --> 00:04:59,980
オーバーフィットに対処する方が

145
00:05:00,310 --> 00:05:01,910
小さめのニューラルネットワークを使うよりも

146
00:05:02,130 --> 00:05:04,160
効果的な事が多い。

147
00:05:05,100 --> 00:05:06,940
そして主要な想定される短所としては

148
00:05:07,130 --> 00:05:09,420
そちらの方が計算量的により高価になりえる、という所。

149
00:05:10,470 --> 00:05:11,940
最後に、決断すべき別の問題として、

150
00:05:12,280 --> 00:05:14,340
隠れレイヤの数を幾つにすべきか、というのがある。

151
00:05:14,480 --> 00:05:16,400
つまり隠れレイヤは

152
00:05:17,030 --> 00:05:18,130
一つにすべきか、

153
00:05:18,380 --> 00:05:19,700
あるいはここに示したように3つの隠れレイヤにすべきか？

154
00:05:20,040 --> 00:05:21,790
あるいは2つの隠れレイヤにすべきか？

155
00:05:23,250 --> 00:05:24,850
そして普段は

156
00:05:24,980 --> 00:05:25,720
前回のビデオで言った通り

157
00:05:26,190 --> 00:05:27,420
一つの隠れレイヤを用いるのは

158
00:05:27,640 --> 00:05:29,570
リーズナブルなデフォルトだ。

159
00:05:29,780 --> 00:05:30,800
たが隠れレイヤの数を

160
00:05:30,890 --> 00:05:32,400
選ぼうという時に、

161
00:05:32,580 --> 00:05:33,610
もう一つ試せる事としては、

162
00:05:34,270 --> 00:05:35,800
トレーニングセット、クロスバリデーションセット、

163
00:05:36,660 --> 00:05:38,320
そしてテストセットに分けて、

164
00:05:38,730 --> 00:05:40,070
そして隠れレイヤ一つ、または隠れレイヤ二つ、

165
00:05:40,260 --> 00:05:41,210
または隠れレイヤ三つで

166
00:05:41,490 --> 00:05:42,810
ニューラルネットワークをトレーニングしてみて、

167
00:05:43,230 --> 00:05:44,300
それらのネットワークのどれが

168
00:05:44,460 --> 00:05:47,460
クロスバリデーションセットで最も良いパフォーマンスを出すかを見てみる。

169
00:05:48,180 --> 00:05:49,190
隠れレイヤを一つ、二つ、三つの

170
00:05:49,660 --> 00:05:50,510
ニューラルネットワークに対して

171
00:05:50,780 --> 00:05:52,410
クロスバリデーション誤差のJcvを

172
00:05:52,570 --> 00:05:53,870
それら全てに対して

173
00:05:54,140 --> 00:05:55,120
計算してみて、

174
00:05:55,240 --> 00:05:56,630
そしてそれを用いて

175
00:05:56,960 --> 00:05:58,350
これらのうちのどのニューラルネットワークを用いるのが

176
00:05:58,690 --> 00:06:00,290
ベストかを選ぶのに使う事が出来る。

177
00:06:02,580 --> 00:06:04,020
以上が、バイアスとバリアンス、

178
00:06:04,230 --> 00:06:05,490
そして学習曲線などを用いて

179
00:06:05,780 --> 00:06:08,170
これらの問題を診断する方法だ。

180
00:06:08,560 --> 00:06:09,860
それを用いて、

181
00:06:09,930 --> 00:06:11,020
学習アルゴリズムのパフォーマンスを改善するのに

182
00:06:11,250 --> 00:06:12,480
何は実り多そうで

183
00:06:12,630 --> 00:06:13,500
何はあまり実りが無さそうかを

184
00:06:13,910 --> 00:06:15,720
判断出来る。

185
00:06:16,960 --> 00:06:18,000
あなたがもしここ数回のビデオの内容を

186
00:06:18,990 --> 00:06:20,700
理解出来ていたら、

187
00:06:20,790 --> 00:06:22,020
そしてそれを実践出来るのなら、

188
00:06:22,630 --> 00:06:24,300
あなたはすでに、、、

189
00:06:24,430 --> 00:06:25,890
学習アルゴリズムを問題に対して適用出来、しかもそれを効率的に行う事が出来る、

190
00:06:26,610 --> 00:06:27,970
しかもそれを、ここシリコンバレーで

191
00:06:28,560 --> 00:06:29,810
こんにちフルタイムの仕事としてやっているエンジニアの

192
00:06:30,540 --> 00:06:31,860
かなりの割合、ひょっとしたら大多数の

193
00:06:32,060 --> 00:06:34,760
実践家よりもうまく、だ。

194
00:06:35,820 --> 00:06:37,560
さて、これらの

195
00:06:37,990 --> 00:06:39,110
診断の時の経験に対する

196
00:06:39,560 --> 00:06:41,420
アドバイスが、

197
00:06:42,730 --> 00:06:44,110
学習アルゴリズムを

198
00:06:44,790 --> 00:06:47,270
効率的に適用し、

199
00:06:48,000 --> 00:06:49,300
とてもうまく機能出来るようになる、助けとなるといいな。