このビデオでは、学習曲線について議論したい。 学習曲線はプロットするととても便利な物で、 アルゴリズムがちゃんと機能している、という サニティチェック(簡単な正当性チェック)をしたい時や アルゴリズムのパフォーマンスを改善したい時に役に立つ物だ。 そして学習曲線は 実際私が バイアスやバリアンスやその混合の問題が 起きてないかを診断したい時に しょっちゅう使っている物だ。 その学習曲線とはこんな物だ。 学習曲線をプロットするには 普通私はJ train、つまり トレーニングセットの 二乗誤差の平均をプロットするか またはJ cv、つまり クロスバリデーションセットの平均の二乗誤差をプロットする。 そしてそのプロットは mの関数としてプロットする、 つまりトレーニング手本の数の 関数という事だ。 普通、mは定数で、例えば100トレーニング手本とかだ。 だがここで私は、 人工的にトレーニングセットのサイズを 減らす、という事をやる訳だ。 つまり自分で、自分自身に 10とか20とか30とか40の トレーニング手本だけを使う、という制限を課す訳だ。 そしてそれらのトレーニング誤差がどうなってるか そしてクロスバリデーションの誤差がどうなっているかをプロットする。 この小さいトレーニングセットに対して。 ではそのプロットがどんな感じか、見てみよう。 トレーニング手本が 一つしか無いとしよう、 こんな風に最初の手本だけ、 そして二次関数をフィッティングしてるとしよう。 トレーニング手本が一つだけなので 完全にフィットさせる事が出来る。 でしょ？二次関数をフィットさせるだけで 一つのトレーニング手本に対しては 誤差0に出来る。 トレーニング手本が2つの時は、、、この場合も二次関数はよくフィットさせられる。 正規化してても たぶんかなり良くフィットさせられるだろう。 そしてもし正規化してなければ これに完璧にフィットさせられる。 そしてトレーニング手本が3つの時も 二次関数を完全に フィッティング出来る。 つまり、もしm=1かm=2かm=3なら これらのトレーニングセットに対する トレーニング誤差は 正規化してなければ0になると予想され、 正規化してたら 0よりちょっとだけ 大きい値が予想される。 ところで、 もしたくさんのトレーニングセットがあり、 それをJ train向けに トレーニングセットのサイズを制限したら、 ここを、仮にm=3に してみたら、 そして3つの手本だけでトレーニングしてみたら、 この図で3つの手本に 対してだけの 実際にフィッティングしてる対象に対してだけの トレーニング誤差を測る事になり だから100個のトレーニング手本が ある訳だけど、m=3だけのトレーニング誤差を プロットしようという訳だ。 つまり、3つの手本だけに対して 仮説をフィッティングして そのトレーニング誤差を測るわけ。 そしてその他のトレーニング手本を わざと学習プロセスから抜くわけ。 まとめると、ここまで見てきた事から、 トレーニングセットのサイズが 小さい時は トレーニング誤差も小さくなる。 何故なら、 トレーニングセットが小さければ とても簡単にフィッティング出来るから。 トレーニングセットに とても良くフィットさせられる、時には 完全に一致させられる事も。 さて、ここでm=4となると、 二次関数はもはや 完全にはデータセットに フィットさせられなくなる。 そしてm=5となると、、、 まぁこの位ならそこそこフィットしたままかもしれん。 でもそこからトレーニングセットを大きくしていくと 全てのトレーニング手本の上を 完全に通る 二次関数を見つけるのは どんどん難しくなる。 つまりトレーニングセットのサイズを 大きくしていくと、 トレーニング誤差の平均は 実際に増加していく。 だからこの図をプロットすると トレーニングセット誤差は それは仮説の誤差の 平均だが、それは mが増加するにつれて増加する。
その直感的な理解を繰り返すと、 mが小さい時は、トレーニング手本がとても ちょっとしか無い時には、 トレーニング手本を一つ一つ 完璧に通過するようにフィッティングするのはとても簡単だ。 だから誤差も 小さくなるだろう。 他方、mが大きくなると 全てのトレーニング手本を 完璧に通るのはどんどん難しくなる。 だからトレーニングセットの誤差は より大きくなる。
ではクロスバリデーションの誤差はどうだろう？ クロスバリデーションの誤差とは このクロスバリデーションセットでの 誤差で、これはまだ見ていないデータだ。 だからとても小さな トレーニングセットでは しっかりと一般化は出来ない。 これだけじゃうまくやれない。 つまりこの仮説は そんなにいい仮説じゃない。 トレーニングセットをより大きくしていく事で 初めて、、、 初めていくらか データにより良くフィットするような 仮説が得られ始めるのだ。 つまり、クロスバリデーション誤差と テストセットの誤差は、 トレーニングセットのサイズを 大きくするにつれて、減少する傾向にある、 なぜならデータが多くあればある程、 新しいサンプルに対して一般化するのもうまく出来るだろうからだ。 ようするに、データを多く使えば使うほど、フィッティングで得られる仮説も良くなっていくはず。 だからJ trainとJ cvを プロットすると、こんな感じの物が得られるはず。 ではここで、もし高バイアスだったり 高バリアンスだったりといった 問題を被った時に、 どうなるか見てみよう。 仮説が高バイアスだったとしよう、 それを説明する為に、 例として、あまり 直線ではうまくフィット出来ないようなデータに対し 直線をフィッティングさせる場合を 考えてみよう。 すると仮説は、こんな感じになる。 ここでトレーニングセットのサイズを 大きくしていくと、 何が起こるか考えてみよう。 つまりここに書いた5つだけの 手本の代わりに、 もっとたくさんのトレーニングの手本があると想像してみよう。 うーん、これに直線をフィッティングしたら、どうなるだろう？ 結局の所得られる結果は ほとんど同じような直線だろう。 つまり、このデータに うまくフィットするのが不可能な直線は さらに大量のデータを増やしても その直線はたいして変わらない、という事。 これがこのデータにもっとも 適合する直線だ。それでも、 この直線はこのデータセットに そんなに良くフィットさせる事は出来ない。 もしクロスバリデーション誤差をプロットしたら こんな感じの結果となるだろう。 グラフの左側では、トレーニングセットをすごく小さく、 例えば1つのトレーニング手本に制限した場合で、そんなに良くは無いだろう。 だがある程度の数の トレーニング手本の数に到達する頃には、 ほとんど確実に、 可能な範囲でベストな直線を得る事になる。 そしてそこからさらに トレーニングセットのサイズを 増やしたところで、 つまりよりmの値を大きくした所で、 基本的には同じ直線を得る事になる。 だからクロスバリデーション誤差は、 ラベルをつけておこう、 またはテストセットの誤差は 極めてすぐに 平坦になってしまう。一旦ある一定の トレーニング手本の数に 到達した後には。 その頃には可能な範囲でかなりベストに近い直線を得たことになる。 ではトレーニング誤差はどうだろう？ トレーニング誤差は今回も小さい所から始まるが、 高バイアスの場合、 トレーニング誤差は 最終的には クロスバリデーション誤差に 近くなる。何故なら あまりにもちょっとのパラメータしか無く データはたくさんある、、、少なくともmが大きい所ではそうなので、 トレーニングセットとクロスバリデーションセットの 誤差は似たりよったりとなる。 だからこんな感じの学習曲線が 得られる事になる。 もし高バイアスのアルゴリズムの場合には。 最後になるが、高バイアスの問題は クロスバリデーション誤差と トレーニング誤差が 両方とも高い、という形で あらわれる。 つまり最終的に 比較的高い J cvとJ trainの値に落ち着く。 これはまた、とても興味深い事を 示唆している。 それはもし学習アルゴリズムが高バイアスだと、 もっとトレーニング手本を 増やしていったとしても、 つまりこの図の右側に 移動していっても、 クロスバリデーション誤差は たいして下がらない事が分かる。 それは高い所でだいたい水平になってしまってる。 だから学習アルゴリズムが実際に 高バイアスの問題を被ってる時は トレーニングデータを増やす事それ自体は そんなには助けにならないだろう。 この図によれば、 この右側の図の例では ここでは5つのトレーニング手本しか無い。 そして何らかの直線をフィットさせてる。 そしてそこに大量のトレーニングデータを 追加しても、 ほとんど同じ直線のまま。 だから学習アルゴリズムが高バイアスな所に もっとたくさんのトレーニングデータを追加しても、 テストセット誤差や クロスバリデーション誤差を たいして低下させない。 だからあなたの学習アルゴリズムが 高バイアスの問題を被っているかは 知ると便利な事だと思う、 何故ならそれを知る事で、 役に立たない所で たくさんのトレーニングデータを集めてしまうという 無駄を避ける事が出来るから。 次に学習アルゴリズムが 高バリアンスの 場合を見てみよう。 とても小さなトレーニングセット、 右の図だと5つしか無いような場合で とても高次の多項式、 ここでは100次の多項式を 描いた。それは誰も使わないような物だが、例示の為に。 この高次の多項式でフィッティングしたら トレーニング誤差が どうなるかを 見てみよう。 そしてとても小さな ラムダの値、 0では無いがとても小さな値を 使えば、 最終的には このデータにすこぶる良くフィットする事が出来て、 ようするにそれはオーバーフィットする事になる。 すると、トレーニングセットのサイズが 小さい時には トレーニング誤差、つまりJ train のシータは 小さくなるだろう。 そしてトレーニングセットのサイズを少し増やしても たぶんまだこのデータに オーバーフィットしたままだが でも多少は データセットに完全にフィットさせるのは 難しくなる。 だからトレーニングセットのサイズを 増加させると、J trainも 増えていくのが見られるだろう、 何故ならトレーニングセットに 完全にフィットさせるのはちょっと難しくなるだろうから、 もっとトレーニング手本が増えると。
でも増えると言ってもトレーニングセット誤差はまだ極めて低いままだろう。 さて、クロスバリデーション誤差はどうなるだろう？ 高バリアンスの設定では 仮説はオーバーフィットしているのだから クロスバリデーション誤差は 高いままに留まる、 トレーニング手本の数を ある程度 増やしたとしても。 だから、クロスバリデーション誤差は こんな感じとなる。 高バリアンスの問題が 起こってる時に特徴的な診断ポイントとしては トレーニング誤差と クロスバリデーション誤差との間に とても大きなギャップがある、というもの。 そしてこの図を見ると、 もっとトレーニングデータを 追加する事を考えると この図を外挿して 右に伸ばすと、 この2つのカーブは 青いカーブと マゼンダのカーブは、同じ所に収束していくだろう事が分かる。 つまり、仮にこの図を 右へと外挿し続けていくと、 たぶん、 トレーニング誤差は 増加し続け、 そしてクロスバリデーション誤差は 減少し続ける。 そして我らが本当に問題としてるのはクロスバリデーション誤差か テストセット誤差でしょ？ だからこの種の図では、 トレーニング手本を さらに追加していく事で 右に外挿してく事で、 クロスバリデーション誤差は 減少していく事が分かる。 だから高バリアンスの 状況では、 トレーニングデータをさらに増やす事は 実際に状況を改善する。 だからこの場合もまた、 学習アルゴリズムが 高バリアンスの問題を被っているかを 知るのは有益な事だと思う、 何故ならそれを知る事で例えば もっと多くのデータを取りに行く 価値があるかが分かるからだ。 前のスライドとこのスライドでは かなりクリーンで理想化された カーブを描いた。 実際の学習アルゴリズムに対して これらの曲線をプロットしたら、 時には私が描いたのに とても似たカーブを見る事もあると思うが、 時にはもうちょっとノイズが入った、 もっととっちらかったようなカーブを 見る事もあるだろう。 だがこれらのような学習曲線を プロットする事はしばしば あなたの学習アルゴリズムが バイアスの問題を被ってるか バリアンスの問題を被ってるか、またはその両方がちょっとずつ混ざってるかを知る助けとなる。 だから学習アルゴリズムの パフォーマンスを改善しようと 試みる時に、 ほぼ必ずやる事としては、 これらの学習曲線を プロットする、というのがある。 そしてだいたいは、バイアスかバリアンスの問題があるかについて、より良い感覚を与えてくれる。 次のビデオでは、 これがどのように、次に取るべきアクション、 または取るべきでないアクションを考える助けとなるかを 見ていく事にする。 学習アルゴリズムのパフォーマンスを改善しようとする時に。