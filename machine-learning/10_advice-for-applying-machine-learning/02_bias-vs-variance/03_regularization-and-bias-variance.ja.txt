正規化がどうオーバーフィットを 防止するかは、これまで見てきた。 でもそれは、どんな風に 学習アルゴリズムのバイアスと分散に影響を与えるか？ このビデオでは、 バイアスと分散の問題について より深く見ていきたい。 そして学習アルゴリズムの 正規化とどう相互作用し、 影響を与えるか議論していきたい。 とても高い次数の多項式を 線形回帰で フィッティングしたとする。 だがオーバーフィットを防ぐ為に、 ここにあるような正規化を行うとする。 このような高次の多項式を フィッティングするが、 そこでオーバーフィットを 避けるために ここに示したような正規化を行うとする。 つまりこの正規化の項で パラメータを値を 小さく保とうとするという訳。 そしていつも通り、正規化の和は jが1からmまで取り、 jが0からmでは無い。 では3つのケースを考えてみよう。 最初のケースは 正規化パラメータのラムダの値が とても大きな場合。 例えば ラムダ=10,000 とか 巨大な値の時。 この場合、 これらのパラメータ全て シータ1、シータ2、シータ3などなど、が、 とても重く ペナルティを課される。 すると、これらのパラメータの値の ほとんどがゼロになり、 結果として、仮説はだいたい h(x)は単にだいたい 近似としてはイコール シータ0となる。 だから仮説は多かれ少なかれ そんな感じに見える。 これは多かれ少なかれフラットで、定数の直線。 つまり仮説は高バイアスで、 これらのデータセットに対してアンダーフィットしている。 だから水平の直線は 単にこれらのデータセットの とても良い仮説という訳では無い。 反対側の極端では、 とても小さい値ラムダの時、 たとえばラムダが イコール0の時。 その場合、高次の多項式で フィッティングしているなら、 これは良くある オーバーフィットの状況だ。 その場合、高次の多項式で フィッティングしていて、 基本的には正規化無しか とても少ない正規化しかしないとすると、 通常の高分散と、オーバーフィットの状況となる。 何故ならもしラムダがイコール0なら それは単に 正規化無しでフィッティングしているのだから、 仮説はオーバーフィットする事となる。 そしてもしラムダの値を中間に 大きすぎず、小さすぎずにパラメータを設定出来た時だけ、 データに対して リーズナブルにフィッティング出来る。 では、どうやったら正規化パラメータのラムダの 良い値を自動的に 選ぶ事が出来るだろうか？ ここに、我らのモデル、学習アルゴリズムの目的関数を再掲しておく。 正規化を使っている状況として、 Jtrainのシータを最適化の目的関数とは 分けて定義しよう。 正規化項を抜いて定義する。 以前は、前のビデオで 正規化をしていない時は JtrainのシータをJのシータ、つまり コスト関数と同じ物として定義していた。 だが正規化を用いるべく追加のラムダの項を足したら、 Jtrain、つまりトレーニングセットの誤差を トレーニングセットの 誤差の二乗の和だけで定義する、 トレーニングセットの 平均の誤差の二乗で 正規化項を考慮に入れないで定義するのだ。 そして同様に クロスバリデーションセット誤差とテストセット誤差も 以前と同様に定義する、つまり クロスバリデーションセットとテストセットの 誤差の二乗の和の 平均。 まとめると、 JtrainとJcvと Jtestの定義は 単なる誤差の二乗の 平均、、、じゃなくて トレーニングセットとバリデーションセットと テストセットの誤差の二乗の平均で 追加の正規化項が無しの物と 定義する。 そしてこんな風に正規化パラメータのラムダを自動で選ぶ。 普通やるのは、試したいラムダの値の範囲を あらかじめ持っておく。 例えば正規化しない、 というのもあるかもしれないし、 幾つかの試してみたい値も入れてあるかもしれない。 例えばラムダを 0.01、0.02、0.04、、、などに渡って構築するかもしれない。 そして見ての通り、普通私は、 これらのステップを 2倍づつで進めて、ある程度の大きい数字までやる。 これを二の倍数とすると、 実際は10.24だ。 ぴったり10じゃないけど だいたい同じだ。 桁数で三桁目とか四桁目の 数は結果にはそんなに影響を与えない。 これは、えーと、 12個の異なるモデルを与える。 私が試す事になるモデルの選択肢を、 12個の異なる 正規化パラメータに対応した。 もちろん、0.01以下の値とか 10より大きい値を試したって いい。 でもここでは簡単のためにこんだけで切った。 これらの12のモデルを所与とすると 我らが行うのは 以下の事だ: この最初のモデル ラムダ=0を取り、 コスト関数Jのシータを 最小化する。 この結果、あるパラメータベクトルのシータが得られ 前回のビデオと同様に これをシータの上付き添字1で 示す事にしよう。 そして次に、二番目のモデル、 ラムダを0.01として取り、 コスト関数を 最小化する。 今回は当然 ラムダ=0.01を使って さっきとは別のパラメータベクトル、シータを得る。 それをシータ2と記す。 次は三番目のモデルから シータ3を 得て、 それを以下同様に 最後のモデルである ラムダ=10 まで、 または10.24までやり、結局このシータ12を得る。 次に、これらの仮説全て、 これら全てのパラメータを取り、 クロスバリデーションセットを それらを評価する為に使う。 最初のモデル、 二番目のモデルを見て、 これら異なる正規化パラメータの値に対し フィッティングして、 それらをクロスバリデーションセットに対して 評価する。
基本的にはこれらのパラメータ毎に、誤差の二乗の平均を クロスバリデーションセットに対して測る。 そしてこれら12個のモデルから 一番低いクロスバリデーションセット誤差が得られる物を 選びだす。 そして、ここで話を進める為に 結果として シータ5、つまり5次の多項式の モデルを選んだとしよう。 何故ならこれが一番クロスバリデーションの誤差が小さかったから、とする。 それを終えて、最後に テストセットの誤差を レポートしたければ、やることは、 パラメータのシータ5、 これは私の選んだ物だが、 それを取って、それが テストセットに対してとれくらい良いかを見る。 ここでも、このパラメータの シータはクロスバリデーションセットに対して フィッティングしたのだから、 テストセットはそれとは別に とっておいて、 初めて見る手本に対して どれくらいうまく一般化出来ているかを 見積もるのを よりうまくやる為に 使うのです。 以上がモデル選択を 正規化パラメータのラムダを選ぶのに 適用した場合です。 このビデオで最後にやりたい事は、 正規化パラメータのラムダを 変化させていくと、 クロスバリデーションとトレーニングの誤差が どう変化していくかを より良く理解する事です。 備忘録として、 これが元のコスト関数、Jのシータでした。 だが今回の目的では、 正規化パラメータ無しで トレーニング誤差と クロスバリデーション誤差を 定義する、 正規化パラメータ無しで。 そしてこのJtrainと Jcvをプロットしたい。 意図する所は、 仮説がどれくらい トレーニングセットと クロスバリデーションセットに対して 良いか、が、 正規化パラメータのラムダを変化させていくと どう変わっていくかを見ていきたい。 で、以前に見たように、 ラムダが小さい時は、 あまり正規化をしない、という事なので よりオーバーフィットの危険性にさらされる。 他方、ラムダが大きくなると、 つまり、この横軸の 右側の部分だと、 その時は より大きなラムダとなるので、 バイアスの問題に遭遇するリスクが上がる。 という訳でJtrainとJcvを プロットすると、以下の事に気付く。 小さなラムダの値では 相対的にはトレーニングセットには 良くフィット出来る、何故なら あまり正規化しないという事だから。 だから小さなラムダの値では 正規化の項は基本的には どっかに行ってしまうような物で、 単に誤差の二乗を最小化しているのに、極めて近い事をしている。 だからラムダが小さい時は 結果としてはJtrainは小さくなるが、 他方ラムダが大きいと、 高バイアス問題となり トレーニングセットにはうまくフィットしなくなる。 だから値は結局、上昇する。 つまり、Jtrainのシータは ラムダを増加させると それに連れて増加する傾向にある。 というのは、ラムダの大きな値は 高バイアスに対応していて、 そこではトレーニングセットに対してすら うまくフィット出来ないだろう。 他方で小さな値のラムダは 考えてみれば分かるが 自由にとても高い次数の多項式でデータにフィッティング出来る。 クロスバリデーション誤差については、結局こんな図となる。 この右側は、 ラムダの値が 大きい時は 結果としてはアンダーフィットしがち。 だからこれはバイアスのレジームとなる。 えーと、だから、クロスバリデーション誤差は 高くなり ちょっとラベルをつけておこう、 これはJcvのシータで、 高バイアスではフィッティングしない、、、 クロスバリデーションセットに対しては良く無いだろう。 他方、ここ、左の方では、これは高分散のレジームで、 そこではラムダの値が小さすぎて、 だからデータにオーバーフィットしている 可能性がある。 だからデータにオーバーフィットしている事により、 クロスバリデーション誤差も 高くなるだろう。 以上で、これがクロスバリデーション誤差と トレーニング誤差が どんな見た目になるか、だ。 パラメータのラムダを 変更していった時に。 正規化パラメータのラムダを変更していった時に。 繰り返しておくと、 しばしば、ある中間のラムダの値が いわゆる「ちょうどぴったし」の またはクロスバリデーション誤差か テストセット誤差が どれだけ小さいかという観点で最良となる。 ところでここで描いた曲線は いくらか漫画的というか、理想化された物だ。 だから実際のデータセットにおいては、 得られるプロットの結果は もうちょっとごちゃごちやしていて、 もっとノイジーかもしれない。 データセットによっては、 あまりトレンドらしき物が 分からない事もある。 全体やクロスバリデーション誤差の プロットを見る事で 人力で、または 自動でクロスバリデーション誤差を 最小化する点を 選び、 そして低いクロスバリデーション誤差に対応する ラムダの値を選ぶ。 学習アルゴリズムの 正規化パラメータラムダを選ぶ時は このような図を プロットすることは 何が起きているのか 理解しやすくしてくれて、 実際に正しい正規化パラメータの値を 選んでいる、という事を 確認しやすくしてくれる事が多い。 以上で、正規化と その学習アルゴリズムのバイアスや分散への 影響に関する洞察を 深めてくれてるといいな。 今や、バイアスと分散を たくさんの異なる視点から見てきた事になる。 次のビデオでやりたい事としては、 ここまで見てきた たくさんの洞察を組み合わせて その上に学習曲線と言われる 診断を 作り上げたい、 それは学習アルゴリズムが バイアス問題にあっているか、 分散問題にあっているか、 またはその両方かを診断する為に 私が良く使う ツールです。