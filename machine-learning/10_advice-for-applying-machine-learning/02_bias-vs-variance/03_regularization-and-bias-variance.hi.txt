अपने देखा है कि कैसे रेगुलराइज़ेशन सहायता कर सकता है ओवर-फ़िटिंग से बचने में. लेकिन यह कैसे प्रभावित करता है बाइयस और वेरीयन्स को एक लर्निंग अल्गोरिद्म के? इस वीडियो में मैं जाना चाहूँगा गहराई में इस बाइयस और वेरीयन्स के मुद्दे में और बताऊँगा कि यह कैसे प्रभावित करता है और कैसे प्रभावित होता है रेगुलराइज़ेशन से आपके लर्निंग अल्गोरिद्म के. मान लो हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल, जैसे दिखाया है यहाँ, लेकिन ओवरफ़िटिंग से बचने के लिए हमें आवश्यकता है इस्तेमाल करने की रेगुलराइज़ेशन, जैसे दिखाया है यहाँ. तो हमारे पास है यह रेगुलराइज़ेशन टर्म रखने के लिए पेरमिटर्स की वैल्यूज़ को कम. और हमेशा की तरह, रेगुलराइज़ेशन आता है j=1 से m, बजाय j =0 से m तक से. चलो लेते हैं तीन केस. पहला केस है बहुत बड़ी वैल्यू रेगुलराइज़ेशन पेरामिटर लैम्डा की, जैसे कि लैम्डा होता बराबर 10,000 के. कुछ बहुत बड़ी वैल्यू. इस केस में, सारे ये पेरमिटर्स, थीटा 1, थीटा 2, थीटा 3 तथा इसी प्रकार आगे होंगे बहुत अधिक प्रभावित होंगे और इसलिए हमें मिलेंगे इनमें से बहुत से पेरमिटर्स ज़ीरो के नज़दीक. और हायपॉथिसस होगी लगभग h ऑफ़ x, सिर्फ़ बराबर या लगभग बराबर थीटा ज़ीरो के. तब हमें मिलती है एक हायपॉथिसस जो लगभग ऐसी दिखती है, जैसे एक समतल, कॉन्स्टंट सीधी लाइन. और इसलिए इस हायपॉथिसस में हाई बाइयस और यह बुरी तरह से अंडरफ़िट होती है इस डेटा सेट को, तो हॉरिज़ॉंटल सीधी लाइन नहीं है एक अच्छा माडल इस डेटा सेट के लिए. और दूसरा छोर है कि यदि हमारे पास है एक बहुत छोटी वैल्यू लैम्डा की, जैसे कि लैम्डा होता बराबर ज़ीरो के. उस केस में, दिया होने पर कि हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल, यह है एक सामान्य ओवर-फ़िटिंग की सेटिंग. उस केस में, दिया होने पर कि हम फ़िट कर रहे हैं एक बड़ी डिग्री का पालिनोमीयल, मूल रूप से, बिना रेगुलराइज़ेशन के या बहुत कम रेगुलराइज़ेशन से, हमें मिलता हमारा हमेशा की तरह हाई-वेरीयन्स, ओवर-फ़िटिंग सेटिंग. यह है मूलत: यदि हमारा लैम्डा है बराबर ज़ीरो, हम सिर्फ़ फ़िट कर रहे हैं बिना रेगुलराइज़ेशन के, तो वह ओवर-फ़िट होता है हायपॉथिसस को. और यह केवल तब जब हमारे पास कुछ मध्यम स्तर की वैल्यू लैम्डा की जो है न ज़्यादा बड़ी न ज़्यादा छोटी कि हमें मिलते हैं पेरामिटर थीटा जो देते हैं हमें एक उचित फ़िट इस डेटा को. तो, कैसे हम अपने आप चुन सकते हैं एक उचित वेल्युु रेगुलराइज़ेशन पेरामिटर की? सिर्फ़ दोहराने के लिए, यहाँ है हमारा मॉडल, और यहाँ है हमारा लर्निंग अल्गोरिद्म का अब्जेक्टिव. उस सेटिंग में जब हम इस्तेमाल कर रहे हैं रेगुलराइज़ेशन, मैं परिभाषित करता हूँ J ट्रेन (थीटा) कुछ अलग ढंग से, कि वह हमारा ऑप्टिमायज़ेशन अब्जेक्टिव हो, लेकिन बिना रेगुलराइज़ेशन टर्म के. पहले, एक पिछले वीडियो में, जब हम नहीं इस्तेमाल कर रहे थे रेगुलराइज़ेशन तब मैंने परिभाषित किया था J ट्रेन ऑफ़ थीटा समान J ऑफ़ थीटा के जो था कॉस्ट फ़ंक्शन लेकिन जब हम ले रहे हैं रेगुलराइज़ेशन हम परिभाषित करेंगे J ट्रेन मेरे ट्रेनिंग सेट का सिर्फ़ सम स्क्वेर्ड एरर का ट्रेनिंग सेट पर या मेरा औसत स्क्वेर्ड एरर ट्रेनिंग सेट पर बिना ध्यान दिए रेगुलराइज़ेशन पर. और इसी तरह मैं तब परिभाषित करूँगा क्रॉस वैलिडेशन सेट एरर और टेस्ट सेट एरर पहले जैसे औसत सम स्क्वेर्ड एरर का क्रॉस वैलिडेशन सेट और टेस्ट सेट पर इसलिए सारांश में मेरी परिभाषाएँ J ट्रेन J CV और J टेस्ट की हैं सिर्फ़ औसत स्क्वेर्ड एरर क्रॉस-वैलिडेशन सेट पर और दूसरी है स्क्वेर्ड एरर टेस्ट सेट पर बिना अतिरिक्त रेगुलराइज़ेशन टर्म के. तो यह है कि कैसे हम चुन सकते हैं रेगुलराइज़ेशन पैरामीटर लैम्डा. तो मैं अक्सर क्या करता हूँ कि लेता हूँ एक रेंज लैम्डा की वैल्यूज़ की जो मैं चाहता हूँ जाँचना. तो मैं शायद नहीं सोच रहा हूँ इस्तेमाल करने की रेगुलराइज़ेशन या ये हैं कुछ वैल्यूज़ जिन पर मैं शायद विचार करूँ लैम्डा = 0.01, 0.02, 0.04 और इसी प्रकार आगे. और आमतौर पर मैं इनको सेट करता हूँ दो के गुणकों / मल्टिपल्स में, कुछ शायद बड़ी वैल्यू तक यदि मुझे करना होता 2 के गुणकों/ मल्टिपल्स में मैं समाप्त करता हूँ 10.24 पर. यह नहीं हैं पूरा 10, लेकिन काफ़ी नज़दीक है. और तीन से चार दशमलव स्थान आपके परिणाम को उतना प्रभावित नहीं करेंगे. तो, वह देता है मुझे शायद 12 भिन्न मॉडल. और मैं चुनना चाह रहा हूँ एक मॉडल 12 भिन्न वैल्यूज़ में से रेगुलराइज़ेशन पेरामिटर लैम्डा की. निश्चय ही आप ले सकते हैं एक वैल्यू 0.01 से कम या 10 से बड़ी वैल्यूज़ लेकिन मैंने सुविधा के लिए इतना ही लिया है. दिए होने पर ये 12 मॉडल, मैं तब क्या कर सकता हूँ है निम्नलिखित, हम ले सकते हैं पहला मॉडल जिसमें लैम्डा है ज़ीरो और मिनमायज़ करते हैं मेरा कॉस्ट फ़ंक्शन J ऑफ़ थीटा और यह देगा मुझे कुछ पेरामिटर थीटा वेक्टर के. और पिछले वीडियो के समान, मैं इसे डिनोट करता हूँ थीटा सूपरस्क्रिप्ट एक से. और फिर मैं ले सकता हूँ मेरा दूसरा मॉडल जिसमें लैम्डा है 0.01 और मिनमायज़ करता हूँ मेरा कॉस्ट फ़ंक्शन अब लेते हुए लैम्डा बराबर 0.01 ही. मिलेगा कुछ भिन्न पेरामिटर वेक्टर थीटा. उसे मैं डिनोट करता हूँ थीटा (2) से. और फिर मुझे मिलता है थीटा(3). जो है मेरे तीसरे मॉडल के लिए. और इसी प्रकार मेरे अंतिम मॉडल तक जिसमें लैम्डा है 10 या 10.24, और मुझे मिलता है यह थीटा (12). आगे, में ले सकता हूँ मेरी सारी हायपॉथिसस, सारे ये पेरमिटर्स और इस्तेमाल करता हूँ मेरा क्रॉस वैलिडेशन सेट इन्हें वैलिडेट/ सत्यापित करने के लिए अत: मैंने लिया मेरा पहला मॉडल, मेरा दूसरा मॉडल, जो फ़िट किए गए हैं इन भिन्न वैल्यूज़ से रेगुलराइज़ेशन पेरामिटर की, और जाँचता हूँ उन्हें मेरे क्रॉस वैलिडेशन सेट से जो निर्भर करता है औसत स्क्वेर्ड एरर पर प्रत्येक इन स्क्वेर्ड वेक्टर पेरमिटर्स थीटा की मेरे क्रॉस वैलिडेशन सेट्स पर. और फिर मैं लूँगा वह जो इन 12 मॉडल में से देता हैं मुझे सबसे कम एरर क्रॉस-वैलिडेशन सेट पर. और मान लो, इस उदाहरण के लिए, कि मैं चुनता हूँ थीटा 5, वह 5थ डिग्री पालिनोमीयल था क्योंकि उसकी सबसे कम क्रॉस वैलिडेशन एरर थी. वैसा कर लेने के बाद, अंत में मैं करूँगा यदि मुझे रिपोर्ट करना है प्रत्येक टेस्ट सेट की एरर, कि लेता पेरामिटर थीटा 5 जो मैंने चुना है, और देखता कि कितना सही यह करता है मेरे टेस्ट सेट पर. तो एक बार फिर, यहाँ है ऐसे जैसे हमने फ़िट किया है यह पेरामिटर, थीटा, मेरे क्रॉस-वैलिडेशन सेट को, यही वजह है मैं अलग रखता हूँ एक अलग टेस्ट सेट जिसका में इस्तेमाल करूँगा पाने के लिए एक बेहतर अनुमान कि कितना सही मेरा पेरामिटर वेक्टर, थीटा, जनरलाइज करेगा पहले से अनदेखे इग्ज़ाम्पल्ज़ पर. तो वह है मॉडल सिलेक्शन जो अप्लाई किया है चुनने के लिए रेगुलराइज़ेशन पेरामिटर लैम्डा. अंतिम काम जो मैं करना चाहता हूँ इस वीडियो में है कि पाना एक बेहतर समझ कि कैसे क्रॉस वैलिडेशन और ट्रेनिंग एरर घटते बढ़ते हैं रेगुलराइज़ेशन पेरामिटर लैम्डा के साथ. और सिर्फ़ एक याद दिलाने के लिए, वह था हमारा प्रारम्भिक कॉस्ट फ़ंक्शन J ऑफ़ थीटा. लेकिन इस हेतु हम परिभाषित करेंगे ट्रेनिंग एरर बिना इस्तेमाल किए रेगुलराइज़ेशन पेरामिटर, और क्रॉस वैलिडेशन एरर बिना इस्तेमाल किए रेगुलराइज़ेशन पेरामिटर. और मैं करना चाहता हूँ कि प्लॉट करूँ इस J ट्रेन को और प्लॉट करूँ इस Jcv को, देखने के लिए कैसे काम करती है मेरी हायपाथिसस ट्रेनिंग सेट पर और कितनी सही मेरी हायपॉथिसस करती हैं क्रॉस वैलिडेशन सेट पर. जैसे मैं बढ़ाता या कम करता हूँ रेगुलराइज़ेशन पेरामिटर लैम्डा. तो जैसे कि हमने पहले देखा था कि यदि लैम्डा बहुत छोटा है तब हम ज़्यादा रेगुलराइज़ नहीं कर रहे और अधिक सम्भावना है ओवर फ़िटिंग की जबकि यदि लैम्डा है बड़ा तब हम हैं जैसे दाईं तरफ़ इस हॉरिज़ॉंटल ऐक्सिस के तब, लैम्डा की बड़ी वैल्यूज़ अधिक सम्भावना हो जाती है एक बाइयस्ड समस्या की, तो यदि आप प्लॉट करते हैं J ट्रेन और J cv, आपको क्या मिलता है कि, लैम्डा की छोटी वैल्यूज़ के लिए, आप फ़िट कर सकते हैं अपेक्षाकृत आसानी से क्योंकि आप रेगुलराइज़ नहीं कर रहे हैं. अत:, लैम्डा की छोटी वैल्यूज़ के लिए, रेगुलराइज़ेशन टर्म वास्तव में न के बराबर है, और आप मिनमायज़ कर रहे हैं ये ग्रे ऐरोज़. तो जब लैम्ब्डा छोटा है, आपको मिलती है Jट्रेन की कम वैल्यू, जबकि यदि लैम्ब्डा बड़ा है तो आपको मिलती है एक हाई बाइयस की समस्या, और आप शायद न फ़िट करें आपके ट्रेनिंग सेट को उचित रूप से, आपको मिलती है एक वैल्यू ऊपर यहाँ. तो Jट्रेन बढ़ता है जब लैम्डा बढ़ता है, क्योंकि लैम्डा की एक बड़ी वैल्यू से मिलता है हाई बाइयस जहाँ आप शायद फ़िट ही न करें आपके ट्रेनिंग सेट को सही ढंग से, जबकि लैम्डा की एक छोटी वैल्यू से मिलता है, यदि फ़िट कर पाते हैं एक बहुत बड़ी डिग्री का पालिनोमीयल आपके डेटा को, मान लो. क्रॉस वैलिडेशन एरर के बाद हमें मिलता है एक चित्र इस तरह का, जहाँ यहाँ दाईं तरफ़, यदि हमारे पास है एक बड़ी वैल्यू लैम्डा की, हम शायद अंडर फ़िटिंग करेंगे और इसलिए यह बाइयस का क्षेत्र है. और इसलिए क्रॉस वैलिडेशन एरर भी ज़्यादा होगी. मैं छोड़ देता हूँ यह सब J cv(थीटा) पर क्योंकि हाई बाइयस में, हम फ़िट नही करेंगे, हम अच्छा नहीं करेंगे क्रॉस वैलिडेशन सेट्स पर, जबकि यहाँ बाईं तरफ, यह है हाई वेरीयन्स का क्षेत्र, जहाँ हमारे पास हैं बहुत कम वैल्यूज़ लैम्डा की तब हम शायद ओवर फ़िट करेंगे डेटा को. और इसलिए ओवर फ़िट करने से डेटा को, क्रॉस वैलिडेशन एरर भी ज़्यादा होगी. और इसलिए, यह है जो क्रॉस वैलिडेशन एरर है और यह है जो ट्रेनिंग एरर है जो दिखती है ट्रेनिंग सेट पर जैसे हम बढ़ाते या कम करते हैं रेगुलराइज़ेशन पेरामिटर लैम्डा. और एक बार फिर, यह अक्सर कुछ मध्यम स्तर की वैल्यू लैम्डा की होगी जो होगी उचित या जो सबसे अच्छा काम करेगी पाने के लिए एक कम क्रॉस वैलिडेशन एरर या एक कम टेस्ट सेट एरर. और जबकि जो कर्व मैंने बनाए है यहाँ वह थोड़े कार्टून जैसे हैं और और कुछ हद तक सम्पूर्ण है अत: वास्तविक डेटा पर जो कर्व आपको मिलेंगे वे दिखेंगे थोड़े टेड़े मेड़ें, सिर्फ़ थोड़े टेड़े मेड़ें इस से. कुछ डेटा सेट्स में आप वाक़ई देख पाएँगे इस तरह के ट्रेंड और देखने से प्लॉट क्रॉस-वैलिडेशन एरर का आप कर सकते हैं या तो हाथ से या प्रोग्राम से, चुनाव एक पोईँट का जो मिनमायज़ करता है क्रॉस वैलिडेशन एरर को और चुन सकते हैं लैम्डा की वैल्यू जो जुड़ी है न्यूनतम क्रॉस वैलिडेशन एरर से. जब मैं प्रयास कर रहा हूँ चुनने का रेगुलराइज़ेशन पेरामिटर लैम्डा लर्निंग अल्गोरिद्म के लिए, अक्सर मैं पाता हूँ कि प्लॉट करने से एक चित्र इस तरह का, जैसा दिखाया है यहाँ सहायता करता है मुझे समझने में कि क्या चल रहा है और सहायता करता है मुझे सत्यापित करने में कि मैंने वाक़ई में ली है एक सही वैल्यू रेगुलराइज़ेशन पेरामिटर लैम्डा की. तो उम्मीद है उससे आपको मिली होगी एक ज़्यादा समझ रेगुलराइज़ेशन की और उसके प्रभाव बाइयस और वेरीयन्स पर एक लर्निंग अल्गोरिद्म के. अब तक आपने देखा बाइयस और वेरीयन्स विभिन्न दृष्टिकोणों से. और हम क्या करना चाहते हैं अगले वीडियो में कि लें यह सब समझ जो हमने सीखा है और बनाए उससे एक डाइयग्नास्टिक जिसे कहते हैं लर्निंग कर्व्ज़, जो है एक टूल जो मैं अक्सर इस्तेमाल करता हूँ पहचानने के लिए कि क्या लर्निंग अल्गोरिद्म में हाई बाइयस की समस्या है या एक वेरीयन्स की समस्या है या थोड़ा बहुत दोनो ही.