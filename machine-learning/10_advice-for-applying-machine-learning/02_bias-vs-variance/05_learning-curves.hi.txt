इस वीडियो में मैं आप को बताना चाहता हूँ लर्निंग कर्व्ज़ के बारे में. लर्निंग कर्व्ज़ है अक्सर एक उपयोगी चीज़ प्लॉट करने के लिए. यदि आप चाहते हैं चेक करना कि आपका अल्गोरिद्म सही काम कर रहा हैं, या यदि आप चाहते हैं लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स को बेहतर करना. और लर्निंग कर्व है एक टूल जो मैं वास्तव में इस्तेमाल करता हूँ अक्सर कोशिश में पहचानने के लिए कि क्या लर्निंग अल्गोरिद्म में शायद बाइयस, या वेरीयन्स या थोड़ा बहुत दोनो की समस्या है. यहाँ है कि लर्निंग कर्व क्या है. प्लॉट करने के लिए एक लर्निंग कर्व, क्या मैं अक्सर करता हूँ कि प्लॉट करता हूँ J ट्रेन जो है, मान लो, औसत स्क्वेर्ड एरर मेरे ट्रेनिंग सेट पर या J cv जो है औसत स्क्वेर्ड एरर मेरे क्रॉस वैलिडेशन सेट पर. और मैं करूँगा प्लॉट उसे एक फ़ंक्शन की तरह m के, अर्थात् एक फ़ंक्शन ट्रेनिंग इग्ज़ाम्पल्ज़ की संख्या की तरह जो मेरे पास हैं. और इसलिए m है आमतौर पर एक स्थिरांक/ कॉन्स्टंट जैसे शायद मेरे पास हैं सिर्फ़, आप जानते हैं, एक 100 ट्रेनिंग इग्ज़ाम्पल लेकिन मैं क्या करूँगा कि कृत्रिम रूप से इस्तेमाल करूँगा कुछ ही ट्रेनिंग सेट इग्ज़ाम्पल्ज़. तो, मैं स्वेच्छा से, सीमित करूँगा इस्तेमाल करने के लिए केवल, मान लो, 10 या 20 या 30 या 40 ट्रेनिंग इग्ज़ाम्पलज़ और प्लॉट करूँगा कि क्या है ट्रेनिंग एरर और क्या है क्रॉस वैलिडेशन इस सबसे छोटे ट्रेनिंग सेट के लिए. तो चलो देखते हैं कैसे ये प्लॉट्स शायद दिखें. मान लो मेरे पास है केवल एक ट्रेनिंग इग्ज़ाम्पल उस तरह का दिखाया हुआ इस पहले उदाहरण में यहाँ और मान लो मैं फ़िट कर रहा हूँ एक क्वाड्रैटिक फ़ंक्शन. ठीक है, मेरे पास है केवल एक ट्रेनिंग इग्ज़ाम्पल. मैं कर पाऊँगा फ़िट इसे पूरी तरह से ठीक है? आप जानते हैं, सिर्फ़ फ़िट करें क्वाड्रैटिक फ़ंक्शन. मुझे मिलेगी 0 एरर एक ट्रेनिंग इग्ज़ाम्पल पर. यदि मेरे पास हैं दो ट्रेनिंग इग्ज़ाम्पल्ज़. ठीक है क्वाड्रैटिक फ़ंक्शन फ़िट हो सकता है उसे भी अच्छे से. अत:, यदि मैं इस्तेमाल कर रहा हूँ रेगुलराइज़ेशन भी, मैं शायद फ़िट कर सकता हूँ उसे भी अच्छी तरह. और यदि मैं इस्तेमाल नहीं कर रहा हूँ रेगुलराइज़ेशन, मैं तब भी फ़िट करता हूँ इसे पूरी तरह से और यदि मेरे पास हैं तीन ट्रेनिंग इग्ज़ाम्पल्ज़ फिर से, हाँ, मैं फ़िट कर सकता हूँ एक क्वाड्रैटिक फ़ंक्शन पूरी तरह से तो यदि m बराबर है 1 या m बराबर है 2 या m बराबर है 3, मेरी ट्रेनिंग एरर मेरे ट्रेनिंग सेट पर होगी 0 मान कर कि मैं नहीं इस्तेमाल कर रहा रेगुलराइज़ेशन या यह होगी थोड़ी ज़्यादा 0 से यदि मैं इस्तेमाल कर रहा हूँ रेगुलराइज़ेशन और वैसे तो यदि मेरे पास है एक बड़ा ट्रेनिंग सेट और मैं कृत्रिम रूप से सीमित साइज़ मेरे ट्रेनिंग सेट का पाने के लिए J ट्रेन. यहाँ यदि मैं सेट करता हूँ m बराबर 3, मान लो, और मैं ट्रेन करता हूँ केवल तीन इग्ज़ाम्पल्ज़ पर, तब, इस चित्र के लिए मैं मापूँगा मेरी ट्रेनिंग एरर केवल तीन इग्ज़ाम्पल्ज़ पर जो वास्तव में फ़िट करते हैं डेटा को भी और इसलिए यद्यपि मेरे पास है एक 100 इग्ज़ाम्पल्ज़, लेकिन यदि मैं प्लॉट करना चाहता हूँ क्या मेरी ट्रेनिंग एरर है तब m है 3. मैं क्या करूँगा कि मापूँगा ट्रेनिंग एरर तीन इग्ज़ाम्पल्ज़ पर जो मैंने वास्तव में फ़िट किये हैं हायपॉथिसस में. और न कि बाक़ी के इग्ज़ाम्पल्ज़ जो मैंने जानबूझ कर छोड़े हैं ट्रेनिंग प्रक्रिया से. तो सारांश में हमने क्या देखा है कि यदि ट्रेनिंग सेट का साइज़ छोटा है तब ट्रेनिंग एरर भी होगी कम ही. क्योंकि आप जानते हैं, हमारे पास है एक छोटा ट्रेनिंग सेट जो है बहुत आसान फ़िट करना ट्रेनिंग सेट को बहुत अच्छे से यहाँ तक कि बिल्कुल सही, अब, मान लो हमारे पास है m बराबर 4 उदाहरण के लिए. ठीक है तब एक क्वाड्रैटिक फ़ंक्शन किया जा सकता है शायद फ़िट इस डेटा सेट को पूरी तरह से और अगर मेरे पास है m बराबर 5 तब आप जानते हैं, शायद क्वाड्रैटिक फ़ंक्शन अभी भी फ़िट हो सकता है वहाँ इसलिए तो, जैसे मेरा ट्रेनिंग सेट बड़ा होता जाता हैं, यह कठिनतर होता जाता है सुनिश्चित करना कि मैं ढूँढ सकता हूँ एक क्वाड्रैटिक फ़ंक्शन जो जाता है मेरे सारे इग्ज़ाम्पल्ज़ से पूरी तरह. तो वास्तव में जैसे ट्रेनिंग सेट का साइज़ बढ़ता है आपको क्या मिलता है कि मेरी औसत ट्रेनिंग एरर वास्तव में बढ़ती है और इसलिए यदि आप प्लॉट करते हैं यह चित्र, आपको क्या मिलता है कि ट्रेनिंग सेट एरर जो है औसत एरर आपकी हायपॉथिसस पर बढ़ती है जैसे m बढ़ता है और सिर्फ़ दोहराने के लिए कि अनुभव क्या है कि जब m है छोटा जब आपके पास हैं बहुत कम ट्रेनिंग इग्ज़ाम्पल. यह होता है बहुत आसान फ़िट करना प्रत्येक आपके एक ट्रेनिंग इग्ज़ाम्पल को पूरी तरह से और इसलिए आपकी ट्रेनिंग एरर होगी कम जबकि जब m है बड़ा तब यह हो जाता है कठिन सारे ट्रेनिंग इग्ज़ाम्पल्ज़ का पूरी तरह फ़िट होना और इसलिए आपकी ट्रेनिंग सेट एरर हो जाती है अधिक अब, तो क्रॉस वैलिडेशन एरर कितनी है. ठीक है, क्रॉस वैलिडेशन है मेरी एरर इस क्रॉस वैलिडेशन सेट पर जो मैंने नहीं देखा है और इसलिए, आप जानते हैं, जब मेरे पास है एक बहुत छोटा ट्रेनिंग सेट, मैं नहीं कर पाऊँगा जनरलाइज़ सही ढंग से, तो अच्छा नहीं कर पाऊँगा उस पर. तो, ठीक है, यह हायपॉथिसस यहाँ नहीं दिखती एक संतोषजनक, और केवल तब जब मुझे मिलता है एक बड़ा ट्रेनिंग सेट जिसमें, आप जानते हैं, मुझे मिलने लगी है हायपॉथिसस जो शायद फ़िट होती है डेटा को थोड़ा बेहतर. तो आपकी क्रॉस-वैलिडेशन एरर और आपकी टेस्ट सेट एरर होने लगती है कम जैसे आपका ट्रेनिंग सेट का साइज़ बढ़ता है क्योंकि जितना अधिक डेटा आपके पास होगा, उतना बेहतर आप जनरलाइज़ कर पाएँगे नए इग्ज़ाम्पल्स पर. तो, जितना अधिक डेटा होगा आपके पास, उतना बेहतर आप फ़िट कर पाएँगे हायपॉथिसस. तो यदि आप प्लॉट करते हैं J ट्रेन, और Jcv यह है जिस प्रकार की चीज़ आपको मिलती है. अब, चलो देखते हैं कि कैसे दिखते हैं शायद लर्निंग कर्व्स हमारे पास या तो हाई बाइयस या हाई वेरीयन्स की समस्या है. मान लो आपकी हायपॉथिसस में है हाई बाइयस और इसे समझाने के लिए मैं करूँगा इस्तेमाल एक, सेट करुँगा एक उदाहरण, फ़िट करने का एक सीधी लाइन डेटा को जो, आप जानते है, नहीं हो वास्तव में एक सीधी लाइन में फ़िट. तब हमें मिलती है एक हायपॉथिसस जो शायद ऐसी दिखती है. चलो अब सोचते हैं कि क्या होगा यदि हमें बढ़ाना होता ट्रेनिंग सेट का साइज़. तो यदि बजाय पाँच इग्ज़ाम्पल्ज़ के जैसे मैंने बनाए हैं वहाँ, कल्पना करो कि हमारे पास हैं बहुत अधिक ट्रेनिंग इग्ज़ाम्पल्ज़. ठीक है क्या होता है, यदि आप फ़िट करते हैं एक सीधी लाइन इसमें. आप क्या पाते है, कि आपको मिलती है, आप जानते हैं, वही सीधी लाइन. मेरा मतलब है एक सीधी लाइन जो सिर्फ़ फ़िट नहीं हो सकती इस डेटा में और लेने से और अधिक डेटा, ठीक है, सीधी लाइन नहीं बदलेगी उतना. यह सबसे अधिक संभव सीधी लाइन है जो फ़िट होती है इस डेटा को, लेकिन सीधी लाइन फ़िट हो ही नहीं सकती इस डेटा सेट को उतने अच्छे से. तो, यदि आप प्लॉट करते हैं क्रॉस-वैलिडेशन एरर, वह ऐसी दिखेगी. बाईं तरफ़ है, यदि आपके पास है एक बहुत छोटा ट्रेनिंग सेट का साइज़ जैसे आप जानते है, शायद सिर्फ़ एक ट्रेनिंग इग्ज़ाम्पल और यह नही करेगा सही काम. लेकिन जब तक पहुँचते हैं एक निश्चित संख्या पर ट्रेनिंग इग्ज़ाम्पल्ज़ की, आपने लगभग फ़िट कर ली है सबसे अधिक सम्भव सीधी लाइन, और भले ही आपको मिले एक अधिक बड़ा ट्रेनिंग सेट साइज़, एक बहुत अधिक वैल्यू m की, आप जानते हैं, आपको मूल रूप से मिलेगी वही सीधी लाइन. और इसलिए, क्रॉस वैलिडेशन एरर - चलो मैं लेबल करता हूँ उसे- या टेस्ट सेट एरर समतल हो जाएगी बहुत जल्द, एक बार आप पहुँच जाते हैं एक निश्चित संख्या से परे ट्रेनिंग इग्ज़ाम्पल्ज़ की, जब तक कि आप फ़िट नहीं कर पाते सबसे अधिक संभव सीधी लाइन. और मेरी ट्रेनिंग एरर का क्या? ठीक है, ट्रेनिंग एरर फिर से कम होगी. और आप क्या पाते हैं कि हाई बाइयस के केस में ट्रेनिंग एरर होगी लगभग बराबर क्रॉस वैलिडेशन एरर के, क्योंकि आपके पास हैं इतने कम पेरामीटरज़ और इतना अधिक डेटा, कम से कम तब जब m बड़ा है. पर्फ़ॉर्मन्स ट्रेनिंग सेट पर और क्रॉस वैलिडेशन सेट पर होगी समान. और इसलिए, यह है जो आपके लर्निंग कर्व्स दिखेंगे, यदि आपके पास है एक अल्गोरिद्म जिसमें हाई बाइयस है. और अंत में, समस्या हाई बाइयस की दिखती है इस बात से कि दोनो क्रॉस-वैलिडेशन एरर और ट्रेनिंग एरर बहुत अधिक हैं, और इसलिए आपको मिलती है एक अपेक्षाकृत अधिक वैल्यू दोनो J cv और J ट्रेन की. यह सूचित भी करता है कुछ बहुत दिलचस्प, जो है कि, यदि एक अल्गोरिद्म में हाई बाइयस है, जैसे हमें मिलते हैं और अधिक ट्रेनिंग इग्ज़ाम्पल्ज़, अर्थात्, जैसे हम बढ़ते हैं दाईं तरफ़ इस चित्र में, हम करेंगे नोटिस कि क्रॉस वैलिडेशन एरर नहीं हो रही है कम, यह मूलरूप से समतल हो गई है, और इसलिए यदि लर्निंग अल्गोरिद्म में हाई बाइयस की समस्या है. लेना और अधिक ट्रेनिंग डेटा ख़ुद से वास्तव में सहायता नहीं करेगा उतनी, और जैसे कि हमारा चित्र के उदाहरण में चित्र में दाईं तरफ़, यहाँ हमारे पास थे केवल पाँच ट्रेनिंग इग्ज़ाम्पल्ज़, और हमने फ़िट की कोई सीधी लाइन. और जब हमारे पास थे बहुत अधिक ट्रेनिंग डेटा, हमें अभी भी मिलती है लगभग वही सीधी लाइन. और इसलिए यदि लर्निंग अल्गोरिद्म में है हाई बाइयस, उसे देना और अधिक ट्रेनिंग डेटा, वह वास्तव में सहायता नहीं करता आपकी कम करने में क्रॉस वैलिडेशन एरर या टेस्ट सेट एरर. तो जान लेने से कि क्या आपके लर्निंग अल्गोरिद्म में समस्या है हाई बाइयस की प्रतीत होती है एक उपयोगी चीज़ जानने के लिए क्योंकि यह बचा सकती है आपको व्यर्थ करने से बहुत सा समय इकट्ठा करने में और अधिक ट्रेनिंग डेटा जबकि वह उपयोगी हो ही नहीं सकता. आगे, चलिए देखते हैं सेटिंग एक लर्निंग अल्गोरिद्म की जिसमें हो सकता हाई वेरीयन्स. चलो सिर्फ़ देखते हैं ट्रेनिंग एरर पर लगभग वहाँ जहाँ आपके पास है एक बहुत छोटा ट्रेनिंग सेट जैसे पाँच ट्रेनिंग इग्ज़ाम्पल्ज़ दिखाए हैं चित्र में दाईं तरफ़ और यदि हम फ़िट कर रहे हैं मान लो एक बहुत बड़ी डिग्री का पालिनोमीयल, और मैंने लिखा है एक सौ डिग्री का पालिनोमीयल जो वास्तव में कोई इस्तेमाल नही करता, लेकिन सिर्फ़ उदाहरण ले लिए. अब यदि हम इस्तेमाल कर रहे हैं एक काफ़ी छोटी वैल्यू लैम्डा की, शायद ज़ीरो नही, लेकिन एक काफ़ी काफ़ी छोटी वैल्यू लैम्डा की, तब हमें मिलेगा, आप जानते हैं, यह डेटा फ़िट होते हुए बहुत अच्छे से एक फ़ंक्शन से जो ओवरफ़िट होता है इसे. तो, यदि ट्रेनिंग सेट का साइज़ है छोटा, हमारी ट्रेनिंग एरर, जो है, J ट्रेन ऑफ़ थीटा होगी कम. और जैसे यह ट्रेनिंग सेट का साइज़ बढ़ता है थोड़ा, आप जानते हैं, हम शायद अभी भी ओवरफ़िट कर रहे हैं इस डेटा को थोड़ा बहुत लेकिन यह होता जाता है थोड़ा कठिन भी फ़िट करना इस डेटा सेट को पूरी तरह से, और इसलिए, जैसे ट्रेनिंग सेट का साइज़ बढ़ता है, हम पाएँगे कि J ट्रेन बढ़ता है, क्योंकि यह है सिर्फ़ थोड़ा कठिन फ़िट करना ट्रेनिंग सेट पूरी तरह से जब हमारे पास हैं अधिक इग्ज़ाम्पल्ज़, लेकिन ट्रेनिंग सेट एरर अभी भी काफ़ी कम होगी. अब, तो क्रॉस वैलिडेशन एरर कितनी है? ठीक है, हाई वेरीयन्स की सेटिंग में, एक हायपॉथिसस है ओवरफ़िटिंग और इसलिए क्रॉस-वैलिडेशन एरर रहेगी ज़्यादा, जब हमारे पास हैं आप जानते हैं, एक मध्यम संख्या भी ट्रेनिंग इग्ज़ाम्पल्ज़ की और, इसलिए शायद, क्रॉस वैलिडेशन एरर शायद वैसे दिखेगी. और सांकेतिक लक्षण कि हमारे पास हाई वेरीयन्स की समस्या है, है कि वहाँ है यह बड़ा अंतर बीच में ट्रेनिंग एरर और क्रॉस वैलिडेशन एरर के. और देखने से इस चित्र को, यदि हम सोचते हैं लेने से और ट्रेनिंग डेटा, अर्थात्, लेने से इस चित्र को और विस्तार करने से इसका दाईं तरफ, हम एक प्रकार से बता सकते हैं कि, आप जानते हैं दो कर्व्ज़, नीला कर्व और मजेंटा कर्व, एक दूसरे के नज़दीक पहुँच रहे हैं. और इसलिए, यदि हमें करना होता विस्तार इस चित्र का दाईं तरफ़, तब ऐसा लगता सम्भावित कि ट्रेनिंग एरर बढ़ती जाएगी और क्रॉस-वैलिडेशन एरर कम होती जाएगी. जिस चीज़ की हम वास्तव में परवाह करते हैं वह है क्रॉस वैलिडेशन एरर या टेस्ट सेट एरर, ठीक है? तो इस तरह के चित्र में, हम बता सकते हैं कि यदि हम जोड़ते रहेंगे ट्रेनिंग इग्ज़ाम्पल्ज़ और विस्तार करते रहेंगे दाईं तरफ, ठीक है, हमारी क्रॉस वैलिडेशन एरर कम होती रहेगी. और, इसलिए, हाई वेरीयन्स की सेटिंग में, लेना और ट्रेनिंग डेटा है, वास्तव में, सम्भावित कि सहायता करेगा. और इसलिए फिर से, यह लगता है एक उपयोगी चीज़ जानने के लिए यदि आपके लर्निंग अल्गोरिद्म में समस्या है हाई वेरीयन्स की, क्योंकि वह बताता है आपको, उदाहरण के लिए, कि यह होगा शायद आपके प्रयास योग्य देखना कि क्या आपको मिल सकता है और अधिक ट्रेनिंग डेटा. अब पिछली स्लाइड पर और इस स्लाइड पर, मैंने बनाए हैं उचित साफ़ सटीक कर्व. यदि आप प्लॉट करते हैं कर्व एक वास्तविक लर्निंग अल्गोरिद्म के लिए, कभी-कभी आप वाक़ई देखेंगे, आप जानते हैं, काफ़ी कुछ ऐसे कर्व, जैसे मैंने बनाए हैं यहाँ. हालांकि, कभी कभी देखेंगे कर्व थोड़े बहुत इधर-उधर और थोड़े बहुत टेड़े मेड़ें इससे. लेकिन प्लॉट करने से लर्निंग कर्व्स, इस तरह के अक्सर बताते हैं आपको, अक्सर मदद कर सकते हैं आपकी समझने में कि आपके लर्निंग अल्गोरिद्म में बाइयस, या वेरीयन्स या थोड़ा बहुत दोनो की समस्या है. तो जब मैं प्रयास करता हूँ पर्फ़ॉर्मन्स को बेहतर करने की एक लर्निंग अल्गोरिद्म की, एक काम मैं लगभग हमेशा करता हूँ कि प्लॉट करता हूँ ये लर्निंग कर्व्ज़, और बहुधा यह देता है आपको एक बेहतर समझ कि क्या वहाँ एक बाइयस या वेरीयन्स की समस्या है. और अगले वीडियो में, हम देखेंगे कि कैसे यह कर सकता है मदद सुझाने में विशेष काम जो किए जा सकते हैं, या नहीं किए जा सकते, आपके लर्निंग अल्गोरिद्म की पर्फ़ॉर्मन्स को बेहतर करने के लिए.