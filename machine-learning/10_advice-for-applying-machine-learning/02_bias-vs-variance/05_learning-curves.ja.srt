1
00:00:00,090 --> 00:00:02,040
このビデオでは、学習曲線について議論したい。

2
00:00:03,310 --> 00:00:05,850
学習曲線はプロットするととても便利な物で、

3
00:00:06,710 --> 00:00:08,170
アルゴリズムがちゃんと機能している、という

4
00:00:08,430 --> 00:00:09,590
サニティチェック(簡単な正当性チェック)をしたい時や

5
00:00:10,400 --> 00:00:12,730
アルゴリズムのパフォーマンスを改善したい時に役に立つ物だ。

6
00:00:13,950 --> 00:00:15,200
そして学習曲線は

7
00:00:15,310 --> 00:00:16,410
実際私が

8
00:00:16,820 --> 00:00:17,920
バイアスやバリアンスやその混合の問題が

9
00:00:18,290 --> 00:00:20,030
起きてないかを診断したい時に

10
00:00:20,180 --> 00:00:23,220
しょっちゅう使っている物だ。

11
00:00:27,170 --> 00:00:28,070
その学習曲線とはこんな物だ。

12
00:00:28,830 --> 00:00:30,550
学習曲線をプロットするには

13
00:00:30,700 --> 00:00:31,760
普通私はJ train、つまり

14
00:00:32,210 --> 00:00:33,950
トレーニングセットの

15
00:00:35,030 --> 00:00:36,050
二乗誤差の平均をプロットするか

16
00:00:36,440 --> 00:00:39,090
またはJ cv、つまり

17
00:00:39,340 --> 00:00:41,130
クロスバリデーションセットの平均の二乗誤差をプロットする。

18
00:00:41,590 --> 00:00:42,900
そしてそのプロットは

19
00:00:43,140 --> 00:00:44,160
mの関数としてプロットする、

20
00:00:44,500 --> 00:00:46,380
つまりトレーニング手本の数の

21
00:00:47,230 --> 00:00:51,260
関数という事だ。

22
00:00:51,950 --> 00:00:53,420
普通、mは定数で、例えば100トレーニング手本とかだ。

23
00:00:53,650 --> 00:00:55,220
だがここで私は、

24
00:00:55,330 --> 00:00:57,670
人工的にトレーニングセットのサイズを

25
00:00:57,860 --> 00:00:59,280
減らす、という事をやる訳だ。

26
00:00:59,500 --> 00:01:01,460
つまり自分で、自分自身に

27
00:01:01,840 --> 00:01:03,440
10とか20とか30とか40の

28
00:01:03,660 --> 00:01:06,040
トレーニング手本だけを使う、という制限を課す訳だ。

29
00:01:06,170 --> 00:01:07,610
そしてそれらのトレーニング誤差がどうなってるか

30
00:01:07,740 --> 00:01:09,640
そしてクロスバリデーションの誤差がどうなっているかをプロットする。

31
00:01:10,040 --> 00:01:12,260
この小さいトレーニングセットに対して。

32
00:01:12,620 --> 00:01:14,090
ではそのプロットがどんな感じか、見てみよう。

33
00:01:14,270 --> 00:01:15,530
トレーニング手本が

34
00:01:15,730 --> 00:01:17,210
一つしか無いとしよう、

35
00:01:17,390 --> 00:01:18,450
こんな風に最初の手本だけ、

36
00:01:18,860 --> 00:01:19,970
そして二次関数をフィッティングしてるとしよう。

37
00:01:22,470 --> 00:01:24,490
トレーニング手本が一つだけなので

38
00:01:25,040 --> 00:01:26,100
完全にフィットさせる事が出来る。

39
00:01:26,650 --> 00:01:28,590
でしょ？二次関数をフィットさせるだけで

40
00:01:28,760 --> 00:01:30,000
一つのトレーニング手本に対しては

41
00:01:30,150 --> 00:01:32,240
誤差0に出来る。

42
00:01:32,570 --> 00:01:34,170
トレーニング手本が2つの時は、、、この場合も二次関数はよくフィットさせられる。

43
00:01:37,050 --> 00:01:38,550
正規化してても

44
00:01:38,750 --> 00:01:40,220
たぶんかなり良くフィットさせられるだろう。

45
00:01:41,080 --> 00:01:41,970
そしてもし正規化してなければ

46
00:01:42,030 --> 00:01:45,200
これに完璧にフィットさせられる。

47
00:01:45,440 --> 00:01:46,400
そしてトレーニング手本が3つの時も

48
00:01:47,260 --> 00:01:48,380
二次関数を完全に

49
00:01:48,660 --> 00:01:51,320
フィッティング出来る。

50
00:01:51,550 --> 00:01:52,590
つまり、もしm=1かm=2かm=3なら

51
00:01:54,850 --> 00:01:56,770
これらのトレーニングセットに対する

52
00:01:57,350 --> 00:01:58,870
トレーニング誤差は

53
00:01:59,110 --> 00:02:01,180
正規化してなければ0になると予想され、

54
00:02:01,220 --> 00:02:02,760
正規化してたら

55
00:02:03,150 --> 00:02:04,290
0よりちょっとだけ

56
00:02:04,560 --> 00:02:06,400
大きい値が予想される。

57
00:02:06,500 --> 00:02:07,350
ところで、

58
00:02:07,740 --> 00:02:08,980
もしたくさんのトレーニングセットがあり、

59
00:02:09,940 --> 00:02:11,040
それをJ train向けに

60
00:02:11,120 --> 00:02:13,080
トレーニングセットのサイズを制限したら、

61
00:02:13,830 --> 00:02:14,770
ここを、仮にm=3に

62
00:02:15,110 --> 00:02:16,720
してみたら、

63
00:02:17,040 --> 00:02:18,290
そして3つの手本だけでトレーニングしてみたら、

64
00:02:19,270 --> 00:02:21,030
この図で3つの手本に

65
00:02:21,110 --> 00:02:22,430
対してだけの

66
00:02:22,830 --> 00:02:24,450
実際にフィッティングしてる対象に対してだけの

67
00:02:24,550 --> 00:02:25,580
トレーニング誤差を測る事になり

68
00:02:27,150 --> 00:02:28,130
だから100個のトレーニング手本が

69
00:02:28,290 --> 00:02:31,160
ある訳だけど、m=3だけのトレーニング誤差を

70
00:02:31,430 --> 00:02:32,620
プロットしようという訳だ。

71
00:02:34,270 --> 00:02:35,200
つまり、3つの手本だけに対して

72
00:02:35,340 --> 00:02:36,660
仮説をフィッティングして

73
00:02:36,750 --> 00:02:39,870
そのトレーニング誤差を測るわけ。

74
00:02:41,290 --> 00:02:42,900
そしてその他のトレーニング手本を

75
00:02:43,010 --> 00:02:44,940
わざと学習プロセスから抜くわけ。

76
00:02:45,140 --> 00:02:46,750
まとめると、ここまで見てきた事から、

77
00:02:46,960 --> 00:02:48,460
トレーニングセットのサイズが

78
00:02:48,820 --> 00:02:50,560
小さい時は

79
00:02:50,630 --> 00:02:52,630
トレーニング誤差も小さくなる。

80
00:02:52,960 --> 00:02:53,900
何故なら、

81
00:02:53,930 --> 00:02:55,150
トレーニングセットが小さければ

82
00:02:55,350 --> 00:02:56,790
とても簡単にフィッティング出来るから。

83
00:02:56,900 --> 00:02:58,080
トレーニングセットに

84
00:02:58,720 --> 00:02:59,490
とても良くフィットさせられる、時には

85
00:02:59,790 --> 00:03:02,970
完全に一致させられる事も。

86
00:03:03,190 --> 00:03:04,460
さて、ここでm=4となると、

87
00:03:04,680 --> 00:03:06,800
二次関数はもはや

88
00:03:06,920 --> 00:03:07,900
完全にはデータセットに

89
00:03:08,100 --> 00:03:09,680
フィットさせられなくなる。

90
00:03:09,790 --> 00:03:11,350
そしてm=5となると、、、

91
00:03:11,460 --> 00:03:13,830
まぁこの位ならそこそこフィットしたままかもしれん。

92
00:03:14,090 --> 00:03:15,940
でもそこからトレーニングセットを大きくしていくと

93
00:03:16,980 --> 00:03:18,460
全てのトレーニング手本の上を

94
00:03:18,620 --> 00:03:19,860
完全に通る

95
00:03:20,060 --> 00:03:21,820
二次関数を見つけるのは

96
00:03:21,960 --> 00:03:25,460
どんどん難しくなる。

97
00:03:25,840 --> 00:03:27,300
つまりトレーニングセットのサイズを

98
00:03:27,690 --> 00:03:28,770
大きくしていくと、

99
00:03:29,300 --> 00:03:30,960
トレーニング誤差の平均は

100
00:03:31,310 --> 00:03:33,080
実際に増加していく。

101
00:03:33,500 --> 00:03:34,650
だからこの図をプロットすると

102
00:03:35,220 --> 00:03:36,860
トレーニングセット誤差は

103
00:03:37,130 --> 00:03:38,520
それは仮説の誤差の

104
00:03:38,940 --> 00:03:40,660
平均だが、それは

105
00:03:41,300 --> 00:03:44,730
mが増加するにつれて増加する。
その直感的な理解を繰り返すと、

106
00:03:45,020 --> 00:03:46,200
mが小さい時は、トレーニング手本がとても

107
00:03:46,500 --> 00:03:48,070
ちょっとしか無い時には、

108
00:03:48,350 --> 00:03:49,420
トレーニング手本を一つ一つ

109
00:03:49,790 --> 00:03:51,350
完璧に通過するようにフィッティングするのはとても簡単だ。

110
00:03:51,610 --> 00:03:52,840
だから誤差も

111
00:03:52,940 --> 00:03:54,540
小さくなるだろう。

112
00:03:54,710 --> 00:03:56,100
他方、mが大きくなると

113
00:03:56,460 --> 00:03:57,900
全てのトレーニング手本を

114
00:03:58,220 --> 00:03:59,900
完璧に通るのはどんどん難しくなる。

115
00:04:00,430 --> 00:04:01,830
だからトレーニングセットの誤差は

116
00:04:02,370 --> 00:04:05,840
より大きくなる。
ではクロスバリデーションの誤差はどうだろう？

117
00:04:06,720 --> 00:04:08,460
クロスバリデーションの誤差とは

118
00:04:08,590 --> 00:04:10,100
このクロスバリデーションセットでの

119
00:04:10,350 --> 00:04:12,660
誤差で、これはまだ見ていないデータだ。

120
00:04:12,880 --> 00:04:14,600
だからとても小さな

121
00:04:14,720 --> 00:04:15,900
トレーニングセットでは

122
00:04:16,080 --> 00:04:16,890
しっかりと一般化は出来ない。

123
00:04:17,020 --> 00:04:19,610
これだけじゃうまくやれない。

124
00:04:19,850 --> 00:04:21,220
つまりこの仮説は

125
00:04:21,620 --> 00:04:22,720
そんなにいい仮説じゃない。

126
00:04:23,020 --> 00:04:23,970
トレーニングセットをより大きくしていく事で

127
00:04:24,050 --> 00:04:25,270
初めて、、、

128
00:04:25,500 --> 00:04:26,380
初めていくらか

129
00:04:26,890 --> 00:04:28,100
データにより良くフィットするような

130
00:04:28,480 --> 00:04:30,810
仮説が得られ始めるのだ。

131
00:04:31,380 --> 00:04:32,050
つまり、クロスバリデーション誤差と

132
00:04:32,260 --> 00:04:35,650
テストセットの誤差は、

133
00:04:35,890 --> 00:04:37,160
トレーニングセットのサイズを

134
00:04:37,470 --> 00:04:39,150
大きくするにつれて、減少する傾向にある、

135
00:04:39,250 --> 00:04:40,700
なぜならデータが多くあればある程、

136
00:04:40,990 --> 00:04:43,410
新しいサンプルに対して一般化するのもうまく出来るだろうからだ。

137
00:04:44,010 --> 00:04:46,730
ようするに、データを多く使えば使うほど、フィッティングで得られる仮説も良くなっていくはず。

138
00:04:47,560 --> 00:04:48,560
だからJ trainとJ cvを

139
00:04:49,420 --> 00:04:51,670
プロットすると、こんな感じの物が得られるはず。

140
00:04:52,490 --> 00:04:53,550
ではここで、もし高バイアスだったり

141
00:04:53,770 --> 00:04:54,940
高バリアンスだったりといった

142
00:04:55,360 --> 00:04:56,550
問題を被った時に、

143
00:04:56,930 --> 00:04:58,210
どうなるか見てみよう。

144
00:04:58,920 --> 00:05:00,530
仮説が高バイアスだったとしよう、

145
00:05:00,830 --> 00:05:02,150
それを説明する為に、

146
00:05:02,370 --> 00:05:03,780
例として、あまり

147
00:05:03,940 --> 00:05:05,250
直線ではうまくフィット出来ないようなデータに対し

148
00:05:05,440 --> 00:05:06,500
直線をフィッティングさせる場合を

149
00:05:06,770 --> 00:05:08,240
考えてみよう。

150
00:05:09,540 --> 00:05:12,330
すると仮説は、こんな感じになる。

151
00:05:13,910 --> 00:05:15,450
ここでトレーニングセットのサイズを

152
00:05:15,750 --> 00:05:16,840
大きくしていくと、

153
00:05:17,470 --> 00:05:18,880
何が起こるか考えてみよう。

154
00:05:19,160 --> 00:05:20,480
つまりここに書いた5つだけの

155
00:05:20,590 --> 00:05:22,400
手本の代わりに、

156
00:05:22,570 --> 00:05:24,080
もっとたくさんのトレーニングの手本があると想像してみよう。

157
00:05:25,280 --> 00:05:27,230
うーん、これに直線をフィッティングしたら、どうなるだろう？

158
00:05:27,980 --> 00:05:29,700
結局の所得られる結果は

159
00:05:30,040 --> 00:05:31,360
ほとんど同じような直線だろう。

160
00:05:31,690 --> 00:05:32,940
つまり、このデータに

161
00:05:33,530 --> 00:05:35,110
うまくフィットするのが不可能な直線は

162
00:05:35,270 --> 00:05:37,320
さらに大量のデータを増やしても

163
00:05:37,890 --> 00:05:39,460
その直線はたいして変わらない、という事。

164
00:05:40,230 --> 00:05:41,400
これがこのデータにもっとも

165
00:05:41,840 --> 00:05:42,770
適合する直線だ。それでも、

166
00:05:42,890 --> 00:05:44,160
この直線はこのデータセットに

167
00:05:44,320 --> 00:05:45,630
そんなに良くフィットさせる事は出来ない。

168
00:05:45,870 --> 00:05:47,420
もしクロスバリデーション誤差をプロットしたら

169
00:05:49,260 --> 00:05:50,170
こんな感じの結果となるだろう。

170
00:05:51,320 --> 00:05:54,470
グラフの左側では、トレーニングセットをすごく小さく、

171
00:05:55,410 --> 00:05:57,710
例えば1つのトレーニング手本に制限した場合で、そんなに良くは無いだろう。

172
00:05:58,550 --> 00:05:59,470
だがある程度の数の

173
00:05:59,660 --> 00:06:00,760
トレーニング手本の数に到達する頃には、

174
00:06:00,940 --> 00:06:02,350
ほとんど確実に、

175
00:06:02,810 --> 00:06:04,010
可能な範囲でベストな直線を得る事になる。

176
00:06:04,200 --> 00:06:05,400
そしてそこからさらに

177
00:06:05,490 --> 00:06:06,260
トレーニングセットのサイズを

178
00:06:06,480 --> 00:06:07,790
増やしたところで、

179
00:06:07,970 --> 00:06:09,170
つまりよりmの値を大きくした所で、

180
00:06:10,010 --> 00:06:12,040
基本的には同じ直線を得る事になる。

181
00:06:12,370 --> 00:06:14,190
だからクロスバリデーション誤差は、

182
00:06:14,480 --> 00:06:15,420
ラベルをつけておこう、

183
00:06:15,650 --> 00:06:17,040
またはテストセットの誤差は

184
00:06:17,140 --> 00:06:18,660
極めてすぐに

185
00:06:18,990 --> 00:06:20,480
平坦になってしまう。一旦ある一定の

186
00:06:20,910 --> 00:06:22,920
トレーニング手本の数に

187
00:06:23,270 --> 00:06:24,700
到達した後には。

188
00:06:25,130 --> 00:06:27,480
その頃には可能な範囲でかなりベストに近い直線を得たことになる。

189
00:06:28,390 --> 00:06:29,540
ではトレーニング誤差はどうだろう？

190
00:06:30,120 --> 00:06:33,050
トレーニング誤差は今回も小さい所から始まるが、

191
00:06:34,620 --> 00:06:36,280
高バイアスの場合、

192
00:06:36,760 --> 00:06:38,080
トレーニング誤差は

193
00:06:38,210 --> 00:06:40,770
最終的には

194
00:06:41,000 --> 00:06:42,510
クロスバリデーション誤差に

195
00:06:42,830 --> 00:06:44,700
近くなる。何故なら

196
00:06:44,810 --> 00:06:46,370
あまりにもちょっとのパラメータしか無く

197
00:06:46,590 --> 00:06:48,070
データはたくさんある、、、少なくともmが大きい所ではそうなので、

198
00:06:48,900 --> 00:06:49,840
トレーニングセットとクロスバリデーションセットの

199
00:06:50,220 --> 00:06:52,500
誤差は似たりよったりとなる。

200
00:06:53,800 --> 00:06:54,750
だからこんな感じの学習曲線が

201
00:06:54,870 --> 00:06:56,460
得られる事になる。

202
00:06:56,770 --> 00:06:58,850
もし高バイアスのアルゴリズムの場合には。

203
00:07:00,220 --> 00:07:01,470
最後になるが、高バイアスの問題は

204
00:07:01,630 --> 00:07:03,260
クロスバリデーション誤差と

205
00:07:03,450 --> 00:07:04,930
トレーニング誤差が

206
00:07:05,580 --> 00:07:07,350
両方とも高い、という形で

207
00:07:07,420 --> 00:07:09,130
あらわれる。

208
00:07:09,560 --> 00:07:10,440
つまり最終的に

209
00:07:10,650 --> 00:07:12,040
比較的高い

210
00:07:12,280 --> 00:07:14,250
J cvとJ trainの値に落ち着く。

211
00:07:15,370 --> 00:07:16,820
これはまた、とても興味深い事を

212
00:07:17,120 --> 00:07:18,520
示唆している。

213
00:07:18,800 --> 00:07:19,990
それはもし学習アルゴリズムが高バイアスだと、

214
00:07:20,360 --> 00:07:22,250
もっとトレーニング手本を

215
00:07:22,390 --> 00:07:23,430
増やしていったとしても、

216
00:07:24,060 --> 00:07:25,100
つまりこの図の右側に

217
00:07:25,210 --> 00:07:26,600
移動していっても、

218
00:07:26,740 --> 00:07:27,880
クロスバリデーション誤差は

219
00:07:28,220 --> 00:07:29,430
たいして下がらない事が分かる。

220
00:07:29,740 --> 00:07:31,020
それは高い所でだいたい水平になってしまってる。

221
00:07:31,560 --> 00:07:32,820
だから学習アルゴリズムが実際に

222
00:07:32,950 --> 00:07:35,020
高バイアスの問題を被ってる時は

223
00:07:36,640 --> 00:07:38,200
トレーニングデータを増やす事それ自体は

224
00:07:38,370 --> 00:07:39,710
そんなには助けにならないだろう。

225
00:07:40,190 --> 00:07:41,580
この図によれば、

226
00:07:41,760 --> 00:07:43,120
この右側の図の例では

227
00:07:43,210 --> 00:07:45,670
ここでは5つのトレーニング手本しか無い。

228
00:07:46,060 --> 00:07:47,970
そして何らかの直線をフィットさせてる。

229
00:07:48,550 --> 00:07:49,270
そしてそこに大量のトレーニングデータを

230
00:07:49,540 --> 00:07:50,730
追加しても、

231
00:07:51,040 --> 00:07:52,710
ほとんど同じ直線のまま。

232
00:07:53,200 --> 00:07:54,290
だから学習アルゴリズムが高バイアスな所に

233
00:07:54,440 --> 00:07:57,090
もっとたくさんのトレーニングデータを追加しても、

234
00:07:57,650 --> 00:07:59,060
テストセット誤差や

235
00:07:59,830 --> 00:08:01,290
クロスバリデーション誤差を

236
00:08:01,890 --> 00:08:02,890
たいして低下させない。

237
00:08:03,730 --> 00:08:04,950
だからあなたの学習アルゴリズムが

238
00:08:05,250 --> 00:08:06,600
高バイアスの問題を被っているかは

239
00:08:06,780 --> 00:08:07,620
知ると便利な事だと思う、

240
00:08:08,100 --> 00:08:09,500
何故ならそれを知る事で、

241
00:08:09,640 --> 00:08:11,140
役に立たない所で

242
00:08:11,290 --> 00:08:12,520
たくさんのトレーニングデータを集めてしまうという

243
00:08:12,920 --> 00:08:15,440
無駄を避ける事が出来るから。

244
00:08:16,200 --> 00:08:17,070
次に学習アルゴリズムが

245
00:08:17,140 --> 00:08:18,530
高バリアンスの

246
00:08:19,470 --> 00:08:20,340
場合を見てみよう。

247
00:08:21,590 --> 00:08:22,880
とても小さなトレーニングセット、

248
00:08:23,550 --> 00:08:24,260
右の図だと5つしか無いような場合で

249
00:08:25,120 --> 00:08:26,350
とても高次の多項式、

250
00:08:26,680 --> 00:08:28,730
ここでは100次の多項式を

251
00:08:29,130 --> 00:08:30,720
描いた。それは誰も使わないような物だが、例示の為に。

252
00:08:31,150 --> 00:08:32,170
この高次の多項式でフィッティングしたら

253
00:08:32,200 --> 00:08:33,050
トレーニング誤差が

254
00:08:34,380 --> 00:08:36,530
どうなるかを

255
00:08:37,090 --> 00:08:38,750
見てみよう。

256
00:08:39,920 --> 00:08:41,460
そしてとても小さな

257
00:08:41,550 --> 00:08:43,160
ラムダの値、

258
00:08:43,800 --> 00:08:44,920
0では無いがとても小さな値を

259
00:08:45,070 --> 00:08:46,830
使えば、

260
00:08:47,040 --> 00:08:47,980
最終的には

261
00:08:48,190 --> 00:08:50,590
このデータにすこぶる良くフィットする事が出来て、

262
00:08:50,860 --> 00:08:53,390
ようするにそれはオーバーフィットする事になる。

263
00:08:54,380 --> 00:08:55,640
すると、トレーニングセットのサイズが

264
00:08:55,990 --> 00:08:57,820
小さい時には

265
00:08:58,320 --> 00:08:59,530
トレーニング誤差、つまりJ train のシータは

266
00:09:00,030 --> 00:09:01,810
小さくなるだろう。

267
00:09:03,130 --> 00:09:04,330
そしてトレーニングセットのサイズを少し増やしても

268
00:09:04,940 --> 00:09:05,870
たぶんまだこのデータに

269
00:09:06,000 --> 00:09:07,160
オーバーフィットしたままだが

270
00:09:07,330 --> 00:09:08,810
でも多少は

271
00:09:09,780 --> 00:09:11,880
データセットに完全にフィットさせるのは

272
00:09:12,020 --> 00:09:12,970
難しくなる。

273
00:09:13,940 --> 00:09:15,140
だからトレーニングセットのサイズを

274
00:09:15,350 --> 00:09:16,810
増加させると、J trainも

275
00:09:16,960 --> 00:09:19,390
増えていくのが見られるだろう、

276
00:09:19,840 --> 00:09:21,040
何故ならトレーニングセットに

277
00:09:21,260 --> 00:09:22,720
完全にフィットさせるのはちょっと難しくなるだろうから、

278
00:09:22,890 --> 00:09:25,700
もっとトレーニング手本が増えると。
でも増えると言ってもトレーニングセット誤差はまだ極めて低いままだろう。

279
00:09:26,530 --> 00:09:28,600
さて、クロスバリデーション誤差はどうなるだろう？

280
00:09:29,220 --> 00:09:30,590
高バリアンスの設定では

281
00:09:31,040 --> 00:09:32,760
仮説はオーバーフィットしているのだから

282
00:09:32,980 --> 00:09:34,190
クロスバリデーション誤差は

283
00:09:34,290 --> 00:09:35,680
高いままに留まる、

284
00:09:36,120 --> 00:09:37,650
トレーニング手本の数を

285
00:09:37,750 --> 00:09:38,930
ある程度

286
00:09:39,260 --> 00:09:40,520
増やしたとしても。

287
00:09:41,170 --> 00:09:42,950
だから、クロスバリデーション誤差は

288
00:09:43,730 --> 00:09:45,520
こんな感じとなる。

289
00:09:45,660 --> 00:09:47,720
高バリアンスの問題が

290
00:09:47,830 --> 00:09:49,200
起こってる時に特徴的な診断ポイントとしては

291
00:09:50,210 --> 00:09:51,490
トレーニング誤差と

292
00:09:51,720 --> 00:09:54,010
クロスバリデーション誤差との間に

293
00:09:54,340 --> 00:09:56,440
とても大きなギャップがある、というもの。

294
00:09:57,440 --> 00:09:58,180
そしてこの図を見ると、

295
00:09:58,720 --> 00:10:00,170
もっとトレーニングデータを

296
00:10:00,440 --> 00:10:01,810
追加する事を考えると

297
00:10:02,110 --> 00:10:03,660
この図を外挿して

298
00:10:03,790 --> 00:10:05,220
右に伸ばすと、

299
00:10:05,330 --> 00:10:06,830
この2つのカーブは

300
00:10:07,030 --> 00:10:08,120
青いカーブと

301
00:10:08,480 --> 00:10:10,480
マゼンダのカーブは、同じ所に収束していくだろう事が分かる。

302
00:10:11,420 --> 00:10:12,360
つまり、仮にこの図を

303
00:10:12,520 --> 00:10:13,840
右へと外挿し続けていくと、

304
00:10:13,980 --> 00:10:21,230
たぶん、

305
00:10:21,360 --> 00:10:23,000
トレーニング誤差は

306
00:10:23,170 --> 00:10:24,120
増加し続け、

307
00:10:24,270 --> 00:10:25,740
そしてクロスバリデーション誤差は

308
00:10:27,130 --> 00:10:29,040
減少し続ける。

309
00:10:30,000 --> 00:10:32,340
そして我らが本当に問題としてるのはクロスバリデーション誤差か

310
00:10:33,010 --> 00:10:35,150
テストセット誤差でしょ？

311
00:10:35,300 --> 00:10:36,460
だからこの種の図では、

312
00:10:36,730 --> 00:10:37,850
トレーニング手本を

313
00:10:38,230 --> 00:10:39,420
さらに追加していく事で

314
00:10:39,820 --> 00:10:40,930
右に外挿してく事で、

315
00:10:41,050 --> 00:10:42,650
クロスバリデーション誤差は

316
00:10:43,290 --> 00:10:44,610
減少していく事が分かる。

317
00:10:45,120 --> 00:10:46,090
だから高バリアンスの

318
00:10:46,330 --> 00:10:47,980
状況では、

319
00:10:48,180 --> 00:10:49,550
トレーニングデータをさらに増やす事は

320
00:10:50,170 --> 00:10:51,240
実際に状況を改善する。

321
00:10:51,520 --> 00:10:52,810
だからこの場合もまた、

322
00:10:53,060 --> 00:10:54,180
学習アルゴリズムが

323
00:10:54,330 --> 00:10:55,830
高バリアンスの問題を被っているかを

324
00:10:56,150 --> 00:10:57,460
知るのは有益な事だと思う、

325
00:10:57,810 --> 00:10:59,150
何故ならそれを知る事で例えば

326
00:10:59,220 --> 00:11:00,100
もっと多くのデータを取りに行く

327
00:11:00,680 --> 00:11:02,430
価値があるかが分かるからだ。

328
00:11:03,700 --> 00:11:04,920
前のスライドとこのスライドでは

329
00:11:05,330 --> 00:11:06,450
かなりクリーンで理想化された

330
00:11:06,970 --> 00:11:08,510
カーブを描いた。

331
00:11:08,900 --> 00:11:10,050
実際の学習アルゴリズムに対して

332
00:11:10,170 --> 00:11:11,970
これらの曲線をプロットしたら、

333
00:11:12,500 --> 00:11:13,910
時には私が描いたのに

334
00:11:14,560 --> 00:11:15,900
とても似たカーブを見る事もあると思うが、

335
00:11:16,600 --> 00:11:17,730
時にはもうちょっとノイズが入った、

336
00:11:18,150 --> 00:11:19,160
もっととっちらかったようなカーブを

337
00:11:19,230 --> 00:11:20,820
見る事もあるだろう。

338
00:11:21,090 --> 00:11:22,440
だがこれらのような学習曲線を

339
00:11:22,620 --> 00:11:23,850
プロットする事はしばしば

340
00:11:24,120 --> 00:11:25,460
あなたの学習アルゴリズムが

341
00:11:25,570 --> 00:11:26,650
バイアスの問題を被ってるか

342
00:11:26,950 --> 00:11:29,080
バリアンスの問題を被ってるか、またはその両方がちょっとずつ混ざってるかを知る助けとなる。

343
00:11:29,170 --> 00:11:31,030
だから学習アルゴリズムの

344
00:11:31,200 --> 00:11:32,700
パフォーマンスを改善しようと

345
00:11:32,760 --> 00:11:34,060
試みる時に、

346
00:11:34,260 --> 00:11:35,720
ほぼ必ずやる事としては、

347
00:11:35,960 --> 00:11:37,440
これらの学習曲線を

348
00:11:37,970 --> 00:11:39,460
プロットする、というのがある。

349
00:11:39,490 --> 00:11:41,710
そしてだいたいは、バイアスかバリアンスの問題があるかについて、より良い感覚を与えてくれる。

350
00:11:44,280 --> 00:11:45,180
次のビデオでは、

351
00:11:45,420 --> 00:11:46,440
これがどのように、次に取るべきアクション、

352
00:11:46,650 --> 00:11:48,370
または取るべきでないアクションを考える助けとなるかを

353
00:11:48,450 --> 00:11:49,580
見ていく事にする。

354
00:11:50,260 --> 00:11:53,250
学習アルゴリズムのパフォーマンスを改善しようとする時に。