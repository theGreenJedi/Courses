对于降维问题来说 目前 最流行 最常用的算法是 主成分分析法 (Principal Componet Analysis, PCA） 在这段视频中 我想首先开始讨论 PCA问题的公式描述 也就是说 我们用公式准确地精确地描述 我们想让 PCA 来做什么 假设 我们有这样的一个数据集 这个数据集含有 二维实数空间内的样本X 假设我想 对数据进行降维 从二维降到一维 也就是说 我想找到 一条直线 将数据投影到这条直线上 那怎么找到一条好的直线来投影这些数据呢？ 这样的一条直线也许是个不错的选择 你认为 这是一个不错的选择的原因是 如果你观察 投影到直线上的点的位置 我将这个点 投影到直线上 得到这个点 这点被投影到这里 这里 这里 以及这里 我们发现 每个点到它们对应的 投影到直线上的点之间的距离非常小 也就是说 这些蓝色的 线段非常的短 所以 正式的说 PCA 所做的就是 寻找一个低维的面 在这个例子中 其实是一条直线 数据投射在上面  使得 这些蓝色小线段的平方和 达到最小值 这些蓝色线段的长度 时常被叫做 投影误差 所以 PCA 所做的就是寻找 一个投影平面 对数据进行投影 使得这个能够最小化 另外 在应用PCA之前 通常的做法是 先进行均值归一化和 特征规范化 使得 特征 x1 和 x2 均值为0 数值在可比较的范围之内 在这个例子里 我已经这么做了 但是 在后面 我还将回过来讨论更多有关 PCA背景下的特征规范化和均值归一化问题 回到这个例子 对比 我刚画好的红线 这是另一条对数据进行投影的直线 这条品红色的线 如你所见 你知道 用这条品红色直线 来投影我的数据 是一个非常糟糕的方向 对吧？ 所以 如果我将数据 投影到这条品红色的直线上 像我们刚才做的那样 那么投影误差 就是这些蓝色的线段 将会很大 所以 这些点将会 移动很长一段距离 才能投影到 才能 投影到这条品红色直线上 因此 这就是为什么 PCA 主成分分析法会选择 红色的这条直线 而不是品红色的这条直线 我们正式一点地写出 PCA 问题 PCA 的目标是 如果我们将数据从二维 降到一维的话 我们将试着寻找 一个向量 向量 u(i) 属于 n 维空间中的向量 在这个例子中是二维的 我们将寻找一个对数据进行投影的方向 使得投影误差能够最小 在这个例子里 我们希望 PCA 寻找到 这个向量 我将它叫做 u(1) 所以 当我把数据投影到 我定义的这条直线 通过延长这个向量得到的直线 最后我得到非常小的 重建误差 看上去是这样的 另外 我应该指出的是 无论PCA 给出的是这个 u(1) 还是负的 u(1) 都没关系 如果它给出的是正的向量 在这个方向上 这没问题 如果给出的是相反的向量 在相反的方向上 也就是 -u(1)  用蓝色画出来 无论给的是正的 还是负的 u(1) 都没关系 因为 这两个方向都定义了 相同的红色直线 也就是我将投影的方向 这就是将 二维数据降到一维的例子 更一般的情况是 我们有 n 维的数据 想降到 k 维 在这种情况下 我们不仅仅只寻找单个的向量 来对数据进行投影 我们要找到 k 个方向 来对数据进行投影 从而最小化投影误差 这是一个例子 如果我有一些三维数据点 比如说像这样的 我想要做的是 是寻找两个向量 我将这些向量叫做... 我们用红线画出来 我要寻找两个向量 从原点延伸出来 这是 u(1) 这是第二个向量 u(2) 这两个向量一起 定义了一个平面 或者说 定义了一个二维面 就像这样 二维平面 我将把数据投影到上面 对于你们其中 熟悉线性代数的人来说 对于你们其中真的 精通线性代数的人来说 对这个正式的定义是 我们将寻找一组向量 u(1) u(2) 也许 一直到 u(k) 我们将要做的是 将数据投影到 这 k 个向量展开的线性子空间上 但是如果你不熟悉 线性代数 那就想成是 寻找 k 个方向 而不是只寻找一个方向 对数据进行投影 所以 寻找一个 k 维的平面 在这里是寻找二维的平面 如图所示 这里我们用 k 个方向来定义平面中这些点的位置 这就是为什么 对于PCA 我们要寻找 k 个向量来对数据进行投影 因此 更正式一点的说 在PCA中 我们想做的就是 寻找到这种方式 对数据进行投影 进而最小化投影距离 也就是数据点和投影后的点之间的距离 在这个三维的例子里 给定一个点 我们想将这个点 投影到二维平面上 当你完成了那个 因此投影误差就是 也就是 这点与投影到 二维平面之后的点之间的距离 因此 PCA 做的就是 寻找一条直线 或者平面 诸如此类等等 对数据进行投影 来最小化平方投影 90度的或者正交的投影误差 最后 一个我有时会被问到的问题是 PCA 和线性回归有怎么样的关系？ 因为当我解释 PCA 的时候 我有时候会以 画出这样的图 看上去有点像线性回归 但是 事实是 PCA不是线性回归 尽管看上去有一些相似 但是它们确实是两种不同的算法 如果我们做线性回归 我们做的是 看左边 我们想要 在给定某个输入特征 x 的情况下 预测某个变量 y 的数值 因此 对于线性回归 我们想做的是 拟合一条直线 来最小化 点和直线之间的平方误差 所以我们要最小化的是 这些蓝线幅值的平方 注意我画的这些 蓝色的垂直线 这是垂直距离 它是某个点 与通过假设的得到的其预测值之间的距离 与此想反 PCA要做的是 最小化这些蓝色直线的幅值 倾斜地画出来的 这实际上是最短的 直角距离 也就是点 x 跟红色直线之间的最短距离 这是一种非常不同的效果 取决于数据集 更更更一般的是 当你做 线性回归的时候 有一个 特别的变量 y 是我们将要预测的 线性回归所要做的就是 用 x 的所有的值来 预测 y 然而在 PCA 中 没有这么一个特别的或者 特殊的变量 y 是我们要预测的 我们所拥有的是 特征x1 x2 等 一直到xn 所有的这些特征都是被同样地对待 因此 它们中没有一个是特殊的 最后一个例子 如果我有三维数据 我要将这些数据 从三维降到二维 我就要找到两个方向 也就是 u(1) 和 u(2) 将数据投影到它们上面 然后我得到的是 我有3个特征 x1 x2 x3 所有的这些都是被同样地对待 这些都是被均等地对待 没有特殊的变量 y 需要被预测 因此 PCA 不是线性回归 尽管有一定程度的相似性 使得它们看上去是有关联的 但它们实际上是非常不同的算法 因此 希望你们能理解 PCA 是做什么的 它是寻找到一个低维的平面 对数据进行投影 以便 最小化投影误差的平方 最小化每个点 与投影后的对应点之间的距离的平方值 在下一段视频中 我们将开始讨论 如何真正地找到这个低维平面 来对数据进行投影 【教育无边界字幕组】翻译：柳桦 校对/审核：所罗门捷列夫