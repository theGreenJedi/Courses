Nesse vídeo gostaria de contar-lhes sobre o algoritmo de Análise dos Componentes Principais E ao final desse vídeo você saberá como implementar o ACP por conta própria e utilizá-lo para reduzir a dimensão de seus dados. Antes de aplicar o PCA, existe um passo de pré-processamento dos dados que sempre deveria ser seguido. Dados os conjuntos de troca dos exemplos, é importante que sempre se realize a normalização média e então, dependendo dos seus dados, talvez utilizar também o redimensionamento. Isso é bastante semelhante aos processos de normalização média e de redimencionamento que temos para o Aprendizado Supervisionado. Na realidade, é exatamente o mesmo procedimento, exceto pelo fato de estarmos executando-os agora para nossos dados não rotulados, do X1 até o Xm. Assim, para a normalização média primeiro calculamos a média das variáveis para então substituir cada variável X por X menos sua média fazendo com que cada variável agora tenha exatamente a média zero. As variáveis diferentes tem escalas muito diferentes. Assim, por exemplo, se x1 for do tamanho de uma casa e x2 for o número de quartos, utilizando nosso exemplo anterior, também escalamos cada variável para estarem em uma faixa de valores comparáveis. E, semelhante ao que tivemos com Aprendizado Supervisionado, tomamos x, i substitui j, a variável j para então subtrairmos a média, e dividirmos por sj, que é aqui uma medida dos valores beta da variável j. Portanto, pode ser o valor máximo menos o valor mínimo ou de forma mais comum, é o desvio padrão da variável j. Após fazer este pré-processamento de dados, este é o resultado do algoritmo PCA. Vimos no vídeo anterior que que o que o PCA faz é tentar encontrar o menor espaço multi-dimensional no qual os dados serão projetados para minimizar os erros de projeção quadrática, minimizar a soma de erros na projeção quadrática e também o tamanho do quadrado destas linhas azuis. O que queremos fazer especificamente é encontrar o vetor, u1, que define esta direção ou no caso de 2D queremos encontrar dois vetores, u1 e u2 que definem esta superfície nas quais projetamos os dados. Então, um lembrete rápido sobre o que significa reduzir as dimensões dos dados para este exemplo à esquerda, recebemos os exemplos x1 que estão em r2, e o que queremos é encontrar um conjunto de números, z1 em r que usaremos para representar nossos dados. Isto é o que a redução de 2D para 1D significa. Especificamente, ao projetar dados nesta linha vermelha aqui precisaremos de apenas um número para definir a posição dos pontos na linha. Chamarei este número de z ou z1. Este Z aqui pertence aos números reais então  este é um vetor unidimensional. Logo, z1 refere-se ao primeiro componente desta matriz um-a-um, ou vetor de uma dimensão. Precisamos apenas de um número para especificar a posição do ponto, Portanto, se este exemplo aqui fosse meu exemplo X1 talvez seja mapeado aqui e se este exemplo fosse X2 talvez ele esteja mapeado aqui. Então este ponto aqui será Z1 e este ponto aqui será Z2. De maneira similar, teríamos estes outros pontos por aqui, talvez X3, X4, X5 sejam mapeados para Z1,Z2, Z3. O que o PCA tem a fazer é que precisamos encontrar uma forma de processar duas coisas. Uma coisa é processar estes vetores u1, neste caso, u1 e u2, e a outra coisa é como processar estes números Z. Então, no exemplo à esquerda estamos reduzindo os dados de 2D para 1D. E no exemplo à direita estamos reduzindo dados de 3D, portanto,  X de i pertence a R3 para Z de i pertence a R2, que é agora bidimensional. Então o vetor z é agora 2D Este seria o z1 logo este é z2 e assim por diante. Precisamos ter uma forma de processar essa nova representação, z1 e z2 também Então, como processamos todas estas quantidades? Há uma derivação matemática e também uma prova matemática para o valor correto de U1, U2, U3, Z2 e assim por diante. Esta prova matemática é muito complicada e fora do escopo deste curso mas uma vez que você aplique essa derivação matemática o procedimento para encontrar o valor de u1, que é o que você quer, não é tão difícil, ainda que a prova matemática de que este é o valor correto esteja além do que vamos mostrar aqui. Mas, deixe-me apenas descrever o procedimento específico que você teria de implementar para processar todas estas coisas, os vetores u1, u2 e o vetor z. Aqui está o procedimento. Digamos que queremos reduzir dados de n dimensões para k dimensões. O que faremos primeiro é calcular algo que chamamos matriz de covariância. A conotação para matriz de covariância é esta letra do alfabeto grego, a letra Sigma em maiúsculo. Infelizmente, símbolo grego Sigma é exatamente igual ao símbolo de somatório. Então a letra Sigma do alfabeto grego é usada para denotar a matriz e aqui está o símbolo de somatório. Espero que nestes slides não haja ambiguidade sobre o que é Sigma para calcular matriz de covariância e o símbolo para somatório e espero que o contexto torne mais claro quando estou usando um ou outro. Como você processa esta matriz? Digamos que queremos guardá-la nesta variável chamada Sigma. O que nós precisamos fazer é calcular algo que chamamos de vetor de transformação linear da matriz sigma. E, para se fazer isto, você usa este comando u s v igual a SVD de sigma. SVD significa Decomposição de Valor Singular. Isto é um valor singular de decomposição muito mais avançado. Esta é algebra linear muito mais avançada que você realmente precisa saber. Quando sigma é igual à matriz de covariância há algumas formas de calcular estes vetores e se você for um especialista em álgebra linear e se você já ouviu falar de pico em vetores, talvez você saiba que há outra função octal chamada I que pode também ser usada para calcular a mesma coisa e acontece que a função SVD e a função I retornarão os mesmos vetores, apenas que SVD é um pouco mais estável numericamente Eu costumo usar SVD enquanto meus amigos usam a função I para fazer isto. Mas, quando você aplica isto para encontrar a matriz de covariância sigma dá no mesmo. Isto porque a matriz de covariância sempre satisfaz uma propriedade matemática chamada positivo finito simétrico. Você  só precisar saber que as funções SVD e I são diferentes e que quando aplicadas numa matriz de covariância - que sempre satisfaz esta propriedade matemática -- estas funções sempre dão o mesmo resultado. Isto é provavelmente muito mais álgebra que você precisa saber portanto não se preocupe se nada disso faça sentido. Tudo o que você precisa saber é estes são os comandos que você precisa implementar no Octave. Se você estiver implementando  em uma linguage diferente que Octave ou MATLAB você precisa encontrar a biblioteca numérica de álgebra linear capaz de processar o SVD ou decomposição de valor singular. Há muitas bibliotecas como esta em provavelmente todas liguagens de programação. Pessoas podem usar estas (bibliotecas) para encontrar a rotina que calcula matrizes u, s, e d para matriz de covariância sigma. Então, em mais esta matriz de covariância sigma será uma matriz n por n. Uma forma de ver isto é se você olhar a definição. Este é um vetor N por 1 e aqui I é o vetor transposto 1 por N. O produto destes dois será uma matriz N por N. 1 por N transposto, 1 X N portanto haverá uma matriz N por N e quando adicionamos tudo isto, ainda teremos uma matriz NxN. A saída do SVD são três matrizes, u, s e v. O que você realmente quer do SVD é a matriz u. A matriz u também é uma matriz NxN. Se você olhar para as colunas da matriz U você verá que as colunas da matriz U serão exatamente os vetores u1, u2 e assim por diante. Logo, u é uma matriz NxN. Se quisermos reduzir os dados de n dimensões para k dimensões temos apenas que pegar os primeiros vetores k. Assim temos de u1 até uk, que nos dá a direção K para onde queremos projetar os dados. O resto do procedimento da rotina de álgebra linear SVD nos dá esta matriz u. Vamos chamar essas colunas de u1-uN. Para encerrar a descrição do restante do procedimento a partir da rotina de álgebra linear SVD, temos essas matrizes, u,s, e d. Usaremos as primeiras colunas K desta matriz para termos u1-uK. A outra coisa a fazer é pegar o conjunto original de dados X, que é um RN e encontrar uma dimensão menor de representação Z, que é R K para estes dados. Então, a forma com que faremos isto é pegar primeiras colunas K da matriz U, construir esta matriz, empilhar U1, U2 e etc até U K nas colunas. Isto é basicamente pegar esta parte da matriz, as primeiras colunas K desta matriz. Então é assim uma matriz N por K. Eu darei um nome para esta matriz. Chamarei esta matriz de U subscrito 'reduzido', ou uma versão reduzida da matriz U. Usarei isto para reduzir a dimensão dos meus dados. Calcularei Z tal que Z será igual à matriz tranposta reduzida U multiplicada por x. Alternativamente, escrevemos o que esta transposta significa. Quando eu pego a transposta da matriz U, o que terei no final são estes vetores, agora em linhas. Eu tenho U1 transposta para UK transposta. Então multiplicamos por X e é assim que obtenho meu vetor Z. Para ter certeza que essas dimensões são realistas, esta matriz aqui deve ser K por N e o x aqui será N por 1 e então este produto z aqui em cima será k por 1. Logo, z é k-dimensional, que é exatamente o que queremos. É claro que estes x aqui podem ser exemplos do nosso conjunto de treinamento. Podem ser exemplos no conjunto de validação cruzada do conjunto de testes. Por exemplo, se eu quisesse pegar um exemplo de treinamento i, posso escrever isto como xi, e isto me dará ZI lá. Resumindo, aqui está o algoritmo PCA num lado do slide. Após normalização da média para garantir que cada característica possui média zero e funcionalidade escalável opcional, Você deve fazer fucionalidade escalável se suas características usam faixas de valores diferentes. Após este pré-processamento nós computamos a matriz portadora Sigma, e a propósito, se seus dados são representados com matriz como esta, se você tem dados apresentados em linhas como estas. Se você tem uma matriz X que é seu conjunto de treinamento escritos em linhas onde x1 é transposto para x1 transposto esta matriz de covariância sigma tem uma implementação vetorizada boa. Você pode implementar em Octave você pode até rodar sigma igual a um sobre m multiplicado por x, que é a matriz aqui em cima, transposta por x vezes e esta expressão simples que é a implementação vetorizada para calcular a matriz sigma. Eu não provarei isto hoje. Está é a vetorização correta. Se você quiser,  você pode testar numericamente usando Octave para ter certeza que estas duas implementações dão o mesmo resultado. Você pode provar matematicamente. De qualquer forma, esta é a implementação de vetorização correta. Podemos aplicar a rotina SVD para obter u, s e d. E então pegamos as primeiras colunas k da matriz u. Você reduz e, finalmente, isto define como iniciamos com uma característica de vetor x para terminar com essa representação com dimensões reduzidas z. E de forma similar ao k-médias, se você aplicar PCA, você aplicaria isto nos vetores X e RN. Então isto não é feito com x-0 1. Então isso é o algoritmo PCA. Uma coisa que não fiz foi fornecer uma prova matemática que este procedimento de fato é a projeção dos dados no subespaço k-dimensional na superfície K-dimensional  que realmente minimiza a projeção quadrática de erros. está fora do conteúdo coberto neste curso. Felizmente o algortimo PCA pode ser implementado com poucas linhas de código. Se você implementar isto no Octave ou no MATLAB você terá de fato um algoritmo de redução de dimensionalidade bastante efetivo. Então, este é o algoritmo PCA. Um coisa que não fiz foi fornecer uma prova matemática que u1,u2 e etc e o z e assim por diante que você terá como resultado deste procedimento é de fato as escolhas que minimizariam os erros de  projeção quadrática. Lembre-se  que dissemos que o que PCA tenta fazer é encontrar uma superfície, ou linha, no qual projetar os dados, de maneira a minimizar os erros de projeção quadrática. Eu não provei isto. A prova matemática está fora do conteúdo coberto neste curso. Felizmente o algoritmo PCA pode ser implementado com  poucas linhas de código no Octave. Se você implementá-lo ele funcionará e funcionará bem e se você implementar este algoritmo você terá um algoritmo efetivo de redução dimensional que corretamente minimiza os erros  projeção do erro quadrática.