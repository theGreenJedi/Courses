次元削減の問題において、 現在の所一番人気で、 一番良く使われている アルゴリズムは、 主成分分析(Principal Components Analysis)、またはPCAと呼ばれる物だ。 このビデオでは、 PCAの問題の定式化について 議論を開始する。 言い換えると、 PCAに何をして欲しいのか、を 詳細かつ厳密に定式化しよう。 こんなデータセットがあるとしよう。 つまり手本xがR 2にあるような データセット。 そしてデータの次元を 2次元から1次元に 削減したいとしよう。 言い換えると、私は、データを射影する先の 直線を探したい。 では、データを射影するのに良さそうな直線とは、どんな物か？ こんな感じの直線はかなり良い選択だと言えそうだ。 そしてこれが良い選択だと 思う理由は、 もし射影された点が どこに行くのかを見てみると、 つまりこの点に対して、ここに射影するとこれを得る。 この点はここに射影される、ここ、 ここ、ここ、ここ、 各点と射影された点との 距離がとても小さくなっている事に 気付くだろう。 つまり、この青い線は きわめて短い。 つまり、PCAがやる事は より低次元の平面、この場合は 実際は直線になるが、 そういう平面で 射影する先として、 これらの青い線分の 二乗和を最小化する物を 探そうと試みる事だ。 これらの青い線分の長さは 射影誤差と呼ばれる事もある。 するとPCAがやる事は それを最小化するような 射影先の平面を 探す事と言える。 ちよっと脇にそれるが、 PCAを適用する前には ます平均標準化を、 そしてフィーチャースケーリングを かけておく、そうする事で フィーチャーx1とフィーチャーx2が 平均0で、 比較可能な範囲の値を持つようにしておく。 この例に関しては既に私がやった。 だがこの件については、 PCAという文脈でのフィーチャースケーリングと 平均標準化については、後でもっと詳しく議論する事にする。 この例に戻ると、 このさっき引いた 赤の直線とは別の データを射影する直線もあり得る。 このマゼンタの直線とか。 見て分かるように、 このマゼンタの直線は データの射影先としては、よりまずい方向だ。でしょ？ つまりもしこのマゼンタの直線に データを射影すると、 他の点もこんな感じで。 そして射影誤差、つまり 青い線分は巨大になる。 つまりこれらの点は マゼンタの直線の上に 移動するには、つまり 射影するには、 大きな距離を移動しなくてはならない。 つまりPCA、Principal Component Analysis(主成分分析)は、 ここのマゼンタの直線のような物じゃなく 赤い線みたいな物を 選ぶ物だ。 PCAの問題をよりフォーマルに書こう。 PCAのゴールは 仮にデータを2次元から 1次元に削減したいとすると、 以下のようなベクトルを探す、と言える。 そのベクトルは uiと呼ぶ事にしよう、 それはR nのベクトルで、 この場合はR 2だ。 それは射影誤差を最小化するような データの射影先の方向を持つようなベクトルだ。 つまりこの例では、 PCAにはこのベクトルを探してくれる事を期待している、 それをu1と呼ぼう、 それの持つ性質は、 そのベクトルを延長して定義した 直線にデータを射影すると、 とても小さな 射影誤差となるような ベクトルだ。 データの例はこんな感じだ。 ところで、 PCAはu1を与えるか-u1を与えるかは、 重要では無い、という事は言っておこう。 だからPCAが正のベクトルで この向きの物を与えたら、それはそれで良い。
またもし、 反対方向のベクトル、反対の向きを向いた ベクトルを与えたら、つまり -u1だったとして、代わりに青で描くと、 正のu1を与えようが-u1を与えようが、 それはどっちでも良い。 何故ならこれらのベクトルはどちらも、 同じ赤い直線を 定義する物で、そこに私はデータを 射影する訳だから。 以上が、データを2次元から1次元に 削減するケースだ。 より一般的には、 N次元のデータセットを持っていて、 そしてそれをK次元へと削減したい。 その場合は、データを射影する先の 単独のベクトルを 探したいのでは無く、 データを射影するK次元を 探したい。 射影誤差を最小化するように。 例えばこんなだ。 仮に、こんな感じの 3D点の雲があったとしよう、 そこて私が見つけたいのは、ベクトル、、、 じゃなかった、ベクトルのペアを探したい。 このベクトルをどう呼ぼうか、、、 これらは赤で描く事にしよう。 私はベクトルのペアを探したい、 ここを原点として、 これがu1、 これが二番目のベクトルu2と呼ぼう。 そしてこれらを合わせて、これら二つのベクトルが、 平面を定義する。 言い換えるとそれらが2D平面を定義する。 たとえばこんな感じの2D平面、 データを射影する先の。 線形代数に通じた 視聴者の方々には、 真の線形代数マスターの 方々にとっては、 これの正式な定義は、 我らはu1, u2,...,ukまでの ベクトルを探す、 そして我らがやる事は、 このk本のベクトルが張る 線形部分空間に、 データを射影する、という事だ。 だが線形代数にあんまり 慣れていないなら、 データを射影する先として、 1方向の代わりにk方向を探す、と考えておけばよろしい。 さて、k次元の平面を探す為に、 このケースでは実際は2D平面だが、 この図に示したように、 ここでは平面上の点の位置を、 k本の方向で定義出来る。 そんな訳で、PCAにおいては、 我らはデータを射影する先となるk本のベクトルを見つけたい。 つまり、よりフォーマルに言うと、 PCAにおいては、我らがやりたい事は、 ある種の射影距離、 つまり射影先の点と射影元の点との 距離を最小化するような、 射影方法を見つけたい。 つまりこの3Dの例でも、 所与の点に対し、 この各点を2Dの平面に 射影する。 それを行う時には、 射影誤差は、 元の点と2D平面へ 射影した先の点との 距離となる。 つまりPCAがやる事とは、 データを射影する、直線なり平面なり それ以外なり、とにかく射影先で、 射影の二乗を最小化するような物を 探すという事だ、 射影とは90度、または直行する射影の誤差だ。 最後に、たまに質問される事の 一つに、PCAと線形回帰の 関係について、というのがある。 何故なら私がPCAを 説明する時に、たまにこんな図を描くので、 これが線形回帰みたいだからだろう。 実はPCAは、線形回帰では無い。 表面上はある程度似ているにもかかわらず、 これらは実は全く異なるアルゴリズムなのだ。 線形回帰をやる時というのは、 我らがやるのは、この左側で、 あるフィーチャーxを入力として ある変数yを予想しようと 試みる、という事だ。 つまり線形回帰においては、 我らがやっている事は、 点と直線との距離による 二乗誤差を最小化するような、 直線をフィッティングしているのだ。 だから我らが最小化するのは、 この青い線の二乗だ。 そして私はこれらの青い線を 垂直に書いた事に気づいただろうか。 それらは点と仮説が予測した値との 垂直な距離となっている、 一方で対称的に PCAにおいては、 これらの青い線の 大きさを最小化したい、 これは角度をつけて描いてあり、 実際は直行した最短距離で、 点とこの赤い直線との 最短距離で、 そしてこれは、 データセットによっては、 大きく異なった結果となる。 より一般的には、、、 より一般的には、線形回帰をする時には、 特別な変数yという物が 予測しようとする変数として 存在する。線形回帰とは、 Xを全て使って Yを予測しようと する物だ。 一方でPCAは、 特別な役割の変数Y、 予測しようとする変数Yは 存在しない。 その代わりに、フィーチャーのリスト x1, x2...とxnまでのフィーチャーがあって、 これらのフィーチャーは全て 等しく扱われる。 つまりどれも特別では無い。 最後の例として、 三次元のデータの場合、 そしてこれを3Dから2Dに削減したい場合、 つまり例えば 二つの方向、u1とu2を 探して、 そこにデータを射影したい。 その場合、最初にあるのは、 3つのフィーチャーx1, x2, x3で、 これらは全て同じように扱われる。 これら三つは全て対称的に扱われて、 予測をしたいと思う 特別な変数yは存在していない。 つまり、PCAは 線形回帰じゃないって事だ。 表面上はある程度 関係しているように見えるかもしれないが、 これらは実際のところ、とても異なったアルゴリズムだ。 以上で、PCAが何をしているか、 わかったかな。PCAは、 以下の条件を満たすような、 より低い次元の平面を 探す事を試みる、その条件とは この二乗射影誤差を最小化するような物、 各点と射影先の点の位置との距離の二乗を 最小化する、という条件。 次のビデオでは、データを射影する先の 低次元の平面を 実際にどうやって探すのかについて、 議論を開始する。