Nesse vídeo gostaria de contar-lhes sobre o algoritmo de Análise dos Componentes Principais E ao final desse vídeo, você saberá como implementar o ACP por conta própria e utilizá-lo para reduzir a dimensão de seus dados. Antes de aplicar o PCA, existe um passo de pré-processamento dos dados que sempre deveria ser seguido. Dados os conjuntos de troca dos exemplos, é importante que sempre se realize a normalização média e então, dependendo dos seus dados, talvez utilizar também o redimensionamento. Isso é bastante semelhante aos processos de normalização média e de redimencionamento que temos para o Aprendizado Supervisionado. Na realidade, é exatamente o mesmo procedimento, exceto pelo fato de estarmos executando-os agora para nossos dados não rotulados, do X1 até o Xm. Assim, para a normalização média primeiro calculamos a média de das variáveis para então substituir cada variável X por X menos sua média fazendo com que cada variável agora tenha exatamente a média zero As variáveis diferentes tem escalas muito diferentes. Assim, por exemplo, se x1 for do tamanho de uma casa e x2 for o número de quartos, para utilizar nosso exemplo, também escalamos cada variável para estarem em uma faixa de valores comparáveis. E, semelhante ao que tivemos com Aprendizado Supervisionado, tomaríamos x, i substitui j, a variável j para então subtrairmos a média, e dividimos por sj valor máximo menos o valor mínimo são o desvio padrão mais comum de ocorrer numa feature j; após fazer este pré-processamento de dados,
este é o resultado do algoritmo PCA Nós vimos no vídeo anterior que o PCA faz é tentar encontrar o menor espaço multi-dimensional no qual os dados serão projetados para minimizar os erros de projeção quadrática minimizar a soma de erros
na projeção quadrática e também o tamanho do quadrado destas linhas azuis O que queremos fazer especificamente é encontrar o vetor, u1, que que define esta direção ou no caso de 2D queremos queremos encontrar dois vetores, u1 e u2, que definem esta superfície nas quais projetar os dados. Então, um lembrete rápido sobre o que significa reduzir as dimensões dos dados para este exemplo à esquerda recebemos os exemplos x1 que estão em r2 E o que queremos fazer é encontrar um conjunto de números, z1 em r que usaremos para representar 
nossos dados Este é o significado de
 redução de 2D para 1D Especificamente, ao projetar dados nesta linha vermelha aqui Precisaremos de apenas um número para definir a posição dos pontos
na linha Chamarei este número de z ou z1 este Z aqui pertence aos Reais então 
este é um vetor unidimensional Logo z1 refere-se ao primeiro componente deste esta matriz um-a-um, ou vetor 
de uma dimensão Precisamos de apenas de um número para especificar a posição do ponto Então neste exemplo que foi meu exemplo X1 talvez seja mapeado aqui E se este exemplo fosse X2 talvez este exemplo esteja mapeado aqui Então este ponto aqui será Z1 e este ponto aqui será Z2 de maneira similar nós teríamos estes outros pontos por aqui, talvez X3 x3, X4, X5 sejam mapeados 
para Z1,Z2, Z3 O que o PCA tem a fazer é que precisamos encontrar uma forma de 
processar duas coisas. Uma coisa é processar estes vetores u1, neste caso, u1 e u2 E a outra coisa é como processamos 
estes Z números Então, no exemplo à esquerda estamos reduzindo os dados de 2D para 1D E no exemplo à direita estamos reduzindo dados de 3D X pertence ao conjunto R3 para Z de i pertence a R2 (2 dimensões) Então o vetor z é agora 2D Este seria o z1 logo este é z2 
e assim por diante Precisamos ter uma forma de processar essa nova representação, z1 e z2 também Então, como processamos
todas estas quantidades? Ocorre que há uma derivação matemática e também uma prova matemática para o valor correto para U1, U2, U3, Z2 e assim por diante Esta prova matemática é muito complicada e fora do escopo deste curso mas uma vez que você aplique essa
derivação matemática ocorre que o procedimento para encontrar o valor de u1 que é o que você quer não é tão difícil, ainda que a prova matemática seja este valor é o valor correto se alguém mais estiver envolvido Apenas descreverei o procedimento específico que você teria de implementar para processar todas estas coisas, os vetores u1,u2 e o vetor z. Eis o procedimento Digamos que queremos
reduzir dados de dimensões n para k dimensões O que faremos primeiro é processar o valor chamado de matriz de covariância A conotação para matriz de covariância é esta letra do alfabeto do grego, a letra Sigma em maiúsculo É um pouco infeliz mas o símbolo grego Sigma seja
exatamente igual ao símbolo de somatório Então este é a letra Sigma do alfabeto grego usada para denotar a matriz e aqui está o símbolo
de somatório Espero que nestes slides não haja ambiguidade sobre o que é Sigma para calcular matriz de covariância e o símbolo para somatório e espero que o contexto torne mais claro quando estou usando um ou outro; Como você processa esta matriz. digamos que queremos guardá-la nesta variável chamada Sigma O que nós precisamos fazer é processar algo que chamamos vetor de transformação linear da matriz E um oitavo, a forma que você faz isto, é usar este comando u s v igual s v d de sigma SVD significa Decomposição de Valor
Singular Isto é muito mais avançado Isto é muito mais avançado álgebra linear que você realmente precisa saber mas ocorre que quando sigma é igual a matriz de covariância há algumas formas de processar estes vetores e se você for um especialista em álgebra linear e se você já ouviu sobre
pico em vetores antes talvez você saiba que há outra função octal chamada I que pode também ser usada para processar
a mesma coisa e ocorre que a função SVD e a função I retornarão os mesmos vetores apenas que SVD é um pouco mais estável
numericamente Então eu tendo a usar SVD ainda que alguns amigos meus usem a função I para fazer isto; mas quando você aplica isto para
encontrar a matriz de covariância sigma dá no mesmo Porque a matriz de covariância sempre satisfaz uma propriedade matemática chamada 
positivo finito simétrico Você não precisa saber o que isto significa mas as funções SVD e I são diferentes mas quando aplicadas numa matriz de covariância - que sempre satisfaz esta propriedade matemática 
comprovadamente estas funções sempre dão o mesmo
resultado Okay isto foi provavelmente muito mais
álgebra que você precisa saber; caso nada faça
sentido não se preocupe Tudo o que você precisa saber é estes são os comandos que você
precisa implementar no Octave E se você está implementado isto numa linguagem diferente que Octave ou MATLAB então você deve encontrar a biblioteca numérica de álgebra linear capaz de processa o SVD ou decomposição de valor singular e há muitas bibliotecas como esta em provavelmente todas liguagens
de programação Pessoas pode usar estas (bibliotecas) para encontrar a rotina que processa
matrizes u s e d para matriz de covariância sigma Então para fornecer mais detalhes esta matriz de covariância sigma será uma matriz n por n e uma forma de ver isto se você olhar a definição este é um vetor n por 1 e aqui I é o vetor transposto 1 por N e o produto destes dois será uma matriz N por N 1 por N transposto, 1 X N então haverá uma matriz N por N e nós adicionamos todos estes
ainda teremos uma matriz NxN A saída do SVD são três matrizes, u, s e v E aquilo que você realmente quer do
SVD é a matriz u A matriz um também é uma matriz NxN Se você olhar as colunas da matriz U ocorre que as colunas da matriz U serão exatamente os vetores u1 u2 e assim por diante Logo, u é uma matriz NxN e se queremos reduzir os dados de n dimensões para para k dimensões então o que temos que fazer é apenas pegar os primeiros
vetores k e assim temos de u1 até uk que nos dá a direção K para onde queremos projetar os dados. O resto do procedimento a partir rotina de álgebra linear SVD nos temos esta matriz u. E vamos chamar essas colunas u1-uN Então apenas para encerrar a descrição do restante do procedimento a partir da rotina de álgebra linear SVD temos essas matrizes, u,s e d aqui está a matriz U nós usaremos as primeiras colunas K desta matriz para termos u1-uK A outra coisa que temos que fazer é encontrar uma forma de obter o conjunto original de dados X é um RN e encontrar uma dimensão menor de representação,Z que é R K para estes dados Então o que faremos é obter o jeito que faremos isto Pegar primeiras colunas K da matriz U Construir esta matriz Empilhe U1, U2 e etc até U K nas colunas Isto é basicamente pegar esta parte da matriz as primeiras colunas K desta matriz Então ela será uma matriz N por K Eu darei um nome para esta matriz Chamarei esta matriz de U, subscrito reduzido uma versão reduzida da matriz U, talvez Eu usarei isto para reduzir a dimensão
dos meus dados Processare Z tal que Z será igual a matriz tranposta reduzida U multiplicada por X. Alternativamente, escrever o que esta transposta significa. Quando eu pego a transposta da matriz U, o que terei no final são estes vetores, agora em linhas Eu tenho U1 transposta para UK transposta Então multiplicada por X e é assim que obtenho meu vetor Z Apenas para ter certeza que essas
dimensões são realistas Esta matriz aqui deve ser k por n e o x aqui será n por 1 e então este produto z aqui em cima será k por 1 Logo, z é k k-dimensional e o vetor é k-dimensional isto é exatamente o que queremos é claro que este x aqui pode ser exemplos do nosso conjunto de treinamento em nosso conjunto de 
validação cruzada exemplos do conjunto de testes e por exemplo se eu quisesse pegar um exemplo de treino i Eu posso escrever isto como xi XI e isto me dará ZI lá Então para resumit aqui está o algoritmo PCA num lado do slide Após siginificativa normalização para
garantir que cada feature possui valor zero Opcionalmente um funcionalidade escalável você deveria fazer para 
fucionalidade escalável se sua funcionalidade usa diferentes
extensão de valores Após este pré-processamento nós computamos a matriz portadora Sigma e a propósito se seus dados são representados com matriz como esta, se você tem dados apresentados em linhas como estas Se você tem uma matriz X que é seu conjunto de treino escritos em linhas onde x1 é transposto para x1 transposto esta esta matriz de covariância sigma tem
uma implementação vetorizada boa. você pode implementar em Octave sigma é igual a 1 sobre m multiplicado por X que é a matriz aqui em cima transposta por x vezes e e esta expressão simples está é a implementação vetorizada para processar matriz sigma Eu não provarei isto hoje Está é a correta vetorização, se você quiser você pode testar numericamente por você mesmo usando Octave para ter certeza que estas duas aqui e aqui estas implementações resultam igual.
Você pode provar matematicamente De qualquer forma, isto é a implementação de vetorização correta Podemos aplicar a rotina SVD para obter u, s e d e então pegamos as primeiras colunas k da matriz u você reduz e finalmente isto define como iniciamos com uma funcionalidade de vetor x para terminar com essa representação com dimensões
reduzidas z e semelhante a k significa que se você aplicar PCA então você aplicaria nos vetores X e RN Então isto não está pronto com a convenção de que X de 0 é igual a 1 Então este foi o algoritmo PCA Uma coisa que não fiz foi fornecer a prova matemática que este procedimento de fato é a projeção dos dados na de dimensões K do subespaço da superfície de dimensões K que realmente minimiza a projeção quadrática de erros A prova matemática está fora do escopo
deste curso Felizmente o algortimo PCA pode ser implementado em algumas linhas de código e se você implementar este no Octave ou no MATLAB você terá de fato dimensões Então este foi o algoritmo PCA Um coisa que não fiz foi fornecer uma prova matemática que u1,u2 e etc e o Z assim por diante você terá como resultado deste procedimento é de fato escolhas capazes de minimizar a projeção quadrática de erros Certo, lemnre que dissemos que O que PCA tenta fazer é encontrar uma superfície, ou lina no qual projetar os dados, de maneira a minimizar a projeção quadrática de erros Então eu não provei isto e a prova matemática disto está fora do escopo do curso Mas felizmente o algoritmo PCA pode ser implementado em poucas linhas de código no Octave E se você implementá-lo ele funcionará e e funcionará bem e se você implementar este algoritmo você terá um algoritmo efetivo de redução dimensional Que faz a coisa certa para minimizar a projeção do erro quadrático