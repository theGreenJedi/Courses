1
00:00:00,340 --> 00:00:01,410
En este video, me gustaría

2
00:00:01,550 --> 00:00:03,020
hablarle sobre el algoritmo de análisis

3
00:00:03,340 --> 00:00:04,570
de componente principal

4
00:00:05,600 --> 00:00:06,560
y al final de este

5
00:00:06,710 --> 00:00:09,200
video usted sabrá cómo poner en práctica el ACP

6
00:00:10,170 --> 00:00:12,540
y usarlo para reducir la dimensión de sus datos.

7
00:00:13,100 --> 00:00:14,690
Antes de aplicar el ACP,

8
00:00:14,800 --> 00:00:17,760
hay una etapa de pre-procesamiento de datos que siempre se debe hacer.

9
00:00:18,510 --> 00:00:20,220
Dados los conjuntos de entrenamiento de ejemplos m no etiquetados,

10
00:00:20,520 --> 00:00:22,290
es importante realizar

11
00:00:22,600 --> 00:00:24,070
siempre la normalización de media y

12
00:00:25,330 --> 00:00:26,140
después, dependiendo de sus datos,

13
00:00:26,840 --> 00:00:28,540
tal vez realizar una escalación de variables.

14
00:00:29,620 --> 00:00:30,950
Esto es muy similar a la

15
00:00:31,650 --> 00:00:33,250
normalización de media y al proceso de escalación de variables

16
00:00:34,080 --> 00:00:36,580
que tenemos para aprendizaje supervisado.

17
00:00:36,910 --> 00:00:38,240
De hecho, es exactamente el mismo

18
00:00:38,390 --> 00:00:40,160
procedimiento, excepto que lo estamos

19
00:00:40,310 --> 00:00:41,790
haciendo ahora en nuestros datos sin valores asignados,

20
00:00:42,930 --> 00:00:43,670
de x1 a xm.

21
00:00:44,180 --> 00:00:45,530
Entonces, para la normalización de media,

22
00:00:45,720 --> 00:00:47,080
primero computamos la media de

23
00:00:47,390 --> 00:00:49,070
cada variable y a continuación,

24
00:00:49,340 --> 00:00:50,900
reemplazamos cada variable x

25
00:00:51,150 --> 00:00:52,680
con x menos su media,

26
00:00:52,810 --> 00:00:54,120
esto hace que cada variable

27
00:00:54,520 --> 00:00:57,450
ahora tenga exactamente una media de cero.

28
00:00:58,690 --> 00:01:00,690
Las diferentes variables tienen escalas muy distintas.

29
00:01:01,540 --> 00:01:03,050
Por ejemplo, si x1

30
00:01:03,080 --> 00:01:04,060
es el tamaño de una casa, y

31
00:01:04,100 --> 00:01:05,390
x2 es el número de habitaciones,

32
00:01:05,580 --> 00:01:07,370
para usar nuestro ejemplo anterior,

33
00:01:07,480 --> 00:01:08,680
ahora escalamos también cada variable

34
00:01:09,130 --> 00:01:10,540
para tener una gama de valores comparables.

35
00:01:10,980 --> 00:01:12,490
Y así, similar a lo

36
00:01:12,680 --> 00:01:13,860
que teníamos en el aprendizaje supervisado,

37
00:01:14,060 --> 00:01:16,200
tomaríamos xi subíndice j,

38
00:01:16,680 --> 00:01:17,620
es la variable j,

39
00:01:23,250 --> 00:01:25,530
y la resto

40
00:01:25,890 --> 00:01:27,610
de la media,

41
00:01:28,370 --> 00:01:29,520
eso es lo que tenemos en la parte superior y luego dividimos entre Sj.

42
00:01:29,610 --> 00:01:30,020
Aquí, Sj es cierta medida de la gama de valores de la variable j.  Así que podría ser el valor máximo

43
00:01:30,080 --> 00:01:31,310
menos el mínimo, o más comúnmente,

44
00:01:31,890 --> 00:01:33,540
es la desviación estándar de

45
00:01:33,640 --> 00:01:35,520
la variable j. Después de haber hecho

46
00:01:36,230 --> 00:01:39,480
este tipo procesamiento de datos previo, esto es lo que hace el algoritmo ACP:

47
00:01:40,620 --> 00:01:41,630
Vimos en el video anterior

48
00:01:41,960 --> 00:01:43,050
que lo que el ACP hace es que

49
00:01:43,170 --> 00:01:44,520
trata de encontrar un subespacio

50
00:01:44,790 --> 00:01:46,080
de dimensiones inferiores sobre el cual

51
00:01:46,170 --> 00:01:47,500
proyectar los datos para

52
00:01:47,650 --> 00:01:49,780
minimizar los errores de proyección

53
00:01:50,540 --> 00:01:51,660
al cuadrado, la suma de los

54
00:01:51,740 --> 00:01:53,080
errores de proyección al cuadrado, así como el

55
00:01:53,420 --> 00:01:54,800
cuadrado de la longitud de

56
00:01:54,870 --> 00:01:56,790
estos segmentos de líneas azules y lo que queríamos

57
00:01:57,110 --> 00:01:58,510
hacer específicamente es encontrar

58
00:01:59,210 --> 00:02:02,730
un vector, u1, el cual

59
00:02:03,280 --> 00:02:04,750
especifica esa dirección o

60
00:02:05,040 --> 00:02:06,630
en el caso 2D queremos

61
00:02:06,880 --> 00:02:08,760
encontrar dos vectores, u1 y

62
00:02:10,640 --> 00:02:12,980
u2, para definir esta superficie

63
00:02:13,590 --> 00:02:14,610
sobre la cual proyectar los datos.

64
00:02:16,620 --> 00:02:17,920
Entonces, sólo como un rápido

65
00:02:18,040 --> 00:02:19,160
recordatorio de lo que significa la reducción de

66
00:02:19,730 --> 00:02:20,820
dimensión de los datos,

67
00:02:21,490 --> 00:02:22,430
para este ejemplo de la

68
00:02:22,470 --> 00:02:23,560
izquierda, se nos proporcionaron los

69
00:02:23,680 --> 00:02:26,010
ejemplos xi, que están en R2.

70
00:02:26,300 --> 00:02:28,390
Y lo que

71
00:02:28,660 --> 00:02:29,500
nos gustaría hacer, es encontrar

72
00:02:29,970 --> 00:02:32,400
un conjunto de números zi en

73
00:02:33,000 --> 00:02:34,950
"R" para representar nuestros datos.

74
00:02:36,000 --> 00:02:37,820
Así que eso es lo que significa la reducción de 2D a 1D.

75
00:02:39,020 --> 00:02:41,450
Específicamente, mediante la proyección

76
00:02:42,710 --> 00:02:44,080
de datos sobre esta línea roja,

77
00:02:44,800 --> 00:02:46,320
sólo necesitamos un número para

78
00:02:46,450 --> 00:02:48,340
especificar la posición de los puntos sobre la línea.

79
00:02:48,590 --> 00:02:49,380
Así que voy a llamar a ese número

80
00:02:50,700 --> 00:02:51,830
z o z1.

81
00:02:52,020 --> 00:02:54,850
Z aquí es un número real, por lo que es como un vector unidimensional.

82
00:02:55,380 --> 00:02:56,650
Así z1 sólo se refiere al

83
00:02:56,690 --> 00:02:58,080
primer componente de esta

84
00:02:58,280 --> 00:03:00,430
matriz de uno por uno, o a este vector unidimensional.

85
00:03:01,670 --> 00:03:03,170
Y por eso necesitamos

86
00:03:03,490 --> 00:03:05,590
un solo número para especificar la posición de un punto.

87
00:03:06,330 --> 00:03:07,940
Así que si este ejemplo

88
00:03:08,460 --> 00:03:09,510
fuera mi ejemplo x1,

89
00:03:10,610 --> 00:03:13,160
quizá se mapee  aquí

90
00:03:13,900 --> 00:03:15,450
y si este ejemplo fuera x2

91
00:03:15,680 --> 00:03:17,250
tal vez ese ejemplo se mapearía aquí.

92
00:03:17,530 --> 00:03:18,790
Y entonces este punto

93
00:03:19,060 --> 00:03:20,400
aquí será z1

94
00:03:20,840 --> 00:03:21,920
y este punto aquí será

95
00:03:22,080 --> 00:03:24,240
z2 y de manera similar, tendríamos

96
00:03:24,620 --> 00:03:26,410
esos otros puntos

97
00:03:26,840 --> 00:03:30,230
para estos, tal vez x3,

98
00:03:30,510 --> 00:03:32,550
x4, x5 se mapearían a z1, z2,z3.

99
00:03:34,360 --> 00:03:35,940
Entonces, lo que el ACP

100
00:03:36,050 --> 00:03:36,830
tiene que hacer es que tenemos que

101
00:03:36,930 --> 00:03:38,920
encontrar una manera de calcular dos cosas:

102
00:03:39,310 --> 00:03:40,710
la primera es computar estos vectores,

103
00:03:41,830 --> 00:03:44,970
u1, y en este caso u1 y u2

104
00:03:45,230 --> 00:03:46,880
y la otra es

105
00:03:47,130 --> 00:03:48,140
cómo podemos computamos estos números

106
00:03:49,360 --> 00:03:51,200
z. Así que en el

107
00:03:51,430 --> 00:03:53,910
ejemplo de la izquierda estamos reduciendo los datos de 2D a 1D.

108
00:03:55,290 --> 00:03:56,100
En el ejemplo de la derecha,

109
00:03:56,510 --> 00:03:58,100
estaríamos reduciendo los datos de

110
00:03:58,450 --> 00:04:00,600
3 dimensiones, xi en R3

111
00:04:00,710 --> 00:04:04,840
a zi, que ahora es de dos dimensiones.

112
00:04:05,390 --> 00:04:07,790
Así que estos vectores z serían ahora de dos dimensiones.

113
00:04:08,450 --> 00:04:09,590
Así que sería z1,

114
00:04:10,150 --> 00:04:11,410
z2 como tal y

115
00:04:11,640 --> 00:04:12,940
tenemos que computar

116
00:04:13,670 --> 00:04:15,410
estas nuevas representaciones, z1

117
00:04:15,570 --> 00:04:17,350
y z2 de los datos también.

118
00:04:18,280 --> 00:04:20,350
Entonces, ¿cómo calculamos todas estas cantidades?

119
00:04:20,520 --> 00:04:21,520
Se puede usar una derivación matemática,

120
00:04:22,490 --> 00:04:23,660
o la demostración matemática

121
00:04:24,300 --> 00:04:26,020
para saber cuál es

122
00:04:26,090 --> 00:04:27,970
el valor correcto para u1, u2, z1,

123
00:04:28,290 --> 00:04:29,480
z2 y así sucesivamente.

124
00:04:29,690 --> 00:04:31,230
Esa demostración matemática es muy

125
00:04:31,480 --> 00:04:32,890
complicada y más allá del

126
00:04:32,950 --> 00:04:34,620
alcance del curso.

127
00:04:35,280 --> 00:04:37,290
Pero una vez que has hecho toda la derivación matemática, el

128
00:04:37,590 --> 00:04:38,590
procedimiento para

129
00:04:39,350 --> 00:04:40,570
encontrar realmente el valor

130
00:04:41,200 --> 00:04:42,210
de u1 que buscamos

131
00:04:42,950 --> 00:04:43,950
no es tan difícil, a pesar de que

132
00:04:44,180 --> 00:04:45,640
la demostración matemática de

133
00:04:45,840 --> 00:04:46,940
que este valor es el correcto

134
00:04:47,260 --> 00:04:48,450
se trata de algo más complejo y

135
00:04:48,700 --> 00:04:49,960
más de lo que me quiero adentrar.

136
00:04:50,880 --> 00:04:52,070
Pero permítame describir el

137
00:04:52,480 --> 00:04:53,830
procedimiento específico que

138
00:04:53,960 --> 00:04:55,250
tiene que implementar para

139
00:04:55,440 --> 00:04:56,450
para calcular todos estas

140
00:04:56,570 --> 00:04:57,840
cosas, los vectores, u1, u2,

141
00:04:58,910 --> 00:05:00,980
el vector z.  Aquí está el procedimiento:

142
00:05:02,070 --> 00:05:02,970
Digamos que queremos reducir

143
00:05:03,170 --> 00:05:04,220
los datos, de "n" dimensiones

144
00:05:04,840 --> 00:05:05,760
a "k" dimensiones. Lo que vamos a

145
00:05:06,760 --> 00:05:07,640
hacer primero es

146
00:05:07,900 --> 00:05:09,400
calcular algo que se llama la

147
00:05:09,830 --> 00:05:11,140
matriz de covarianza y esta matriz

148
00:05:11,700 --> 00:05:13,620
se denota comúnmente por medio

149
00:05:13,820 --> 00:05:15,050
de esta letra griega, que es

150
00:05:15,190 --> 00:05:16,880
la «sigma» mayúscula del «alfa»beto griego.

151
00:05:18,000 --> 00:05:19,210
Es un poco lamentable que la

152
00:05:19,310 --> 00:05:21,080
«sigma» del «alfa»beto griego luzca exactamente

153
00:05:21,760 --> 00:05:22,710
igual que los símbolos de suma.

154
00:05:23,210 --> 00:05:24,620
Así que esta «sigma»

155
00:05:24,700 --> 00:05:26,220
del «alfa»beto griego se usa

156
00:05:26,420 --> 00:05:29,540
para denotar una matriz y este es un símbolo de suma.

157
00:05:30,510 --> 00:05:32,330
Así que espero que en estas diapositivas

158
00:05:32,680 --> 00:05:34,190
no haya ambigüedad sobre cuál símbolo

159
00:05:34,410 --> 00:05:36,340
se refiere a la matriz de covarianza «sigma» y

160
00:05:36,520 --> 00:05:37,850
cual al símbolo de suma

161
00:05:38,090 --> 00:05:39,620
y espero que

162
00:05:39,940 --> 00:05:41,460
sea claro a partir del contexto cuando

163
00:05:41,820 --> 00:05:43,510
esté usando cada uno.

164
00:05:43,740 --> 00:05:44,790
¿Cómo se puede calcular esta matriz «sigma»?

165
00:05:45,530 --> 00:05:46,550
Digamos que queremos almacenarla

166
00:05:47,135 --> 00:05:47,640
en una variable de Octave

167
00:05:48,120 --> 00:05:49,970
llamada «sigma».

168
00:05:50,840 --> 00:05:51,890
Lo que tenemos que hacer es

169
00:05:52,030 --> 00:05:53,660
calcular algo que se llama la

170
00:05:54,130 --> 00:05:56,190
vectores propios de la matriz «sigma».

171
00:05:57,560 --> 00:05:58,450
En Octave, la forma de

172
00:05:58,590 --> 00:05:59,820
hacer esto es usando este

173
00:05:59,970 --> 00:06:01,020
comando, [U, S, V]=

174
00:06:01,350 --> 00:06:02,600
svd («sigma»),

175
00:06:03,650 --> 00:06:06,090
SVD, por cierto, significa descomposición de valor singular.

176
00:06:08,520 --> 00:06:10,590
Esta

177
00:06:10,790 --> 00:06:12,660
es

178
00:06:14,450 --> 00:06:15,560
álgebra lineal mucho más avanzada,

179
00:06:15,800 --> 00:06:16,950
más de lo que realmente necesita saber,

180
00:06:16,950 --> 00:06:18,770
pero resulta

181
00:06:18,950 --> 00:06:20,250
que cuando «sigma» es una matriz

182
00:06:20,480 --> 00:06:21,800
de covarianza existen

183
00:06:21,880 --> 00:06:23,420
algunas maneras de calcular estos

184
00:06:23,610 --> 00:06:25,810
vectores propios y si usted

185
00:06:25,960 --> 00:06:27,350
es un experto en álgebra lineal

186
00:06:27,700 --> 00:06:28,610
y si ha oído hablar de los vectores propios

187
00:06:28,860 --> 00:06:30,170
antes, usted quizá sepa que

188
00:06:30,350 --> 00:06:31,660
hay otra función de Octave

189
00:06:31,990 --> 00:06:33,420
llamada "eig", que puede

190
00:06:33,520 --> 00:06:35,030
usarse también para calcular la misma cosa.

191
00:06:35,950 --> 00:06:36,980
Y resulta que

192
00:06:37,370 --> 00:06:39,180
la función SVD y la

193
00:06:39,290 --> 00:06:40,310
función "eig" le darán los

194
00:06:40,370 --> 00:06:42,170
mismos vectores propios, aunque SVD

195
00:06:42,840 --> 00:06:44,210
es un poco más estable numéricamente.

196
00:06:44,540 --> 00:06:45,890
Así que yo tiendo a usar SVD, aunque

197
00:06:46,140 --> 00:06:47,040
tengo un par de amigos que usan

198
00:06:47,280 --> 00:06:48,720
la función "eig" para hacer esto también

199
00:06:48,920 --> 00:06:50,050
pero cuando usted

200
00:06:50,130 --> 00:06:51,270
aplica esto a una matriz de covarianza

201
00:06:51,750 --> 00:06:52,960
«sigma», le da el mismo resultado.

202
00:06:53,870 --> 00:06:55,070
Esto es porque la matriz de covarianza

203
00:06:55,500 --> 00:06:57,250
siempre satisface una propiedad matemática

204
00:06:57,940 --> 00:07:00,560
llamada semidefinita positiva simétrica.

205
00:07:01,360 --> 00:07:02,140
Usted realmente no necesita saber

206
00:07:02,280 --> 00:07:03,890
lo que eso significa, sólo que las funciones SVD

207
00:07:05,340 --> 00:07:07,090
y "eig" son diferentes pero

208
00:07:07,400 --> 00:07:08,670
cuando se aplican a una

209
00:07:08,780 --> 00:07:10,410
matriz de covarianza que puede

210
00:07:10,550 --> 00:07:12,080
ser probada para satisfacer siempre esta

211
00:07:13,190 --> 00:07:15,220
propiedad matemática, estas funciones siempre darán el mismo resultado.

212
00:07:16,580 --> 00:07:19,180
Bien, eso fue probablemente mucho más álgebra lineal de lo que necesitaba saber.

213
00:07:19,260 --> 00:07:22,380
En caso de que nada de eso tuviera sentido, no se preocupe por ello,

214
00:07:22,560 --> 00:07:23,490
todo lo que necesita saber es que

215
00:07:24,130 --> 00:07:27,840
este comando que debe

216
00:07:28,140 --> 00:07:29,690
implementar en Octave.

217
00:07:30,080 --> 00:07:30,550
Y si usted está implementándolo en un

218
00:07:30,710 --> 00:07:32,120
lenguaje diferente a Octave o MATLAB,

219
00:07:32,710 --> 00:07:33,790
lo que debe hacer es encontrar

220
00:07:34,190 --> 00:07:35,860
la biblioteca de álgebra lineal numérica

221
00:07:36,730 --> 00:07:37,960
que puede calcular la SVD

222
00:07:38,230 --> 00:07:40,460
o la descomposición en valor singular, y

223
00:07:40,970 --> 00:07:42,680
hay muchas de estas bibliotecas para

224
00:07:43,570 --> 00:07:45,060
probablemente todos los principales lenguajes de programación.

225
00:07:45,300 --> 00:07:46,920
Se puede utilizar eso para

226
00:07:47,050 --> 00:07:48,920
calcular las matrices u,

227
00:07:49,200 --> 00:07:52,770
s, y d, de la matriz de covarianza «sigma».

228
00:07:53,340 --> 00:07:54,490
Entonces, sólo para añadir

229
00:07:54,620 --> 00:07:56,090
algunos detalles más, esta «sigma» de matriz de covarianza

230
00:07:56,660 --> 00:07:58,080
será

231
00:07:58,250 --> 00:08:01,480
una matriz nxn.

232
00:08:02,250 --> 00:08:03,240
Una manera de ver esto

233
00:08:03,510 --> 00:08:04,220
es que si nos fijamos en la definición,

234
00:08:05,250 --> 00:08:06,280
este es un vector

235
00:08:06,660 --> 00:08:08,680
nx1 y esto que se muestra aquí,

236
00:08:08,920 --> 00:08:10,830
xi traspuesta es

237
00:08:11,010 --> 00:08:13,260
1xn, entonces el

238
00:08:13,380 --> 00:08:14,480
producto de estas dos cosas

239
00:08:15,150 --> 00:08:15,800
va a ser una matriz

240
00:08:16,570 --> 00:08:17,530
nxn.

241
00:08:19,100 --> 00:08:22,130
Y

242
00:08:22,280 --> 00:08:22,840
cuando

243
00:08:22,910 --> 00:08:23,710
sumamos todo esto, todavía

244
00:08:23,840 --> 00:08:26,140
tiene una matriz nxn.

245
00:08:27,600 --> 00:08:29,920
Y los resultados de la SVD son

246
00:08:30,500 --> 00:08:32,580
las tres matrices, u, s, y

247
00:08:32,830 --> 00:08:35,070
v.  Lo que realmente necesita de la SVD es la matriz u.

248
00:08:36,230 --> 00:08:40,160
La matriz u también será una matriz nxn

249
00:08:41,510 --> 00:08:42,280
y si nos fijamos en las

250
00:08:42,350 --> 00:08:43,260
columnas de la matriz u,

251
00:08:43,480 --> 00:08:45,330
resulta que las

252
00:08:45,630 --> 00:08:47,210
columnas

253
00:08:48,570 --> 00:08:50,180
de la matriz u serán

254
00:08:50,350 --> 00:08:53,860
exactamente esos vectores, u1,

255
00:08:54,260 --> 00:08:56,290
u2 y así sucesivamente, que es lo que queremos.

256
00:08:57,640 --> 00:08:59,330
Así que u, será una matriz nxn

257
00:09:00,910 --> 00:09:01,830
y si queremos reducir

258
00:09:02,230 --> 00:09:03,200
los datos, de "n" dimensiones

259
00:09:03,800 --> 00:09:05,380
hasta "k" dimensiones, entonces lo que

260
00:09:05,490 --> 00:09:07,950
tenemos que hacer es tomar los primeros vectores k

261
00:09:09,800 --> 00:09:12,670
que nos dan u1 hasta

262
00:09:12,860 --> 00:09:14,700
uk, que son las

263
00:09:14,780 --> 00:09:16,930
direcciones k en las que

264
00:09:17,200 --> 00:09:19,770
queremos proyectar los datos.

265
00:09:20,090 --> 00:09:21,640
Así que, para describir el resto del procedimiento a partir de

266
00:09:22,410 --> 00:09:24,170
esta rutina de álgebra

267
00:09:24,490 --> 00:09:25,580
lineal numérica de SVD, lo que tenemos

268
00:09:25,840 --> 00:09:27,140
es esta matriz u  y voy a nombrar

269
00:09:27,530 --> 00:09:29,080
estas columnas de u1-un.

270
00:09:30,580 --> 00:09:31,620
Así que, para concluir la

271
00:09:31,830 --> 00:09:32,520
descripción del resto del

272
00:09:32,540 --> 00:09:34,550
procedimiento a partir de

273
00:09:35,320 --> 00:09:36,940
la rutina de álgebra lineal numérica SVD

274
00:09:37,240 --> 00:09:38,650
tenemos estas matrices u, s,

275
00:09:38,830 --> 00:09:41,320
y d, aquí está la matriz u y vamos

276
00:09:41,900 --> 00:09:44,460
a utilizar las primeras columnas k

277
00:09:45,050 --> 00:09:46,310
de esta matriz para obtener u1 hasta uk.

278
00:09:48,710 --> 00:09:49,460
Ahora, otra cosa que necesitamos es encontrar la forma de

279
00:09:49,700 --> 00:09:53,730
tomar mi grupo de datos original

280
00:09:54,110 --> 00:09:55,430
x que está

281
00:09:55,630 --> 00:09:57,080
en Rn y encontrar una

282
00:09:57,250 --> 00:09:59,210
representación dimensional menor, z , que

283
00:09:59,420 --> 00:10:01,280
está en Rk para estos datos.

284
00:10:01,570 --> 00:10:02,800
Así que la forma en que haremos

285
00:10:02,900 --> 00:10:03,930
esto será tomar

286
00:10:04,180 --> 00:10:06,690
las primeras columnas k de la matriz u,

287
00:10:08,330 --> 00:10:09,790
voy a construir esta matriz,

288
00:10:11,060 --> 00:10:13,040
apilando u1, u2 y

289
00:10:14,170 --> 00:10:16,690
así sucesivamente hasta uk en columnas.

290
00:10:17,350 --> 00:10:19,120
Básicamente se trata de tomar, ya sabe,

291
00:10:19,280 --> 00:10:20,350
esta parte de la matriz, las

292
00:10:20,530 --> 00:10:22,260
primeras columnas k de esta matriz.

293
00:10:23,420 --> 00:10:25,370
Y entonces esto es

294
00:10:25,600 --> 00:10:26,920
una matriz

295
00:10:27,200 --> 00:10:28,580
nxk, le voy a

296
00:10:29,500 --> 00:10:30,690
dar a esta matriz un nombre,

297
00:10:30,880 --> 00:10:32,200
voy a llamar a esta matriz

298
00:10:32,930 --> 00:10:35,760
U, con el subíndice "reducir", una especie

299
00:10:36,090 --> 00:10:38,620
de versión reducida de la matriz U, quizá,

300
00:10:39,140 --> 00:10:41,250
voy a usarla para reducir la dimensión de mis datos.

301
00:10:43,040 --> 00:10:43,950
Y la forma en que voy a calcular Z es que

302
00:10:44,250 --> 00:10:45,960
Z será igual a esta

303
00:10:46,220 --> 00:10:49,570
matriz reducida u, transpuesta

304
00:10:50,010 --> 00:10:52,030
transferencia negativo “X” de «theta». O de forma alternativa,

305
00:10:52,510 --> 00:10:53,860
anotar lo que significa esta transposición.

306
00:10:54,630 --> 00:10:55,910
Cuando tomo esta transposición de

307
00:10:56,010 --> 00:10:57,920
esta matriz U "reducir", con lo

308
00:10:58,010 --> 00:11:00,680
que voy a terminar es con estos vectores ahora en filas,

309
00:11:00,950 --> 00:11:04,540
tengo U1 transpuesta hasta uk transpuesta,

310
00:11:07,060 --> 00:11:08,860
elevo esto a la X

311
00:11:09,700 --> 00:11:10,740
y así es como me obtengo

312
00:11:10,920 --> 00:11:12,670
mi vector Z. Sólo para

313
00:11:12,740 --> 00:11:14,280
asegurarse de que estas dimensiones tienen sentido,

314
00:11:15,370 --> 00:11:16,380
esta matriz aquí va

315
00:11:16,560 --> 00:11:17,450
a ser kxn

316
00:11:18,270 --> 00:11:19,350
y «x» aquí va

317
00:11:19,420 --> 00:11:20,530
a ser nx1,

318
00:11:20,750 --> 00:11:21,810
de modo que el producto

319
00:11:22,320 --> 00:11:24,330
aquí será kx1

320
00:11:24,820 --> 00:11:27,920
y así, z es de "k" dimensiones,

321
00:11:28,790 --> 00:11:29,810
es un vector k dimensional,

322
00:11:30,010 --> 00:11:31,230
que es exactamente

323
00:11:32,000 --> 00:11:33,180
lo que queríamos.

324
00:11:33,550 --> 00:11:34,640
Y, por supuesto, estas «x’s»  aquí ahora pueden ser

325
00:11:34,890 --> 00:11:36,010
ejemplos en nuestro conjunto

326
00:11:36,100 --> 00:11:36,970
de entrenamiento, pueden ser ejemplos

327
00:11:37,540 --> 00:11:38,750
en nuestro grupo de validación cruzada,

328
00:11:38,980 --> 00:11:40,330
pueden ser ejemplos en el conjunto de prueba, y

329
00:11:40,500 --> 00:11:41,590
por ejemplo, si

330
00:11:41,930 --> 00:11:43,830
quisiera tomar el ejemplo de entrenamiento i,

331
00:11:44,260 --> 00:11:45,910
puedo escribir esto como xi,

332
00:11:47,270 --> 00:11:48,430
xi y esto es lo que me va a dar

333
00:11:48,510 --> 00:11:50,080
Zi allá.

334
00:11:50,940 --> 00:11:53,140
Así que, para resumir, aquí está el

335
00:11:53,460 --> 00:11:54,820
algoritmo de ACP en una diapositiva.

336
00:11:56,250 --> 00:11:58,200
Después de la normalización de media, para garantizar

337
00:11:58,420 --> 00:11:59,230
que cada variable es media cero,

338
00:11:59,610 --> 00:12:01,420
y opcionalmente, escalación de característcas.

339
00:12:02,280 --> 00:12:03,780
-Realmente debe hacer escalación de variables si

340
00:12:03,890 --> 00:12:05,820
sus variables adquieren rangos de valores muy diferentes.-

341
00:12:06,620 --> 00:12:08,610
Después de este procesamiento previo, calculamos

342
00:12:09,130 --> 00:12:12,010
la matriz de covarianza «sigma», de este modo y

343
00:12:12,240 --> 00:12:14,070
por cierto,

344
00:12:14,210 --> 00:12:16,340
si sus datos se dan

345
00:12:16,610 --> 00:12:17,780
en forma de matriz

346
00:12:18,030 --> 00:12:18,960
como esta, si sus datos se presentan

347
00:12:19,230 --> 00:12:22,580
en filas de esta forma, si usted

348
00:12:22,780 --> 00:12:24,370
tiene una matriz X,

349
00:12:24,960 --> 00:12:26,190
que son todos sus grupos de entrenamiento

350
00:12:27,030 --> 00:12:28,830
escritos en filas en las que tenemos x1

351
00:12:29,210 --> 00:12:30,400
transpuesta hasta xm transpuesta,

352
00:12:31,530 --> 00:12:32,700
esta matriz de covarianza «sigma» tiene en realidad

353
00:12:33,020 --> 00:12:36,040
una implementación de vectorización buena.

354
00:12:37,390 --> 00:12:38,980
La puede aplicar en Octave,

355
00:12:39,440 --> 00:12:41,130
incluso se puede ejecutar «sigma» = 1

356
00:12:41,670 --> 00:12:45,250
sobre m a la x,

357
00:12:45,550 --> 00:12:46,440
que es esta matriz aquí,

358
00:12:47,250 --> 00:12:50,770
transpuesta a la x, y

359
00:12:50,980 --> 00:12:53,320
esta simple expresión, es

360
00:12:53,570 --> 00:12:55,070
la aplicación de vectorización de cómo

361
00:12:55,220 --> 00:12:56,910
calcular la matriz «sigma».

362
00:12:58,020 --> 00:12:59,020
No voy a comprobar si

363
00:12:59,160 --> 00:13:00,600
esta es la vectorización correcta pero si

364
00:13:00,740 --> 00:13:02,460
desea, puede probar esto numéricamente

365
00:13:02,870 --> 00:13:03,900
usted mismo con

366
00:13:03,980 --> 00:13:05,100
Octave, asegurándose de que

367
00:13:05,840 --> 00:13:06,890
tanto esto, como las implementaciones

368
00:13:06,920 --> 00:13:10,050
den las mismas respuestas o puede tratar de probarlo usted mismo matemáticamente,

369
00:13:11,250 --> 00:13:12,330
puede hacerlo de cualquier modo pero esta es la

370
00:13:12,430 --> 00:13:14,580
implementación de vectorización correcta de cómo calcular «sigma».

371
00:13:16,480 --> 00:13:17,570
A continuación podemos aplicar la rutina SVD

372
00:13:17,920 --> 00:13:19,050
para obtener u, s,

373
00:13:19,250 --> 00:13:20,840
y d, posteriormente, tomamos

374
00:13:21,100 --> 00:13:22,720
las primeras columnas

375
00:13:23,050 --> 00:13:24,450
k de la matriz para obtener U

376
00:13:24,660 --> 00:13:26,550
"reducir" y

377
00:13:26,650 --> 00:13:28,540
finalmente, esto define la forma en que

378
00:13:28,740 --> 00:13:29,980
vamos a partir de una variable

379
00:13:30,290 --> 00:13:31,600
del vector x a esta

380
00:13:31,850 --> 00:13:34,340
representación z de dimensiones reducidas. De forma

381
00:13:34,540 --> 00:13:35,760
similar a Media k

382
00:13:36,090 --> 00:13:37,860
si aplica el ACP, la forma

383
00:13:38,030 --> 00:13:40,300
de aplicarlo es con vectores x en Rn ¿cierto?,

384
00:13:41,100 --> 00:13:43,990
por lo tanto, esto no se hace con la convención x0=1.

385
00:13:44,200 --> 00:13:46,080
Entonces, eso fue

386
00:13:46,990 --> 00:13:48,680
el algoritmo de ACP,

387
00:13:50,120 --> 00:13:51,380
una cosa que no hice fue

388
00:13:51,590 --> 00:13:53,190
la demostración matemática de que

389
00:13:53,520 --> 00:13:54,600
este procedimiento realmente permite

390
00:13:54,970 --> 00:13:56,560
la proyección de los datos en

391
00:13:57,230 --> 00:13:58,730
el subespacio de k dimensiones en la superficie

392
00:13:58,870 --> 00:14:00,620
k dimensional que en realidad

393
00:14:02,170 --> 00:14:04,800
minimiza el error de proyección de cuadrados, la prueba matemática

394
00:14:05,110 --> 00:14:07,170
de esto está más allá del alcance de este curso.

395
00:14:07,700 --> 00:14:09,110
Afortunadamente, el algoritmo de ACP

396
00:14:09,470 --> 00:14:10,940
puede ser implementado en no

397
00:14:11,320 --> 00:14:12,510
demasiadas líneas de código Octave y

398
00:14:13,190 --> 00:14:14,510
si lo implementa en Octave o MATLAB,

399
00:14:14,640 --> 00:14:16,120
en realidad obtendrá

400
00:14:16,520 --> 00:14:17,590
un algoritmo para reducción

401
00:14:18,110 --> 00:14:19,710
de dimensionalidad muy efectivo.

402
00:14:22,430 --> 00:14:23,850
Ese fue el algoritmo de ACP,

403
00:14:25,010 --> 00:14:26,290
una cosa que no hice fue

404
00:14:26,840 --> 00:14:28,420
la demostración matemática de que

405
00:14:29,170 --> 00:14:30,360
U1 y U2 y así sucesivamente,

406
00:14:30,720 --> 00:14:31,630
de Z y estos elementos

407
00:14:31,770 --> 00:14:32,830
obtenidos por

408
00:14:32,980 --> 00:14:34,330
este procedimiento son realmente las

409
00:14:34,680 --> 00:14:35,870
opciones que reducirían al mínimo

410
00:14:36,500 --> 00:14:37,800
estos errores de proyección al cuadrado.

411
00:14:38,140 --> 00:14:39,350
Bien, recuerde que dijimos que lo que ACP

412
00:14:39,610 --> 00:14:40,660
intenta hacer es tratar

413
00:14:40,960 --> 00:14:42,170
de encontrar una superficie o línea

414
00:14:42,570 --> 00:14:43,690
sobre la cual proyectar los datos

415
00:14:44,280 --> 00:14:46,340
y así reducir al mínimo este error de proyección al cuadrado.

416
00:14:46,700 --> 00:14:48,610
Así que no demostré que esto se lograba realmente

417
00:14:49,140 --> 00:14:50,680
y la demostración matemática

418
00:14:50,970 --> 00:14:52,520
de esto está más allá del alcance de este curso.

419
00:14:53,170 --> 00:14:55,550
Afortunadamente, el algoritmo de ACP

420
00:14:55,730 --> 00:14:58,890
puede implementarse en no demasiadas líneas de código Octave y

421
00:14:59,350 --> 00:15:00,740
si lo implementa,

422
00:15:01,430 --> 00:15:02,560
esto es lo que realmente

423
00:15:02,770 --> 00:15:03,730
funciona o funcionará bien y

424
00:15:04,710 --> 00:15:05,940
si implementa este algoritmo,

425
00:15:06,500 --> 00:15:09,220
obtendrá un algoritmo muy eficaz de reducción de dimensionalidad

426
00:15:09,780 --> 00:15:10,650
que funciona correctamente para

427
00:15:11,050 --> 00:15:13,460
minimizar este error de proyección al cuadrado.