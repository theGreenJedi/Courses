このビデオでは、 主成分分析のアルゴリズムについて お話ししたい。 このビデオが終わる頃には、 あなたはPCAを自分自身で実装する方法を知る事になる。 そしてそれを用いてあなたのデータの次元を削減する。 PCAを適用する前に、 必ずやらなくてはいけないデータの前処理のステップがある。 トレーニングセットの手本が与えられたとして、 必ずやらなくてはいけない事は、 平均標準化だ。 そして次にデータによっては、 フィーチャースケーリングも実行する場合がある。 これは教師あり学習での 平均標準化とフィーチャースケーリングのプロセスと とても似た物だ。 実のところ、それは ラベル無しデータのx1からxmに対して行う、という 事以外は 完全に同じ物だ。 平均標準化の為に、 各フィーチャーの平均を まず計算する必要がある。 そして各フィーチャーxを x-平均 で置き換える。 この結果、各フィーチャーは厳密に 平均0となる。 もしそれぞれのフィーチャーがとても異なるスケールを持つなら、 例えば x1が家の大きさ、 x2が寝室の数なら、 以前の例を引っ張り出してみたが、 そうすると各フィーチャーが 比較可能な範囲になるように、スケールする必要もある。 だから教師あり学習でやったのと 同様に、 x(i)の下付き添字jを これはjのフィーチャーだが、 そこから 平均を引いて、 これが分子となる、そしてそれをsjで割る。 ここでsjはフィーチャーjの値の範囲に関する、なんらかの指標だ。
つまり、max - minでも良いし、 もっと良く使われている物としては、 フィーチャーjの標準偏差がある。 この種のデータの前処理を 行った上で、PCAはこんな事をする。 前回のビデオで我らは PCAがやる事は、 より低い次元の部分空間で、 そこにデータを射影すると 射影誤差の二乗が 最小になるような物を、 探す、という事を見たのだった。 射影誤差の二乗の和、 これらの青い線の 長さの二乗。 つまり具体的には、 この方向を示すベクトルu1を 見つけたい。または 2Dのケースでは二つのベクトル、 u1とu2を見つけたい、 データを射影するこの平面を 定義するような。 ではちょっと簡単に、 データの次元を削減する、というのが 何を意味するのかを振り返ってみよう。 この例では、 左側には手本xiが与えられていて、 それはR 2に存在する。 そして我らがやりたい事は、 我らのデータを表す Rの数字ziを、 探す事だ。 以上が2Dから1Dへの削減、の意味する所だ。 具体的には、この赤い直線へ データを射影する事で、 この直線の上の点の位置を 示すのは、数字一つで十分だ。 その数字を、zとかz1と 呼ぶ事にしよう。 ここでzは実数だ。つまりこれは一次元ベクトルみたいな物だ。 z1は単にこの 1x1行列、または1次元ベクトルの 最初の要素を表す。 つまり点の位置を示すのに 一つの数しか必要としない。 つまりこの手本が 手本x1だとすると、 それはここにマップされるだろうか。 そしてこの手本がx2なら、 それはここにマップされるかな。 つまりこのここの点は z1となり、 ここのこの点はz2となる。 同様に、これら他の点も、 x3, x4, x5とすると、 これらはz3, z4, z5に マップされるなど。 つまりPCAの任務は、 二つの事を計算する方法を 考え出す必要がある、という事だ。 一つ目はこれらのベクトル、 u1と、こちらのケースではu1とu2。 そしてもう一つは、 これらの数、Zを計算する方法。 つまり左側の例では、 データを2Dから1Dへと削減した。 右側の例では、 データを3次元、R 3から、 zi、これはここでは2次元へと 削減している。 つまりこれらのzベクトルは、いまは2次元だ。 つまりそれはz1とz2、 みたいな形。 だから我らはこれらの新しい表現 z1とz2を計算する方法を 同様に知らなくてはならない。 ではこれら全ての量を、どうやって計算したら良いだろうか？ 数学的な導出は、 数学的な証明も、 u1, u2, z1, z2などの 正しい値は何か、を 証明する事は、 その数学的証明はとても複雑で、 このコースの範囲を 越える。 だが一度その手順をマスターしてしばえば、 実際に望みのu1の値を 探す手続きというのは、 そんなに大変でも 無い事が分かる。 この値が正しい値だ、と 数学的に証明するのは、 よりしっかりとやらないと分からない事で、 それは私がこのコースでやりたいレベルを越えてしまう。 だが、具体的な手順は、 記述してみよう、 これら全てのベクトル、 u1, u2, ベクトルzを 計算する為にやる必要のある事を。 これがその手順だ。 我らはデータを n次元からk次元へと 削減したいとする。 そこで我らがやるのは、 まず、共分散行列と呼ばれる物を 計算する。共分散行列は 普通ギリシャ文字の 大文字のギリシャ文字のシグマで 表す慣例となっている。 和のシグマと ギリシャ文字のシグマが まったく同じなのは、ちょっとした不幸だ。 つまりこの ギリシャ文字シグマは 行列を示すのに使われ、このこれは和の記号だ。 これらのスライドにおいては、 どれがSigma行列、行列か、 それが和の記号か、 どれがどちらか、 その区別は、 文脈から私がどっちを使ってるかは はっきり分かると思う。 この行列をどう計算するか？ Octaveの変数Sigmaに それが入っていると しよう。 我らがやるべき事は、 行列Sigmaの固有ベクトルと呼ばれる物を 計算する事だ。 Octaveでは、それをやる方法は、 以下のコマンドを使う事だ： U S Vイコールの svdのSigma。 ところで、svdは、Singular Value Decomposition（特異値分解）の略。 特異値分解は、 よりアドバンスドな、、、 あなたが実際にしる必要がある範囲よりも よりアドバンスドな線形代数の範疇なのだが、 知られている事として、 Sigmaが共分散行列とする時、 固有ベクトルを求める方法が 幾つか存在する。 そしてもしあなたが 線形代数のエキスパートで、 前から固有ベクトルを知ってたなら、 ひょっとしたらoctaveの eigと呼ばれる関数を知ってるかもしれない。 これもまた同じ物を 計算するのに使う事が出来る。 結局は、svd関数と eig関数は、 同じベクトルを返す。 でもsvdの方がちょっとだけ 数値計算的に安定だが。 だから私はsvdを使う事にしている、 でも私には、同じ事をするのに eig関数を使う友人が 何人かいるが。だが、これを 共分散行列Sigmaに適用する場合、 どちらも同じ結果を返す。 これは共分散行列は、 いつでも正定値で対称、 という数学的性質を満たす為だ。 それの意味する所は知る必要は無いが、 svdとeig関数は、 異なる関数だが、 共分散行列に 適用する時には、 いつもこの数学的性質を満たす事が証明されていて、 いつも同じ結果を返す、という事だ。 まあいいや。以上はたぶん知る必要があるよりも大分たくさんの線形代数の詳細だろう。 これら全部が何言ってるかさっぱり分からなくても、気にしなくてよろしい。 知らなきゃいけない事の全ては、 Octaveで、このシステムコマンドを 実装しなくてはいけない、って事だけ。 そしてもしOctaveやMATLAB以外の 別の言語でこれを実装しなくてはいけない、としたら、 その時はあなたは、数値計算の 線形代数ライブラリで、svd、つまり特異値分解が 計算出来る物を 探すべきだ。 そんなライブラリは、主要なプログラミング言語なら どれにもたくさんあるはず。 人々はそれを用いて 共分散行列シグマの行列U, S, Dを 計算するのに使う事が出来る。 ではより詳細を埋めていこう。 この共分散行列シグマは、 n掛けるnの 行列だ。 それを確認する一つの方法としては、 定義を見ると、 これはn掛ける1ベクトルで、 そしてこれは xiの転置は 1掛けるn。 だからこれら二つの積は、 n掛けるnの 行列となる。 1xNの転置と1xNだから、 NxN行列となり、 これらを全て足しあわせても、 NxN行列のまま。 そしてsvdの出力は、 3つの行列U, S, Vだ。 svdの結果でこの場合本当に必要とするのは、U行列のみだ。 U行列もまたNxN行列だ。 そしてもしU行列の列を 見てみると、 U行列の列は、 ベクトルu1, u2, などなどと まったく同じ事が 分かる。 分かる。 つまりUは行列。 そしてN次元からk次元へと データを削減 したい場合、 やるべき事はまず最初のk本のベクトルを取り それによって我らがデータを射影したいと思う、 u1からukまでの、 k本の方向が 得られる。 そこからの手順としては、 SVD数値計算線形代数ライブラリのルーチンから この行列Uが得られる。 これらの列を u(1)からu(n)、と呼ぼう。 さて、残りの手順を まとめて記述してみよう。 SVD数値計算線形代数ルーチンから、 これらの行列、 U, S, Dを得る。 この行列から、 最初のk列を取り出し、 u(1)-u(k)を得る。 さて、次にやるべき事は、 元になるデータセットのXを持ってきて、 このXはR nだが、 そしてより低じ次元の表現である zを見つけてくる、 それはR kだ。 それを行う方法としては、 U行列の最初のk列を 取り出して、 この行列を構築する。 u1, u2, ...とukまで 列に積み上げる。 それは基本的には、 行列のこの部分を取り出す事だ、 この行列の最初のk列を。 だからこれは、 n掛けるk行列と なる。 この行列に名前を与えておこう。 この行列を、Uの下付き添字reduce、と 呼ぶ事にする、 reduce（削減）されたバージョンのU、みたいな意味だ。 これを私のデータの次元を削減するのに使っていく。 そしてzを計算する方法は、 zイコールの、 このU reduce行列の転置に、掛ける事のx。 または別の書き方をすると、 この転置が意味する所を書きくだして、 このU行列の転置を取ると、 最終的に得られるのは、 今や列に並んでいるこれらのベクトルだ。 u1転置からuk転置まで。 そしてそれに、掛けることx。 これがベクトルzを 得る方法だ。 これらの次元を納得する為に確認してみよう。 このここの行列は、 k掛けるnとなり、 ここのxは、 n掛ける1となる。 だからその積は、 k掛ける1となる。 つまりzはk次元の、、、 k次元ベクトルで、 それはまさに 我らの求める物だ。 そしてもちろんこれらのxは、 トレーニングセットの 手本でも良いし、 クロスバリデーションセットの手本でも良いし、 テストセットの手本でも良い。 例えば、トレーニング手本の xiをとりたいとする。 そうするとここをxiと書けば良くて、 ここもxiで、するとここは ziとなる。 ではまとめだ。 これはPCAアルゴリズムを一つのスライドに納めた。 平均標準化の後で、 各フィーチャーの平均は0なのを保証した後で、 さらにオプションでフィーチャースケーリングも、 もし本当にフィーチャースケーリングが必要なら、 もしあなたのフィーチャーがそれぞれとても異なるレンジの値を取るなら、行う。 この前処理の後で、 共分散行列Sigmaを計算する。 ところで、 こんな形で 行列としてデータがある時に、 こんな風にデータが 行で与えられている時には、 もし行列Xとして トレーニングセットが行の形で 書かれていて、 x1の転置から、x nの転置までが並んでいるとしたら、 これの共分散行列Sigmaは実は、 ナイスなベクトル化された実装が存在する。 octaveではこんな風に実装出来る。 Sigmaイコール、 1/m 掛ける事の、 ここにあるXに転置をとって、 さらにXを掛ける。 このシンプルな式が、 行列Sigmaを計算する為の ベクトル化した実装だ。 今日のところはこれを証明したりはしない。 これは正しいベクトル化だ。 もしやりたければ、簡単に自分で Octaveで試す事で、数値的に確認出来るし、 つまりこれと、 この実装が同じ結果を返す、と。 または自分で数学的に証明する事も、やれば出来る。 どちらにしろ、これは正しい ベクトル化した実装だ。計算量がより少ない。 次にsvdルーチンを適用し、 U、S、Dを取得する。 そして次に、 U行列の最初のk列を つかみとって、 U reduceとし、 そして最後に、これが フィーチャーベクトルxから、 この削減された次元の表現zへと 変換する方法だ。 そしてK-meansのように、 PCAを適用する時も、 それを適用するベクトルxはR nだ。 つまりx0 = 1となるx0無しで行う。 以上がPCA アルゴリズムだ。 ここまでで一つやっていない事として、 以下の数学的証明を与えていない事だ。 それはk次元部分空間への、 k次元平面への データの射影が、実際に 二乗射影誤差を 最小化する、という事。 それの証明はこのコースのスコープを越える。 幸運な事に、PCAのアルゴリズムは そんなたくさんのコードにならずに 実装する事が可能だ。 そしてこのアルゴリズムを octaveで実装してしまえば、 あなたはとても効率的な次元削減の アルゴリズムを得る事が出来る。 以上がPCAアルゴリズムだ。 一つまだやってない事としては、 u1とかu2などや、zなどが、 ここまでの手続きで得られる これらのベクトルが これらの二乗射影誤差を 本当に最小化している、という 数学的な証明を 与える事だ。 いいかい？思い出してみよう。 PCAがやろうとしているのは、 データを射影する 平面とか直線で、 二乗射影誤差を最小化する物を探す、という事だった。 そしてこれが、その条件をみたしている、とは証明していない。 そしてその数学的証明は このコースの範囲を超えちゃう。 だが幸運な事に、PCAのアルゴリズムは Octaveのコードではそんなにたくさんの行をかけずに実装出来る。 そしてもしこれを実装すれば、 これは実際にうまく 機能する。 そしてこのアルゴリズムを実装する事で、 とても有効な次元削減のアルゴリズムを得る事が出来るのだ。 それは射影二乗誤差を最小化するという 仕事を正しくこなす。