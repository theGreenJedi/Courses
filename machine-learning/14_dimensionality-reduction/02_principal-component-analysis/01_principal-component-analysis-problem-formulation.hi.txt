डिमेन्शनैलिटी रिडक्शन की प्रॉब्लम के लिए, अब तक का सबसे लोकप्रिय, अब तक का सबसे अधिक इस्तेमाल किया जाने वाला अल्गोरिद्म है कुछ जिसे कहते हैं प्रिन्सिपल कम्पोनेंट अनालिसिस, या PCA. इस वीडियो में मैं आप को बताना चाहता हूँ प्रॉब्लम फ़ॉर्म्युलेशन के बारे में PCA के लिए. दूसरे शब्दों में, चलो प्रयास करते हैं फ़ॉर्म्युलेट करने का विधिवत रूप से, जो वास्तव में हम चाहते हैं कि PCA करे. मान लो हमारे पास है एक डेटा सेट इस तरह का. तो, यह है एक डेटा सेट इग्ज़ाम्पल्ज़ x का जो R2 में है और मान लो मैं चाहता हूँ कम करना डिमेन्शन डेटा की दो-डिमेन्शनल से एक-डिमेन्शनल. दूसरे शब्दों में, मैं जानना चाहता हूँ एक लाइन जिस पर प्रोजेक्ट कर सकूँ डेटा को. तो क्या लगती है एक अच्छी लाइन जिस पर प्रोजेक्ट कर सकूँ डेटा को, यह है एक लाइन इस तरह की, यह हो सकता है काफ़ी अच्छा विकल्प. और क्यों हम सोचते है कि यह एक अच्छा विकल्प है क्योंकि यदि आप देखे कहाँ प्रोजेक्ट किए हुए पोईँट के वर्ज़नज़ स्केल होता है, तो मैं लेता हूँ यह पोईँट और प्रोजेक्ट करता हूँ इसे नीचे यहाँ. देखा क्या, यह पोईँट प्रोजेक्ट होता है यहाँ, यहाँ, यहाँ. यदि हम निकालें दूरी प्रत्येक पोईँट के बीच और प्रोजेक्ट किए वर्ज़न के बीच वह बहुत कम है. मतलब कि ये नीले लाइन सेगमेंट्स बहुत छोटे हैं. तो PCA क्या करता है कि यह चाहता है ढूँढना एक निचली डिमेन्शन की सरफ़ेस, वास्तव में एक लाइन इस केस में, जिस पर प्रोजेक्ट कर सके डेटा को ताकि सम स्क्वेर्ज़ का इन छोटी नीली लाइन सेगमेंट्स का हो कम से कम. लंबाई उन नीली सेगेमेंट्स की, जिसे कभी कभी प्रोजेक्शन एरर भी कहते हैं. और इसलिए PCA क्या करता है यह कोशिश करता है जानने कि 
एक सरफ़ेस जिस पर प्रोजेक्ट कर सके डेटा को ताकि न्यूनतम कर सके उसे. सिर्फ़ थोड़ा अलग से, PCA अप्लाई करने से पहले, यह एक स्टैंडर्ड प्रैक्टिस है कि पहले करते हैं मीन नॉर्मलाइज़ेशन, फ़ीचर स्केलिंग के लिए ताकि फ़ीचर्ज़ x1 और x2 की हो ज़ीरो मीन, और होनी चाहिए तुलनात्मक वैल्यूज़. मैंने पहले ही किया है यह इस उदाहरण के लिए, लेकिन मैं वापिस इस पर आऊँगा बाद में और बात करूँगा फ़ीचर स्केलिंग की और नोर्मलाइज़ेशन की PCA के संदर्भ में बाद में. लेकिन वापिस आते हुए इस उदाहरण पर, लाल लाइन का विपरीत जो मैंने अभी बनाई थी, यहाँ है एक भिन्न लाइन जिस पर मैं प्रोजेक्ट कर सकता हूँ मेरा डेटा, जो है यह मजेंटा लाइन. और, जैसे कि हम देखेंगे, यह मजेंटा लाइन है एक बहुत ग़लत दिशा में मेरे डेटा को प्रोजेक्ट करने के लिए, ठीक है? तो यदि मुझे करना होता प्रोजेक्ट मेरा डेटा इस मजेंटा लाइन पर, हमें मिलेगा एक सेट पोएंट्स का इस तरह का. और प्रोजेक्शन एरर, जो हैं ये नीले लाइन सेगमेंट्स, होंगे बहुत बड़े. तो इन पोईँट्स को चलना पड़ेगा काफ़ी दूर होने के लिए प्रोजेक्ट मजेंटा लाइन पर. और इसीलिए PCA, प्रिन्सिपल कम्पोनेंट अनालिसिस, चुनेगा कुछ लाल लाइन जैसा बजाय मजेंटा लाइन नीचे यहाँ. चलो लिखते हैं PCA प्रॉब्लम थोड़ी विधिवत. उद्देश्य PCA का, यदि हम कम करना चाहते हैं डेटा को दो-डिमेन्शनल से एक डिमेन्शनल पर, हम प्रयास कर रहे हैं ढूँढने के लिए एक वेक्टर अर्थात् एक वेक्टर u1, जो होगा Rn, तो वह होगा R2 इस केस में. मैं जानना चाहता हूँ दिशा जिस पर मुझे प्रोजेक्ट करना है डेटा, ताकि प्रोजेक्शन एरर न्यूनतम हो जाए. तो, इस उदाहरण में मैं उम्मीद कर रहा हूँ कि PCA ढूँढेगा इस वेक्टर को, जिसे मैं कहूँगा u(1), ताकि जब मैं प्रोजेक्ट करूँ डेटा उस लाइन पर जो मैं परिभाषित करता हूँ बढ़ाते हुए इस वेक्टर को, मुझे मिलती है बहुत कम प्रोजेक्शन एरर. और वह डेटा जो दिखता है ऐसा. और वैसे तो, मुझे बताना चाहिए कि कहाँ PCA देता है मुझे u(1) या -उ(1), उससे अंतर नहीं पड़ता. तो यदि यह देता है मुझे एक पॉज़िटिव वेक्टर इस दिशा में, वह ठीक है. यदि यह देता है मुझे विपरीत वेक्टर, विपरीत दिशा में, तो वह होगा जैसे माइनस u(1). चलो बनाते हैं उसे नीले से इसके बजाय, ठीक है? लेकिन यह दे एक पॉज़िटिव u(1) या नेगेटिव u(1), उससे अंतर नहीं पड़ता क्योंकि प्रत्येक यह वेक्टर परिभाषित करता है वही लाल लाइन 
जिस पर मैं प्रोजेक्ट कर रहा था मेरा डेटा. तो यह है एक केस काम करने का डेटा को दो-डिमेन्शनल से एक-डिमेन्शनल पर. अधिक सामान्य रूप से हमारे पास है n-डिमेन्शनल डेटा और हम चाहेंगे उसे कम करना k-डिमेन्शन्स में. उस केस में हम चाहते हैं ढूँढना न केवल एक वेक्टर जिस पर प्रोजेक्ट करें डेटा को लेकिन हम चाहते हैं ढूँढना k-डिमेन्शन्स जिस पर प्रोजेक्ट करें डेटा को. ताकि प्रोजेक्शन एरर न्यूनतम हो जाए. तो यहाँ है उदाहरण. यदि मेरे पास है एक 3D पोईँट क्लाउड इस तरह का, 
तब शायद मैं क्या चाहता हूँ करना कि जान पाऊँ वेक्टर्स. तो मिलता है एक युग्म वेक्टर्स का. और मैं कहूँगा इन्हें वेक्टर्स. मैं बनाता हूँ इन्हें लाल रंग से. यहाँ है u(1), और यहाँ है मेरा दूसरा वेक्टर, u(2). यहाँ है u(1), और यहाँ है मेरा दूसरा वेक्टर, u(2). और एक साथ, ये दो वेक्टर्स परिभाषित करते हैं एक प्लेन, या वे परिभाषित करते हैं एक 2D सरफ़ेस, ठीक है? इस तरह की एक 2D सरफ़ेस जिस पर मैं प्रोजेक्ट करूँगा मेरा डेटा. आप में से वे जो परिचित हैं लिनीअर ऐल्जेब्रा से, आप में से वे जो विशेषज्ञ हैं लिनीअर ऐल्जेब्रा में, विधिवत परिभाषा इसकी है कि हम पाना चाहते हैं एक सेट वेक्टर्स u(1), u(2) से u(k) तक. और हम क्या करेंगे कि प्रोजेक्ट करेंगे डेटा को लिनीअर सबस्पेस पर जो बनती हैं इस सेट से k वेक्टर्स के. लेकिन यदि आप परिचित नहीं है लिनीअर ऐल्जेब्रा से, सिर्फ़ सोचें इसे ढूँढने की तरह k दिशाएँ बजाय सिर्फ़ एक दिशा के जिस पर प्रोजेक्ट करना है डेटा को. तो जानना एक k-डिमेन्शनल सरफ़ेस है वास्तव में जानना एक 2D प्लेन इस केस में. जो दिखाया है इस चित्र में, जहाँ हम परिभाषित कर सकते हैं स्थान पोईँट्स का एक प्लेन मैं इस्तेमाल करके k दिशाएँ. और यही कारण है PCA चाहता है ढूँढना k-डिमेन्शन्स जिस पर प्रोजेक्ट करें डेटा को. और अधिक विधिवत रूप में PCA में, हम चाहते हैं जानना 
यह रास्ता प्रोजेक्ट करने के लिए डेटा को ताकि प्रोजेक्शन एरर न्यूनतम हो जाए, जो है दूरी पोईँट और प्रोजेक्शन के बीच की. और ऐसा ही है इस 3D उदाहरण में भी. दिया होने पर एक पोईँट हम लेंगे पोईँट और उसे प्रोजेक्ट करेंगे इस 2D सरफ़ेस पर. हमने कर लिया वह. और इसलिए प्रोजेक्शन एरर होगी, दूरी पोईँट और जहाँ यह प्रोजेक्ट होता है नीचे मेरी 2D सरफ़ेस पर. और इसलिए PCA क्या करता है कि यह ढूँढता है लाइन, या एक प्लेन, या जो भी, जिस पर डेटा को प्रोजेक्ट करना है, न्यूनतम करने के लिव वह स्क्वेर प्रोजेक्शन, वह 90 डिग्री, या वह ओर्थोग्नल प्रजेक्शन एरर. अंत में, एक प्रश्न जो मुझसे कभी कभी पूछा जाता है कि कैसे PCA सम्बंधित करता है लिनीअर रेग्रेशन से? क्योंकि जब समझाता हूँ PCA, मैं कभी कभी बनाता हूँ चित्र इस तरह के और वह लगता है थोड़ा लिनीअर रेग्रेशन जैसे. ऐसा है कि PCA नहीं है लिनीअर रेग्रेशन, और दिखने में समानता होने के बावजूद, ये हैं वास्तव में बिल्कुल भिन्न अल्गोरिद्म्स. यदि हम कर रहे होते लिनीअर रेग्रेशन, हम जो करते वह होता, बाईं तरफ हम करते प्रिडिक्ट वैल्यू किसी वेरीयबल y की दी होने पर कुछ जानकारी फ़ीचर्ज़ x की. और इसलिए लिनीअर रेग्रेशन, हम कोशिश कर रहे हैं फ़िट करने की एक लाइन ताकि न्यूनतम कर सकें स्क्वेर एरर को पोईँट और इस सीधी लाइन में. और इसलिए हम जो न्यूनतम कर रहे हैं वह होगी लम्बाई इन नीली लाइन्स की. और ध्यान दें कि बना रहा हूँ ये नीली लाइएस वर्टिकल रूप में. कि ये नीली लाइन्स हैं वर्टिकल दूरी पोईँट और वैल्यू में जो हायपॉथिसस ने प्रिडिक्ट की है. जबकि इसके विपरीत, PCA में, यह क्या करता है कि न्यूनतम करता है लम्बाई इन नीली लाइन्स की, जो हैं बनाई एक कोण पर. ये हैं वास्तव में न्यूनतम ऑर्थोग्नल दूरियाँ. न्यूनतम दूरी पोईँट x और इस नीली लाइन में. और यह देता है एक बिल्कुल भिन्न प्रभाव निर्भर करते हुए डेटा सेट पर. और अधिक सामान्य रूप में, जब आप कर रहे हैं लिनीअर रेग्रेशन, वहाँ है यह अलग वेरीयबल y जो हम प्रिडिक्ट कर रहे हैं. तो लिनीअर रेग्रेशन क्या करता है कि लेता है सारी वैल्यूज़ x और इस्तेमाल करते हुए उसे प्रिडिक्ट करता है y. जबकि PCA में, कुछ अलग नहीं है, या नहीं है विशेष वेरीयबल y जो हम प्रिडिक्ट कर रहे हैं. और इसके बजाय, हमारे पास है एक लिस्ट फ़ीचर्ज़ की, x1, x2 आगे xn तक, और सारे ये फ़ीचर्ज़ हैं एक समान, तो कोई एक उनमें से नहीं है विशेष. एक आख़िरी उदाहरण के रूप में, यदि मेरे पास है तीन-डिमेन्शनल डेटा और मैं चाहता हूँ कम करना 3D से 2D पर, तो शायद मैं जानना चाहता हूँ दो दिशाएँ, u(1) और u(2), जिस पर मैं प्रोजेक्ट कर सकता हूँ मेरा डेटा, तब मेरे पास क्या है कि मेरे पास हैं तीन फ़ीचर्ज़ x1, x2, x3, और सब ये एक समान हैं. सारे ये सेमेट्रिक हैं और नहीं है कोई विशेष वेरीयबल y जो मैं प्रिडिक्ट कर रहा हूँ. और इसलिए PCA नहीं है एक लिनीअर रेग्रेशन, और हालाँकि शायद वे समान दिखते हैं कुछ हद तक, वे सम्बंधित नहीं हैं, ये हैं वास्तव में बिल्कुल भिन्न अल्गोरिद्म्स. तो उम्मीद है अब आप समझ गए होंगे कि PCA क्या कर रहा है. यह जानना चाह रहा है एक कम डिमेन्शन की सरफ़ेस जिस पर प्रोजेक्ट करना है डेटा, ताकि यह स्क्वेर्ड प्रोजेक्शन एरर न्यूनतम हो जाए. न्यूनतम करने के लिए स्क्वेर्ड दूरी प्रत्यके पोईँट और स्थान जहाँ यह प्रोजेक्ट हुआ है. अगले वीडियो में, हम शुरू करेंगे बात करना कि कैसे वास्तव में ढूढे / जाने यह एक कम डिमेन्शन की सरफ़ेस जिस पर प्रोजेक्ट करना है डेटा.