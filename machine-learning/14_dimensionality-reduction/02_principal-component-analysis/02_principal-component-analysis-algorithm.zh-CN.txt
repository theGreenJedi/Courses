在这段视频中 我想介绍一下 主成成分分析(PCA)的算法 听完这段视频 你就应该知道 PCA 的实现过程 并且应用 PCA 来给你的数据降维了 在使用 PCA 之前 我们通常会有一个数据预处理的过程 拿到某组有 m 个无标签样本的训练集 一般先进行均值归一化 (mean normalization) 这一步很重要 然后 还可以进行特征缩放 (feature scaling) 这根据你的数据而定 这跟我们之前 在监督学习中提到的 均值归一和特征缩放是一样的 实际上 它们是完全一样的步骤 只不过现在我们针对的 是一系列无标签的数据 x(1) 到 x(m) 因此 对于均值归一 我们首先应该计算出 每个特征的均值 μ 然后我们用 x - μ 来替换掉 x 这样就使得 所有特征的均值为0 然后 由于不同特征的取值范围都很不一样 比如说 如果 x1 表示房子的面积 x2 表示房屋的卧室数量 这里沿用我们之前的例子 然后我们可以把每个特征 进行缩放 使其处于同一可比的范围内 同样地 跟之前的监督学习类似 我们可以用 x(i)j 减去平均值 μj 除以 sj 来替换掉第 j 个特征 x(i)j 这里的 sj 表示特征 j 的某个量度范围 因此它可以表示最大值减最小值 或者更普遍地 它可以表示特征 j 的标准差 进行完以上这些数据预处理后 接下来就正式进入 PCA 的算法部分 在之前的视频中 我们已经知道了 PCA 的原理 PCA 是在试图找到一个 低维的子空间 然后把原数据投影到子空间上 并且最小化平方投影误差的值 或者说 投影误差的平方和 也就是 这些蓝色线段 长度的平方和 因此 我们想要做的 是找到某个具体的向量 u(1) 指定这条投影线的方向 或者 在2D的情况下 我们想要找到两个向量 u(1) 和 u(2) 来定义一个投影平面 对数据进行投影 因此 我们再来回忆一下 降低数据的维度是什么意思 对于左边这个例子 我们的数据是 二维实数 x(i) 我们想要做的 是找到一系列 一维实数 z(i) 来表示我们的数据 因此这就是从二维降低到一维的意思 具体来说就是 要把数据投影到这条红线上 我们只需要一个数 来指明点在线上的位置 我把这个数称为 z 或者 z1 这里的 z 就是一个实数 等于是一个一维向量 所以 z1 就指的是 这个 1×1 的 z 向量的 第一个元素 因此 我们只需要一个数 就能指定点在线上的位置了 所以 如果这个样本 是我们的样本 x(1) 那么它可能投影到这里 如果这是我的样本 x(2) 那么它可能被投影到这里 因此这个投影点 就是 z(1) 而这个投影点 就是 z(2) 类似地 我们也可以有其他点 比如 x(3) x(4) x(5) 等等 投影到 z(1) z(2) z(3) (老师口误,译者注) 因此 PCA 要做的事儿 就是要得到一种方法 来计算两个东西 其一是计算这些向量 比如这里的 u(1) 这里的 u(1) u(2) 另一个问题是 怎样计算出这些 z 对于左边这个例子 我们要把数据从二维降到一维 对于右边这个例子 我们要把数据从三维 降到二维 从原来的 x(i) 变为现在的 z(i) 新的 z 向量是二维的 它应该是 {z1 z2] 这样的一个向量 因此 我们需要 找到某种办法 来算出这些新的变量 也就是 z1 和 z2 那么应该怎样来计算这些值呢? 实际上 这个问题 有它在数学上的推导 或者说有完整的数学证明 来解释 什么才是 u(1) u(2) z1 z2 的正确值 这个数学证明过程 是非常复杂的 同时也超出了本课程的范围 但如果你推导一遍 这个数学证明过程 你就会发现要找到 u(1) 的值 也不是一件很难的事 但是要证明 这个值就应该是正确的取值 这是别的老师的任务 不是我要讲的内容 我只简单描述一下 你要实现 PCA 要计算这些值 计算这些向量 u(1) u(2) 以及 z 向量 所需要进行的步骤 假如说 我们想要 把数据从 n 维 降低到 k 维 我们首先要做的 是计算出 这个协方差矩阵 通常是用 希腊字母大写的西格玛 ∑ 来表示 很不幸的是 这个希腊符号和 求和符号重复了 这里说的是 表示协方差矩阵的符号 而这里表示的是求和符号 希望在这部分幻灯片中 我的表达让你不要混淆 哪个是表示协方差的 ∑ 矩阵 哪个是求和符号 希望从我使用的环境中 你能看出来 计算出这个协方差矩阵后 假如我们 把它存为 Octave 中的 一个变量 叫 Sigma 我们需要做的 是计算出 Sigma 矩阵的特征向量 (eigenvectors) 在 Octave 中 你可以使用如下命令 来实现这一功能 [U,S,V] = svd(Sigma); 顺便说一下 svd 表示奇异值分解 (singular value decomposition) 这是某种更高级的 奇异值分解 这是比较高级的 线性代数的内容 你不必掌握这些 但实际上 Sigma 是一个 协方差矩阵 有很多种方法 来计算它的特征向量 如果你线性代数学得很好 或者你之前听说过 特征向量的话 那也许知道在 Octave 中 还有另一个 eig 命令 可以用来计算特征向量 实际上 svd 命令 和 eig 命令 将得到相同的结果 虽然说 svd 其实要更稳定一些 所以我一般选择用 svd 但我也有一些朋友 他们喜欢用 eig 函数 但你用 Sigma 命令 用在这里的协方差矩阵上 你会得到同样的答案 这是因为协方差均值 总满足一个数学性质 称为对称正定 (symmetric positive definite) 你不必细究 这个具体是什么意思 你只要知道 svd 和 eig 是不同的函数 但当它们用在 协方差矩阵时 可以证明它始终是满足 这个数学性质的 因此用两个命令的结果一样 好了 这就是你需要了解的一点线性代数知识 如果有任何地方不清楚的话 不必在意 你只需要知道 这条在 Octave 中 你需要执行的语句就行了 如果你用除了 Octave 或者 MATLAB 之外的其他编程环境 你要做的是找到 某个可以计算 svd 即奇异值分解的 函数库文件 在主流的编程语言中 应该有不少这样的库文件 我们可以用它们 来计算出协方差矩阵的 U S V 矩阵 我再提一下 几个细节问题 这个协方差矩阵 Sigma 应该是一个 n×n 的矩阵 一种证实的办法是 如果你看定义 这是一个 n×1 的向量 这一项有一个转置 因此是一个 1×n 的向量 两个向量相乘 得到的结果 自然是 n×n的矩阵 这是 n×1 的转置 1×n 所以这是一个 n×n 的矩阵 然后把所有这些加起来 当然还是 n×n 矩阵 然后 svd 将输出三个矩阵 分别是 U S V 你真正需要的是 U 矩阵 U 矩阵也是一个 n×n 矩阵 如果我们看 U 矩阵的列 实际上 U 矩阵的 列元素 就是我们 需要的 u(1) u(2) 等等 所以 U 矩阵是 n×n 的 如果我们想 将数据的维度从 n 降低到 k 的话 我们只需要提取前 k 列向量 这样我们就得到了 u(1) 到 u(k) 也就是我们用来投影数据的 k 个方向 剩下的步骤就很简单了 通过这个 svd 我们得到了矩阵 U 我们把它的列向量 叫做 u(1) 到 u(n) 然后我们继续 把剩下的步骤讲完 通过这个 svd 过程 我们可以得到 矩阵 U S V 我们取出 U 矩阵的 前 k 列 得到一个新的矩阵 u(1) 到 u(k) 接下来我们要做的事是 我们需要找到一个办法 对于原始数据集 x x 是一个 n 维实数 然后我们要找到一个 低维的表达 z z 是 k 维实数 所以方法是 我们把 U 矩阵的前 k 列取出来 我们构建这样一个矩阵 把 u(1) u(2) 一直到 u(k) 并列地合起来 其实就是取出 这个 U 矩阵的 前 k 列元素 因此这就是一个 n × k 维的矩阵 n × k 维的矩阵 我给这个矩阵起个名字 我把这个矩阵 叫做 U 下标 reduce 表示 U 矩阵约减后的版本 我将用它来约减我的数据 然后 计算 z 的方法是 z 等于这个 Ureduce 矩阵的转置乘以 x 或者也可以这样写 这里的转置可以换一种写法 对 U 矩阵求转置 实际上就会得到 这样一个行矩阵 从 u(1) 转置一直到 u(k) 转置 然后用它乘以 x 这样我就得到了 z 矩阵的表达 我们来标出矩阵的维度 这个矩阵的维度 应该是 k × n 这里 x 的维度 应该是 n × 1 因此这两个相乘 维度应该是 k × 1 因此 z 是 k 维的 向量 正好也就是 我们所希望的 我们所希望的 当然 这里所说的 x 可以是训练集中的样本 也可以是 交叉验证集中的样本 也可以是测试集样本 比如 如果我想处理第 i 个训练样本 那么我可以把这个写成 x(i) 这里也是 x(i) 降维得到的就是 z(i) 总结一下 这就是 PCA 的全过程 首先进行均值归一化 保证所有的特征量都是均值为0的 然后可以选择进行特征缩放 如果不同特征量的范围跨度很大的话 你确实需要进行特征缩放这一步 在以上的预处理之后 我们计算出这个协方差 Sigma 矩阵 就像这样 顺便说一下 如果你的数据 是像这样的一个矩阵给出的 也就是说你的数据是一行一行给出的 比如数据是一个大 X 矩阵 每一组训练样本用一行来表示 从 x(1) 转置 一直到 x(m) 转置 那么这个协方差矩阵 就能写成一个向量化的表示 你可以在 Octave 中实现 在 Octave中 你可以 执行 Sigma = (1/m) * X' * X; 执行 Sigma = (1/m) * X' * X; X 就是上面这个矩阵 你只要执行这个简单的语句 这用向量化的表达 计算出了 Sigma 矩阵 今天我不会在这里证明 这个计算公式的正确性 当然如果你愿意 你也可以自己推导一下 或者在 Octave 中测试一下 确保这两个式子其实是一样的 或者你动手自己算一算吧 随便你怎么证明 反正这个向量表达是正确的 然后我们可以应用 svd 函数 来计算出 U S V 矩阵 然后 我们取出 U 矩阵的 前 k 列元素 组成新的 Ureduce 矩阵 最后这个式子 给出了我们从 原来的特征 x 变成降维后的 z 的过程 另外 跟 k均值算法类似 如果你使用 PCA 的话 你的 x 应该是 n 维实数 所以 没有 x0 = 1 这一项 好了 这就是 PCA 算法 有一件事儿我没做 u(1) u(2) 等等 通过将数据 投影到 k 维的子平面上 确实使得 投影误差的平方和为最小值 我并没有证明这一点 已经超出了这门课的范围 幸运的是 PCA 算法 能够用不多的几行代码 就能实现 如果你在 Octave 或者 MATLAB 中 自己实现一下的话 你就已经获得了一种 非常有效的维度约减的算法 这就是 PCA 算法 我没有证明 u(1) u(2) 等等 以及 z 等等 这些我们在算法中的选择 确实能够使得 平方投影误差最小化 对吧? 还记得吗? 我们之前说过的 PCA 的目标就是 尽量找到一个平面 或者一条线 来对数据进行投影 这个平面或线应该最小化平方投影误差 我并没有证明这一点 这个问题的数学证明 已经超出了这门课的范围 但很幸运的一点是 PCA 算法可以在 Octave 中 用短短几行代码执行出来 如果你自己执行一遍 它一定会成功运行的 而且效果会很好 如果你亲自实现这个算法的话 你就得到了这个非常有用的降维算法 它确实能够很好地最小化 平方投影误差的值 【无边界字幕组】翻译: 所罗门捷列夫 校对: 竹二个 审核：Naplessss