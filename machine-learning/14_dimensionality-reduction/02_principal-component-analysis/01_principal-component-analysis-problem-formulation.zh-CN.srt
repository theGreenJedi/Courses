1
00:00:00,090 --> 00:00:01,010
对于降维问题来说

2
00:00:01,920 --> 00:00:03,420
目前

3
00:00:03,490 --> 00:00:04,620
最流行

4
00:00:04,690 --> 00:00:06,180
最常用的算法是

5
00:00:06,390 --> 00:00:08,460
主成分分析法 (Principal Componet Analysis, PCA）

6
00:00:10,200 --> 00:00:11,160
在这段视频中

7
00:00:11,220 --> 00:00:12,610
我想首先开始讨论

8
00:00:12,740 --> 00:00:14,240
PCA问题的公式描述

9
00:00:14,910 --> 00:00:16,090
也就是说

10
00:00:16,260 --> 00:00:18,630
我们用公式准确地精确地描述

11
00:00:18,900 --> 00:00:19,980
我们想让 PCA 来做什么

12
00:00:20,670 --> 00:00:21,820
假设 我们有这样的一个数据集

13
00:00:22,020 --> 00:00:23,050
这个数据集含有

14
00:00:23,360 --> 00:00:24,710
二维实数空间内的样本X

15
00:00:25,040 --> 00:00:26,140
假设我想

16
00:00:26,470 --> 00:00:27,640
对数据进行降维

17
00:00:27,810 --> 00:00:29,850
从二维降到一维

18
00:00:31,170 --> 00:00:32,130
也就是说 我想找到

19
00:00:32,690 --> 00:00:34,400
一条直线 将数据投影到这条直线上

20
00:00:35,140 --> 00:00:37,680
那怎么找到一条好的直线来投影这些数据呢？

21
00:00:38,730 --> 00:00:40,760
这样的一条直线也许是个不错的选择

22
00:00:41,510 --> 00:00:42,790
你认为

23
00:00:43,020 --> 00:00:43,990
这是一个不错的选择的原因是

24
00:00:44,150 --> 00:00:45,420
如果你观察

25
00:00:46,020 --> 00:00:48,230
投影到直线上的点的位置

26
00:00:48,530 --> 00:00:51,180
我将这个点 投影到直线上 得到这个点

27
00:00:51,640 --> 00:00:53,500
这点被投影到这里

28
00:00:53,640 --> 00:00:55,220
这里 这里 以及这里

29
00:00:56,120 --> 00:00:57,360
我们发现

30
00:00:57,420 --> 00:00:58,860
每个点到它们对应的

31
00:00:59,460 --> 00:01:02,520
投影到直线上的点之间的距离非常小

32
00:01:03,790 --> 00:01:06,490
也就是说 这些蓝色的

33
00:01:06,690 --> 00:01:08,210
线段非常的短

34
00:01:09,270 --> 00:01:10,260
所以 正式的说

35
00:01:10,430 --> 00:01:11,730
PCA 所做的就是

36
00:01:12,180 --> 00:01:14,320
寻找一个低维的面

37
00:01:14,340 --> 00:01:15,250
在这个例子中

38
00:01:15,330 --> 00:01:16,660
其实是一条直线

39
00:01:16,740 --> 00:01:18,260
数据投射在上面  使得

40
00:01:18,520 --> 00:01:20,130
这些蓝色小线段的平方和

41
00:01:20,360 --> 00:01:22,570
达到最小值

42
00:01:23,550 --> 00:01:24,780
这些蓝色线段的长度

43
00:01:25,020 --> 00:01:26,530
时常被叫做

44
00:01:27,100 --> 00:01:29,710
投影误差

45
00:01:29,750 --> 00:01:30,480
所以 PCA 所做的就是寻找

46
00:01:30,770 --> 00:01:31,840
一个投影平面

47
00:01:32,010 --> 00:01:33,350
对数据进行投影

48
00:01:33,480 --> 00:01:35,050
使得这个能够最小化

49
00:01:35,090 --> 00:01:37,460
另外 在应用PCA之前

50
00:01:37,960 --> 00:01:39,750
通常的做法是

51
00:01:39,960 --> 00:01:41,300
先进行均值归一化和

52
00:01:41,820 --> 00:01:43,190
特征规范化 使得

53
00:01:43,560 --> 00:01:44,760
特征 x1 和 x2

54
00:01:44,880 --> 00:01:46,770
均值为0

55
00:01:46,880 --> 00:01:48,740
数值在可比较的范围之内

56
00:01:49,110 --> 00:01:50,320
在这个例子里 我已经这么做了

57
00:01:50,490 --> 00:01:51,590
但是

58
00:01:51,680 --> 00:01:52,990
在后面 我还将回过来讨论更多有关

59
00:01:53,190 --> 00:01:54,960
PCA背景下的特征规范化和均值归一化问题

60
00:01:58,600 --> 00:01:59,420
回到这个例子

61
00:02:00,260 --> 00:02:01,470
对比

62
00:02:01,710 --> 00:02:03,300
我刚画好的红线

63
00:02:03,530 --> 00:02:05,970
这是另一条对数据进行投影的直线

64
00:02:06,810 --> 00:02:08,260
这条品红色的线

65
00:02:08,520 --> 00:02:09,260
如你所见

66
00:02:09,370 --> 00:02:10,660
你知道 用这条品红色直线

67
00:02:10,810 --> 00:02:13,920
来投影我的数据 是一个非常糟糕的方向 对吧？

68
00:02:14,090 --> 00:02:15,020
所以 如果我将数据

69
00:02:15,120 --> 00:02:16,430
投影到这条品红色的直线上

70
00:02:16,730 --> 00:02:18,050
像我们刚才做的那样

71
00:02:19,140 --> 00:02:21,240
那么投影误差

72
00:02:21,420 --> 00:02:24,460
就是这些蓝色的线段 将会很大

73
00:02:24,910 --> 00:02:25,930
所以 这些点将会

74
00:02:26,010 --> 00:02:28,170
移动很长一段距离

75
00:02:28,320 --> 00:02:29,840
才能投影到

76
00:02:30,360 --> 00:02:31,760
才能

77
00:02:31,930 --> 00:02:33,440
投影到这条品红色直线上

78
00:02:33,740 --> 00:02:35,390
因此 这就是为什么 PCA

79
00:02:36,010 --> 00:02:37,540
主成分分析法会选择

80
00:02:37,860 --> 00:02:38,840
红色的这条直线

81
00:02:39,230 --> 00:02:41,410
而不是品红色的这条直线

82
00:02:42,870 --> 00:02:45,280
我们正式一点地写出 PCA 问题

83
00:02:46,140 --> 00:02:47,660
PCA 的目标是

84
00:02:47,810 --> 00:02:49,150
如果我们将数据从二维

85
00:02:49,360 --> 00:02:50,580
降到一维的话

86
00:02:51,450 --> 00:02:52,160
我们将试着寻找

87
00:02:52,640 --> 00:02:54,590
一个向量

88
00:02:54,970 --> 00:02:56,160
向量 u(i)

89
00:02:57,150 --> 00:02:58,250
属于 n 维空间中的向量

90
00:02:58,780 --> 00:03:00,170
在这个例子中是二维的

91
00:03:01,130 --> 00:03:02,300
我们将寻找一个对数据进行投影的方向

92
00:03:02,600 --> 00:03:04,990
使得投影误差能够最小

93
00:03:05,400 --> 00:03:06,710
在这个例子里

94
00:03:07,190 --> 00:03:09,180
我们希望 PCA 寻找到

95
00:03:09,380 --> 00:03:10,590
这个向量 我将它叫做

96
00:03:10,720 --> 00:03:12,960
u(1) 所以

97
00:03:13,120 --> 00:03:14,340
当我把数据投影到

98
00:03:15,590 --> 00:03:17,620
我定义的这条直线

99
00:03:18,170 --> 00:03:19,840
通过延长这个向量得到的直线

100
00:03:20,370 --> 00:03:21,650
最后我得到非常小的

101
00:03:22,100 --> 00:03:23,400
重建误差

102
00:03:24,310 --> 00:03:25,220
看上去是这样的

103
00:03:26,180 --> 00:03:26,640
另外

104
00:03:26,840 --> 00:03:28,310
我应该指出的是 无论PCA

105
00:03:28,920 --> 00:03:32,150
给出的是这个 u(1) 还是负的 u(1) 都没关系

106
00:03:32,650 --> 00:03:33,630
如果它给出的是正的向量

107
00:03:33,890 --> 00:03:35,530
在这个方向上 这没问题

108
00:03:35,950 --> 00:03:37,910
如果给出的是相反的向量

109
00:03:38,330 --> 00:03:40,160
在相反的方向上

110
00:03:40,720 --> 00:03:43,150
也就是 -u(1)  用蓝色画出来

111
00:03:43,300 --> 00:03:44,400
无论给的是正的

112
00:03:45,120 --> 00:03:46,310
还是负的 u(1) 

113
00:03:46,440 --> 00:03:48,120
都没关系 因为

114
00:03:48,230 --> 00:03:50,030
这两个方向都定义了

115
00:03:50,110 --> 00:03:51,660
相同的红色直线

116
00:03:51,870 --> 00:03:54,430
也就是我将投影的方向

117
00:03:54,610 --> 00:03:56,300
这就是将

118
00:03:56,680 --> 00:03:58,120
二维数据降到一维的例子

119
00:03:58,920 --> 00:04:00,220
更一般的情况是

120
00:04:00,350 --> 00:04:01,680
我们有 n 维的数据

121
00:04:01,840 --> 00:04:03,790
想降到 k 维

122
00:04:04,970 --> 00:04:06,010
在这种情况下

123
00:04:06,160 --> 00:04:07,450
我们不仅仅只寻找单个的向量

124
00:04:07,940 --> 00:04:09,020
来对数据进行投影

125
00:04:09,320 --> 00:04:10,660
我们要找到 k 个方向

126
00:04:11,520 --> 00:04:12,420
来对数据进行投影

127
00:04:13,290 --> 00:04:15,680
从而最小化投影误差

128
00:04:16,440 --> 00:04:17,100
这是一个例子

129
00:04:17,480 --> 00:04:19,100
如果我有一些三维数据点

130
00:04:19,390 --> 00:04:21,030
比如说像这样的

131
00:04:21,290 --> 00:04:22,620
我想要做的是

132
00:04:23,880 --> 00:04:26,120
是寻找两个向量

133
00:04:27,020 --> 00:04:28,180
我将这些向量叫做...

134
00:04:29,080 --> 00:04:30,530
我们用红线画出来

135
00:04:30,710 --> 00:04:32,210
我要寻找两个向量

136
00:04:32,580 --> 00:04:33,580
从原点延伸出来

137
00:04:34,490 --> 00:04:37,280
这是 u(1) 

138
00:04:37,580 --> 00:04:39,800
这是第二个向量 u(2) 

139
00:04:40,180 --> 00:04:42,110
这两个向量一起

140
00:04:42,320 --> 00:04:43,850
定义了一个平面

141
00:04:44,400 --> 00:04:45,590
或者说 定义了一个二维面

142
00:04:46,790 --> 00:04:47,900
就像这样

143
00:04:48,270 --> 00:04:51,140
二维平面 我将把数据投影到上面

144
00:04:52,050 --> 00:04:52,900
对于你们其中

145
00:04:53,080 --> 00:04:54,980
熟悉线性代数的人来说

146
00:04:55,170 --> 00:04:56,010
对于你们其中真的

147
00:04:56,230 --> 00:04:57,380
精通线性代数的人来说

148
00:04:57,780 --> 00:04:58,820
对这个正式的定义是

149
00:04:59,230 --> 00:05:00,500
我们将寻找一组向量

150
00:05:00,610 --> 00:05:01,680
u(1) u(2) 也许

151
00:05:01,800 --> 00:05:03,370
一直到 u(k) 

152
00:05:03,460 --> 00:05:04,490
我们将要做的是

153
00:05:04,980 --> 00:05:06,600
将数据投影到

154
00:05:06,830 --> 00:05:09,520
这 k 个向量展开的线性子空间上

155
00:05:10,520 --> 00:05:11,570
但是如果你不熟悉

156
00:05:12,070 --> 00:05:13,200
线性代数 那就想成是

157
00:05:13,400 --> 00:05:14,790
寻找 k 个方向

158
00:05:15,510 --> 00:05:18,380
而不是只寻找一个方向 对数据进行投影

159
00:05:18,740 --> 00:05:19,950
所以 寻找一个 k 维的平面

160
00:05:20,610 --> 00:05:21,560
在这里是寻找二维的平面

161
00:05:22,370 --> 00:05:23,870
如图所示

162
00:05:24,040 --> 00:05:25,340
这里我们用

163
00:05:26,800 --> 00:05:29,700
k 个方向来定义平面中这些点的位置

164
00:05:30,410 --> 00:05:31,690
这就是为什么 对于PCA

165
00:05:31,950 --> 00:05:34,440
我们要寻找 k 个向量来对数据进行投影

166
00:05:35,030 --> 00:05:36,920
因此 更正式一点的说

167
00:05:37,050 --> 00:05:38,430
在PCA中 我们想做的就是

168
00:05:38,700 --> 00:05:40,400
寻找到这种方式

169
00:05:40,590 --> 00:05:41,940
对数据进行投影

170
00:05:42,040 --> 00:05:43,570
进而最小化投影距离

171
00:05:43,850 --> 00:05:46,210
也就是数据点和投影后的点之间的距离

172
00:05:47,060 --> 00:05:48,060
在这个三维的例子里

173
00:05:48,560 --> 00:05:50,100
给定一个点

174
00:05:50,280 --> 00:05:51,450
我们想将这个点

175
00:05:51,980 --> 00:05:53,950
投影到二维平面上

176
00:05:55,560 --> 00:05:56,580
当你完成了那个

177
00:05:57,280 --> 00:05:58,690
因此投影误差就是

178
00:05:58,870 --> 00:06:00,830
也就是

179
00:06:01,440 --> 00:06:03,160
这点与投影到

180
00:06:03,970 --> 00:06:05,360
二维平面之后的点之间的距离

181
00:06:05,880 --> 00:06:06,990
因此 PCA 做的就是

182
00:06:07,070 --> 00:06:08,480
寻找一条直线

183
00:06:08,620 --> 00:06:10,430
或者平面 诸如此类等等

184
00:06:10,660 --> 00:06:11,810
对数据进行投影

185
00:06:12,010 --> 00:06:14,160
来最小化平方投影

186
00:06:15,100 --> 00:06:17,430
90度的或者正交的投影误差

187
00:06:18,100 --> 00:06:19,240
最后

188
00:06:19,280 --> 00:06:20,060
一个我有时会被问到的问题是

189
00:06:20,280 --> 00:06:22,100
PCA 和线性回归有怎么样的关系？

190
00:06:22,350 --> 00:06:24,180
因为当我解释 PCA 的时候

191
00:06:24,600 --> 00:06:25,780
我有时候会以

192
00:06:26,190 --> 00:06:28,720
画出这样的图 看上去有点像线性回归

193
00:06:30,790 --> 00:06:32,130
但是 事实是

194
00:06:32,370 --> 00:06:33,950
PCA不是线性回归

195
00:06:34,350 --> 00:06:37,560
尽管看上去有一些相似 但是它们确实是两种不同的算法

196
00:06:38,680 --> 00:06:39,680
如果我们做线性回归

197
00:06:40,770 --> 00:06:42,170
我们做的是

198
00:06:42,270 --> 00:06:42,940
看左边 我们想要

199
00:06:43,230 --> 00:06:44,400
在给定某个输入特征 x 的情况下

200
00:06:44,540 --> 00:06:45,830
预测某个变量 y 的数值

201
00:06:46,120 --> 00:06:47,330
因此 对于线性回归

202
00:06:47,570 --> 00:06:48,760
我们想做的是

203
00:06:49,150 --> 00:06:50,350
拟合一条直线

204
00:06:51,900 --> 00:06:52,970
来最小化

205
00:06:53,390 --> 00:06:56,160
点和直线之间的平方误差

206
00:06:56,360 --> 00:06:57,270
所以我们要最小化的是

207
00:06:57,900 --> 00:07:00,320
这些蓝线幅值的平方

208
00:07:00,790 --> 00:07:02,240
注意我画的这些

209
00:07:02,550 --> 00:07:04,650
蓝色的垂直线

210
00:07:05,150 --> 00:07:06,500
这是垂直距离

211
00:07:06,520 --> 00:07:07,700
它是某个点

212
00:07:08,090 --> 00:07:10,470
与通过假设的得到的其预测值之间的距离

213
00:07:10,510 --> 00:07:13,100
与此想反

214
00:07:13,190 --> 00:07:14,170
PCA要做的是

215
00:07:14,320 --> 00:07:16,890
最小化这些蓝色直线的幅值

216
00:07:17,460 --> 00:07:19,550
倾斜地画出来的

217
00:07:19,980 --> 00:07:21,590
这实际上是最短的

218
00:07:22,090 --> 00:07:23,900
直角距离

219
00:07:24,050 --> 00:07:26,620
也就是点 x

220
00:07:27,000 --> 00:07:28,320
跟红色直线之间的最短距离

221
00:07:28,530 --> 00:07:29,870
这是一种非常不同的效果

222
00:07:30,600 --> 00:07:32,050
取决于数据集

223
00:07:32,400 --> 00:07:34,610
更更更一般的是

224
00:07:34,760 --> 00:07:35,890
当你做

225
00:07:36,150 --> 00:07:37,740
线性回归的时候 有一个

226
00:07:38,160 --> 00:07:39,810
特别的变量 y

227
00:07:40,000 --> 00:07:41,130
是我们将要预测的

228
00:07:41,560 --> 00:07:43,610
线性回归所要做的就是

229
00:07:44,060 --> 00:07:45,060
用 x 的所有的值来

230
00:07:45,260 --> 00:07:46,930
预测 y

231
00:07:47,210 --> 00:07:48,920
然而在 PCA 中

232
00:07:49,230 --> 00:07:50,200
没有这么一个特别的或者

233
00:07:50,400 --> 00:07:51,900
特殊的变量 y

234
00:07:52,040 --> 00:07:52,770
是我们要预测的

235
00:07:53,230 --> 00:07:54,100
我们所拥有的是

236
00:07:54,740 --> 00:07:56,130
特征x1 x2 等

237
00:07:56,280 --> 00:07:57,830
一直到xn

238
00:07:57,940 --> 00:07:59,460
所有的这些特征都是被同样地对待

239
00:08:00,360 --> 00:08:01,560
因此 它们中没有一个是特殊的

240
00:08:02,980 --> 00:08:05,180
最后一个例子

241
00:08:05,400 --> 00:08:07,220
如果我有三维数据

242
00:08:07,390 --> 00:08:08,660
我要将这些数据

243
00:08:08,820 --> 00:08:10,110
从三维降到二维

244
00:08:10,380 --> 00:08:11,630
我就要找到两个方向

245
00:08:12,780 --> 00:08:14,110
也就是 u(1) 和 u(2)

246
00:08:14,920 --> 00:08:16,030
将数据投影到它们上面

247
00:08:16,960 --> 00:08:17,840
然后我得到的是

248
00:08:18,390 --> 00:08:20,190
我有3个特征 x1 x2

249
00:08:20,860 --> 00:08:22,410
x3 所有的这些都是被同样地对待

250
00:08:22,780 --> 00:08:24,100
这些都是被均等地对待

251
00:08:25,020 --> 00:08:26,240
没有特殊的变量

252
00:08:26,740 --> 00:08:27,740
y 需要被预测

253
00:08:28,870 --> 00:08:30,320
因此 PCA

254
00:08:30,650 --> 00:08:33,210
不是线性回归

255
00:08:34,020 --> 00:08:35,870
尽管有一定程度的相似性

256
00:08:36,040 --> 00:08:37,260
使得它们看上去是有关联的

257
00:08:37,600 --> 00:08:41,580
但它们实际上是非常不同的算法

258
00:08:41,810 --> 00:08:43,360
因此 希望你们能理解

259
00:08:43,630 --> 00:08:44,960
PCA 是做什么的

260
00:08:45,220 --> 00:08:46,520
它是寻找到一个低维的平面

261
00:08:47,130 --> 00:08:48,290
对数据进行投影

262
00:08:48,680 --> 00:08:50,230
以便

263
00:08:50,450 --> 00:08:52,420
最小化投影误差的平方

264
00:08:52,650 --> 00:08:54,140
最小化每个点

265
00:08:54,390 --> 00:08:56,660
与投影后的对应点之间的距离的平方值

266
00:08:57,800 --> 00:08:59,040
在下一段视频中

267
00:08:59,340 --> 00:09:00,490
我们将开始讨论

268
00:09:00,900 --> 00:09:02,350
如何真正地找到这个低维平面

269
00:09:03,210 --> 00:09:04,470
来对数据进行投影 【教育无边界字幕组】翻译：柳桦 校对/审核：所罗门捷列夫