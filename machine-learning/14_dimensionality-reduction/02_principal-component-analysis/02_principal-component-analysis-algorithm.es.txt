En este video, me gustaría hablarle sobre el algoritmo de análisis de componente principal y al final de este video usted sabrá cómo poner en práctica el ACP y usarlo para reducir la dimensión de sus datos. Antes de aplicar el ACP, hay una etapa de pre-procesamiento de datos que siempre se debe hacer. Dados los conjuntos de entrenamiento de ejemplos m no etiquetados, es importante realizar siempre la normalización de media y después, dependiendo de sus datos, tal vez realizar una escalación de variables. Esto es muy similar a la normalización de media y al proceso de escalación de variables que tenemos para aprendizaje supervisado. De hecho, es exactamente el mismo procedimiento, excepto que lo estamos haciendo ahora en nuestros datos sin valores asignados, de x1 a xm. Entonces, para la normalización de media, primero computamos la media de cada variable y a continuación, reemplazamos cada variable x con x menos su media, esto hace que cada variable ahora tenga exactamente una media de cero. Las diferentes variables tienen escalas muy distintas. Por ejemplo, si x1 es el tamaño de una casa, y x2 es el número de habitaciones, para usar nuestro ejemplo anterior, ahora escalamos también cada variable para tener una gama de valores comparables. Y así, similar a lo que teníamos en el aprendizaje supervisado, tomaríamos xi subíndice j, es la variable j, y la resto de la media, eso es lo que tenemos en la parte superior y luego dividimos entre Sj. Aquí, Sj es cierta medida de la gama de valores de la variable j.  Así que podría ser el valor máximo menos el mínimo, o más comúnmente, es la desviación estándar de la variable j. Después de haber hecho este tipo procesamiento de datos previo, esto es lo que hace el algoritmo ACP: Vimos en el video anterior que lo que el ACP hace es que trata de encontrar un subespacio de dimensiones inferiores sobre el cual proyectar los datos para minimizar los errores de proyección al cuadrado, la suma de los errores de proyección al cuadrado, así como el cuadrado de la longitud de estos segmentos de líneas azules y lo que queríamos hacer específicamente es encontrar un vector, u1, el cual especifica esa dirección o en el caso 2D queremos encontrar dos vectores, u1 y u2, para definir esta superficie sobre la cual proyectar los datos. Entonces, sólo como un rápido recordatorio de lo que significa la reducción de dimensión de los datos, para este ejemplo de la izquierda, se nos proporcionaron los ejemplos xi, que están en R2. Y lo que nos gustaría hacer, es encontrar un conjunto de números zi en "R" para representar nuestros datos. Así que eso es lo que significa la reducción de 2D a 1D. Específicamente, mediante la proyección de datos sobre esta línea roja, sólo necesitamos un número para especificar la posición de los puntos sobre la línea. Así que voy a llamar a ese número z o z1. Z aquí es un número real, por lo que es como un vector unidimensional. Así z1 sólo se refiere al primer componente de esta matriz de uno por uno, o a este vector unidimensional. Y por eso necesitamos un solo número para especificar la posición de un punto. Así que si este ejemplo fuera mi ejemplo x1, quizá se mapee  aquí y si este ejemplo fuera x2 tal vez ese ejemplo se mapearía aquí. Y entonces este punto aquí será z1 y este punto aquí será z2 y de manera similar, tendríamos esos otros puntos para estos, tal vez x3, x4, x5 se mapearían a z1, z2,z3. Entonces, lo que el ACP tiene que hacer es que tenemos que encontrar una manera de calcular dos cosas: la primera es computar estos vectores, u1, y en este caso u1 y u2 y la otra es cómo podemos computamos estos números z. Así que en el ejemplo de la izquierda estamos reduciendo los datos de 2D a 1D. En el ejemplo de la derecha, estaríamos reduciendo los datos de 3 dimensiones, xi en R3 a zi, que ahora es de dos dimensiones. Así que estos vectores z serían ahora de dos dimensiones. Así que sería z1, z2 como tal y tenemos que computar estas nuevas representaciones, z1 y z2 de los datos también. Entonces, ¿cómo calculamos todas estas cantidades? Se puede usar una derivación matemática, o la demostración matemática para saber cuál es el valor correcto para u1, u2, z1, z2 y así sucesivamente. Esa demostración matemática es muy complicada y más allá del alcance del curso. Pero una vez que has hecho toda la derivación matemática, el procedimiento para encontrar realmente el valor de u1 que buscamos no es tan difícil, a pesar de que la demostración matemática de que este valor es el correcto se trata de algo más complejo y más de lo que me quiero adentrar. Pero permítame describir el procedimiento específico que tiene que implementar para para calcular todos estas cosas, los vectores, u1, u2, el vector z.  Aquí está el procedimiento: Digamos que queremos reducir los datos, de "n" dimensiones a "k" dimensiones. Lo que vamos a hacer primero es calcular algo que se llama la matriz de covarianza y esta matriz se denota comúnmente por medio de esta letra griega, que es la «sigma» mayúscula del «alfa»beto griego. Es un poco lamentable que la «sigma» del «alfa»beto griego luzca exactamente igual que los símbolos de suma. Así que esta «sigma» del «alfa»beto griego se usa para denotar una matriz y este es un símbolo de suma. Así que espero que en estas diapositivas no haya ambigüedad sobre cuál símbolo se refiere a la matriz de covarianza «sigma» y cual al símbolo de suma y espero que sea claro a partir del contexto cuando esté usando cada uno. ¿Cómo se puede calcular esta matriz «sigma»? Digamos que queremos almacenarla en una variable de Octave llamada «sigma». Lo que tenemos que hacer es calcular algo que se llama la vectores propios de la matriz «sigma». En Octave, la forma de hacer esto es usando este comando, [U, S, V]= svd («sigma»), SVD, por cierto, significa descomposición de valor singular. Esta es álgebra lineal mucho más avanzada, más de lo que realmente necesita saber, pero resulta que cuando «sigma» es una matriz de covarianza existen algunas maneras de calcular estos vectores propios y si usted es un experto en álgebra lineal y si ha oído hablar de los vectores propios antes, usted quizá sepa que hay otra función de Octave llamada "eig", que puede usarse también para calcular la misma cosa. Y resulta que la función SVD y la función "eig" le darán los mismos vectores propios, aunque SVD es un poco más estable numéricamente. Así que yo tiendo a usar SVD, aunque tengo un par de amigos que usan la función "eig" para hacer esto también pero cuando usted aplica esto a una matriz de covarianza «sigma», le da el mismo resultado. Esto es porque la matriz de covarianza siempre satisface una propiedad matemática llamada semidefinita positiva simétrica. Usted realmente no necesita saber lo que eso significa, sólo que las funciones SVD y "eig" son diferentes pero cuando se aplican a una matriz de covarianza que puede ser probada para satisfacer siempre esta propiedad matemática, estas funciones siempre darán el mismo resultado. Bien, eso fue probablemente mucho más álgebra lineal de lo que necesitaba saber. En caso de que nada de eso tuviera sentido, no se preocupe por ello, todo lo que necesita saber es que este comando que debe implementar en Octave. Y si usted está implementándolo en un lenguaje diferente a Octave o MATLAB, lo que debe hacer es encontrar la biblioteca de álgebra lineal numérica que puede calcular la SVD o la descomposición en valor singular, y hay muchas de estas bibliotecas para probablemente todos los principales lenguajes de programación. Se puede utilizar eso para calcular las matrices u, s, y d, de la matriz de covarianza «sigma». Entonces, sólo para añadir algunos detalles más, esta «sigma» de matriz de covarianza será una matriz nxn. Una manera de ver esto es que si nos fijamos en la definición, este es un vector nx1 y esto que se muestra aquí, xi traspuesta es 1xn, entonces el producto de estas dos cosas va a ser una matriz nxn. Y cuando sumamos todo esto, todavía tiene una matriz nxn. Y los resultados de la SVD son las tres matrices, u, s, y v.  Lo que realmente necesita de la SVD es la matriz u. La matriz u también será una matriz nxn y si nos fijamos en las columnas de la matriz u, resulta que las columnas de la matriz u serán exactamente esos vectores, u1, u2 y así sucesivamente, que es lo que queremos. Así que u, será una matriz nxn y si queremos reducir los datos, de "n" dimensiones hasta "k" dimensiones, entonces lo que tenemos que hacer es tomar los primeros vectores k que nos dan u1 hasta uk, que son las direcciones k en las que queremos proyectar los datos. Así que, para describir el resto del procedimiento a partir de esta rutina de álgebra lineal numérica de SVD, lo que tenemos es esta matriz u  y voy a nombrar estas columnas de u1-un. Así que, para concluir la descripción del resto del procedimiento a partir de la rutina de álgebra lineal numérica SVD tenemos estas matrices u, s, y d, aquí está la matriz u y vamos a utilizar las primeras columnas k de esta matriz para obtener u1 hasta uk. Ahora, otra cosa que necesitamos es encontrar la forma de tomar mi grupo de datos original x que está en Rn y encontrar una representación dimensional menor, z , que está en Rk para estos datos. Así que la forma en que haremos esto será tomar las primeras columnas k de la matriz u, voy a construir esta matriz, apilando u1, u2 y así sucesivamente hasta uk en columnas. Básicamente se trata de tomar, ya sabe, esta parte de la matriz, las primeras columnas k de esta matriz. Y entonces esto es una matriz nxk, le voy a dar a esta matriz un nombre, voy a llamar a esta matriz U, con el subíndice "reducir", una especie de versión reducida de la matriz U, quizá, voy a usarla para reducir la dimensión de mis datos. Y la forma en que voy a calcular Z es que Z será igual a esta matriz reducida u, transpuesta transferencia negativo “X” de «theta». O de forma alternativa, anotar lo que significa esta transposición. Cuando tomo esta transposición de esta matriz U "reducir", con lo que voy a terminar es con estos vectores ahora en filas, tengo U1 transpuesta hasta uk transpuesta, elevo esto a la X y así es como me obtengo mi vector Z. Sólo para asegurarse de que estas dimensiones tienen sentido, esta matriz aquí va a ser kxn y «x» aquí va a ser nx1, de modo que el producto aquí será kx1 y así, z es de "k" dimensiones, es un vector k dimensional, que es exactamente lo que queríamos. Y, por supuesto, estas «x’s»  aquí ahora pueden ser ejemplos en nuestro conjunto de entrenamiento, pueden ser ejemplos en nuestro grupo de validación cruzada, pueden ser ejemplos en el conjunto de prueba, y por ejemplo, si quisiera tomar el ejemplo de entrenamiento i, puedo escribir esto como xi, xi y esto es lo que me va a dar Zi allá. Así que, para resumir, aquí está el algoritmo de ACP en una diapositiva. Después de la normalización de media, para garantizar que cada variable es media cero, y opcionalmente, escalación de característcas. -Realmente debe hacer escalación de variables si sus variables adquieren rangos de valores muy diferentes.- Después de este procesamiento previo, calculamos la matriz de covarianza «sigma», de este modo y por cierto, si sus datos se dan en forma de matriz como esta, si sus datos se presentan en filas de esta forma, si usted tiene una matriz X, que son todos sus grupos de entrenamiento escritos en filas en las que tenemos x1 transpuesta hasta xm transpuesta, esta matriz de covarianza «sigma» tiene en realidad una implementación de vectorización buena. La puede aplicar en Octave, incluso se puede ejecutar «sigma» = 1 sobre m a la x, que es esta matriz aquí, transpuesta a la x, y esta simple expresión, es la aplicación de vectorización de cómo calcular la matriz «sigma». No voy a comprobar si esta es la vectorización correcta pero si desea, puede probar esto numéricamente usted mismo con Octave, asegurándose de que tanto esto, como las implementaciones den las mismas respuestas o puede tratar de probarlo usted mismo matemáticamente, puede hacerlo de cualquier modo pero esta es la implementación de vectorización correcta de cómo calcular «sigma». A continuación podemos aplicar la rutina SVD para obtener u, s, y d, posteriormente, tomamos las primeras columnas k de la matriz para obtener U "reducir" y finalmente, esto define la forma en que vamos a partir de una variable del vector x a esta representación z de dimensiones reducidas. De forma similar a Media k si aplica el ACP, la forma de aplicarlo es con vectores x en Rn ¿cierto?, por lo tanto, esto no se hace con la convención x0=1. Entonces, eso fue el algoritmo de ACP, una cosa que no hice fue la demostración matemática de que este procedimiento realmente permite la proyección de los datos en el subespacio de k dimensiones en la superficie k dimensional que en realidad minimiza el error de proyección de cuadrados, la prueba matemática de esto está más allá del alcance de este curso. Afortunadamente, el algoritmo de ACP puede ser implementado en no demasiadas líneas de código Octave y si lo implementa en Octave o MATLAB, en realidad obtendrá un algoritmo para reducción de dimensionalidad muy efectivo. Ese fue el algoritmo de ACP, una cosa que no hice fue la demostración matemática de que U1 y U2 y así sucesivamente, de Z y estos elementos obtenidos por este procedimiento son realmente las opciones que reducirían al mínimo estos errores de proyección al cuadrado. Bien, recuerde que dijimos que lo que ACP intenta hacer es tratar de encontrar una superficie o línea sobre la cual proyectar los datos y así reducir al mínimo este error de proyección al cuadrado. Así que no demostré que esto se lograba realmente y la demostración matemática de esto está más allá del alcance de este curso. Afortunadamente, el algoritmo de ACP puede implementarse en no demasiadas líneas de código Octave y si lo implementa, esto es lo que realmente funciona o funcionará bien y si implementa este algoritmo, obtendrá un algoritmo muy eficaz de reducción de dimensionalidad que funciona correctamente para minimizar este error de proyección al cuadrado.