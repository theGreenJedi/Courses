1
00:00:00,340 --> 00:00:01,410
在这段视频中

2
00:00:01,550 --> 00:00:03,020
我想介绍一下

3
00:00:03,340 --> 00:00:04,570
主成成分分析(PCA)的算法

4
00:00:05,600 --> 00:00:06,560
听完这段视频

5
00:00:06,710 --> 00:00:09,200
你就应该知道 PCA 的实现过程

6
00:00:10,170 --> 00:00:12,540
并且应用 PCA 来给你的数据降维了

7
00:00:13,100 --> 00:00:14,690
在使用 PCA 之前

8
00:00:14,800 --> 00:00:17,760
我们通常会有一个数据预处理的过程

9
00:00:18,510 --> 00:00:20,220
拿到某组有 m 个无标签样本的训练集

10
00:00:20,520 --> 00:00:22,290
一般先进行均值归一化 (mean normalization)

11
00:00:22,600 --> 00:00:24,070
这一步很重要

12
00:00:25,330 --> 00:00:26,140
然后 还可以进行特征缩放 (feature scaling)

13
00:00:26,840 --> 00:00:28,540
这根据你的数据而定

14
00:00:29,620 --> 00:00:30,950
这跟我们之前

15
00:00:31,650 --> 00:00:33,250
在监督学习中提到的

16
00:00:34,080 --> 00:00:36,580
均值归一和特征缩放是一样的

17
00:00:36,910 --> 00:00:38,240
实际上

18
00:00:38,390 --> 00:00:40,160
它们是完全一样的步骤

19
00:00:40,310 --> 00:00:41,790
只不过现在我们针对的

20
00:00:42,930 --> 00:00:43,670
是一系列无标签的数据 x(1) 到 x(m)

21
00:00:44,180 --> 00:00:45,530
因此 对于均值归一

22
00:00:45,720 --> 00:00:47,080
我们首先应该计算出

23
00:00:47,390 --> 00:00:49,070
每个特征的均值 μ

24
00:00:49,340 --> 00:00:50,900
然后我们用 x - μ

25
00:00:51,150 --> 00:00:52,680
来替换掉 x

26
00:00:52,810 --> 00:00:54,120
这样就使得

27
00:00:54,520 --> 00:00:57,450
所有特征的均值为0

28
00:00:58,690 --> 00:01:00,690
然后 由于不同特征的取值范围都很不一样

29
00:01:01,540 --> 00:01:03,050
比如说 

30
00:01:03,080 --> 00:01:04,060
如果 x1 表示房子的面积

31
00:01:04,100 --> 00:01:05,390
x2 表示房屋的卧室数量

32
00:01:05,580 --> 00:01:07,370
这里沿用我们之前的例子

33
00:01:07,480 --> 00:01:08,680
然后我们可以把每个特征

34
00:01:09,130 --> 00:01:10,540
进行缩放 使其处于同一可比的范围内

35
00:01:10,980 --> 00:01:12,490
同样地

36
00:01:12,680 --> 00:01:13,860
跟之前的监督学习类似

37
00:01:14,060 --> 00:01:16,200
我们可以用 x(i)j

38
00:01:16,680 --> 00:01:17,620
减去平均值 μj

39
00:01:23,250 --> 00:01:25,530
除以 sj

40
00:01:25,890 --> 00:01:27,610
来替换掉第 j 个特征 x(i)j

41
00:01:28,370 --> 00:01:29,520
这里的 sj 表示特征 j 的某个量度范围

42
00:01:29,610 --> 00:01:30,020
因此它可以表示最大值减最小值

43
00:01:30,080 --> 00:01:31,310
或者更普遍地

44
00:01:31,890 --> 00:01:33,540
它可以表示特征 j 的标准差

45
00:01:33,640 --> 00:01:35,520
进行完以上这些数据预处理后

46
00:01:36,230 --> 00:01:39,480
接下来就正式进入 PCA 的算法部分

47
00:01:40,620 --> 00:01:41,630
在之前的视频中

48
00:01:41,960 --> 00:01:43,050
我们已经知道了 PCA 的原理

49
00:01:43,170 --> 00:01:44,520
PCA 是在试图找到一个

50
00:01:44,790 --> 00:01:46,080
低维的子空间

51
00:01:46,170 --> 00:01:47,500
然后把原数据投影到子空间上

52
00:01:47,650 --> 00:01:49,780
并且最小化平方投影误差的值

53
00:01:50,540 --> 00:01:51,660
或者说

54
00:01:51,740 --> 00:01:53,080
 投影误差的平方和

55
00:01:53,420 --> 00:01:54,800
也就是 这些蓝色线段

56
00:01:54,870 --> 00:01:56,790
长度的平方和

57
00:01:57,110 --> 00:01:58,510
因此 我们想要做的

58
00:01:59,210 --> 00:02:02,730
是找到某个具体的向量 u(1)

59
00:02:03,280 --> 00:02:04,750
指定这条投影线的方向

60
00:02:05,040 --> 00:02:06,630
或者 在2D的情况下

61
00:02:06,880 --> 00:02:08,760
我们想要找到两个向量

62
00:02:10,640 --> 00:02:12,980
u(1) 和 u(2)

63
00:02:13,590 --> 00:02:14,610
来定义一个投影平面 对数据进行投影

64
00:02:16,620 --> 00:02:17,920
因此

65
00:02:18,040 --> 00:02:19,160
我们再来回忆一下

66
00:02:19,730 --> 00:02:20,820
降低数据的维度是什么意思

67
00:02:21,490 --> 00:02:22,430
对于左边这个例子

68
00:02:22,470 --> 00:02:23,560
我们的数据是

69
00:02:23,680 --> 00:02:26,010
二维实数 x(i)

70
00:02:26,300 --> 00:02:28,390
我们想要做的

71
00:02:28,660 --> 00:02:29,500
是找到一系列

72
00:02:29,970 --> 00:02:32,400
一维实数 z(i)

73
00:02:33,000 --> 00:02:34,950
来表示我们的数据

74
00:02:36,000 --> 00:02:37,820
因此这就是从二维降低到一维的意思

75
00:02:39,020 --> 00:02:41,450
具体来说就是

76
00:02:42,710 --> 00:02:44,080
要把数据投影到这条红线上

77
00:02:44,800 --> 00:02:46,320
我们只需要一个数

78
00:02:46,450 --> 00:02:48,340
来指明点在线上的位置

79
00:02:48,590 --> 00:02:49,380
我把这个数称为

80
00:02:50,700 --> 00:02:51,830
z 或者 z1

81
00:02:52,020 --> 00:02:54,850
这里的 z 就是一个实数 等于是一个一维向量

82
00:02:55,380 --> 00:02:56,650
所以 z1 就指的是

83
00:02:56,690 --> 00:02:58,080
这个 1×1 的 z 向量的

84
00:02:58,280 --> 00:03:00,430
第一个元素

85
00:03:01,670 --> 00:03:03,170
因此 我们只需要一个数

86
00:03:03,490 --> 00:03:05,590
就能指定点在线上的位置了

87
00:03:06,330 --> 00:03:07,940
所以 如果这个样本

88
00:03:08,460 --> 00:03:09,510
是我们的样本 x(1)

89
00:03:10,610 --> 00:03:13,160
那么它可能投影到这里

90
00:03:13,900 --> 00:03:15,450
如果这是我的样本 x(2)

91
00:03:15,680 --> 00:03:17,250
那么它可能被投影到这里

92
00:03:17,530 --> 00:03:18,790
因此这个投影点

93
00:03:19,060 --> 00:03:20,400
就是 z(1)

94
00:03:20,840 --> 00:03:21,920
而这个投影点

95
00:03:22,080 --> 00:03:24,240
就是 z(2)

96
00:03:24,620 --> 00:03:26,410
类似地 我们也可以有其他点

97
00:03:26,840 --> 00:03:30,230
比如 x(3)

98
00:03:30,510 --> 00:03:32,550
x(4) x(5) 等等 投影到 z(1) z(2) z(3) (老师口误,译者注)

99
00:03:34,360 --> 00:03:35,940
因此 PCA 要做的事儿

100
00:03:36,050 --> 00:03:36,830
就是要得到一种方法

101
00:03:36,930 --> 00:03:38,920
来计算两个东西

102
00:03:39,310 --> 00:03:40,710
其一是计算这些向量

103
00:03:41,830 --> 00:03:44,970
比如这里的 u(1) 这里的 u(1) u(2)

104
00:03:45,230 --> 00:03:46,880
另一个问题是

105
00:03:47,130 --> 00:03:48,140
怎样计算出这些 z

106
00:03:49,360 --> 00:03:51,200
对于左边这个例子

107
00:03:51,430 --> 00:03:53,910
我们要把数据从二维降到一维

108
00:03:55,290 --> 00:03:56,100
对于右边这个例子

109
00:03:56,510 --> 00:03:58,100
我们要把数据从三维

110
00:03:58,450 --> 00:04:00,600
降到二维

111
00:04:00,710 --> 00:04:04,840
从原来的 x(i) 变为现在的 z(i)

112
00:04:05,390 --> 00:04:07,790
新的 z 向量是二维的

113
00:04:08,450 --> 00:04:09,590
它应该是 {z1 z2] 这样的一个向量

114
00:04:10,150 --> 00:04:11,410
因此 我们需要

115
00:04:11,640 --> 00:04:12,940
找到某种办法

116
00:04:13,670 --> 00:04:15,410
来算出这些新的变量

117
00:04:15,570 --> 00:04:17,350
也就是 z1 和 z2

118
00:04:18,280 --> 00:04:20,350
那么应该怎样来计算这些值呢?

119
00:04:20,520 --> 00:04:21,520
实际上 这个问题

120
00:04:22,490 --> 00:04:23,660
有它在数学上的推导

121
00:04:24,300 --> 00:04:26,020
或者说有完整的数学证明

122
00:04:26,090 --> 00:04:27,970
来解释 什么才是 u(1) u(2) z1 z2

123
00:04:28,290 --> 00:04:29,480
的正确值

124
00:04:29,690 --> 00:04:31,230
这个数学证明过程

125
00:04:31,480 --> 00:04:32,890
是非常复杂的

126
00:04:32,950 --> 00:04:34,620
同时也超出了本课程的范围

127
00:04:35,280 --> 00:04:37,290
但如果你推导一遍

128
00:04:37,590 --> 00:04:38,590
这个数学证明过程

129
00:04:39,350 --> 00:04:40,570
你就会发现要找到

130
00:04:41,200 --> 00:04:42,210
u(1) 的值

131
00:04:42,950 --> 00:04:43,950
也不是一件很难的事

132
00:04:44,180 --> 00:04:45,640
但是要证明

133
00:04:45,840 --> 00:04:46,940
这个值就应该是正确的取值

134
00:04:47,260 --> 00:04:48,450
这是别的老师的任务

135
00:04:48,700 --> 00:04:49,960
不是我要讲的内容

136
00:04:50,880 --> 00:04:52,070
我只简单描述一下

137
00:04:52,480 --> 00:04:53,830
你要实现 PCA 

138
00:04:53,960 --> 00:04:55,250
要计算这些值

139
00:04:55,440 --> 00:04:56,450
计算这些向量 u(1) u(2)

140
00:04:56,570 --> 00:04:57,840
以及 z 向量

141
00:04:58,910 --> 00:05:00,980
所需要进行的步骤

142
00:05:02,070 --> 00:05:02,970
假如说 我们想要

143
00:05:03,170 --> 00:05:04,220
把数据从 n 维

144
00:05:04,840 --> 00:05:05,760
降低到 k 维

145
00:05:06,760 --> 00:05:07,640
我们首先要做的

146
00:05:07,900 --> 00:05:09,400
是计算出

147
00:05:09,830 --> 00:05:11,140
这个协方差矩阵

148
00:05:11,700 --> 00:05:13,620
通常是用

149
00:05:13,820 --> 00:05:15,050
希腊字母大写的西格玛

150
00:05:15,190 --> 00:05:16,880
 ∑ 来表示

151
00:05:18,000 --> 00:05:19,210
很不幸的是

152
00:05:19,310 --> 00:05:21,080
这个希腊符号和

153
00:05:21,760 --> 00:05:22,710
求和符号重复了

154
00:05:23,210 --> 00:05:24,620
这里说的是

155
00:05:24,700 --> 00:05:26,220
表示协方差矩阵的符号

156
00:05:26,420 --> 00:05:29,540
而这里表示的是求和符号

157
00:05:30,510 --> 00:05:32,330
希望在这部分幻灯片中

158
00:05:32,680 --> 00:05:34,190
我的表达让你不要混淆

159
00:05:34,410 --> 00:05:36,340
哪个是表示协方差的

160
00:05:36,520 --> 00:05:37,850
 ∑ 矩阵

161
00:05:38,090 --> 00:05:39,620
哪个是求和符号

162
00:05:39,940 --> 00:05:41,460
希望从我使用的环境中

163
00:05:41,820 --> 00:05:43,510
你能看出来

164
00:05:43,740 --> 00:05:44,790
计算出这个协方差矩阵后

165
00:05:45,530 --> 00:05:46,550
假如我们

166
00:05:47,135 --> 00:05:47,640
把它存为 Octave 中的

167
00:05:48,120 --> 00:05:49,970
一个变量 叫 Sigma

168
00:05:50,840 --> 00:05:51,890
我们需要做的

169
00:05:52,030 --> 00:05:53,660
是计算出

170
00:05:54,130 --> 00:05:56,190
Sigma 矩阵的特征向量 (eigenvectors)

171
00:05:57,560 --> 00:05:58,450
在 Octave 中

172
00:05:58,590 --> 00:05:59,820
你可以使用如下命令

173
00:05:59,970 --> 00:06:01,020
来实现这一功能

174
00:06:01,350 --> 00:06:02,600
[U,S,V] = svd(Sigma);

175
00:06:03,650 --> 00:06:06,090
顺便说一下 svd 表示奇异值分解 (singular value decomposition) 

176
00:06:08,520 --> 00:06:10,590
这是某种更高级的

177
00:06:10,790 --> 00:06:12,660
奇异值分解

178
00:06:14,450 --> 00:06:15,560
这是比较高级的

179
00:06:15,800 --> 00:06:16,950
线性代数的内容

180
00:06:16,950 --> 00:06:18,770
你不必掌握这些

181
00:06:18,950 --> 00:06:20,250
但实际上 Sigma 是一个

182
00:06:20,480 --> 00:06:21,800
协方差矩阵

183
00:06:21,880 --> 00:06:23,420
有很多种方法

184
00:06:23,610 --> 00:06:25,810
来计算它的特征向量

185
00:06:25,960 --> 00:06:27,350
如果你线性代数学得很好

186
00:06:27,700 --> 00:06:28,610
或者你之前听说过

187
00:06:28,860 --> 00:06:30,170
特征向量的话

188
00:06:30,350 --> 00:06:31,660
那也许知道在 Octave 中

189
00:06:31,990 --> 00:06:33,420
还有另一个 eig 命令

190
00:06:33,520 --> 00:06:35,030
可以用来计算特征向量

191
00:06:35,950 --> 00:06:36,980
实际上 svd 命令

192
00:06:37,370 --> 00:06:39,180
和 eig 命令

193
00:06:39,290 --> 00:06:40,310
将得到相同的结果

194
00:06:40,370 --> 00:06:42,170
虽然说 svd

195
00:06:42,840 --> 00:06:44,210
其实要更稳定一些

196
00:06:44,540 --> 00:06:45,890
所以我一般选择用 svd

197
00:06:46,140 --> 00:06:47,040
但我也有一些朋友

198
00:06:47,280 --> 00:06:48,720
他们喜欢用 eig 函数

199
00:06:48,920 --> 00:06:50,050
但你用 Sigma 命令

200
00:06:50,130 --> 00:06:51,270
用在这里的协方差矩阵上

201
00:06:51,750 --> 00:06:52,960
你会得到同样的答案

202
00:06:53,870 --> 00:06:55,070
这是因为协方差均值

203
00:06:55,500 --> 00:06:57,250
总满足一个数学性质

204
00:06:57,940 --> 00:07:00,560
称为对称正定 (symmetric positive definite)

205
00:07:01,360 --> 00:07:02,140
你不必细究

206
00:07:02,280 --> 00:07:03,890
这个具体是什么意思

207
00:07:05,340 --> 00:07:07,090
你只要知道 svd 和 eig 是不同的函数

208
00:07:07,400 --> 00:07:08,670
但当它们用在

209
00:07:08,780 --> 00:07:10,410
协方差矩阵时

210
00:07:10,550 --> 00:07:12,080
可以证明它始终是满足

211
00:07:13,190 --> 00:07:15,220
这个数学性质的 因此用两个命令的结果一样

212
00:07:16,580 --> 00:07:19,180
好了 这就是你需要了解的一点线性代数知识

213
00:07:19,260 --> 00:07:22,380
如果有任何地方不清楚的话 不必在意

214
00:07:22,560 --> 00:07:23,490
你只需要知道

215
00:07:24,130 --> 00:07:27,840
这条在 Octave 中

216
00:07:28,140 --> 00:07:29,690
你需要执行的语句就行了

217
00:07:30,080 --> 00:07:30,550
如果你用除了 Octave 或者 MATLAB

218
00:07:30,710 --> 00:07:32,120
之外的其他编程环境

219
00:07:32,710 --> 00:07:33,790
你要做的是找到

220
00:07:34,190 --> 00:07:35,860
某个可以计算 svd 

221
00:07:36,730 --> 00:07:37,960
即奇异值分解的

222
00:07:38,230 --> 00:07:40,460
函数库文件

223
00:07:40,970 --> 00:07:42,680
在主流的编程语言中

224
00:07:43,570 --> 00:07:45,060
应该有不少这样的库文件

225
00:07:45,300 --> 00:07:46,920
我们可以用它们

226
00:07:47,050 --> 00:07:48,920
来计算出协方差矩阵的

227
00:07:49,200 --> 00:07:52,770
U S V 矩阵

228
00:07:53,340 --> 00:07:54,490
我再提一下

229
00:07:54,620 --> 00:07:56,090
几个细节问题

230
00:07:56,660 --> 00:07:58,080
这个协方差矩阵 Sigma

231
00:07:58,250 --> 00:08:01,480
应该是一个 n×n 的矩阵

232
00:08:02,250 --> 00:08:03,240
一种证实的办法是

233
00:08:03,510 --> 00:08:04,220
如果你看定义

234
00:08:05,250 --> 00:08:06,280
这是一个 n×1 的向量

235
00:08:06,660 --> 00:08:08,680
这一项有一个转置

236
00:08:08,920 --> 00:08:10,830
因此是一个

237
00:08:11,010 --> 00:08:13,260
1×n 的向量

238
00:08:13,380 --> 00:08:14,480
两个向量相乘

239
00:08:15,150 --> 00:08:15,800
得到的结果

240
00:08:16,570 --> 00:08:17,530
自然是 n×n的矩阵

241
00:08:19,100 --> 00:08:22,130
这是 n×1 的转置 1×n

242
00:08:22,280 --> 00:08:22,840
所以这是一个 n×n 的矩阵

243
00:08:22,910 --> 00:08:23,710
然后把所有这些加起来

244
00:08:23,840 --> 00:08:26,140
当然还是 n×n 矩阵

245
00:08:27,600 --> 00:08:29,920
然后 svd 将输出三个矩阵

246
00:08:30,500 --> 00:08:32,580
分别是 U S V

247
00:08:32,830 --> 00:08:35,070
你真正需要的是 U 矩阵

248
00:08:36,230 --> 00:08:40,160
U 矩阵也是一个 n×n 矩阵

249
00:08:41,510 --> 00:08:42,280
如果我们看 U 矩阵的列

250
00:08:42,350 --> 00:08:43,260
实际上

251
00:08:43,480 --> 00:08:45,330
U 矩阵的

252
00:08:45,630 --> 00:08:47,210
列元素

253
00:08:48,570 --> 00:08:50,180
就是我们

254
00:08:50,350 --> 00:08:53,860
需要的

255
00:08:54,260 --> 00:08:56,290
u(1) u(2) 等等

256
00:08:57,640 --> 00:08:59,330
所以 U 矩阵是 n×n 的

257
00:09:00,910 --> 00:09:01,830
如果我们想

258
00:09:02,230 --> 00:09:03,200
将数据的维度从 n

259
00:09:03,800 --> 00:09:05,380
降低到 k 的话

260
00:09:05,490 --> 00:09:07,950
我们只需要提取前 k 列向量

261
00:09:09,800 --> 00:09:12,670
这样我们就得到了

262
00:09:12,860 --> 00:09:14,700
u(1) 到 u(k)

263
00:09:14,780 --> 00:09:16,930
也就是我们用来投影数据的

264
00:09:17,200 --> 00:09:19,770
 k 个方向

265
00:09:20,090 --> 00:09:21,640
剩下的步骤就很简单了

266
00:09:22,410 --> 00:09:24,170
通过这个 svd 

267
00:09:24,490 --> 00:09:25,580
我们得到了矩阵 U

268
00:09:25,840 --> 00:09:27,140
我们把它的列向量

269
00:09:27,530 --> 00:09:29,080
叫做 u(1) 到 u(n)

270
00:09:30,580 --> 00:09:31,620
然后我们继续

271
00:09:31,830 --> 00:09:32,520
把剩下的步骤讲完

272
00:09:32,540 --> 00:09:34,550
通过这个 svd 过程

273
00:09:35,320 --> 00:09:36,940
我们可以得到

274
00:09:37,240 --> 00:09:38,650
矩阵 U S V

275
00:09:38,830 --> 00:09:41,320
我们取出 U 矩阵的

276
00:09:41,900 --> 00:09:44,460
前 k 列

277
00:09:45,050 --> 00:09:46,310
得到一个新的矩阵 u(1) 到 u(k)

278
00:09:48,710 --> 00:09:49,460
接下来我们要做的事是

279
00:09:49,700 --> 00:09:53,730
我们需要找到一个办法

280
00:09:54,110 --> 00:09:55,430
对于原始数据集 x

281
00:09:55,630 --> 00:09:57,080
x 是一个 n 维实数

282
00:09:57,250 --> 00:09:59,210
然后我们要找到一个

283
00:09:59,420 --> 00:10:01,280
低维的表达 z

284
00:10:01,570 --> 00:10:02,800
z 是 k 维实数

285
00:10:02,900 --> 00:10:03,930
所以方法是

286
00:10:04,180 --> 00:10:06,690
我们把 U 矩阵的前 k 列取出来

287
00:10:08,330 --> 00:10:09,790
我们构建这样一个矩阵

288
00:10:11,060 --> 00:10:13,040
把 u(1) u(2)

289
00:10:14,170 --> 00:10:16,690
一直到 u(k) 并列地合起来

290
00:10:17,350 --> 00:10:19,120
其实就是取出

291
00:10:19,280 --> 00:10:20,350
这个 U 矩阵的

292
00:10:20,530 --> 00:10:22,260
前 k 列元素

293
00:10:23,420 --> 00:10:25,370
因此这就是一个

294
00:10:25,600 --> 00:10:26,920
n × k 维的矩阵

295
00:10:27,200 --> 00:10:28,580
n × k 维的矩阵

296
00:10:29,500 --> 00:10:30,690
我给这个矩阵起个名字

297
00:10:30,880 --> 00:10:32,200
我把这个矩阵

298
00:10:32,930 --> 00:10:35,760
叫做 U 下标 reduce

299
00:10:36,090 --> 00:10:38,620
表示 U 矩阵约减后的版本

300
00:10:39,140 --> 00:10:41,250
我将用它来约减我的数据

301
00:10:43,040 --> 00:10:43,950
然后 计算 z 的方法是

302
00:10:44,250 --> 00:10:45,960
z 等于这个

303
00:10:46,220 --> 00:10:49,570
Ureduce 矩阵的转置乘以 x

304
00:10:50,010 --> 00:10:52,030
或者也可以这样写

305
00:10:52,510 --> 00:10:53,860
这里的转置可以换一种写法

306
00:10:54,630 --> 00:10:55,910
对 U 矩阵求转置

307
00:10:56,010 --> 00:10:57,920
实际上就会得到

308
00:10:58,010 --> 00:11:00,680
这样一个行矩阵

309
00:11:00,950 --> 00:11:04,540
从 u(1) 转置一直到 u(k) 转置

310
00:11:07,060 --> 00:11:08,860
然后用它乘以 x

311
00:11:09,700 --> 00:11:10,740
这样我就得到了

312
00:11:10,920 --> 00:11:12,670
z 矩阵的表达

313
00:11:12,740 --> 00:11:14,280
我们来标出矩阵的维度

314
00:11:15,370 --> 00:11:16,380
这个矩阵的维度

315
00:11:16,560 --> 00:11:17,450
应该是 k × n

316
00:11:18,270 --> 00:11:19,350
这里 x 的维度

317
00:11:19,420 --> 00:11:20,530
应该是 n × 1

318
00:11:20,750 --> 00:11:21,810
因此这两个相乘

319
00:11:22,320 --> 00:11:24,330
维度应该是 k × 1

320
00:11:24,820 --> 00:11:27,920
因此 z 是 k 维的

321
00:11:28,790 --> 00:11:29,810
向量 正好也就是

322
00:11:30,010 --> 00:11:31,230
我们所希望的

323
00:11:32,000 --> 00:11:33,180
我们所希望的

324
00:11:33,550 --> 00:11:34,640
当然 这里所说的 x

325
00:11:34,890 --> 00:11:36,010
可以是训练集中的样本

326
00:11:36,100 --> 00:11:36,970
也可以是

327
00:11:37,540 --> 00:11:38,750
交叉验证集中的样本

328
00:11:38,980 --> 00:11:40,330
也可以是测试集样本

329
00:11:40,500 --> 00:11:41,590
比如

330
00:11:41,930 --> 00:11:43,830
如果我想处理第 i 个训练样本

331
00:11:44,260 --> 00:11:45,910
那么我可以把这个写成

332
00:11:47,270 --> 00:11:48,430
x(i) 这里也是 x(i)

333
00:11:48,510 --> 00:11:50,080
降维得到的就是 z(i)

334
00:11:50,940 --> 00:11:53,140
总结一下

335
00:11:53,460 --> 00:11:54,820
这就是 PCA 的全过程

336
00:11:56,250 --> 00:11:58,200
首先进行均值归一化

337
00:11:58,420 --> 00:11:59,230
保证所有的特征量都是均值为0的

338
00:11:59,610 --> 00:12:01,420
然后可以选择进行特征缩放

339
00:12:02,280 --> 00:12:03,780
如果不同特征量的范围跨度很大的话

340
00:12:03,890 --> 00:12:05,820
你确实需要进行特征缩放这一步

341
00:12:06,620 --> 00:12:08,610
在以上的预处理之后

342
00:12:09,130 --> 00:12:12,010
我们计算出这个协方差 Sigma 矩阵

343
00:12:12,240 --> 00:12:14,070
就像这样

344
00:12:14,210 --> 00:12:16,340
顺便说一下

345
00:12:16,610 --> 00:12:17,780
如果你的数据

346
00:12:18,030 --> 00:12:18,960
是像这样的一个矩阵给出的

347
00:12:19,230 --> 00:12:22,580
也就是说你的数据是一行一行给出的

348
00:12:22,780 --> 00:12:24,370
比如数据是一个大 X 矩阵

349
00:12:24,960 --> 00:12:26,190
每一组训练样本用一行来表示

350
00:12:27,030 --> 00:12:28,830
从 x(1) 转置

351
00:12:29,210 --> 00:12:30,400
一直到 x(m) 转置

352
00:12:31,530 --> 00:12:32,700
那么这个协方差矩阵

353
00:12:33,020 --> 00:12:36,040
就能写成一个向量化的表示

354
00:12:37,390 --> 00:12:38,980
你可以在 Octave 中实现

355
00:12:39,440 --> 00:12:41,130
在 Octave中 你可以

356
00:12:41,670 --> 00:12:45,250
执行 Sigma = (1/m) * X' * X;

357
00:12:45,550 --> 00:12:46,440
执行 Sigma = (1/m) * X' * X;

358
00:12:47,250 --> 00:12:50,770
X 就是上面这个矩阵

359
00:12:50,980 --> 00:12:53,320
你只要执行这个简单的语句

360
00:12:53,570 --> 00:12:55,070
这用向量化的表达

361
00:12:55,220 --> 00:12:56,910
计算出了 Sigma 矩阵

362
00:12:58,020 --> 00:12:59,020
今天我不会在这里证明

363
00:12:59,160 --> 00:13:00,600
这个计算公式的正确性

364
00:13:00,740 --> 00:13:02,460
当然如果你愿意

365
00:13:02,870 --> 00:13:03,900
你也可以自己推导一下

366
00:13:03,980 --> 00:13:05,100
或者在 Octave 中测试一下

367
00:13:05,840 --> 00:13:06,890
确保这两个式子其实是一样的

368
00:13:06,920 --> 00:13:10,050
或者你动手自己算一算吧

369
00:13:11,250 --> 00:13:12,330
随便你怎么证明

370
00:13:12,430 --> 00:13:14,580
反正这个向量表达是正确的

371
00:13:16,480 --> 00:13:17,570
然后我们可以应用 svd 函数

372
00:13:17,920 --> 00:13:19,050
来计算出 U S V 矩阵

373
00:13:19,250 --> 00:13:20,840
然后

374
00:13:21,100 --> 00:13:22,720
我们取出 U 矩阵的

375
00:13:23,050 --> 00:13:24,450
前 k 列元素

376
00:13:24,660 --> 00:13:26,550
组成新的 Ureduce 矩阵

377
00:13:26,650 --> 00:13:28,540
最后这个式子

378
00:13:28,740 --> 00:13:29,980
给出了我们从

379
00:13:30,290 --> 00:13:31,600
原来的特征 x 

380
00:13:31,850 --> 00:13:34,340
变成降维后的 z 的过程

381
00:13:34,540 --> 00:13:35,760
另外 跟 k均值算法类似

382
00:13:36,090 --> 00:13:37,860
如果你使用 PCA 的话

383
00:13:38,030 --> 00:13:40,300
你的 x 应该是 n 维实数

384
00:13:41,100 --> 00:13:43,990
所以 没有 x0 = 1 这一项

385
00:13:44,200 --> 00:13:46,080
好了 这就是

386
00:13:46,990 --> 00:13:48,680
PCA 算法

387
00:13:50,120 --> 00:13:51,380
有一件事儿我没做

388
00:13:51,590 --> 00:13:53,190
u(1) u(2) 等等

389
00:13:53,520 --> 00:13:54,600
通过将数据

390
00:13:54,970 --> 00:13:56,560
投影到 k 维的子平面上

391
00:13:57,230 --> 00:13:58,730
确实使得

392
00:13:58,870 --> 00:14:00,620
投影误差的平方和为最小值

393
00:14:02,170 --> 00:14:04,800
我并没有证明这一点

394
00:14:05,110 --> 00:14:07,170
已经超出了这门课的范围

395
00:14:07,700 --> 00:14:09,110
幸运的是 PCA 算法

396
00:14:09,470 --> 00:14:10,940
能够用不多的几行代码

397
00:14:11,320 --> 00:14:12,510
就能实现

398
00:14:13,190 --> 00:14:14,510
如果你在 Octave 或者 MATLAB 中

399
00:14:14,640 --> 00:14:16,120
自己实现一下的话

400
00:14:16,520 --> 00:14:17,590
你就已经获得了一种

401
00:14:18,110 --> 00:14:19,710
非常有效的维度约减的算法

402
00:14:22,430 --> 00:14:23,850
这就是 PCA 算法

403
00:14:25,010 --> 00:14:26,290
我没有证明

404
00:14:26,840 --> 00:14:28,420
u(1) u(2) 等等

405
00:14:29,170 --> 00:14:30,360
以及 z 等等

406
00:14:30,720 --> 00:14:31,630
这些我们在算法中的选择

407
00:14:31,770 --> 00:14:32,830
确实能够使得

408
00:14:32,980 --> 00:14:34,330
平方投影误差最小化

409
00:14:34,680 --> 00:14:35,870
对吧? 还记得吗?

410
00:14:36,500 --> 00:14:37,800
我们之前说过的

411
00:14:38,140 --> 00:14:39,350
PCA 的目标就是

412
00:14:39,610 --> 00:14:40,660
尽量找到一个平面

413
00:14:40,960 --> 00:14:42,170
或者一条线

414
00:14:42,570 --> 00:14:43,690
来对数据进行投影

415
00:14:44,280 --> 00:14:46,340
这个平面或线应该最小化平方投影误差

416
00:14:46,700 --> 00:14:48,610
我并没有证明这一点

417
00:14:49,140 --> 00:14:50,680
这个问题的数学证明

418
00:14:50,970 --> 00:14:52,520
已经超出了这门课的范围

419
00:14:53,170 --> 00:14:55,550
但很幸运的一点是 PCA 算法可以在 Octave 中

420
00:14:55,730 --> 00:14:58,890
用短短几行代码执行出来

421
00:14:59,350 --> 00:15:00,740
如果你自己执行一遍

422
00:15:01,430 --> 00:15:02,560
它一定会成功运行的

423
00:15:02,770 --> 00:15:03,730
而且效果会很好

424
00:15:04,710 --> 00:15:05,940
如果你亲自实现这个算法的话

425
00:15:06,500 --> 00:15:09,220
你就得到了这个非常有用的降维算法

426
00:15:09,780 --> 00:15:10,650
它确实能够很好地最小化

427
00:15:11,050 --> 00:15:13,460
平方投影误差的值 【无边界字幕组】翻译: 所罗门捷列夫 校对: 竹二个 审核：Naplessss