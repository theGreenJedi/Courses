En este video, me gustaría comenzar a hablar sobre un segundo tipo de problema de aprendizaje no supervisado llamado reducción de dimensionalidad. Hay un par de distintas razones por las cuales uno podría querer hacer una reducción de dimensionalidad. Una de ellas es la compresión de datos, y como veremos más adelante en algunos videos, la compresión de datos no sólo nos permite comprimir los datos y por lo tanto tener que utilizar menos memoria de la computadora o espacio en el disco, pero también nos permitirá acelerar nuestros algoritmos de aprendizaje. Primero, vamos a empezar por hablar de lo que es la reducción de dimensionalidad. Como un ejemplo motivador, digamos que hemos reunido un conjunto de datos con demasiadas variables, y he trazado sólo dos de ellas aquí. Y digamos que, desconocidas para nosotros, dos de las variables eran, de hecho, la longitud de algo en centímetros, y una variable diferente, x2, es la longitud de la misma cosa en pulgadas. Por lo tanto, esto nos da una representación altamente redundante y tal vez en lugar de tener dos funciones separadas, x1 y x2, ambas que básicamente miden la longitud, quizá lo que queremos hacer es reducir los datos a una sola dimensión y sólo tener un número midiendo esta longitud. En caso de que este ejemplo parezca un poco rebuscado, este ejemplo de centímetros y pulgadas, en realidad no es tan irreal y no es tan diferente de las cosas que veo suceder en la industria. Si usted tiene cientos o miles de variables, a menudo es muy fácil perder la pista de que variables uno tiene exactamente. Y a veces uno puede tener diferentes equipos de ingeniería, tal vez un equipo de ingeniería le da doscientas variables, un segundo equipo de ingeniería le da trescientas variables, y un tercer equipo de ingeniería le da quinientas variables, por lo que uno tiene mil variables en conjunto. y realmente se vuelve difícil no perder de vista cuáles variables exactamente obtuvo de qué equipo, y realmente no es tan difícil tener variables altamente redundantes como estas. Así que si la longitud en centímetros se redondeó al centímetro más cercano y la longitud en pulgadas se redondeó a la pulgada más cercana, entonces, es por eso que estos ejemplos no se encuentran perfectamente en una línea recta, debido al error de redondeo al centímetro o pulgada más cercanos. Y si podemos reducir los datos a una sola dimensión en lugar de dos dimensiones, esto reduce la redundancia. Como un ejemplo distinto, de nuevo, quizá uno que pueda parecer menos rebuscado. Durante varios años he estado trabajando con pilotos de helicópteros autónomos, o he estado trabajando con pilotos que vuelan helicópteros. Así que, si usted tuviera que medir-- hacer una encuesta o hacer una prueba de estos diferentes pilotos, usted podría tener una variable, x1, que sería tal vez la habilidad de estos pilotos de helicóptero, y tal vez "x2" podría ser el disfrute del piloto. Es decir, qué tanto disfrutan volar, y tal vez estas dos variables estarán altamente correlacionadas. Y lo que realmente le interesa a usted podría ser está dirección, una variable diferente que realmente mida la aptitud del piloto. Y por supuesto estoy inventando el nombre aptitud, pero de nuevo, si usted tiene variables altamente correlacionadas, quizá usted en realidad quiera reducir la dimensión. Permítame hablarle un poco más acerca de lo que realmente significa reducir la dimensión de los datos, de 2 dimensiones, 2D a 1 dimensión o 1D. Déjeme colorear estos ejemplos usando diferentes colores. En este caso, a lo que me refiero con la reducción de la dimensión, es que me gustaría encontrar tal vez esta línea, esta dirección en la que la mayoría de los datos parece estar, y proyectar todos los datos en esa línea que acabo de dibujar. Y al hacer esto lo que puedo hacer es simplemente medir la posición de cada uno de los ejemplos en esa línea. Y lo que puedo hacer es venir con una nueva variable, z1, y para especificar la posición en la línea necesito un solo número, entonces se dice así que z1 es una nueva variable que especifica la ubicación de cada uno de esos puntos en esta línea verde. Y lo que esto significa es que donde previamente, si tuviera un ejemplo x1, tal vez este fue mi primer ejemplo, x1. Entonces, con el fin de representar x1, x1 originalmente, Necesitaba un número de dos dimensiones, o un vector de variable de dos dimensiones. En cambio, ahora puedo representar z1. Podría utilizar sólo z1 para representar mi primer ejemplo, y ese va a ser un número real. Y del mismo modo con x2, si x2 es mi segundo ejemplo allí, entonces anteriormente, mientras que se requerían dos números para representar, si en vez de esto, computo la proyección de esa cruz negra sobre la línea, ahora necesito un solo número real que es z2 para representar la ubicación de este punto z2 en la línea. Y así sucesivamente, a través de mis ejemplos m. Así que, para resumir, si nos permitimos aproximar el grupo de datos original mediante la proyección de todos mis ejemplos originales en esta línea verde aquí, entonces necesito un solo número, necesito un solo número real para especificar la posición de un punto en la línea y por lo tanto, lo que puedo hacer es utilizar sólo un número para representar la ubicación de cada uno de mis ejemplos de entrenamiento después de que han sido proyectados en esa línea verde. Así que esta es una aproximación a el grupo de entrenamiento original ya que he proyectado todos mis ejemplos de entrenamiento sobre una línea. Pero ahora tengo que mantener sólo un número para cada uno de mis ejemplos, por lo que se reduce a la mitad el requerimiento de memoria, o de espacio, o lo que guste para el como almacenar mis datos. Y quizás es más interesante, más importante, lo que veremos más tarde en el siguiente vídeo, es que esto nos permitirá hacer que nuestros algoritmos de aprendizaje se ejecuten más rápido también. Y esa es en realidad, quizá, la aplicación más interesante de esta compresión de datos, en lugar de reducir la memoria o requerir más espacio en el disco para el almacenamiento de los datos. En la diapositiva anterior, se mostró un ejemplo de la reducción de datos de 2D a 1D. En esta diapositiva voy a mostrar otro ejemplo de reducción de datos de tres dimensiones, 3D, a dos dimensiones, 2D. Por cierto, en el ejemplo más típico de reducción de dimensionalidad, podríamos tener datos de 1000 dimensiones o 1000D, que quisiéramos reducir a digamos un centenar de dimensiones o 100D, pero debido a las limitaciones de lo que puedo dibujar en la diapositiva, voy a utilizar ejemplos de 3D a 2D o de 2D a 1D. Digamos que tenemos un serie de datos como la que se muestra aquí. Y entonces, tendría un conjunto de ejemplos x (i), que son puntos en R3. Entonces, tengo ejemplos de tres dimensiones. Sé que puede ser un poco difícil ver esto en la diapositiva pero voy a mostrar una nube de puntos 3D en un momento. Y puede que sea difícil verlo aquí, pero todos estos datos quizás se ubican aproximadamente en el plano de esta manera. Así que lo que podemos hacer es mediante la reducción de dimensionalidad es tomar todos estos datos y proyectarlos  sobre un plano de dos dimensiones. Lo que he hecho aquí es que he tomado todos los datos y los he proyectado en su totalidad, de modo que todo se encuentra en el plano. Finalmente, con el fin de especificar la ubicación de un punto dentro de un plano, necesitamos dos números, ¿cierto? Tenemos que ,quizá, especificar la ubicación de un punto a lo largo de este eje y  también especificar su ubicación a lo largo de ese eje. Por lo tanto, necesitamos dos números, tal vez llamados z1 y z2 para especificar la ubicación de un punto dentro de un plano. Y así, lo que eso significa, es que ahora podemos representar cada ejemplo, cada ejemplo de entrenamiento, utilizando dos números que he dibujado aquí, z1 y z2. Por lo tanto, nuestros datos pueden ser representados utilizando el vector z que está en R2 y estos subíndices, el subíndice z 1, subíndice z 2, lo que quiero decir con esto es que mis vectores aquí, z, son vectores de dos dimensiones, z1, z2 Así que, si tengo algunos ejemplos particulares, Z (i), ese es el vector de dos dimensiones, z (i) 1, Z (i) 2. En la diapositiva anterior cuando estaba reduciendo los datos a una sola dimensión, entonces sólo tenía z1, ¿verdad? y eso es lo que era un subíndice z1 1 en la diapositiva anterior pero aquí tengo datos bidimensionales, así que tengo z1 y z2 como los dos componentes de los datos. Ahora, permítanme asegurarme que estas cifras tengan sentido. Entonces, permítame volver a mostrar estas tres figuras de nuevo pero con trazados en 3D. Así que el proceso por el cual pasamos fue que mostrado a la izquierda se encuentra el grupo de datos original, en medio el grupo de datos proyectados en 2D, y a la derecha los grupos de datos 2D con z1 y z2 como ejes. Vamos a mirarlos un poco más a detalle. Aquí está mi grupo de datos original mostrado a la izquierda. Había comenzado con una nube de puntos 3D en la que los ejes están etiquetados como x1, x2, x3, así que esta es una nube de puntos 3D pero la mayoría de los datos puede encontrarse aproximadamente no demasiado lejos de un plano 2D. Así que lo que podemos hacer es tomar estos datos,  aquí está mi figura de enmedio, y los voy a proyectar en 2D. Así que he proyectado estos datos de manera que todos ahora yacen en esta superficie 2D. Como se puede ver, todos los datos se encuentran en un plano, porque hemos proyectado todo en un plano y lo que esto significa es que ahora sólo necesito dos números, z1 y z2 para representar la ubicación de un punto en el plano. Entonces, ese es el proceso que podemos realizar para reducir nuestros datos, de tres a dos dimensiones. Así que eso es la reducción de dimensionalidad y cómo podemos usarla para comprimir nuestros datos. Y como veremos más adelante, esto nos permitirá hacer que algunos de nuestros algoritmos de aprendizaje se ejecuten mucho más rápido también pero vamos hablar de esto en un video posterior.