Neste vídeo, eu gostaria de começar a falar sobre um segundo tipo de problema do aprendizado não supervisionado, que é chamado de redução de dimensão. Existem algumas diferentes razões pelas quais pode-se desejar a redução de dimensão. Uma delas é a compressão de dados, e como veremos mais tarde, alguns vídeos mais tarde, a compressão de dados não só nos permite uma economia, em termos de memória computacional e espaço em disco, mas também irá nos ajudar a aumentar a velocidade de nossos algoritmos de aprendizado. Mas antes, vamos começar falando sobre o que é a redução de dimensão. Como exemplo motivacional, digamos que nós coletamos um conjunto de dados com muitas, muitas, muitas variáveis, e que colocamos apenas duas delas neste gráfico. E digamos que não tivéssemos conhecimento que duas de nossas variáveis eram na verdade o comprimento de algo em centímetros, e uma outra variávei, x2, era o mesmo comprimento, mas em polegadas. E assim nós teremos uma representação altamente redundante, e, talvez, ao invés de termos duas variáveis, x1 e x2, ambas basicamente medindo a mesma coisa, talvez o que desejaríamos seria reduzir os nossos dados em uma única dimensão e, assim, termos apenas um número contendo o comprimento. Caso esse exemplo pareça um pouco fictício, esse caso do centímetro junto com as polegadas não é assim tão irreal, e não é tão distante do que eu vejo acontecendo no mercado. Se você tem centenas ou mesmo milhares de variáveis, é realmente fácil perder-se a noção do que cada variável faz. E às vezes também podemos ter diferentes equipes de engenheiros, talvez um dos times forneça duas centenas de variáveis, o segundo time fornece outras três centenas, e um terceiro contribui com cinco centenas, então você terminaria com mil variáveis ao todo, e realmente torna-se difícil manter uma noção de quais variáveis, exatamente, você recebeu de cada time, e assim temos que não é tão difícil que ocorra redundância como mostrado aqui. Então, se o comprimento em centímetros for arredondado para o centímetro mais próximo, assim como as polegadas. Então, esse é o porquê desses exemplos não ajustarem-se perfeitamente em uma linha reta, por causa, como disse, do arredondamento que gerará um erro em centímetros e polegadas. E se nós pudermos reduzir os dados para uma dimensão ao invés de duas, isso reduziria a redundância. Para um exemplo diferente, que pareça menos fictício. Por muitos anos eu trabalho com pilotos autônomos de helicópteros. Ou tenho trabalhado com pilotos de helicóptero. Se você fosse medir, se você fosse interrogar, ou fazer um teste com esses diferentes pilotos, você pode ter uma variável, x1, que será a habilidade desses pilotos de helicóptero, e talvez x2 pudesse ser o proveito do piloto. Isso é, em outras palavras, o quanto ele se diverte pilotando, e talvez essas duas variáveis sejam altamente correlacionadas. E o que você realmente está se preocupando em ter como medida é essa direção, uma variável diferente que capture e messa a aptidão do piloto. E eu estou chamando de "aptidão", mas, novamente, se você tem variáveis altamente correlacionadas, talvez você realmente queira reduzir a sua dimensão. Então, deixe-me mostrar um pouco mais sobre o que significa reduzir a dimensão de um conjunto de dados de 2 dimensões, ou seja 2D, para 1 dimensão, 1D. Bom, vou colorir esses exemplos com diferentes cores. E neste caso ao reduzirmos a dimensão, o que quero passar é que eu talvez gostaria de encontrar essa linha. Essa direção, sabe? Na qual a maioria dos dados parece acomodar-se, e projetar todos os dados nessa linha, e ao fazer isso, o que eu posso fazer agora é somente medir a posição de cada exemplo nessa linha. E o que eu posso fazer é criar uma nova variável, z1, e para especificar a posição nesta linha eu preciso apenas de um número, então diremos que z1 é uma nova variável que especifica a localidade de cada um desses pontos nessa linha verde. E isso significa que onde antes eu tinha um exemplo de x1, talvez esse fosse o meu primeiro exemplo x1, E para que eu pudesse representar o x1 originalmente eu precisava de um número de duas dimensões, ou um vector de variáveis com duas dimensões. Agora eu posso representar como z1, u poderia usar apenas z1 para representar meu primeiro exemplo, e ele será apenas um único número real. E da mesma forma será x2, se ele é meu segundo exemplo aqui, então, novamente, enquanto ele requer dois números para ser representado, se, ao invés disso, eu calcular a sua projeção, a do x negro nessa linha, agora eu precisarei de apenas um número real, que será z2 e representará a posição deste ponto z2 na linha. E da mesma forma será para todos os M exemplos. Então, apenas resumindo, se aproximar-mos o conjunto original de dados ao projetá-los todos nesse linha verde aqui, então precisarei apenas de um número, apenas um único número real para especificar a posição de um ponto na linha, e portanto o que eu posso fazer é usar apenas um número para representar a posição de cada um dos meus exemplos de treinamento após eles terem sido projetados nessa linha verde. Então isso é uma aproximação do conjunto original de dados, pois eu projetei todos os exemplos em uma linha. Mas eu poderia gerar eu preciso de apenas um número para representar cada exemplo. E então isso corta pela metade a memória necessária, ou espaço requerido, ou o que for que você use para armazenar dados. E, talvez, até mais interessante, mais importante, o que veremos mais tarde, nos vídeos posteriores, esse procedimento nos permitirá rodar nossos algoritmos de aprendizado mais rapidamente. E esse é, na verdade, o benefício mais interessante da compressão de dados. ao invés de apenas reduzir a memória ou espaço de disco para armazenar dados. No slide anterior, mostramos um exemplo de redução de dados de 2D para 1D. Nesse slide, irei mostrar outro exemplo de redução de dados, mas de três dimensões 3D para duas dimensões 2D. À propósito, nos exemplos mais típicos de redução de dimensão, nós podemos ter algo como dados com milhares de dimensões ou dados 1000D, que nós desejaríamos reduzir para algo com centenas de dimensões, ou 100D, mas devido as limitações gráficas que posso usar nos slides, irei usar exemplos de 3D para 2D, ou 2D para 1D. Então, se temos um conjunto de dados como o mostrado aqui, e eu teria um conjunto de exemplos, x(i), que são pontos no R3. Portanto, exemplos com três dimensões, eu sei que pode ser um pouco difícil visualizar isso no gráfico, mas temos aqui uma nuvem de pontos 3D no gráfico. E pode ser difícil ver aqui, mas cada um desses pontos, desses dados, podem acomodar-se à grosso modo no plano, desta forma. E o que podemos fazer com a redução de dimensão, é pegar todos esses dados e projetá-los até um plano de duas dimensões. Então, aqui o que eu fiz foi: peguei todos os dados e projetei de maneira que todos ficassem no plano. Agora, finalmente, para especificar o local de um ponto do plano, nós precisamos de dois números, correto? Nós precisamos apenas especificar a posição do ponto ao longo desse eixo, e também fazê-lo para esse outro eixo. Portanto, precisamos de dois números, que seriam z1 e z2, para especificar a localização de um ponto no plano. E o que isso significa é que agora podemos representar cada exemplo, cada dado de treinamento, usando apenas dois números, que defini aqui como z1 e z2. Então nossos dados podem ser representados usando apenas um vetor z que pertence ao R2. E esse subscrito, z1, z2, o que quero dizer com isso é que meu vetor z é um vetor de duas dimensões: z1 e z2. Então, se eu tiver um exemplo em particular, z(i), ele pode ser visto como o vetor de duas dimensões: o primeiro z(i)1, e o segundo z(i)2. E no slide anterior, quando eu estava reduzindo dados para uma dimensão, então eu teria, nesse caso, apenas z1, correto? E era isso que representava o z1 no slide anterior. Mas aqui eu tenho dados com duas dimensões, então eu tenho z1 e z2 como os dois componentes dos dados. Agora só para ter certeza que essas figuras fazem sentido. Mostrarei novamente essas três mesmas imagens, mas com gráficos 3D. Então o processo que aplicamos apresentados com esse conjunto de dados fictícios, e no centro temos o conjunto de dados projetados no plano 2D, e na direita temos os dados em 2D, definidos no eixo z1 e z2. Olhemos para eles um pouco além. Aqui estão os dados iriginais, mostrados à esquerda, então eu tinha começado com uma nuvem de pontos em 3D, onde os eixos eram definidos como x1, x2, x3 e, portanto, temos 3D, mas a maioria dos dados, mesmo grosseiramente, talvez comodem-se em algum plano 2D. Então o que podemos fazer é pegar esses dados e, como vemos aqui na figura central, irei projetá-los no plano 2D. Portanto, projetei todos os dados nessa superfície 2D. Como você pode notar, todos os dados estão no plano 2D, pois projetamos tudo nesse plano, então o que isso significa para nós é que agora nós precisamos de apenas dois números, z1 e z2, para representar a posição de um ponto neste plano. E este é o processo que podemos aplicar para que possamos reduzir os nossos dados de três dimensões para duas dimensões. Ou seja, é a redução de dimensão e é assim que podemos usá-la para comprimir nossos dados. E como veremos mais tarde, isso nos permitirá fazer com que alguns algoritmos rodem mais rapidamente também, mas veremos isso apenas nos próximos vídeos.