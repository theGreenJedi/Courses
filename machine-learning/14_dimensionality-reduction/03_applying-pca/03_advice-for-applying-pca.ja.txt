以前の動画で、 私はPCAは時には、学習アルゴリズムの実行時間を スピードアップする為に使う事が出来る、という事に言及した。 このビデオで私は それを実際にどうやるのかを説明する。 そしてまた、 PCAをどう適用するのかについて、 幾つか助言も与えておきたい。 これがPCAを学習アルゴリズムのスピードアップに使う方法だ。 そしてこの教師あり学習のスピードアップが 私が個人的にPCAを使う もっとも一般的な 用途だ。 教師あり学習の問題に 直面しているとしよう。 入力xとラベルyの 教師あり学習。 そしてあなたの手本xiが 凄い高次元だとしよう。 例えば、あなたの手本xiは、 1万次元のフィーチャーベクトルだとする。 そんな例の一つには、 何らかのコンピュータビジョンの問題に 取り組んでいる時、とかが考えられる。 そこでは100x100の画像だとすると、 つまり100x100は、1万ピクセルだ。 だから例えば、 xiが1万ピクセルの 明度の値の フィーチャーベクトルだとすれば、 1万次元のフィーチャーベクトルを持つ事になる。 そのようなとても高い次元の フィーチャーベクトルの場合、 学習アルゴリズムを走らせると、遅くなりがちだ。 1万次元のフィーチャーベクトルを食わせようとすれば、 それがロジスティック回帰でも ニューラルネットワークでもサポートベクタマシンでも、その他何でも、 単にデータが多量だ、という理由なだけで、 それは1万個の数字なので、 学習アルゴリズムの実行を遅くしうる。 幸運にも、PCAでもって、 我らはこのデータの次元を 減らす事が出来る。 だからアルゴリズムをもっと効率的に走らせる事が出来る。 そのやり方はこうだ。 まず最初に ラベル付けされたトレーニングセットを チェックして、入力のみを引っ張り出す。 x達を引きぬいて、 一時的にyの事は脇にどけておく。 そうすると、ここには、 ラベル無しのトレーニングセット x1からxmが、 得られる。それらは、例えば 1万次元とかのデータだ。 我らの、1万次元の手本。 つまり単に入力ベクトルである所の x1からxmまでを取り出しただけ。 そして次にPCAを適用して、 その結果我らは、 データの削減された次元の表現を得る、 つまり1万次元のフィーチャーベクトル の代わりに、いまや 例えば1000次元のフィーチャーベクトルを得る事になる。 つまりこれは、10倍の節約になる。 つまりこれは我らに、新しいトレーニングセットを与えるのだ。 つまり以前には、私は 手本としてx1, y1を持っていた訳だが、 それが今や最初のトレーニングの入力はz1となり、 つまり我らは、ある種の 新しいトレーニング手本を得る訳だ、 それはz1とy1がペアになって、 同様にz2, y2, 点点点と続き zm, ymまで。 何故なら、トレーニング手本はいまや このより低い次元の表現、 z1, z2, ..., zmで表されるから。 最後に、この削減された次元の トレーニングセットに対して、 これらを学習アルゴリズム、例えば ニューラルネットワークとか、ロジスティック回帰とかに 食わせて、 そして仮説hを学習する事が出来る、 この仮説はこれらの低次元の表現zを 入力として受け取り、予測を試みる物だ。 例えばロジスティック回帰を 使ってるとするなら、 1割ることの 1足すeのマイナス シータ転置 z を出力する仮説を訓練する、 これは 入力としてこれらのzベクトルの一つを受け取り、 予測を試みる。 そして最後に、 新しい手本を得たら、 それは新しいテスト手本のxかもしれないが、 そこであなたがすべきは、 テストの手本xを PCAで見つけられた同じマッピングを用いて 対応するzに マッピングする。 そして次にそのzを この仮説に食わせる、 そしてこの仮説が 入力xに対応する予測を行う。 最後に一つ注意を。 PCAがやっている事は xからzへのマッピングを定義する、という事だ。 そしてこのxからzへのマッピングは PCAをトレーニングセットだけに対して走らせる事で 定義するべきだ。 具体的に言うと、このマッピング、 PCAが学習するこのマッピングは、 パラメータの集合を計算する訳だ。 フィーチャースケーリングして平均標準化して、 そしてこのU reduce行列を計算する。 だがここで重要なのはU reduceだけだ。 これもまたPCAで 学習されたパラメータのような物で、 我らはパラメータのフィッティングは トレーニングセットだけに対して 行うべきだ。そして クロスバリデーションセットやテストセットに対してフィッティングしてはいけない。 だからこれらの事、U reduceとかは、 トレーニングセットだけにPCAを適用して 取得するべきである。 そしてU reduceを見出したら、またフィーチャースケーリングのパラメータを見出したら、 平均標準化して、 スケールでフィーチャーを割って 比較可能なスケールにするのだった。 これら全てのパラメータをトレーニングセットに対して 見出したら、 その次には、同じマッピングをその他の手本、 クロスバリデーションセットとかテストセットにある手本に対して 適用出来るのだ。 まとめておこう。 PCAを走らせる時は、 手持ちのデータのうち トレーニングセットにだけ走らせないといけない。 クロスバリデーションセットとテストセットの部分には実行してはいけない。 そしてそうする事で、xからzへのマッピングの定義が 得られる。そこで次に そのマッピングを クロスバリデーションセットやテストセットに 適用する。 ところで、 私はデータの削減として、 1万次元から1000次元にする、という 話をしているが、 これはそんなに非現実的な数字では無い。 多くの問題で、我らは実際には高次元データを、 5倍とか10倍とか削減して、 しかもほとんどの分散を保持したままに出来て、 だからパフォーマンスにほとんど影響を与えずに行える、 パフォーマンスというのは分類の正確さとかの観点という事だが、 学習アルゴリズムの正確さにほとんど影響を与えずに 行う事が出来る。 そしてより低い次元のデータで 作業を行う事で、 学習アルゴリズムは、しばしばずっと早く走る。 まとめると、ここまでに我らは 以下のようなPCAの応用例を話してきた。 まず、圧縮という応用を話した。 圧縮したいのは、データを保存するのに必要な メモリやディスク容量を 減らす為かもしれないし、 今話したように、学習アルゴリズムを スピードアップする為かもしれない。 これらの応用では、 kを選ぶ為にはしばしば、 保持される分散の パーセンテージを 調べる。 つまり、この学習アルゴリズムのスピードアップという 応用例の場合、よく使われるのは99%の分散を保持する、というライン。 それはとても典型的なkを選ぶ為の 選択と言える。 以上がこれらの圧縮の応用に際し、kを選ぶ方法だ。 他方は可視化という応用。 我らは通常、プロット方法としては、 2次元データとか3次元データの プロット方法しか知らない。 つまり可視化の応用では、 普通はkを2か3と選ぶ。 何故なら我らは2Dか3Dのデータセットしか プロット出来ないから。 以上がPCAの主な応用の 要約だ。 それとそれぞれの応用に際しての kの選び方だ。 PCAの良く見る誤用についても 指摘しておくべきだろう。 あなたも時には、他の人が これをやってしまっている、という事を耳にする事があるだろう。そんなに多くは無いとは思いたいが。 私がこれを言及したいのは、あなたにやって欲しくないからだ。 そんな良く無いPCAの誤用としては、 オーバーフィッティングを防ぐ為に使ってしまう、という物。 それはこういう理由だ。 これはPCAの 良い使い方では無い、 だが、これがこの手法を用いる背後にある理由だ、 それは、えーと、 xiがあるとして、 それがnフィーチャーだったとする。 そしてそのデータを圧縮する、 ziを代わりに使う、 するとフィーチャーの数を k個に減らせられる、 それはnよりもっと低い次元のはずだ。 つまり、もっと少ない数の フィーチャーを持つ事になる。 kが1000でnが1万なら、 我らは1000次元のデータしか 持たなくなるのだから、 オーバーフィットもしにくくなるだろう、 1万次元のデータを使うよりは、 1000個のフィーチャーを使う方が。 つまり、PCAをオーバーフィットを 防止する方法と考える人が居る。 だが、強調しておきたいが、 これはPCAの悪い使い方で、 これをやるのを、私は推奨しない。 それはこの手法が悪い振る舞いをするって訳じゃない。 もしあなたがオーバーフィットを防止する為に データの次元を減らす為に この手法を使ったとする、 たぶんそれはうまく行くと思う。 だが、これは単に オーバーフィッティングに対応するのに 良い方法じゃない、とういだけ。その代わりに、 もしオーバーフィットに悩んでいるなら、 それに対処するもっと良い方法は、 PCAを使ってデータの次元を削減する代わりに 正規化を使うという事だ。 その理由は、 PCAがどう機能するかを考えてみると、 それはラベルyを使わない。 単に入力のxiだけを 見ていって、 そしてそれを用いてデータの 低次元の近似を探していく。 つまりPCAが行うのは、 なんらかの情報を捨て去るって事だが、 PCAはyの値が何かを知らずに データの次元を捨てる、 あるいは削減する。 だから、これはたぶんOKなんだが、 こういう風にPCAを使うのは たぶんOKなんだが、、、 99%の分散とかを保持している限りは。 分散のほとんどを 維持しているのだから。 だが、何らかの貴重な情報を捨て去っている可能性もある。 そして以下のような事も分かるだろう。 分散の99%を保持していようが 分散の95%を保持していようが、 はたまたどれだけの分散を保持していようが、 単に正規化を使うだけの方が どんなに悪くても同程度に良い オーバーフィッティングを防ぐ 手法だという事が分かっている。 そして多くの場合には、正規化の方が 単によりうまく機能する。 何故なら線形回帰やロジスティック回帰や その他の手法を正規化と共に用いる時には、 この最小化の問題が 実際にyの値が何なのかを 知っているから。 だから何か重要な情報を 捨て去ってしまう可能性が低い。 一方でPCAはラベルを 有効利用出来ないので、 重要な情報を捨て去ってしまう可能性が、より高い。 ではまとめよう。 もしあなたの主なモチベーションが 学習アルゴリズムのスピードアップなら、 それはPCAの良い使い方だ。 だがオーバーフィッティングを防止する為にPCAを使うなら、 それは良く無いPCAの使い方だ、 その場合は正規化を代わりに使うべきだ、 それこそが多くの人々が代わりに 推奨している事でもある。 最後に、PCAの誤った使い方を一つ。 PCAはとても便利なアルゴリズムだと言うべきだろう、 私は圧縮とか可視化の目的で、良くPCAを使う。 だが、私が時々目にするのは、 人々がPCAを、使うべきでは無い所でもまた 使ってしまっている事がある。 こんな事を私は良く見かける： 誰かが機械学習のシステムを 設計している時に、 こんなプランを書きだすとする。 学習システムを設計しよう！ トレーニングセットを集めて、 そして次に、PCAを走らせる、 そしてロジスティック回帰を訓練し、 そしてテストデータでテストする、っと。 つまり、しばしばプロジェクトの 本当にしょっぱなの所で、 PCAが組み込まれた、これら四つのステップをやろう、 という、プロジェクトのプランを 書く人が居る。 PCAを用いた こういうプロジェクトのプランを書く前に、 自身に問うてみるのが とても有益な問いは、 PCA無しでこれら全部を行ったら どうだろうか？という事だ。 そしてしばしば人々は、 このような複雑なプロジェクトプランを作り出して PCAを実装したりする前に、 このステップを検討していない。 だから具体的に言うと、 私が良く人々にアドバイスする事としては、 PCAを実装する前に、 私が最初に提案するのは、 あなたがやっている事が なんであれ、 何をやりたいのであれ、 まずはオリジナルの生のデータxiで やってみる事を検討せよ、という事だ。 それが望む結果を生まなかったら、 その時になって初めて、PCAを実装し、ziを使う事を検討すべきだ。 つまりPCAを使う前に、 データの次元を減らす代わりに、 私が代わりに検討するのは、 このPCAのステップをさぼってみよう。 そして検討する事は、 オリジナルのデータに対して 単純に学習アルゴリズムを訓練してみよう、って事だ。 オリジナルの生の入力xiを使ってみよう、 そして私が推奨するのは、 PCAをアルゴリズムに 組み込む代わりに、 あなたがやってる事がなんであれ、最初のxiでやってみる事を推奨する。 そしてそれがうまくいかない、と 信じる理由を得て初めて、 つまりあなたの学習アルゴリズムが あまりにも実行速度が遅いという 結果になって初めて、 または必要メモリの要求量や 必要ディスクの要求量があまりにも大きくなり過ぎて、 だから表現を圧縮したい、と思った時に、、、 それはxiを使ってみて うまく行かなかった時になって、、、 xiではうまく行かない、という 証拠がある時か、または そう信じる強い理由があるようになって、、、 その時初めて、PCAを実装して表現を圧縮する事に使えば良い。 こんな事を言うのは、 人々が最初からプロジェクトのプランに PCAを用いる事を計画していて、 そして時々、 彼らが何をするにせよ、 PCA無しでもうまく行く、という事態を 私はちょくちょく目撃して来たからだ。 だからPCAを使うのは、 代替案も検討した後にすべきだ、 PCAを得たり、どのkを使うべきかとかを見出したりに たくさん時間を費やしたり する前に。 さて、以上がPCAだ。 最後に付したこれらのコメントにも関わらず、 適切な用途で用いれば、 PCAは信じられない程 便利なアルゴリズムだ。 そして現実に、私はPCAをしょっちゅう使う。 私の場合は、 PCAを使うのは、学習アルゴリズムの実行時間を スピードアップするのに使う事が一番多い。 だが私が思うに、 PCAはその他の用途も同じくらい一般的だと思う。 データを圧縮するのに使う、これは メモリやディスク容量の要件を 減らす事が出来るし、 またデータの可視化にも良く使われる。 そしてPCAは教師なし学習のアルゴリズムのうち、 もっとも良く使われていて、 もっともパワフルな物の一つと言えるだろう。 そしてこれらのビデオで学んだ事を用いて、 あなたは、きっと PCAを実装する事が出来て、 これらの目的全てに対しても 使っていく事が出来るだろう。