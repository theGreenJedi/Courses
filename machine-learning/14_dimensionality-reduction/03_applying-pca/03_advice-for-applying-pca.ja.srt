1
00:00:00,090 --> 00:00:01,450
以前の動画で、

2
00:00:01,610 --> 00:00:02,710
私はPCAは時には、学習アルゴリズムの実行時間を

3
00:00:02,840 --> 00:00:05,410
スピードアップする為に使う事が出来る、という事に言及した。

4
00:00:07,070 --> 00:00:08,140
このビデオで私は

5
00:00:08,370 --> 00:00:09,520
それを実際にどうやるのかを説明する。

6
00:00:09,820 --> 00:00:11,270
そしてまた、

7
00:00:11,460 --> 00:00:12,900
PCAをどう適用するのかについて、

8
00:00:12,990 --> 00:00:14,550
幾つか助言も与えておきたい。

9
00:00:17,110 --> 00:00:19,630
これがPCAを学習アルゴリズムのスピードアップに使う方法だ。

10
00:00:20,270 --> 00:00:21,940
そしてこの教師あり学習のスピードアップが

11
00:00:22,270 --> 00:00:23,630
私が個人的にPCAを使う

12
00:00:23,870 --> 00:00:25,870
もっとも一般的な

13
00:00:26,530 --> 00:00:27,720
用途だ。

14
00:00:28,710 --> 00:00:29,640
教師あり学習の問題に

15
00:00:30,300 --> 00:00:31,660
直面しているとしよう。

16
00:00:31,810 --> 00:00:33,380
入力xとラベルyの

17
00:00:33,690 --> 00:00:35,510
教師あり学習。

18
00:00:35,810 --> 00:00:37,330
そしてあなたの手本xiが

19
00:00:37,830 --> 00:00:39,140
凄い高次元だとしよう。

20
00:00:39,840 --> 00:00:41,670
例えば、あなたの手本xiは、

21
00:00:41,800 --> 00:00:44,000
1万次元のフィーチャーベクトルだとする。

22
00:00:45,510 --> 00:00:46,550
そんな例の一つには、

23
00:00:46,700 --> 00:00:48,130
何らかのコンピュータビジョンの問題に

24
00:00:48,540 --> 00:00:50,390
取り組んでいる時、とかが考えられる。

25
00:00:50,650 --> 00:00:52,410
そこでは100x100の画像だとすると、

26
00:00:52,780 --> 00:00:55,550
つまり100x100は、1万ピクセルだ。

27
00:00:55,850 --> 00:00:57,520
だから例えば、

28
00:00:57,780 --> 00:00:59,240
xiが1万ピクセルの

29
00:00:59,760 --> 00:01:01,670
明度の値の

30
00:01:02,470 --> 00:01:03,580
フィーチャーベクトルだとすれば、

31
00:01:04,410 --> 00:01:05,580
1万次元のフィーチャーベクトルを持つ事になる。

32
00:01:06,880 --> 00:01:08,530
そのようなとても高い次元の

33
00:01:09,300 --> 00:01:10,890
フィーチャーベクトルの場合、

34
00:01:11,320 --> 00:01:12,860
学習アルゴリズムを走らせると、遅くなりがちだ。

35
00:01:13,030 --> 00:01:14,300
1万次元のフィーチャーベクトルを食わせようとすれば、

36
00:01:14,790 --> 00:01:16,980
それがロジスティック回帰でも

37
00:01:17,570 --> 00:01:19,780
ニューラルネットワークでもサポートベクタマシンでも、その他何でも、

38
00:01:20,450 --> 00:01:22,000
単にデータが多量だ、という理由なだけで、

39
00:01:22,200 --> 00:01:23,060
それは1万個の数字なので、

40
00:01:24,130 --> 00:01:25,970
学習アルゴリズムの実行を遅くしうる。

41
00:01:27,170 --> 00:01:28,530
幸運にも、PCAでもって、

42
00:01:28,680 --> 00:01:29,810
我らはこのデータの次元を

43
00:01:30,060 --> 00:01:31,050
減らす事が出来る。

44
00:01:31,180 --> 00:01:32,410
だからアルゴリズムをもっと効率的に走らせる事が出来る。

45
00:01:32,920 --> 00:01:34,440
そのやり方はこうだ。

46
00:01:34,590 --> 00:01:35,780
まず最初に

47
00:01:35,980 --> 00:01:37,030
ラベル付けされたトレーニングセットを

48
00:01:37,400 --> 00:01:39,520
チェックして、入力のみを引っ張り出す。

49
00:01:39,800 --> 00:01:41,830
x達を引きぬいて、

50
00:01:42,730 --> 00:01:45,130
一時的にyの事は脇にどけておく。

51
00:01:45,860 --> 00:01:46,750
そうすると、ここには、

52
00:01:47,090 --> 00:01:49,150
ラベル無しのトレーニングセット x1からxmが、

53
00:01:49,400 --> 00:01:51,000
得られる。それらは、例えば

54
00:01:51,240 --> 00:01:53,600
1万次元とかのデータだ。

55
00:01:53,940 --> 00:01:55,800
我らの、1万次元の手本。

56
00:01:55,870 --> 00:01:57,230
つまり単に入力ベクトルである所の

57
00:01:58,370 --> 00:01:58,930
x1からxmまでを取り出しただけ。

58
00:02:00,650 --> 00:02:01,810
そして次にPCAを適用して、

59
00:02:02,700 --> 00:02:03,740
その結果我らは、

60
00:02:03,980 --> 00:02:06,100
データの削減された次元の表現を得る、

61
00:02:06,410 --> 00:02:08,010
つまり1万次元のフィーチャーベクトル

62
00:02:08,260 --> 00:02:09,540
の代わりに、いまや

63
00:02:09,780 --> 00:02:11,880
例えば1000次元のフィーチャーベクトルを得る事になる。

64
00:02:12,330 --> 00:02:13,500
つまりこれは、10倍の節約になる。

65
00:02:15,110 --> 00:02:17,260
つまりこれは我らに、新しいトレーニングセットを与えるのだ。

66
00:02:17,910 --> 00:02:19,430
つまり以前には、私は

67
00:02:19,620 --> 00:02:21,180
手本としてx1, y1を持っていた訳だが、

68
00:02:21,490 --> 00:02:24,340
それが今や最初のトレーニングの入力はz1となり、

69
00:02:24,580 --> 00:02:25,800
つまり我らは、ある種の

70
00:02:26,050 --> 00:02:27,010
新しいトレーニング手本を得る訳だ、

71
00:02:28,210 --> 00:02:29,240
それはz1とy1がペアになって、

72
00:02:30,700 --> 00:02:33,170
同様にz2, y2, 点点点と続き zm, ymまで。

73
00:02:33,770 --> 00:02:35,300
何故なら、トレーニング手本はいまや

74
00:02:35,460 --> 00:02:36,980
このより低い次元の表現、

75
00:02:37,480 --> 00:02:41,040
z1, z2, ..., zmで表されるから。

76
00:02:41,310 --> 00:02:42,340
最後に、この削減された次元の

77
00:02:43,650 --> 00:02:45,060
トレーニングセットに対して、

78
00:02:45,240 --> 00:02:46,540
これらを学習アルゴリズム、例えば

79
00:02:46,640 --> 00:02:47,900
ニューラルネットワークとか、ロジスティック回帰とかに

80
00:02:48,280 --> 00:02:49,450
食わせて、

81
00:02:49,750 --> 00:02:51,990
そして仮説hを学習する事が出来る、

82
00:02:52,230 --> 00:02:53,830
この仮説はこれらの低次元の表現zを

83
00:02:54,330 --> 00:02:56,230
入力として受け取り、予測を試みる物だ。

84
00:02:57,890 --> 00:02:59,030
例えばロジスティック回帰を

85
00:02:59,460 --> 00:03:00,880
使ってるとするなら、

86
00:03:01,060 --> 00:03:02,760
1割ることの

87
00:03:03,080 --> 00:03:04,020
1足すeのマイナス シータ転置 z

88
00:03:04,180 --> 00:03:06,020
を出力する仮説を訓練する、

89
00:03:07,620 --> 00:03:10,150
これは

90
00:03:10,610 --> 00:03:11,530
入力としてこれらのzベクトルの一つを受け取り、

91
00:03:11,960 --> 00:03:13,660
予測を試みる。

92
00:03:15,260 --> 00:03:16,310
そして最後に、

93
00:03:16,630 --> 00:03:17,800
新しい手本を得たら、

94
00:03:17,920 --> 00:03:20,060
それは新しいテスト手本のxかもしれないが、

95
00:03:20,220 --> 00:03:21,340
そこであなたがすべきは、

96
00:03:22,130 --> 00:03:23,730
テストの手本xを

97
00:03:24,960 --> 00:03:26,590
PCAで見つけられた同じマッピングを用いて

98
00:03:26,990 --> 00:03:27,860
対応するzに

99
00:03:28,220 --> 00:03:29,610
マッピングする。

100
00:03:30,390 --> 00:03:31,280
そして次にそのzを

101
00:03:31,950 --> 00:03:33,740
この仮説に食わせる、

102
00:03:33,910 --> 00:03:35,450
そしてこの仮説が

103
00:03:35,750 --> 00:03:36,740
入力xに対応する予測を行う。

104
00:03:38,110 --> 00:03:40,090
最後に一つ注意を。

105
00:03:40,510 --> 00:03:42,350
PCAがやっている事は

106
00:03:42,710 --> 00:03:45,090
xからzへのマッピングを定義する、という事だ。

107
00:03:45,960 --> 00:03:46,970
そしてこのxからzへのマッピングは

108
00:03:47,050 --> 00:03:48,280
PCAをトレーニングセットだけに対して走らせる事で

109
00:03:48,580 --> 00:03:50,840
定義するべきだ。

110
00:03:51,650 --> 00:03:53,310
具体的に言うと、このマッピング、

111
00:03:53,530 --> 00:03:54,770
PCAが学習するこのマッピングは、

112
00:03:54,950 --> 00:03:57,650
パラメータの集合を計算する訳だ。

113
00:03:58,210 --> 00:04:00,500
フィーチャースケーリングして平均標準化して、

114
00:04:01,240 --> 00:04:04,040
そしてこのU reduce行列を計算する。

115
00:04:04,680 --> 00:04:05,510
だがここで重要なのはU reduceだけだ。

116
00:04:05,670 --> 00:04:06,980
これもまたPCAで

117
00:04:07,120 --> 00:04:08,420
学習されたパラメータのような物で、

118
00:04:08,670 --> 00:04:09,950
我らはパラメータのフィッティングは

119
00:04:10,150 --> 00:04:12,270
トレーニングセットだけに対して

120
00:04:12,480 --> 00:04:13,990
行うべきだ。そして

121
00:04:14,040 --> 00:04:16,250
クロスバリデーションセットやテストセットに対してフィッティングしてはいけない。

122
00:04:16,370 --> 00:04:17,560
だからこれらの事、U reduceとかは、

123
00:04:18,180 --> 00:04:19,460
トレーニングセットだけにPCAを適用して

124
00:04:19,820 --> 00:04:22,430
取得するべきである。

125
00:04:23,300 --> 00:04:26,930
そしてU reduceを見出したら、またフィーチャースケーリングのパラメータを見出したら、

126
00:04:27,350 --> 00:04:28,620
平均標準化して、

127
00:04:29,320 --> 00:04:31,790
スケールでフィーチャーを割って

128
00:04:32,180 --> 00:04:34,500
比較可能なスケールにするのだった。

129
00:04:34,760 --> 00:04:36,010
これら全てのパラメータをトレーニングセットに対して

130
00:04:36,570 --> 00:04:38,010
見出したら、

131
00:04:38,220 --> 00:04:41,560
その次には、同じマッピングをその他の手本、

132
00:04:41,820 --> 00:04:45,020
クロスバリデーションセットとかテストセットにある手本に対して

133
00:04:45,180 --> 00:04:46,680
適用出来るのだ。

134
00:04:47,150 --> 00:04:48,340
まとめておこう。

135
00:04:48,450 --> 00:04:49,790
PCAを走らせる時は、

136
00:04:49,900 --> 00:04:51,070
手持ちのデータのうち

137
00:04:51,220 --> 00:04:52,450
トレーニングセットにだけ走らせないといけない。

138
00:04:52,490 --> 00:04:55,880
クロスバリデーションセットとテストセットの部分には実行してはいけない。

139
00:04:56,410 --> 00:04:57,620
そしてそうする事で、xからzへのマッピングの定義が

140
00:04:57,870 --> 00:04:58,770
得られる。そこで次に

141
00:04:58,950 --> 00:05:00,320
そのマッピングを

142
00:05:00,560 --> 00:05:02,240
クロスバリデーションセットやテストセットに

143
00:05:02,290 --> 00:05:03,370
適用する。

144
00:05:03,450 --> 00:05:04,660
ところで、

145
00:05:05,000 --> 00:05:06,660
私はデータの削減として、

146
00:05:06,950 --> 00:05:08,510
1万次元から1000次元にする、という

147
00:05:08,740 --> 00:05:10,350
話をしているが、

148
00:05:10,660 --> 00:05:11,950
これはそんなに非現実的な数字では無い。

149
00:05:12,280 --> 00:05:14,720
多くの問題で、我らは実際には高次元データを、

150
00:05:17,600 --> 00:05:18,700
5倍とか10倍とか削減して、

151
00:05:18,780 --> 00:05:20,910
しかもほとんどの分散を保持したままに出来て、

152
00:05:21,270 --> 00:05:22,680
だからパフォーマンスにほとんど影響を与えずに行える、

153
00:05:23,900 --> 00:05:25,840
パフォーマンスというのは分類の正確さとかの観点という事だが、

154
00:05:26,240 --> 00:05:27,970
学習アルゴリズムの正確さにほとんど影響を与えずに

155
00:05:28,770 --> 00:05:30,320
行う事が出来る。

156
00:05:31,090 --> 00:05:32,140
そしてより低い次元のデータで

157
00:05:32,590 --> 00:05:33,730
作業を行う事で、

158
00:05:34,060 --> 00:05:36,500
学習アルゴリズムは、しばしばずっと早く走る。

159
00:05:36,910 --> 00:05:38,120
まとめると、ここまでに我らは

160
00:05:38,410 --> 00:05:40,920
以下のようなPCAの応用例を話してきた。

161
00:05:41,970 --> 00:05:43,780
まず、圧縮という応用を話した。

162
00:05:44,020 --> 00:05:45,140
圧縮したいのは、データを保存するのに必要な

163
00:05:45,500 --> 00:05:46,440
メモリやディスク容量を

164
00:05:46,590 --> 00:05:47,960
減らす為かもしれないし、

165
00:05:48,240 --> 00:05:49,390
今話したように、学習アルゴリズムを

166
00:05:49,460 --> 00:05:51,630
スピードアップする為かもしれない。

167
00:05:52,100 --> 00:05:53,870
これらの応用では、

168
00:05:54,130 --> 00:05:56,240
kを選ぶ為にはしばしば、

169
00:05:56,420 --> 00:05:58,770
保持される分散の

170
00:05:59,160 --> 00:06:00,590
パーセンテージを

171
00:06:00,810 --> 00:06:03,880
調べる。

172
00:06:04,780 --> 00:06:06,320
つまり、この学習アルゴリズムのスピードアップという

173
00:06:06,570 --> 00:06:10,050
応用例の場合、よく使われるのは99%の分散を保持する、というライン。

174
00:06:10,530 --> 00:06:11,690
それはとても典型的なkを選ぶ為の

175
00:06:12,100 --> 00:06:14,270
選択と言える。

176
00:06:14,730 --> 00:06:16,640
以上がこれらの圧縮の応用に際し、kを選ぶ方法だ。

177
00:06:17,850 --> 00:06:19,590
他方は可視化という応用。

178
00:06:20,760 --> 00:06:22,100
我らは通常、プロット方法としては、

179
00:06:22,230 --> 00:06:23,550
2次元データとか3次元データの

180
00:06:24,020 --> 00:06:25,520
プロット方法しか知らない。

181
00:06:26,540 --> 00:06:28,650
つまり可視化の応用では、

182
00:06:28,830 --> 00:06:29,660
普通はkを2か3と選ぶ。

183
00:06:29,710 --> 00:06:31,930
何故なら我らは2Dか3Dのデータセットしか

184
00:06:32,740 --> 00:06:33,500
プロット出来ないから。

185
00:06:34,510 --> 00:06:35,720
以上がPCAの主な応用の

186
00:06:36,020 --> 00:06:37,230
要約だ。

187
00:06:37,870 --> 00:06:39,580
それとそれぞれの応用に際しての

188
00:06:39,670 --> 00:06:41,540
kの選び方だ。

189
00:06:42,890 --> 00:06:45,710
PCAの良く見る誤用についても

190
00:06:46,400 --> 00:06:48,100
指摘しておくべきだろう。

191
00:06:48,800 --> 00:06:50,300
あなたも時には、他の人が

192
00:06:50,580 --> 00:06:51,820
これをやってしまっている、という事を耳にする事があるだろう。そんなに多くは無いとは思いたいが。

193
00:06:52,230 --> 00:06:54,780
私がこれを言及したいのは、あなたにやって欲しくないからだ。

194
00:06:55,480 --> 00:06:56,460
そんな良く無いPCAの誤用としては、

195
00:06:56,540 --> 00:06:59,170
オーバーフィッティングを防ぐ為に使ってしまう、という物。

196
00:07:00,380 --> 00:07:00,660
それはこういう理由だ。

197
00:07:01,910 --> 00:07:03,080
これはPCAの

198
00:07:03,730 --> 00:07:04,610
良い使い方では無い、

199
00:07:04,670 --> 00:07:05,630
だが、これがこの手法を用いる背後にある理由だ、

200
00:07:05,690 --> 00:07:07,080
それは、えーと、

201
00:07:07,350 --> 00:07:09,090
xiがあるとして、

202
00:07:09,300 --> 00:07:10,660
それがnフィーチャーだったとする。

203
00:07:10,830 --> 00:07:12,640
そしてそのデータを圧縮する、

204
00:07:12,750 --> 00:07:13,700
ziを代わりに使う、

205
00:07:14,270 --> 00:07:15,410
するとフィーチャーの数を

206
00:07:15,560 --> 00:07:17,050
k個に減らせられる、

207
00:07:17,290 --> 00:07:19,300
それはnよりもっと低い次元のはずだ。

208
00:07:19,410 --> 00:07:21,130
つまり、もっと少ない数の

209
00:07:21,490 --> 00:07:22,520
フィーチャーを持つ事になる。

210
00:07:22,770 --> 00:07:25,800
kが1000でnが1万なら、

211
00:07:26,090 --> 00:07:27,010
我らは1000次元のデータしか

212
00:07:27,780 --> 00:07:29,390
持たなくなるのだから、

213
00:07:29,670 --> 00:07:30,580
オーバーフィットもしにくくなるだろう、

214
00:07:31,260 --> 00:07:32,230
1万次元のデータを使うよりは、

215
00:07:33,280 --> 00:07:34,980
1000個のフィーチャーを使う方が。

216
00:07:35,950 --> 00:07:37,160
つまり、PCAをオーバーフィットを

217
00:07:37,360 --> 00:07:39,360
防止する方法と考える人が居る。

218
00:07:39,950 --> 00:07:41,940
だが、強調しておきたいが、

219
00:07:42,110 --> 00:07:44,000
これはPCAの悪い使い方で、

220
00:07:44,260 --> 00:07:46,080
これをやるのを、私は推奨しない。

221
00:07:46,520 --> 00:07:48,430
それはこの手法が悪い振る舞いをするって訳じゃない。

222
00:07:49,000 --> 00:07:49,920
もしあなたがオーバーフィットを防止する為に

223
00:07:50,330 --> 00:07:51,560
データの次元を減らす為に

224
00:07:51,890 --> 00:07:52,830
この手法を使ったとする、

225
00:07:53,690 --> 00:07:54,830
たぶんそれはうまく行くと思う。

226
00:07:55,560 --> 00:07:56,720
だが、これは単に

227
00:07:57,040 --> 00:07:58,340
オーバーフィッティングに対応するのに

228
00:07:58,680 --> 00:08:00,390
良い方法じゃない、とういだけ。その代わりに、

229
00:08:00,510 --> 00:08:01,810
もしオーバーフィットに悩んでいるなら、

230
00:08:02,030 --> 00:08:03,420
それに対処するもっと良い方法は、

231
00:08:03,800 --> 00:08:05,680
PCAを使ってデータの次元を削減する代わりに

232
00:08:05,900 --> 00:08:07,910
正規化を使うという事だ。

233
00:08:08,670 --> 00:08:10,000
その理由は、

234
00:08:11,010 --> 00:08:12,150
PCAがどう機能するかを考えてみると、

235
00:08:12,900 --> 00:08:13,950
それはラベルyを使わない。

236
00:08:14,530 --> 00:08:15,680
単に入力のxiだけを

237
00:08:16,050 --> 00:08:17,220
見ていって、

238
00:08:17,340 --> 00:08:19,070
そしてそれを用いてデータの

239
00:08:19,130 --> 00:08:21,150
低次元の近似を探していく。

240
00:08:21,390 --> 00:08:22,840
つまりPCAが行うのは、

241
00:08:23,190 --> 00:08:25,410
なんらかの情報を捨て去るって事だが、

242
00:08:26,460 --> 00:08:28,040
PCAはyの値が何かを知らずに

243
00:08:28,180 --> 00:08:29,680
データの次元を捨てる、

244
00:08:30,110 --> 00:08:31,390
あるいは削減する。

245
00:08:32,380 --> 00:08:33,700
だから、これはたぶんOKなんだが、

246
00:08:34,250 --> 00:08:35,770
こういう風にPCAを使うのは

247
00:08:35,920 --> 00:08:37,750
たぶんOKなんだが、、、

248
00:08:37,990 --> 00:08:39,190
99%の分散とかを保持している限りは。

249
00:08:39,410 --> 00:08:40,400
分散のほとんどを

250
00:08:40,830 --> 00:08:41,970
維持しているのだから。

251
00:08:42,100 --> 00:08:44,230
だが、何らかの貴重な情報を捨て去っている可能性もある。

252
00:08:45,010 --> 00:08:45,980
そして以下のような事も分かるだろう。

253
00:08:46,310 --> 00:08:47,580
分散の99%を保持していようが

254
00:08:47,820 --> 00:08:49,260
分散の95%を保持していようが、

255
00:08:49,360 --> 00:08:50,940
はたまたどれだけの分散を保持していようが、

256
00:08:51,020 --> 00:08:52,310
単に正規化を使うだけの方が

257
00:08:52,720 --> 00:08:54,650
どんなに悪くても同程度に良い

258
00:08:54,790 --> 00:08:56,010
オーバーフィッティングを防ぐ

259
00:08:56,220 --> 00:08:57,880
手法だという事が分かっている。

260
00:08:58,900 --> 00:09:00,340
そして多くの場合には、正規化の方が

261
00:09:00,590 --> 00:09:02,220
単によりうまく機能する。

262
00:09:02,350 --> 00:09:03,890
何故なら線形回帰やロジスティック回帰や

263
00:09:04,250 --> 00:09:05,240
その他の手法を正規化と共に用いる時には、

264
00:09:05,600 --> 00:09:07,390
この最小化の問題が

265
00:09:08,010 --> 00:09:09,420
実際にyの値が何なのかを

266
00:09:09,480 --> 00:09:10,740
知っているから。

267
00:09:10,960 --> 00:09:12,680
だから何か重要な情報を

268
00:09:12,880 --> 00:09:14,330
捨て去ってしまう可能性が低い。

269
00:09:14,730 --> 00:09:15,790
一方でPCAはラベルを

270
00:09:16,060 --> 00:09:17,810
有効利用出来ないので、

271
00:09:17,850 --> 00:09:19,940
重要な情報を捨て去ってしまう可能性が、より高い。

272
00:09:20,230 --> 00:09:21,370
ではまとめよう。

273
00:09:21,620 --> 00:09:22,900
もしあなたの主なモチベーションが

274
00:09:23,010 --> 00:09:24,380
学習アルゴリズムのスピードアップなら、

275
00:09:24,530 --> 00:09:26,490
それはPCAの良い使い方だ。

276
00:09:26,790 --> 00:09:28,360
だがオーバーフィッティングを防止する為にPCAを使うなら、

277
00:09:28,650 --> 00:09:29,630
それは良く無いPCAの使い方だ、

278
00:09:30,030 --> 00:09:32,270
その場合は正規化を代わりに使うべきだ、

279
00:09:32,900 --> 00:09:36,190
それこそが多くの人々が代わりに

280
00:09:36,440 --> 00:09:40,490
推奨している事でもある。

281
00:09:41,310 --> 00:09:43,350
最後に、PCAの誤った使い方を一つ。

282
00:09:43,750 --> 00:09:45,760
PCAはとても便利なアルゴリズムだと言うべきだろう、

283
00:09:46,270 --> 00:09:49,170
私は圧縮とか可視化の目的で、良くPCAを使う。

284
00:09:50,230 --> 00:09:51,400
だが、私が時々目にするのは、

285
00:09:51,570 --> 00:09:53,310
人々がPCAを、使うべきでは無い所でもまた

286
00:09:53,710 --> 00:09:56,080
使ってしまっている事がある。

287
00:09:56,220 --> 00:09:57,940
こんな事を私は良く見かける：

288
00:09:58,030 --> 00:09:59,140
誰かが機械学習のシステムを

289
00:09:59,330 --> 00:10:00,330
設計している時に、

290
00:10:01,010 --> 00:10:02,130
こんなプランを書きだすとする。

291
00:10:02,200 --> 00:10:04,150
学習システムを設計しよう！

292
00:10:05,060 --> 00:10:06,080
トレーニングセットを集めて、

293
00:10:06,570 --> 00:10:07,350
そして次に、PCAを走らせる、

294
00:10:07,400 --> 00:10:08,700
そしてロジスティック回帰を訓練し、

295
00:10:08,860 --> 00:10:11,200
そしてテストデータでテストする、っと。

296
00:10:11,680 --> 00:10:12,770
つまり、しばしばプロジェクトの

297
00:10:13,090 --> 00:10:14,360
本当にしょっぱなの所で、

298
00:10:14,600 --> 00:10:15,600
PCAが組み込まれた、これら四つのステップをやろう、

299
00:10:15,720 --> 00:10:16,980
という、プロジェクトのプランを

300
00:10:17,310 --> 00:10:18,610
書く人が居る。

301
00:10:20,210 --> 00:10:21,220
PCAを用いた

302
00:10:21,530 --> 00:10:23,350
こういうプロジェクトのプランを書く前に、

303
00:10:23,560 --> 00:10:24,860
自身に問うてみるのが

304
00:10:25,030 --> 00:10:27,110
とても有益な問いは、

305
00:10:27,630 --> 00:10:28,560
PCA無しでこれら全部を行ったら

306
00:10:29,540 --> 00:10:31,470
どうだろうか？という事だ。

307
00:10:32,170 --> 00:10:33,450
そしてしばしば人々は、

308
00:10:33,800 --> 00:10:34,940
このような複雑なプロジェクトプランを作り出して

309
00:10:35,440 --> 00:10:37,080
PCAを実装したりする前に、

310
00:10:37,920 --> 00:10:40,620
このステップを検討していない。

311
00:10:40,810 --> 00:10:42,360
だから具体的に言うと、

312
00:10:43,050 --> 00:10:44,300
私が良く人々にアドバイスする事としては、

313
00:10:44,670 --> 00:10:45,980
PCAを実装する前に、

314
00:10:46,450 --> 00:10:47,970
私が最初に提案するのは、

315
00:10:48,220 --> 00:10:49,410
あなたがやっている事が

316
00:10:49,600 --> 00:10:50,770
なんであれ、

317
00:10:50,850 --> 00:10:52,030
何をやりたいのであれ、

318
00:10:52,450 --> 00:10:53,650
まずはオリジナルの生のデータxiで

319
00:10:53,980 --> 00:10:56,420
やってみる事を検討せよ、という事だ。

320
00:10:56,600 --> 00:10:57,860
それが望む結果を生まなかったら、

321
00:10:57,960 --> 00:10:59,650
その時になって初めて、PCAを実装し、ziを使う事を検討すべきだ。

322
00:11:01,010 --> 00:11:02,420
つまりPCAを使う前に、

323
00:11:03,030 --> 00:11:03,930
データの次元を減らす代わりに、

324
00:11:04,360 --> 00:11:05,710
私が代わりに検討するのは、

325
00:11:06,640 --> 00:11:08,020
このPCAのステップをさぼってみよう。

326
00:11:08,420 --> 00:11:09,690
そして検討する事は、

327
00:11:10,040 --> 00:11:11,460
オリジナルのデータに対して

328
00:11:12,440 --> 00:11:13,560
単純に学習アルゴリズムを訓練してみよう、って事だ。

329
00:11:14,410 --> 00:11:15,990
オリジナルの生の入力xiを使ってみよう、

330
00:11:16,300 --> 00:11:17,770
そして私が推奨するのは、

331
00:11:18,180 --> 00:11:19,550
PCAをアルゴリズムに

332
00:11:19,720 --> 00:11:20,910
組み込む代わりに、

333
00:11:21,030 --> 00:11:23,250
あなたがやってる事がなんであれ、最初のxiでやってみる事を推奨する。

334
00:11:24,090 --> 00:11:25,000
そしてそれがうまくいかない、と

335
00:11:25,150 --> 00:11:26,180
信じる理由を得て初めて、

336
00:11:26,480 --> 00:11:27,590
つまりあなたの学習アルゴリズムが

337
00:11:27,790 --> 00:11:29,470
あまりにも実行速度が遅いという

338
00:11:29,510 --> 00:11:31,100
結果になって初めて、

339
00:11:31,280 --> 00:11:32,680
または必要メモリの要求量や

340
00:11:32,910 --> 00:11:34,140
必要ディスクの要求量があまりにも大きくなり過ぎて、

341
00:11:34,430 --> 00:11:35,850
だから表現を圧縮したい、と思った時に、、、

342
00:11:36,190 --> 00:11:37,810
それはxiを使ってみて

343
00:11:38,000 --> 00:11:39,020
うまく行かなかった時になって、、、

344
00:11:39,360 --> 00:11:40,640
xiではうまく行かない、という

345
00:11:40,950 --> 00:11:41,890
証拠がある時か、または

346
00:11:42,380 --> 00:11:43,890
そう信じる強い理由があるようになって、、、

347
00:11:44,380 --> 00:11:46,730
その時初めて、PCAを実装して表現を圧縮する事に使えば良い。

348
00:11:47,990 --> 00:11:48,830
こんな事を言うのは、

349
00:11:49,100 --> 00:11:50,380
人々が最初からプロジェクトのプランに

350
00:11:50,530 --> 00:11:51,520
PCAを用いる事を計画していて、

351
00:11:52,100 --> 00:11:54,580
そして時々、

352
00:11:54,650 --> 00:11:55,620
彼らが何をするにせよ、

353
00:11:55,820 --> 00:11:57,380
PCA無しでもうまく行く、という事態を

354
00:11:57,660 --> 00:11:59,520
私はちょくちょく目撃して来たからだ。

355
00:11:59,840 --> 00:12:01,650
だからPCAを使うのは、

356
00:12:01,860 --> 00:12:03,130
代替案も検討した後にすべきだ、

357
00:12:03,320 --> 00:12:04,170
PCAを得たり、どのkを使うべきかとかを見出したりに

358
00:12:04,300 --> 00:12:05,570
たくさん時間を費やしたり

359
00:12:05,770 --> 00:12:08,100
する前に。

360
00:12:08,250 --> 00:12:09,330
さて、以上がPCAだ。

361
00:12:09,800 --> 00:12:11,000
最後に付したこれらのコメントにも関わらず、

362
00:12:11,080 --> 00:12:12,380
適切な用途で用いれば、

363
00:12:12,690 --> 00:12:14,060
PCAは信じられない程

364
00:12:14,150 --> 00:12:15,330
便利なアルゴリズムだ。

365
00:12:16,070 --> 00:12:17,480
そして現実に、私はPCAをしょっちゅう使う。

366
00:12:17,770 --> 00:12:19,330
私の場合は、

367
00:12:19,580 --> 00:12:20,650
PCAを使うのは、学習アルゴリズムの実行時間を

368
00:12:20,850 --> 00:12:22,150
スピードアップするのに使う事が一番多い。

369
00:12:22,880 --> 00:12:24,310
だが私が思うに、

370
00:12:24,400 --> 00:12:25,690
PCAはその他の用途も同じくらい一般的だと思う。

371
00:12:26,020 --> 00:12:27,300
データを圧縮するのに使う、これは

372
00:12:27,410 --> 00:12:29,030
メモリやディスク容量の要件を

373
00:12:29,620 --> 00:12:30,650
減らす事が出来るし、

374
00:12:30,990 --> 00:12:33,130
またデータの可視化にも良く使われる。

375
00:12:34,270 --> 00:12:35,710
そしてPCAは教師なし学習のアルゴリズムのうち、

376
00:12:35,750 --> 00:12:36,960
もっとも良く使われていて、

377
00:12:36,990 --> 00:12:39,420
もっともパワフルな物の一つと言えるだろう。

378
00:12:40,060 --> 00:12:41,210
そしてこれらのビデオで学んだ事を用いて、

379
00:12:41,420 --> 00:12:43,120
あなたは、きっと

380
00:12:43,500 --> 00:12:44,710
PCAを実装する事が出来て、

381
00:12:45,150 --> 00:12:46,280
これらの目的全てに対しても

382
00:12:46,500 --> 00:12:47,930
使っていく事が出来るだろう。