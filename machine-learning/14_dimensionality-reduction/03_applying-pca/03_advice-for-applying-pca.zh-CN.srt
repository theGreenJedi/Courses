1
00:00:00,090 --> 00:00:01,450
在之前的视频中 我已经

2
00:00:01,610 --> 00:00:02,710
提到过 PCA 有时可以

3
00:00:02,840 --> 00:00:05,410
用来提高机器学习算法的速度

4
00:00:07,070 --> 00:00:08,140
在本节课的视频中

5
00:00:08,370 --> 00:00:09,520
讲解如何在实际操作中

6
00:00:09,820 --> 00:00:11,270
来实现 同时列举

7
00:00:11,460 --> 00:00:12,900
一些例子 只是希望能够

8
00:00:12,990 --> 00:00:14,550
提供一些应用 PCA 的建议

9
00:00:17,110 --> 00:00:19,630
首先我先介绍如何通过 PCA 来提高学习算法的速度

10
00:00:20,270 --> 00:00:21,940
这种监督学习算法的提速

11
00:00:22,270 --> 00:00:23,630
实际上也是

12
00:00:23,870 --> 00:00:25,870
我个人经常通过使用

13
00:00:26,530 --> 00:00:27,720
PCA 来实现的一种功能

14
00:00:28,710 --> 00:00:29,640
比如说你遇到了一个监督学习问题

15
00:00:30,300 --> 00:00:31,660
注意这个

16
00:00:31,810 --> 00:00:33,380
监督学习算法问题有

17
00:00:33,690 --> 00:00:35,510
输入 x 和 标签 y

18
00:00:35,810 --> 00:00:37,330
假如说你的样本

19
00:00:37,830 --> 00:00:39,140
x(i) 是非常高维的数据

20
00:00:39,840 --> 00:00:41,670
比如说 x(i) 是

21
00:00:41,800 --> 00:00:44,000
一个10,000维的向量

22
00:00:45,510 --> 00:00:46,550
比如说其中的一个例子是

23
00:00:46,700 --> 00:00:48,130
你在解决某个计算机

24
00:00:48,540 --> 00:00:50,390
视觉的问题 在这里有

25
00:00:50,650 --> 00:00:52,410
一张100 × 100的图片 那么

26
00:00:52,780 --> 00:00:55,550
如果是100×100 那就是10000

27
00:00:55,850 --> 00:00:57,520
像素 如果 x(i) 是

28
00:00:57,780 --> 00:00:59,240
包含了

29
00:00:59,760 --> 00:01:01,670
这10000像素

30
00:01:02,470 --> 00:01:03,580
强度值的特征向量 那么

31
00:01:04,410 --> 00:01:05,580
你就会有10000维特征向量

32
00:01:06,880 --> 00:01:08,530
像这样有很高维

33
00:01:09,300 --> 00:01:10,890
的特征向量 

34
00:01:11,320 --> 00:01:12,860
运行会比较慢

35
00:01:13,030 --> 00:01:14,300
如果你输入10,000维

36
00:01:14,790 --> 00:01:16,980
的特征向量到逻辑回归中

37
00:01:17,570 --> 00:01:19,780
或者到一个神经网络、支持向量机中 或者任何别的算法中

38
00:01:20,450 --> 00:01:22,000
由于这是一个很大量的数据集

39
00:01:22,200 --> 00:01:23,060
有10,000个数据

40
00:01:24,130 --> 00:01:25,970
因此这会使得学习算法运行得更慢

41
00:01:27,170 --> 00:01:28,530
幸运的是 通过使用 PCA 我们

42
00:01:28,680 --> 00:01:29,810
能够降低数据的维数

43
00:01:30,060 --> 00:01:31,050
从而使得

44
00:01:31,180 --> 00:01:32,410
算法能够更加高效地运行

45
00:01:32,920 --> 00:01:34,440
这就是其中

46
00:01:34,590 --> 00:01:35,780
的原理 首先我们需要

47
00:01:35,980 --> 00:01:37,030
检查带标签的

48
00:01:37,400 --> 00:01:39,520
的训练数据集并提取出

49
00:01:39,800 --> 00:01:41,830
输入数据 我们只需要提取出 x

50
00:01:42,730 --> 00:01:45,130
并暂时把 y 放在一边

51
00:01:45,860 --> 00:01:46,750
这一步我们会得到

52
00:01:47,090 --> 00:01:49,150
一组无标签的训练集 从 x(1)

53
00:01:49,400 --> 00:01:51,000
到 x(m) 这可能会有

54
00:01:51,240 --> 00:01:53,600
10,000维数据

55
00:01:53,940 --> 00:01:55,800
也就是10,000维数据样本

56
00:01:55,870 --> 00:01:57,230
所以 就是从数据组中

57
00:01:58,370 --> 00:01:58,930
x(1) 到 x(m) 中提取出输入向量

58
00:02:00,650 --> 00:02:01,810
然后我们应用PCA

59
00:02:02,700 --> 00:02:03,740
从中我们

60
00:02:03,980 --> 00:02:06,100
会得到一个降维的

61
00:02:06,410 --> 00:02:08,010
数据 与刚才的

62
00:02:08,260 --> 00:02:09,540
10,000维特征相比 我现在

63
00:02:09,780 --> 00:02:11,880
就只有1000维特征向量

64
00:02:12,330 --> 00:02:13,500
因此这就降低了10倍的维数

65
00:02:15,110 --> 00:02:17,260
这就给了我们一个新的训练集

66
00:02:17,910 --> 00:02:19,430
所以之前我有

67
00:02:19,620 --> 00:02:21,180
这样一个样本 x(1), y(1)

68
00:02:21,490 --> 00:02:24,340
这是我的第一个训练集的输入 现在用 z(1) 来表示

69
00:02:24,580 --> 00:02:25,800
这样我们就有了

70
00:02:26,050 --> 00:02:27,010
一个新的训练集样本

71
00:02:28,210 --> 00:02:29,240
其中 z(1) 与 y(1) 是一对儿

72
00:02:30,700 --> 00:02:33,170
同样地  z(2) 对应 y(2) 等等 一直到 z(m) 对 y(m)

73
00:02:33,770 --> 00:02:35,300
因为现在的训练集

74
00:02:35,460 --> 00:02:36,980
由这样一个

75
00:02:37,480 --> 00:02:41,040
更加低维的数据集所代替 z(1),z(2) 一直到 z(m)

76
00:02:41,310 --> 00:02:42,340
最后 我可以将这个

77
00:02:43,650 --> 00:02:45,060
已经降维的数据集

78
00:02:45,240 --> 00:02:46,540
输入到学习算法 或者是将其

79
00:02:46,640 --> 00:02:47,900
放入到神经网络中 或者是

80
00:02:48,280 --> 00:02:49,450
逻辑回归中 可以

81
00:02:49,750 --> 00:02:51,990
学习出假设 h

82
00:02:52,230 --> 00:02:53,830
把这个作为输入 这些低维的

83
00:02:54,330 --> 00:02:56,230
z 作为输入 并作出预测

84
00:02:57,890 --> 00:02:59,030
所以比如说在使用逻辑回归时

85
00:02:59,460 --> 00:03:00,880
我应该训练得到

86
00:03:01,060 --> 00:03:02,760
某一个假设函数 其输出是

87
00:03:03,080 --> 00:03:04,020
1除以1加e的

88
00:03:04,180 --> 00:03:06,020
负的θ次方 转置

89
00:03:07,620 --> 00:03:10,150
乘以 z  这个式子将

90
00:03:10,610 --> 00:03:11,530
z 向量作为

91
00:03:11,960 --> 00:03:13,660
输入 并得出一个预测值

92
00:03:15,260 --> 00:03:16,310
最后 如果你有

93
00:03:16,630 --> 00:03:17,800
一个新的样本 比如说一个新的

94
00:03:17,920 --> 00:03:20,060
测试样本 x

95
00:03:20,220 --> 00:03:21,340
你所要做的是

96
00:03:22,130 --> 00:03:23,730
将你的测试样本 x

97
00:03:24,960 --> 00:03:26,590
通过同样的过程操作下来

98
00:03:26,990 --> 00:03:27,860
通过 PCA

99
00:03:28,220 --> 00:03:29,610
你会得到所对应的 z

100
00:03:30,390 --> 00:03:31,280
然后这个 z 值

101
00:03:31,950 --> 00:03:33,740
又可以输入到这个假设式子中

102
00:03:33,910 --> 00:03:35,450
这个假设之后会对你输入的

103
00:03:35,750 --> 00:03:36,740
x作出一个预测

104
00:03:38,110 --> 00:03:40,090
最后要注意一点 PCA

105
00:03:40,510 --> 00:03:42,350
定义了从

106
00:03:42,710 --> 00:03:45,090
x到z的对应关系

107
00:03:45,960 --> 00:03:46,970
这种从 x 到

108
00:03:47,050 --> 00:03:48,280
z的对应关系只可以通过

109
00:03:48,580 --> 00:03:50,840
在训练集上运行 PCA 定义出来

110
00:03:51,650 --> 00:03:53,310
具体来讲 这种

111
00:03:53,530 --> 00:03:54,770
PCA所学习出的对应关系

112
00:03:54,950 --> 00:03:57,650
所做的就是计算出一系列的参数

113
00:03:58,210 --> 00:04:00,500
这就是特征缩放和均值归一化

114
00:04:01,240 --> 00:04:04,040
同时也计算出这样一个降维的矩阵Ureduce

115
00:04:04,680 --> 00:04:05,510
但是降维矩阵 Ureduce 中的数据

116
00:04:05,670 --> 00:04:06,980
就像一个

117
00:04:07,120 --> 00:04:08,420
PCA所学习的参数一样

118
00:04:08,670 --> 00:04:09,950
我们需要

119
00:04:10,150 --> 00:04:12,270
使我们的参数唯一地适应

120
00:04:12,480 --> 00:04:13,990
这些训练集 而不是

121
00:04:14,040 --> 00:04:16,250
适应我们的交叉验证或者测试集

122
00:04:16,370 --> 00:04:17,560
因此Ureduce矩阵中的数据

123
00:04:18,180 --> 00:04:19,460
就应该

124
00:04:19,820 --> 00:04:22,430
只通过对训练集运行PCA来获得

125
00:04:23,300 --> 00:04:26,930
找出了降维矩阵Ureduce 或者找出了这些特征扩展的参数之后

126
00:04:27,350 --> 00:04:28,620
均值均一化

127
00:04:29,320 --> 00:04:31,790
并扩展可以

128
00:04:32,180 --> 00:04:34,500
用分隔可以比较的规格

129
00:04:34,760 --> 00:04:36,010
在训练集中找到了

130
00:04:36,570 --> 00:04:38,010
所有这些参数后 就可以

131
00:04:38,220 --> 00:04:41,560
将同样的对应关系应用到其他样本中了 可能

132
00:04:41,820 --> 00:04:45,020
是交叉验证数集样本 或者

133
00:04:45,180 --> 00:04:46,680
用在你的测试数据集中

134
00:04:47,150 --> 00:04:48,340
总结一下 当你在

135
00:04:48,450 --> 00:04:49,790
运行PCA的时候 只是在

136
00:04:49,900 --> 00:04:51,070
训练集那一部分

137
00:04:51,220 --> 00:04:52,450
来进行的

138
00:04:52,490 --> 00:04:55,880
而不是 交叉验证的数据集

139
00:04:56,410 --> 00:04:57,620
这就定义了从

140
00:04:57,870 --> 00:04:58,770
x到z的映射 然后你就可以

141
00:04:58,950 --> 00:05:00,320
将这个映射应用到

142
00:05:00,560 --> 00:05:02,240
交叉验证数据集中和

143
00:05:02,290 --> 00:05:03,370
测试数据集中 通过这个

144
00:05:03,450 --> 00:05:04,660
例子中的这种方式

145
00:05:05,000 --> 00:05:06,660
我们讨论了将数据从

146
00:05:06,950 --> 00:05:08,510
上万维降到

147
00:05:08,740 --> 00:05:10,350
千维 这实际上

148
00:05:10,660 --> 00:05:11,950
并不切实际

149
00:05:12,280 --> 00:05:14,720
因为对于大多数我们实际面对的数据降维问题

150
00:05:17,600 --> 00:05:18,700
 降维到原来的五分之一或者十分之一

151
00:05:18,780 --> 00:05:20,910
依旧保持着原本维度数据的变化情况

152
00:05:21,270 --> 00:05:22,680
改变并不会有多少影响

153
00:05:23,900 --> 00:05:25,840
就分类的精确度而言

154
00:05:26,240 --> 00:05:27,970
数据降维后对学习算法

155
00:05:28,770 --> 00:05:30,320
几乎没有什么影响

156
00:05:31,090 --> 00:05:32,140
如果我们将降维

157
00:05:32,590 --> 00:05:33,730
用在低维数据上

158
00:05:34,060 --> 00:05:36,500
我们的学习算法会运行得更快

159
00:05:36,910 --> 00:05:38,120
总之 

160
00:05:38,410 --> 00:05:40,920
迄今为止我们讨论过的有关PCA的应用中

161
00:05:41,970 --> 00:05:43,780
第一个是数据压缩

162
00:05:44,020 --> 00:05:45,140
我们可以借此

163
00:05:45,500 --> 00:05:46,440
减少内存或者磁盘空间的使用

164
00:05:46,590 --> 00:05:47,960
以存取更多的数据

165
00:05:48,240 --> 00:05:49,390
正如刚刚我们讨论过的

166
00:05:49,460 --> 00:05:51,630
就是如何使用数据压缩以加快学习算法的例子

167
00:05:52,100 --> 00:05:53,870
在这些应用中

168
00:05:54,130 --> 00:05:56,240
为了选择一个k值

169
00:05:56,420 --> 00:05:58,770
我们将会根据

170
00:05:59,160 --> 00:06:00,590
保留方差的百分比

171
00:06:00,810 --> 00:06:03,880
来确定k值

172
00:06:04,780 --> 00:06:06,320
对于一个学习算法来说

173
00:06:06,570 --> 00:06:10,050
加快应用将会保留99%的方差

174
00:06:10,530 --> 00:06:11,690
在如何选择k值的问题上

175
00:06:12,100 --> 00:06:14,270
这就是一个很典型的问题

176
00:06:14,730 --> 00:06:16,640
也就是说k的选择是一个数据压缩的应用

177
00:06:17,850 --> 00:06:19,590
然而对于可视化应用来说

178
00:06:20,760 --> 00:06:22,100
我们通常知道

179
00:06:22,230 --> 00:06:23,550
如何将二维

180
00:06:24,020 --> 00:06:25,520
或者三维的数据进行可视化

181
00:06:26,540 --> 00:06:28,650
所以对于可视化应用

182
00:06:28,830 --> 00:06:29,660
我们选择的K值要么等于2

183
00:06:29,710 --> 00:06:31,930
要么等于3

184
00:06:32,740 --> 00:06:33,500
因为我们能画出二维和三维的数据集

185
00:06:34,510 --> 00:06:35,720
所以

186
00:06:36,020 --> 00:06:37,230
我们来总结一下PCA的主要应用

187
00:06:37,870 --> 00:06:39,580
其实也就是

188
00:06:39,670 --> 00:06:41,540
对于不同的应用来选择K值

189
00:06:42,890 --> 00:06:45,710
我要提醒的是有一个频繁

190
00:06:46,400 --> 00:06:48,100
被误用的PCA应用

191
00:06:48,800 --> 00:06:50,300
你有时或许能听到其他人这么做 

192
00:06:50,580 --> 00:06:51,820
当然我们不希望这样

193
00:06:52,230 --> 00:06:54,780
我只是想提醒你不要这么做

194
00:06:55,480 --> 00:06:56,460
这是一个对PCA不好的应用方面

195
00:06:56,540 --> 00:06:59,170
那就是使用它来避免过拟合

196
00:07:00,380 --> 00:07:00,660
下面是原因

197
00:07:01,910 --> 00:07:03,080
这并不是一个

198
00:07:03,730 --> 00:07:04,610
合适的PCA应用

199
00:07:04,670 --> 00:07:05,630
下面

200
00:07:05,690 --> 00:07:07,080
我就来讲原因

201
00:07:07,350 --> 00:07:09,090
如果我们有x(i)

202
00:07:09,300 --> 00:07:10,660
或许x(i)是有n个特征的数据集

203
00:07:10,830 --> 00:07:12,640
如果我们将数据进行压缩

204
00:07:12,750 --> 00:07:13,700
并用压缩后的数据z(i)来代替原始数据

205
00:07:14,270 --> 00:07:15,410
在降维过程中

206
00:07:15,560 --> 00:07:17,050
我们从n个特征降维到k个

207
00:07:17,290 --> 00:07:19,300
比先前的维度低

208
00:07:19,410 --> 00:07:21,130
例如如果我们有

209
00:07:21,490 --> 00:07:22,520
非常小的特征数目

210
00:07:22,770 --> 00:07:25,800
假如k值为1000

211
00:07:26,090 --> 00:07:27,010
n值为10000

212
00:07:27,780 --> 00:07:29,390
如果我们有1000维度的数据

213
00:07:29,670 --> 00:07:30,580
和我们用10000维度的数据比起来

214
00:07:31,260 --> 00:07:32,230
对于同样是1000个特征来说

215
00:07:33,280 --> 00:07:34,980
或许更不容易过拟合

216
00:07:35,950 --> 00:07:37,160
所以有些人认为

217
00:07:37,360 --> 00:07:39,360
PCA是一种避免过拟合的方法

218
00:07:39,950 --> 00:07:41,940
但我这里要强调一下

219
00:07:42,110 --> 00:07:44,000
PCA在过拟合问题上的应用是不合适的

220
00:07:44,260 --> 00:07:46,080
并且我不建议这么做

221
00:07:46,520 --> 00:07:48,430
不仅仅这个方法的效果很差

222
00:07:49,000 --> 00:07:49,920
如果你想使用

223
00:07:50,330 --> 00:07:51,560
PCA方法来对数据降维

224
00:07:51,890 --> 00:07:52,830
以避免过拟合

225
00:07:53,690 --> 00:07:54,830
PCA方法实际看起来是可以的

226
00:07:55,560 --> 00:07:56,720
但是这并不是

227
00:07:57,040 --> 00:07:58,340
一个用来解决

228
00:07:58,680 --> 00:08:00,390
过拟合问题的算法

229
00:08:00,510 --> 00:08:01,810
如果你比较担心过拟合问题

230
00:08:02,030 --> 00:08:03,420
有更好的方法来解决

231
00:08:03,800 --> 00:08:05,680
那就是使用正则化方法

232
00:08:05,900 --> 00:08:07,910
代替PCA来对数据进行降维

233
00:08:08,670 --> 00:08:10,000
原因是

234
00:08:11,010 --> 00:08:12,150
如果你仔细想想PCA是如何工作的

235
00:08:12,900 --> 00:08:13,950
它并不需要使用数据的标签 

236
00:08:14,530 --> 00:08:15,680
你只需要看好

237
00:08:16,050 --> 00:08:17,220
输入数据x(i)

238
00:08:17,340 --> 00:08:19,070
同时使用这个方法

239
00:08:19,130 --> 00:08:21,150
来寻找更低维度的数据近似

240
00:08:21,390 --> 00:08:22,840
那么 PCA做了什么呢

241
00:08:23,190 --> 00:08:25,410
它把某些信息舍弃掉了

242
00:08:26,460 --> 00:08:28,040
舍弃掉一些数据

243
00:08:28,180 --> 00:08:29,680
并在你对数据标签y值毫不知情的情况下

244
00:08:30,110 --> 00:08:31,390
对数据进行降维

245
00:08:32,380 --> 00:08:33,700
所以

246
00:08:34,250 --> 00:08:35,770
这或许是一个使用PCA方法

247
00:08:35,920 --> 00:08:37,750
的可行之路

248
00:08:37,990 --> 00:08:39,190
如果保留99%

249
00:08:39,410 --> 00:08:40,400
的方差

250
00:08:40,830 --> 00:08:41,970
即保留绝大部分的方差

251
00:08:42,100 --> 00:08:44,230
那也就是舍弃掉某些有用信息

252
00:08:45,010 --> 00:08:45,980
事实证明

253
00:08:46,310 --> 00:08:47,580
当你在保留99%

254
00:08:47,820 --> 00:08:49,260
或者95%

255
00:08:49,360 --> 00:08:50,940
或者其它百分比的方差时

256
00:08:51,020 --> 00:08:52,310
结果表明

257
00:08:52,720 --> 00:08:54,650
就只使用正则化将会给你

258
00:08:54,790 --> 00:08:56,010
一种避免过拟合

259
00:08:56,220 --> 00:08:57,880
绝对好的方法

260
00:08:58,900 --> 00:09:00,340
同时正则化

261
00:09:00,590 --> 00:09:02,220
效果也会比PCA更好

262
00:09:02,350 --> 00:09:03,890
因为当你使用线性回归或者逻辑回归

263
00:09:04,250 --> 00:09:05,240
或其他的方法

264
00:09:05,600 --> 00:09:07,390
配合正则化时

265
00:09:08,010 --> 00:09:09,420
这个最小化问题

266
00:09:09,480 --> 00:09:10,740
实际就变成了y值是什么

267
00:09:10,960 --> 00:09:12,680
才不至于

268
00:09:12,880 --> 00:09:14,330
将有用的信息舍弃掉

269
00:09:14,730 --> 00:09:15,790
然而PCA不需要使用到

270
00:09:16,060 --> 00:09:17,810
这些标签 

271
00:09:17,850 --> 00:09:19,940
更容易将有价值信息舍弃

272
00:09:20,230 --> 00:09:21,370
总之

273
00:09:21,620 --> 00:09:22,900
使用PCA的目的是

274
00:09:23,010 --> 00:09:24,380
加速

275
00:09:24,530 --> 00:09:26,490
学习算法的时候是好的

276
00:09:26,790 --> 00:09:28,360
但是用它来避免过拟合

277
00:09:28,650 --> 00:09:29,630
却并不是一个好的PCA应用

278
00:09:30,030 --> 00:09:32,270
我们使用正则化的方法来代替PCA方法

279
00:09:32,900 --> 00:09:36,190
是很多人

280
00:09:36,440 --> 00:09:40,490
建议的

281
00:09:41,310 --> 00:09:43,350
最后讲一下PCA的误用

282
00:09:43,750 --> 00:09:45,760
我说PCA是一个非常有用的算法

283
00:09:46,270 --> 00:09:49,170
我经常用它在可视化数据上进行数据压缩

284
00:09:50,230 --> 00:09:51,400
但我有时候

285
00:09:51,570 --> 00:09:53,310
会看到有些人

286
00:09:53,710 --> 00:09:56,080
把PCA用在了不应当使用的地方

287
00:09:56,220 --> 00:09:57,940
从中

288
00:09:58,030 --> 00:09:59,140
我都看到一个共同点

289
00:09:59,330 --> 00:10:00,330
如果某人正在设计机器学习系统

290
00:10:01,010 --> 00:10:02,130
他们或许会写下像这样的计划

291
00:10:02,200 --> 00:10:04,150
让我们设计一个学习系统

292
00:10:05,060 --> 00:10:06,080
得到训练集然后

293
00:10:06,570 --> 00:10:07,350
我要做的是

294
00:10:07,400 --> 00:10:08,700
先运行PCA

295
00:10:08,860 --> 00:10:11,200
然后训练逻辑回归之后在测试数据上进行测试

296
00:10:11,680 --> 00:10:12,770
通常在

297
00:10:13,090 --> 00:10:14,360
一个项目的初期

298
00:10:14,600 --> 00:10:15,600
有些人便直接写出

299
00:10:15,720 --> 00:10:16,980
项目计划而不是说

300
00:10:17,310 --> 00:10:18,610
来试试PCA的这四步

301
00:10:20,210 --> 00:10:21,220
在写下一个

302
00:10:21,530 --> 00:10:23,350
使用PCA方法的项目计划前

303
00:10:23,560 --> 00:10:24,860
一个非常好的

304
00:10:25,030 --> 00:10:27,110
问题是

305
00:10:27,630 --> 00:10:28,560
如果我们在整个项目中

306
00:10:29,540 --> 00:10:31,470
不使用前后有怎样的差别

307
00:10:32,170 --> 00:10:33,450
通常人们不会

308
00:10:33,800 --> 00:10:34,940
去思考这个问题

309
00:10:35,440 --> 00:10:37,080
尤其是当人们提出一个复杂的项目

310
00:10:37,920 --> 00:10:40,620
其中使用了PCA或其它方法

311
00:10:40,810 --> 00:10:42,360
有时

312
00:10:43,050 --> 00:10:44,300
我经常建议大家

313
00:10:44,670 --> 00:10:45,980
在你使用PCA之前

314
00:10:46,450 --> 00:10:47,970
首先

315
00:10:48,220 --> 00:10:49,410
我要建议你的是

316
00:10:49,600 --> 00:10:50,770
你要知道自己做的是什么

317
00:10:50,850 --> 00:10:52,030
也就是说你想要做什么

318
00:10:52,450 --> 00:10:53,650
这也是你首先

319
00:10:53,980 --> 00:10:56,420
需要在原始数据x(i)上考虑的问题

320
00:10:56,600 --> 00:10:57,860
只有你没有想过你要的是什么的时候

321
00:10:57,960 --> 00:10:59,650
那么在你使用z(i)前使用PCA

322
00:11:01,010 --> 00:11:02,420
所以在使用PCA之前

323
00:11:03,030 --> 00:11:03,930
确定是减少数据维度

324
00:11:04,360 --> 00:11:05,710
我会将这个问题思考清楚

325
00:11:06,640 --> 00:11:08,020
让我们抛弃PCA这一步

326
00:11:08,420 --> 00:11:09,690
并思考

327
00:11:10,040 --> 00:11:11,460
让我们就试试在学习算法上

328
00:11:12,440 --> 00:11:13,560
使用原始数据

329
00:11:14,410 --> 00:11:15,990
那么 我们就会使用原始数据输入x(i)

330
00:11:16,300 --> 00:11:17,770
同时我也建议

331
00:11:18,180 --> 00:11:19,550
一开始不要将

332
00:11:19,720 --> 00:11:20,910
PCA方法就直接放到算法里

333
00:11:21,030 --> 00:11:23,250
先使用原始数据x(i)看看效果

334
00:11:24,090 --> 00:11:25,000
只有一个原因

335
00:11:25,150 --> 00:11:26,180
让我们相信算法出现了问题

336
00:11:26,480 --> 00:11:27,590
那就是

337
00:11:27,790 --> 00:11:29,470
你的学习算法

338
00:11:29,510 --> 00:11:31,100
收敛地非常缓慢

339
00:11:31,280 --> 00:11:32,680
占用内存

340
00:11:32,910 --> 00:11:34,140
或者硬盘空间非常大

341
00:11:34,430 --> 00:11:35,850
所以你想来压缩

342
00:11:36,190 --> 00:11:37,810
数据

343
00:11:38,000 --> 00:11:39,020
只有当你的x(i)效果不好

344
00:11:39,360 --> 00:11:40,640
只有当你有证据或者

345
00:11:40,950 --> 00:11:41,890
充足的理由来确定

346
00:11:42,380 --> 00:11:43,890
x(i)效果不好的时候

347
00:11:44,380 --> 00:11:46,730
那么就考虑用PCA来进行压缩数据

348
00:11:47,990 --> 00:11:48,830
因为我常常看到

349
00:11:49,100 --> 00:11:50,380
某些人

350
00:11:50,530 --> 00:11:51,520
在项目开始时便将PCA考虑进去

351
00:11:52,100 --> 00:11:54,580
有时他们

352
00:11:54,650 --> 00:11:55,620
并没有仔细思考

353
00:11:55,820 --> 00:11:57,380
他们做了什么使得结果表现地好的

354
00:11:57,660 --> 00:11:59,520
更没有考虑在不用PCA下的情景

355
00:11:59,840 --> 00:12:01,650
试想

356
00:12:01,860 --> 00:12:03,130
这种没有PCA的选择

357
00:12:03,320 --> 00:12:04,170
在花费大量时间

358
00:12:04,300 --> 00:12:05,570
在PCA方法上

359
00:12:05,770 --> 00:12:08,100
去计算 k 值之前

360
00:12:08,250 --> 00:12:09,330
这才是使用PCA的正确流程

361
00:12:09,800 --> 00:12:11,000
尽管有这些需要注意的

362
00:12:11,080 --> 00:12:12,380
PCA仍旧是

363
00:12:12,690 --> 00:12:14,060
一种不可思议的有用的算法

364
00:12:14,150 --> 00:12:15,330
当你把它用在合适的应用上面

365
00:12:16,070 --> 00:12:17,480
PCA方法

366
00:12:17,770 --> 00:12:19,330
我使用地比较频繁

367
00:12:19,580 --> 00:12:20,650
大部分时候我都用它来

368
00:12:20,850 --> 00:12:22,150
加快学习算法

369
00:12:22,880 --> 00:12:24,310
但我认为

370
00:12:24,400 --> 00:12:25,690
PCA通常都是

371
00:12:26,020 --> 00:12:27,300
被用来

372
00:12:27,410 --> 00:12:29,030
压缩数据的

373
00:12:29,620 --> 00:12:30,650
以减少内存使用

374
00:12:30,990 --> 00:12:33,130
或硬盘空间占用 或者用来可视化数据

375
00:12:34,270 --> 00:12:35,710
同时 

376
00:12:35,750 --> 00:12:36,960
是最常用的方法之一

377
00:12:36,990 --> 00:12:39,420
也是一种强有力的无监督学习算法

378
00:12:40,060 --> 00:12:41,210
通过在本视频中学到的

379
00:12:41,420 --> 00:12:43,120
希望

380
00:12:43,500 --> 00:12:44,710
你有能力实现

381
00:12:45,150 --> 00:12:46,280
PCA算法并用它

382
00:12:46,500 --> 00:12:47,930
来实现你的目的 【教育无边界字幕组】翻译：Yuens 校对/审核：所罗门捷列夫