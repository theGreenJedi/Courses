Em um vídeo anterior, eu disse que PCA pode ser usado para acelerar o Algoritmo de Aprendizagem. Neste vídeo, eu gostaria de explicar como fazer isso, e também, dar alguns conselhos sobre como usar PCA. É assim que você pode utilizar PCA para agilizar um algoritmo de aprendizagem. Esse ganho de desempenho em Aprendizado Supervisionado, é o uso mais comum que faço do PCA. Digamos que você tenha um problema de Aprendizado Supervisionado -- observe que esse é um problema com entradas X e rótulos Y -- Digamos que seus exemplos x⁽ᴵ⁾ têm grandes dimensões. Digamos que seus exemplos x⁽ᴵ⁾ são vetores de dimensão 10.000. Um exemplo disso é: se você está trabalhando em um problema de visão computacional, onde você tem imagens 100x100. Assim, se você tem 100x100, são 10.000 pixels, e então, se x⁽ᴵ⁾ são vetores de atributos, que contém seus 10.000 valores de intensidade de pixels, você tem vetores de dimensão 10.000. Com vetores de atributos com dimensões tão grandes como essas seu algoritmo pode ficar lento, certo? Se você usar um vetor de dimensão 10.000 como entrada para um algoritmo de regressão logística, rede neural, ou máquina de vetor suporte, como são muitos dados, seu algoritmo pode ficar muito mais lento. Felizmente, com PCA podemos reduzir a dimensão desses dados, e assim tornar nosso algoritmo mais eficiente. É assim que fazemos isso: vamos primeiramente checar nosso conjunto de treinamento rotulado, e extrair apenas as  entradas. Vamos apenas extrair os X's e, temporariamente, deixar os Y's de lado. Isso nos dará um conjunto de treinamento não-rotulado x⁽¹⁾ até x⁽ᵐ⁾,  que talvez seja um vetor de dados de 10 mil dimensões, dos 10 mil  exemplos dimensionais que temos. Portanto, apenas extrair as entradas x⁽¹⁾ até x⁽ᵐ⁾. Vamos aplicar PCA, e isso me dará uma representação dos dados com dimensão reduzida. Ao invés de vetores com dimensão 10.000, agora talvez eu tenha vetores de atributos de dimensão 1.000. Isso é uma economia de 10 vezes. Isso me dará um novo conjunto de treinamento. Enquanto anteriormente eu tinha um exemplo (x⁽¹⁾ ,y⁽¹⁾), agora minha primeira entrada de treinamento é z⁽¹⁾. Agora eu tenho esse novo tipo de exemplo de treinamento, z⁽¹⁾ emparelhado com y⁽¹⁾, similarmente, z⁽²⁾ com y⁽²⁾, ..., até z⁽ᵐ⁾ com y⁽ᵐ⁾ porque meus exemplos de treinamento são agora representados com essa dimensão muito menor z⁽¹⁾, z⁽²⁾, até z⁽ᵐ⁾. Finalmente, eu posso tomar esse conjunto de treinamento de dimensão reduzida e alimentar um algoritmo de aprendizagem, de Regressão ou Rede Neural, e posso aprender a hipótese H, que tem como entrada essas representações de dimensão reduzida "z",  e tentar fazer previsões. Assim, se eu estivesse usando Regressão Logística, por exemplo, eu treinaria uma hipótese que tem como saída 1/(1+e^(-θᵀz)) que tem como entrada um desses vetores "z", e tenta fazer uma predição. E finalmente, se você tiver um novo exemplo de teste "x", o que você faria é pegar seu exemplo de teste "x", mapear-lo usando o mesmo mapeamento encontrado pelo PCA para obter o correspondente "z". E esse "z" é usado para alimentar essa hipótese, e essa hipótese fará uma predição, para a sua entrada "x". Um último ponto: o que PCA faz é definir um mapeamento de "x" para "z", e esse mapeamento de "x" para "z" deve ser definido rodando PCA apenas no conjunto de treinamento. E esse mapeamento aprendido pelo PCA, em especial, o que ele faz é computar um conjunto de parâmetros - o redimensionamento dos parâmetros e a normalização pela média e também a computação dessa matriz "U-reduce". Mas todas essas coisas, como a matriz "U-reduce", são parâmetros que são aprendidos pelo PCA, e nós devemos ajustar nossos parâmetros apenas ao nosso conjunto de treinamento, e não aos nossos conjuntos de validação cruzada e teste. Então, tudo isso deve ser obtido rodando PCA apenas no conjunto de treinamento. E tendo encontrado "U-reduce", ou os parâmetros para redimensionamento das variáveis: a média para normalização, e a valor pelo qual você divide as variáveis, para colocá-las em escalas compatíveis. Tendo encontrado todos esses parâmetros para o conjunto de treinamento, você pode aplicar o mesmo mapeamento para outros exemplos no seu conjunto de validação cruzada, ou no conjunto de teste, okay? Resumindo: quando você roda PCA, rode o PCA apenas para os dados do conjunto de treinamento, não para dados do conjunto de validação cruzada, ou do conjunto de teste. E isso define o mapeamento de "x" para "z", e você pode, então, aplicar esse mapeamento para os seus dados de validação e teste. Aliás, nesse exemplo, eu falei sobre reduzir os dados de dimensão 10.000 para dimensão 1.000; isso, na verdade, não é tão irreal. Para muitos problemas, podemos reduzir a dimensão dos dados de 5 a 10 vezes, e ainda manter a maior parte da variância. Nós podemos fazer isso sem afetar muito o desempenho, em termos da precisão na classificação do algoritmo de aprendizagem. E trabalhando com dados com dimensão reduzida, nosso algoritmo de aprendizagem pode rodar muito mais rápido. Resumindo, até agora nós falamos sobre as seguintes aplicações de PCA: Primeiro falamos sobre compressão, onde podemos reduzir o espaço de memória necessário para guardar os dados, e agora falamos sobre usar para agilizar um algoritmo de aprendizagem. Nessas aplicações, para escolher "K", normalmente, nós o faremos de acordo com a porcentagem de variância a ser mantida. Para esse algoritmo de aprendizagem, aplicações para agilizá-lo, frequentemente, manterão 99% da variância. Essa seria uma forma típica de como escolher k; é assim que escolhemos k para essas aplicações de compressão. Já para aplicações de visualização, normalmente sabemos como plotar dados apenas com duas ou três dimensões, então para aplicações de visualização, normalmente escolhemos k igual a dois ou k igual a três, porque podemos plotar dados apenas em 2D ou 3D. Então, isso resume as principais aplicações de PCA, assim como a escolha do valor de "k" para essas diferentes aplicações. Eu devo mencionar também que há um mau uso de PCA que é frequente. Às vezes, ouvimos falar sobre algumas pessoas fazendo isso, portanto vou mencionar, para você não repetir esse erro. Há um mau uso de PCA, que é: tentar prevenir o sobreajuste (over-fitting) Este é o raciocínio - Isso não é um boa forma de usar PCA, mas aqui está o raciocínio por trás desse método: se nós temos x⁽ᴵ⁾, com n variáveis, podemos comprimi-las, e utilizar z⁽ᴵ⁾, ao invés de x⁽ᴵ⁾, isso reduziria o número de variáveis para k, que seria uma dimensão muito reduzida. Então, se tivermos um número de variáveis muito menor, se k igual a 1.000" e n igual a 10.000, se k igual a 1.000 e n igual a 10.000, dados de dimensão 1.000, é muito menos provável termos sobreajuste, do que se tivermos 10,000 dimensões. Então, algumas pessoas pensa em PCA como uma forma de prevenir sobreajuste. Mas, eu gostaria de enfatizar que isso é uma má aplicação de PCA, e eu não recomendo essa utilização. E não é que não funcione, se você quiser usar esse método para reduzir a dimensão, para prevenir sobreajuste, isso pode até funcionar. Mas, esta não é uma boa forma de lidar com sobreajuste, ao invés disso, se você está preocupado com sobreajuste, há uma maneira muito melhor de lidar com isso: usar regularização, ao invés de PCA, para reduzir a dimensão dos dados. E a razão é que, se você pensar em como PCA funciona, ele joga fora os rótulos "y". Você olha apenas as entradas x⁽ᴵ⁾, e usa isso para encontrar uma aproximação de menor dimensão, para os seus dados. Então, o que PCA faz, é jogar fora parte da informação. Ele joga fora, ou reduz a dimensão dos seus dados, sem saber quais são os valores de "y". Então, usar PCA dessa forma é okay, provavelmente, apenas se, digamos, 99% da variância é mantida, se você mantiver a maior parte da variância. Mas, ele pode jogar fora informações valiosas. E acontece que, se você mantiver 99% da variância, ou 95% da variância, ou algo assim, usando regularização, você terá um método, pelo menos tão bom quanto, para prevenir sobreajuste. E regularização frequentemente funcionará melhor, porque quando você aplica Regressão Linear, ou Regressão Logística, ou algum outro método com regularização, esse problema de minimização sabe quais são os valores de "y", portanto, é menos provável que ele jogue fora alguma informação valiosa. Já PCA não faz uso dos rótulos, portanto é mais provável que jogue fora informações valiosas. Então, resumindo, é um bom uso de PCA se sua motivação for acelerar seu algoritmo de aprendizagem, mas não é bom usar PCA para prevenir sobreajuste. Regularização é a opção recomendada para esta finalidade. Finalmente, um último mau uso de PCA. Eu devo dizer que PCA é um algoritmo muito útil, eu uso constantemente para compressão, com intuito de visualização. Mas, às vezes, eu vejo que as pessoas usam PCA onde não deveriam. Este é um exemplo comum: alguém está modelando um sistema de Máquina de Aprendizado, esse é um modelo de um sistema de aprendizado - pegar o conjunto de treinamento, rodar o PCA, treinar Regressão Logística, e testar com os dados de teste. Então, normalmente, no começo de um projeto, alguém escreve um plano de projeto que diz: "vamos dar 4 passos com PCA". Antes de escrever um plano de projeto que incorpora PCA como esse, uma boa pergunta a se fazer é: "e se fizermos o projeto todo, sem usar PCA?" Normalmente as pessoas não consideram esse passo, antes de propor um plano de projeto tão complicado, implementando PCA. O conselho que, frequentemente, eu dou às pessoas é: antes de implementar PCA, eu sugiro que, primeiramente, você faça tudo o que precisa fazer, considerando como entrada, o seu dado original, os seus dados brutos, x⁽ᴵ⁾. E, se isso não funcionar, então considere usar PCA e z⁽ᴵ⁾. Então, antes de usar PCA, ao invés de reduzir a dimensão dos dados, eu descartaria o passo com PCA, e consideraria treinar o meu algoritmo de aprendizagem com meus dados originais. Vamos utilizar o meu dado original, x⁽ᴵ⁾, e eu recomendaria que ao invés de colocar PCA no algoritmo, tentar fazer, o que você estiver fazendo com x⁽ᴵ⁾ primeiro e apenas se você tentar fazer, o que você estiver fazendo com x⁽ᴵ⁾ primeiro para acreditar que ele não funciona, se o seu algoritmo estiver muito lento, ou se precisar de muita memória, ou de muito espaço em disco; e você quiser comprimir a sua representação, mas apenas se não funcionar com x⁽ᴵ⁾. Apenas se você tiver evidências, ou fortes razões para acreditar que x⁽ᴵ⁾ não funcionará, implemente PCA, e use a representação comprimida. Porque, muitas vezes, eu vejo que as pessoas começam a trabalhar com um plano de projeto que incorpora PCA, e o que estão fazendo funcionaria bem sem usar PCA. Então, considere essa alternativa também, antes de perder tempo implementando PCA, e buscando o valor de "k", e assim por diante. Então, é isso aí para PCA. Apesar dos últimos comentários, PCA é um algoritmo muito útil, quando usado para aplicações apropriadas, e eu, na verdade, uso PCA frequentemente, para acelerar meus algoritmos de aprendizado. Mas, eu acho que aplicações de PCA, também incluem compressão de  dados, redução da necessidade de memória ou de disco; ou para visualizar dados. PCA é um dos algoritmos de Aprendizado Não-Supervisionado mais utilizados, e mais poderosos. E com o que você aprendeu nesses vídeo, eu espero que você consiga implementar PCA, e utilizá-lo para todos os seus objetivos.
Tradução: Pablo de Morais Andrade | Revisão: