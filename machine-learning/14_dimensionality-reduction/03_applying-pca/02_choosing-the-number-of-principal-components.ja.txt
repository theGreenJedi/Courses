PCAのアルゴリズムにおいては、 n次元のフィーチャーを引数に取り それをある数kの次元のフィーチャーの表現へと縮小する。 この数kがPCAアルゴリズムの パラメータとなる。 この数字kはまた、 主成分の数、あるいは保持する主成分の総数 とも呼ばれる物だ。 そしてこのビデオで、 PCAのパラメータkを選ぶのに 人々がどんなやり方で 選ぶ事が多いのかの ガイドラインを与えたい。 kを選ぶ為に、 それは主成分の数を選ぶという事だが、 その為に有用な幾つかのコンセプトをここに示す。 PCAがやろうとする事は、 二乗射影誤差の平均を 最小化しようとする事だ。 つまりPCAは、 この量を最小化しようと試みる。ここに書きだした奴。 これは元のデータxと 射影したバージョンx-approx-iとの 差分で、 これは前回のビデオで定義した物だ。 PCAはxとそれをより次元の低い表面へと 射影した点との距離の二乗を 最小化しようとする。 これが二乗射影誤差の平均だ。 そしてまた、データ全体の 分散も定義しておこう。 それは これらの手本xiの 長さの二乗の平均だ。 つまりデータ全体の分散は、 トレーニングセット内の 各トレーニング手本の長さの 平均だ。 そしてこれの意味する所は、 「平均では、我らの手本は 全てがゼロのベクトルから、どれだけ離れているか？ 平均では我らのトレーニング手本は 原点からどれだけ離れているだろうか？」 我らがkを選ぼうとする時には、 kを選ぶのに良く使われる 経験則としては、 これらの値の比が、 0.01未満となるように選ぶ、という物がある。 言い換えると、 我らがkを選ぶ とても良くやるやり方は、 二乗射影誤差の平均を求める、という事。 それはxと射影との 距離の平均を、 データ全体の分散で割った物。 この全体の分散は、データがどれほど変化するかを表す。 我らはこの比率を、 例えば0.01未満にしたい、とする。 言い換えると1%未満、と考えても良い。 そしてほとんどの人々が kを選ぶという事を考える時には、 kを直接選ぶのではなく、 多くの人の考え方としては、 この値が幾つか、 という事。これが0.01だろうか？ または別の数だろうか？ そしてもしこれが0.01なら、 PCAの用語を用いてこれを 違う言い方で言うと、99%の分散が保持されている、と言う。 私は本当に、、、 技術的にこのフレーズが一体何を意味しているのかについて、 そんなに気にしないで欲しい。 このフレーズ、「99%の分散が保持されている」というのは、単に この左側の量が0.01未満だと言っているに過ぎない。 だから、もしあなたが PCAを使っていて、そして他の人に、 どれだけの数の主成分を 残したのか、について伝えたい時は、 私は、kを 99%の分散が保持されるように選んだ、 と言う方が、より一般的だ。 そしてそれは、知っておくと役に立つ事だ。 それの意味する所は、 平均の二乗射影誤差を 全体の分散で割ると、 それがたかだが1%だという事だ。 それは考えてみるになかなか 洞察に富んだ事だ。 一方でもしあなたが誰かに 「100個の主成分を得た」とか、 「kはイコール100で、 元の次元は1000次元のデータだった」 とか言っても、 これを聞いた人にとっては 解釈が難しい。 そこでこの数字、0.01というのを人々は良く使う。 その他良く使われるのは0.05。 こちらは5%。 もしこちらを用いると 人の所に行って、こう言う事が出来る： 分散の95%は 保持されています、と。 他の数字としては、分散の90%が保持されている、とか、 85%くらいまではありうるかもしれない。 90%は0.10に対応した物で、 つまり10%。 つまり、値の範囲として、 90, 95, 99, そして低くても85%くらいまでの中に 含まれる値は、 値としてはかなり典型的な範囲だろう。 95から99の値が 人々が用いる値としては もっとも一般的な範囲だと思う。 多くのデータセットにおいて、 99%の分散を保持するのに、 凄いデータの次元を削減出来て、 しかもほとんどの分散を保持したままに出来る事に しばしば驚く事になろう。 何故なら多くの現実世界のデータは、 多くのフィーチャーがお互いに 高い相関を持っているから。 だからデータを 大量に圧縮しつつ、 多くの分散、例えば 99%とか95%の分散を保持する事も 可能となる。
ではこれをどう実装したらいいだろう？ これが一つ、使えそうなアルゴリズムだ。 もしあなたがkの値を 選びたいとすると、 k=1から始める。 そしてPCAを実行する。 つまりUreduceを計算して、 z(1), z(2), ... , z(m)と計算して、 x1 approx, ...とxm approxまでを 全て計算する。 そして分散が99%保持されているかどうかをチェックする。 そしてもし保持されていれば、k=1を使う。 保持されていなければ、次に進み、k=2を試す。 そしてまたこの手続き全体を 繰り返して、そして この式が満たされているかをチェックする、 これは0.01未満だろうか。そして満たしていなかったら、またこれを繰り返す。 k=3を試そう、 次にk=4を試そう、 そうやって、例えば k=17まで行った所で、 99%のデータが保持されている、という事を 見出したとする。 その場合はk=17とする訳だ。 これはkの 99%の分散を保持する最小の値を探す 方法の一つだ。 だが想像出来るように、 この手続は恐ろしいほど効率が悪い。 我らはk=1を試し、k=2、とこれらの計算をやり続ける。 幸運な事に、PCAを実装する時には、 この手順の所で、 実際にはこれらの事を 同じように計算する、もっと簡単な方法を可能にする 量を与えてくれる。 具体的には、これらの行列、 U, S, Vを取る為に svdを呼んだら、 共分散行列 Sigmaに対して svdをコールしたら、 それはSという行列も返す。 このSが何かというと、 SはN掛けるNの 正方行列であり、 実際は、 対角行列でもある。 そして対角成分であるs11, s22, s33、、、とsnnまであるが、 それらだけが この行列の中の非ゼロ成分で、 対角成分から外れた全ての成分は 0となる。 分かりましたか。 ここに描いたこれらの大きなOは、 これの意味する所は、 この行列の 対角成分から外れた成分は 全てゼロとなる、という事だ。 そして証明出来る事として、 ここでその証明をして見るつもりは無いが、 ある所与の値kに対し、 ここにある値は、 もっとシンプルに計算出来る、 という事が、知られている。 そしてその値は、 1引くことの i=1からkまでの 和をとる事のsii、 割ることの 和を取る事のi=1からnまでの sii。 これを言葉で説明すると、 あるいはこれを説明する為に 別の見方で見てみると、 例えばk=3だとすると、 分子を計算する為に 我らがやる事は、 和を取る事のi=1から3までの siiを計算する、 つまり、単にこの最初の三つの要素の和を計算する。 これが分子となる。 そして分母は、 この対角成分全ての和だ。 そして1から引くことのその比、 それがここの量を 与えるのだ。 この青で丸く囲んだこれ。 そこで我らは、 たんにこれが0.01以上かどうかを テストする事が出来る。 または同じ事だが、 i=1からkまでのsiiの和を 割ることの i=1からnまでのsiiの和 これが0.99以上かどうかを テストしても 構わない。 もしあなたが分散の99%を 保持したいと思うのなら。 そこであなたが出来る事としては、 単純にちょっとずつkを増加させていって、 k=1をセットし、k=2をセットし、 k=3をセットし、、、と続けていって、 そしてこの量をテストしていって、 99%の分散を保持する事を保証する中で、 最小となるkの値を見てみる。 もしこれをやれば、 svd関数はたったの一回 呼ぶだけで良い。 何故ならそれでS行列が得られるから。 ひとたびS行列を 得てしまえば、 あなたはこの計算を 分子のkの値を増やしていくだけで 計算していく事が出来て、 だからsvdを 異なる値のkを試す都度何度も何度も 呼びつづける必要は無い。 つまりこの手続きは、 もっとずっと効率的だ。 そしてこの方法により、 PCAを最初から、何度も何度も 走らせる必要無く、 kの値を選ぶ事が出来る。単にsvdを一回走らせるだけで、 これらの対角成分の値を与えてくれる、 これらの値全て、s11、s22とsnnまでの値全て。 そしてそこで、単に この式の kを変えていくだけで、 99%の分散が保持される最小のkを 見つける事が出来る。 ではまとめよう。 PCAを圧縮の目的で 使う時に、 私がkを選ぶ為に良くやるのは、 svdを共分散行列に対して 一回実行して、 そしてこの式を使って、 この式を満たす最小のkを 選び出す。 ところで、 たとえあなたが違うkの値を選んだとしても、 たとえあなたがkの値を 手動で選んで、 例えば1000次元のデータに対して k=100を 選びたい、としよう。 その場合でも、あなたがやった事を 他の人に説明したいと思ったら、 あなたのPCAの実装のパフォーマンスを 他の人に伝える良い方法としては、 実際にこの量を 計算してみる事だ。 この値はあなたに、 分散の何％が保持されているかを教えてくれる。 そしてあなたがこの数字を報告すれば、 PCAに慣れている人たちなら、 そういう人たちなら これを用いる事で、 あなたの100次元の表現が、 元のデータセットをどれくらい良く近似出来ているかを 良く理解する事が出来る。 何故なら99%の分散が保持されているのだから、 それは実際にあなたの 施工誤差の二乗を測る指標だ。 その比が0.01というなら、 人々にあなたのPCAの実装が オリジナルのデータセットの良い近似を 発見したかについての 良い理解を与える。 以上で、数字のkを選ぶ 効率的な手続きを 提供出来ただろうか。このkを選ぶ事で、 データを何次元まで削減するかを 選ぶ事になる。 そしてもしあなたがPCAを とても高い次元のデータセットに適用するなら、 1000次元のデータとか、 とてもよく、データセットが 高い相関を持つフィーチャーを含むので、 これは単に、多くのデータにおいて あなたが目にする事になる性質なので、 あなたは良く、PCAが 分散の99%を保持していながら、とか、 あるいは95とか99とか、 とにかく高い割合の分散を 保持していながら、 とても多くの割合のデータを 圧縮する事が出来る、という状況を目にする事になるだろう。