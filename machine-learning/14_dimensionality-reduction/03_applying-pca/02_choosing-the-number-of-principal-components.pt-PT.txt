No algoritmo de PCA nos usamos as características dimensionais N e as reduzimos a uma representação de uma característica dimensional k. O número k é um parâmetro do algoritmo de PCA. O número k é também chamado de número dos principais componentes ou de número dos principais componentes que mantemos. E neste vídeo gostaria de dar algumas orientações, contar como as pessoas tendem a pensar sobre como escolher este parâmetro k para o PCA. Para escolher k, isto é, para escolher o número dos Componentes Principais, aqui seguem alguns conceitos úteis. O que o PCA tenta fazer é tentar minimizar o erro médio da projeção quadrática. Então ele tenta minimizar essa quantidade, que eu estou escrevendo, que é a diferença entre a o dados x e a versão projetada x-approx-i, que foi definida no último vídeo, ele tenta minimizar a distância quadrática entre x e sua projeção em relação a superfície de menor
ordem dimensional. Então isso é o erro médio da projeção quadrática. E também, eu vou definir a variação total nos dados para serem o a média de comprimento quadrático desses exemplos Xi para que a variação total nos dados seja a média dos meus conjuntos de treinamento do comprimento de cada um dos meus
exemplos de treinamento. E esse aqui diz, "Na média, quão longe estão meus exemplos de treinamento do vetor, de serem todos apenas zeros?" Quão longe, em média, estão meus exemplos de treinamento da origem? Quando nós estamos tentando escolher k, um princípio básico comum para escolher k, é escolher os menores valores de forma que a proporção entre eles seja menor que 0.01. Então, em outras palavras, Um jeito comum de pensar sobre como escolher k é que nós queremos o erro
médio da projeção quadrática. Essa é a distância média entre x e suas projeções divididos pelo total da variação dos dados. Isso é o quanto os dados variam. Nós queremos que essa proporção seja menor que, digamos, 0.01. Ou que seja menor que 1%,
que é uma outra forma de pensar nisso. E a forma que a maioria das pessoas pensa sobre escolher k é ao invés de escolher k diretamente, a forma que a maioria das pessoas fala a respeito disso, é sobre o que é este número, se é 0.01 ou algum outro número. E se for 0.01, outro jeito de falar isso, para usar a linguagem do PCA, é que 99% da variação é mantida. Não se preocupem sobre o que essa frase realmente significa tecnicamente, mas a frase "99% da variação é mantida" apenas quer dizer que essa quantidade na esquerda é menor que 0.01. E assim, se você está usando PCA e você quer dizer a alguém, quantos Componentes Principais você reteve, seria mais comum dizer "bem, eu escolhi k... para que 99% da variação fosse mantida." E isso é uma coisa útil para se saber. Isso quer dizer que o erro médio da projeção quadrática dividida pela variação total que fosse no máximo 1%. É uma coisa interessante para se pensar, pois se você dizer a alguém,
"Bem... eu tinha 100 componentes básicos" ou "k era igual a 100... em milhares de dados dimensionais" é um pouco complicado para que as pessoas possam interpretar. isso. Então o número 0.01 é o que as pessoas costumam usar. Outro valor comum é 0.05, e isso seria 5%, e se você fizer isso então você vai dizer "Bem, 95% ... da variância é mantida", e assim, outros números, talvez 90% da variância é mantida, no ponto mais baixo, talvez até 85%. Então 90% iria corresponder a digamos 0.10 quase 10%. E assim a faixa de valores, de 90, 95, 99, talvez tão baixo quanto 85% das variáveis mantidas, seriam faixas típicas em valores. Talvez de 95 até 99 são na realidade as faixas de valores mais comuns utilizadas. Para muitos conjuntos de dados você ficaria surpreso. Para manter 99% da variância,
você pode reduzir a dimensão dos dados significativamente
e ainda manter a maior parte da variância. Porque a maioria dos dados na vida real diz que muitas variáveis são apenas altamente correlacionadas, e assim se torna possível comprimir muito os dados e ainda assim manter 99% da variância ou 95% da variância.
Então, como você implementa isso? Bem, aqui está um algorítmo que você talvez use. Você pode iniciar, se você quiser escolher o valor de k, nós começamos com k = 1. E então nós executamos o PCA. Assim nós computamos, 
você reduz, computa z1, z2, até zm. Computa todos os x1 approx e assim por diante até xm approx e então nós verificamos se 99% da variância foi mantida. Então nós usamos o k = 1. Mas se isso não for, então o que nós faremos a seguir,
é tentar k = 2. E então novamente nós executamos todo o procedimento e verificamos, se a expressão foi satisfeita. Se isso for menor que 0.01.
E se não for, então nós repetimos o processo. Vamos tentar k = 3, então tentar k = 4, e assim sucessivamente até que talvez nós alcancemos k = 17, e nós encontraremos que 99% dos dados foram mantidos e então nós usamos k = 17. Esta é uma forma de escolher o menor valor para k, para que 99% da variância seja mantida. Mas como você pode imaginar, esse procedimento parece horrivelmente ineficiente. Nós estamos tentando k=1, k=2,
e nós estamos fazendo todos esses cálculos. Felizmente, quando você implementa PCA, na realidade,
nesse passo, ele realmente nos dá a quantidade que torna muito mais fácil de computar essas coisas também. Quando você está chamando svd para pegar essas matrizes, U, S e V, quando você chama svd na matriz de covariância Σ (sigma), isso também nos dá a matriz S e o que S será, é uma matriz quadrada n x n, que é diagonal. Então, a entrada diagonal s, um um, S22, S33 até Snn, serão os únicos elementos não-zero desta matriz, 
e tudo que não estiver na diagonal será zero. Ok? Então, esses grandes O's que eu estou desenhando, com isso eu quero dizer que tudo que estiver fora da diagonal dessa matriz,
todas essas entradas serão iguais a zero. E assim,
o que é possível mostrar e eu não vou provar isso aqui, é que verifica-se que para um valor de k, essa quantidade aqui (área circulada),
pode ser computada de forma mais simples. E aquela quantidade pode ser computada como 1 menos a soma de i = 1 até k de Sii, dividido pela... soma de i = 1 ... através de n de Sii. Então para expressar em palavras, ou mostrar um ponto de vista diferente para explicar, vamos supor que k = 3. O que nós iremos fazer para computar o numerador,
é somar de i = 1 através de 3 de Sii, então apenas computamos a soma desses 3 primeiros elementos. Então esse é o numerador. E então para o denominador, a soma de todas as entradas na diagonal. E 1 menos a proporção disso, isso me dá esta quantidade aqui, onde eu desenhei um círculo azul. E assim. o que nós podemos fazer é testar se isso é menor ou igual a 0.01. Ou, de forma equivalente, nós podemos testar se a soma de i = 1, até k, Sii dividido pela soma de i = 1 até n, Sii se isso for maior que ou igual a 0.99, se você quer ter certeza que 99% da variância é mantida. E assim o que você pode fazer é apenas lentamente incrementar k, definda k igual 1, k igual 2, k igual 3 e assim por diante, e apenas teste essa quantidade para ver qual é o menor valor de k, que garante que 99%
da variância é mantida. E se você fizer isso, então você precisa chamar a função svd apenas uma vez. Porque isso te dá uma matriz S, e uma vez que você possui a matriz S,
você pode então apenas continuar fazendo o cálculo incrementando o valor de K no numerador e assim você não precisa continuar chamando svd repetidas vezes para testar os diferentes valores de k. Então esse procedimento é mais eficiente, e isso permite que você selecione os valores de k sem precisar executar o PCA desde o início repetidamente. Você apenas executa svd apenas uma vez, isso resulta todos esses números da diagonal, todos esses números S11, S22 até Snn, e então você pode apenas variar k nessa expressão para encontrar o menor valor para k, de forma que 99% da variância é mantida. Então para sumarizar,
o caminho que eu costumo usar, a forma com que eu geralmente escolho K quando eu estou usando o PCA para compressão é chamar svd uma vez na matriz de covariância, e então eu usaria esta fórmula e escolheria o menor valor para k para o qual esta expressão seria satisfeita. E por falar nisso, mesmo que você escolhesse valores diferentes de k, mesmo se você escolhesse um valor para k manualmente, talvez você tivesse milhares de dados dimensionais e eu quero apenas escolher k = 100. Então, se você quiser explicar para os outros o que você acabou de fazer uma boa forma para explicar
para eles a performance da sua implementação do PCA, seria pegar essa quantidade e computar o que é isso, e isso vai dizer a você qual era a porcentagem da variância mantida. E se você reportar esse número, então, as pessoas que forem familiarizadas com PCA, e as pessoas podem usar isso para obter um bom entendimento de quão bem sua representação de 100 dimensões está aproximando o seu conjunto de dados original, porque existe 99% de variância mantida. Isto é realmente uma medida do seu erro de construção quadrática, onde a proporção sendo 0.01, dá às pessoas um bom senso intuitivo se a sua implementação do PCA está encontrando uma boa aproximação do seu conjunto de dados original. Então, espero que isso lhe dê um procedimento eficiente para escolher o número de k.
Para escolher qual dimensão para reduzir seus dados, e se você aplicar o seu PCA para conjuntos de dados de altas dimensões, por exemplo dados com mil dimensões. Frequentemente, só por que os
conjuntos de dados tendem a ter variáveis altamente correlacionadas, esta é apenas uma propriedade da maioria dos conjuntos de dados que você vê. Você verá que o PCA poderá manter 99% da variância, ou digamos, 95% ou 99%, alguma fração alta de variância, mesmo quando comprimindo os dados por um grande fator.