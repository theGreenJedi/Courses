En el algoritmo ACP tomamos "n" variables dimensionales y las reducimos a cierta representación de variables de "k" dimensiones. Este número k es un parámetro del algoritmo de ACP, este número "k" también se conoce como el número de componentes principales o el número de componentes principales que hemos retenido. En este video, me gustaría darle algunas pautas, informarle sobre cómo la gente tiende a pensar sobre cómo elegir este parámetro "k" para ACP. Con el fin de elegir "k", esto es, elegir el número de componentes principales, aquí hay un par de conceptos útiles: lo que ACP intenta hacer es tratar de minimizar los errores de proyección al cuadrado promedio, de modo que trata de minimizar esta cantidad que estoy escribiendo, que es la diferencia entre los datos originales X y la versión proyectada, X-aprox-i, que se definió en el último vídeo, por lo que trata de minimizar la distancia al cuadrado entre x y su proyección sobre esa superficie con dimensiones inferiores. Ese es el error de proyección al cuadrado promedio. También permítame definir la variación total en los datos para que sea la longitud promedio al cuadrado de estos ejemplos Xi, de modo que la variación total en los datos es el promedio de mis conjuntos de entrenamiento, de la longitud de cada uno de mis ejemplos de entrenamiento. Y lo que esto dice es "En promedio, ¿qué tan lejos están mis ejemplos de entrenamiento del vector, qué tan lejos están de ser sólo ceros? " ¿Qué tan lejos están, en promedio, qué tan lejos están mis ejemplos de entrenamiento del origen? Cuando estamos tratando de elegir k, una regla general bastante común para elegir k, es elegir los valores más pequeños para que la relación entre estos sea menor que 0.01. En otras palabras, una forma bastante común de pensar en cómo escogemos k, es que queremos el error de proyección al cuadrado promedio, es decir, la distancia promedio entre x y sus proyecciones dividida entre la variación total de los datos, que es la medida en que los datos varían, queremos que esta relación sea menor a, digamos, 0.01 o que sea menor al 1%, que es otra manera de considerarlo. Y la forma en que la mayoría de la gente piensa sobre la elección de k, es en lugar de elegir k directamente, la manera en que la mayoría de la gente habla de esto, es preguntándose qué número es este, ya sea 0.01 o algún otro número. Si es 0.01, otra forma de decir esto usando el lenguaje de ACP es que se retiene 99% de la varianza. En realidad, no quiero que se preocupe por lo que esta frase significa técnicamente pero esta frase: "el 99% de la varianza se retiene" sólo significa que esta cantidad de la izquierda es menor a 0.01, por lo tanto, si está usando ACP y quiere decirle a alguien, ya sabe, cuántos componentes principales tiene retenidos, sería más común decir: bueno, elegí k para que se retuviera el 99% de la varianza y eso es algo muy útil que debemos saber, significa que el error de proyección al cuadrado promedio dividido entre la variación total fue al menos del 1%. Eso es algo muy provechoso en que pensar, si usted le dice a alguien: "bueno, tenía 100 componentes principales o k era igual a 100 en datos de mil dimensiones", es un poco difícil para la gente interpretar un rol en esto. Así que, este número 0.01 es lo que la gente suele usar. Otros valores comunes son: 0.05, por lo que este sería un 5%, al hacer esto, entonces ahora dice: bien, 95% de la varianza es retenida y, otros números tal vez, aproximadamente el 90% de la varianza es retenido, quizá el porcentaje sea tan bajo como 85%, entonces, el 90% correspondería a decir 0.10, digamos un 10%. Así que el rango de valores de 90, 95 99, quizá hasta un porcentaje tan bajo como el 85% de las variables retenidas sería un rango bastante típico de valores. Tal vez 95 a 99 es realmente el rango de valores más común que la gente usa. Para muchos conjuntos de datos, se sorprendería, con el fin de retener el 99% de la varianza, puede a menudo reducir la dimensión de los datos de manera significativa y todavía conservar la mayor parte de la varianza ya que en la mayor parte de conjuntos de datos en la vida real muchas variables están solo altamente correlacionadas, así que resulta posible comprimir los datos en gran manera y aún retener el 99% de la varianza o el 95% de la misma. Entonces, ¿cómo puede implementar esto? Bueno, aquí hay un algoritmo que podría utilizar. Usted puede empezar, si quiere elegir el valor de k, podríamos empezar con k=1 y posteriormente usamos el ACP. Ya sabe, calculamos, usted reduce, calcula z1, z2, hasta zm, calcule todas estas x1aprox y así sucesivamente hasta xm aprox y luego comprobamos si se retiene el 99% de la varianza. Entonces estamos bien y utilizamos k=1 pero si no es así, entonces lo que haremos ahora es tratar con k= 2 y luego vamos a repetir todo este procedimiento y comprobaremos, usted sabe, si esta expresión fue satisfecha, si es menor a 0.01 y si no, entonces lo hacemos de nuevo. Vamos a tratar con k=3, después con k=4, y así sucesivamente hasta llegar a tal vez k=17 y nos encontramos con que el 99% de los datos se ha retenido y luego utilizamos k= 17, ¿cierto? Esa es una manera de elegir el valor más pequeño de k, de manera que se retiene 99% de la varianza. Pero como se puede imaginar, este procedimiento parece terriblemente ineficiente porque estamos tratando con k=1, k =2, estamos haciendo todos estos cálculos. Afortunadamente, cuando se implementa el ACP, en realidad, en este paso, realmente nos da una cantidad que hace que sea mucho más fácil calcular estas cosas también. Específicamente, cuando usa SVD para obtener estas matrices, u, s, y d, cuando esté usando el SVD en la matriz de covarianza «sigma», también nos devuelve esta matriz S y la función de S va a ser una matriz cuadrada, de hecho una matriz nxn, que es diagonal. Por lo tanto, las entradas diagonales son S11, S22, S33 bajando hasta Snn, estas serán los únicos elementos distintos de cero en esta matriz, y todo lo que esté fuera de las diagonales va a ser cero. ¿Ok? Así que los grandes 0 que yo estoy dibujando, con ellos me refiero a todo lo que está fuera de la diagonal de esta matriz, todas esas entradas en ese espacio van a ser ceros. Y así, lo que es posible mostrar, y no voy a probar esto aquí, resulta que para un valor dado de k, esta cantidad aquí se puede calcular de manera mucho más simple y esa cantidad se puede calcular como 1 menos la suma de i=1 hasta k de Sii dividida entre la suma de i=1 hasta n de Sii. Sólo para expresarlo con palabras o sólo para tener otra vista de cómo explicar eso, digamos que si K=3, lo que vamos a hacer para calcular el numerador, la suma desde 1, i=1 hasta 3 de Sii, así que solo calculamos la suma de estos tres primeros elementos, así que ese el numerador. Y luego, para el denominador, bueno, es la suma de todas estas entradas de la diagonal y 1 menos la relación de eso, eso me da esta cantidad aquí, la cual he circulado en azul. Así que lo que podemos hacer es simplemente probar si esto es menor o igual a 0.01 o de manera equivalente, podemos probar si la suma de i=1 hasta k,  Sii dividida entre la suma de i=1 hasta n Sii, si es mayor o igual a 0.99, si quiere asegurarse de que se retiene 99% de la varianza. Lo que puede hacer, es sólo aumentar poco a poco k, escribimos k=1, k=2, k=3 y así sucesivamente, y sólo probar esta cantidad, para ver cuál es el valor más pequeño de k que asegura que se retiene 99% de la varianza. Y si hace esto, entonces usted necesita usar la función SVD sólo una vez. Debido a que le da la matriz S y una vez que tiene la matriz S, puede sólo seguir haciendo este cálculo mediante el aumento del valor de k en el numerador y así no es necesario estar usando la función SVD una y otra vez para probar los diferentes valores de k. Por lo tanto, este procedimiento es mucho más eficiente y esto puede permitirle seleccionar el valor de k sin necesidad de ejecutar el ACP desde cero una y otra vez. De modo que usted ejecuta SVD una vez, esto le da todos estos números diagonales, todos estos números S11, S22 hasta Snn, después usted puede, sólo variar k en esta expresión para encontrar el valor más pequeño de k, de modo que se retiene 99% de la varianza. Así que, para resumir, el modo que utilizo comúnmente, la manera en la que a menudo elijo k cuando estoy usando ACP para compresión, es que usaría una vez SVD en la matriz de covarianza y luego usaría esta fórmula y elegiría el valor más pequeño de k por medio del cual se satisface esta expresión. Por cierto, incluso si usted fuera a elegir valor diferente a k e incluso si fuera a elegir el del valor de k manualmente, tal vez usted podría tener datos de 1000 dimensiones y sólo quiero elegir k=100, entonces, si quiere explicar a otros lo que acaba de hacer, una buena manera de explicar el desempeño de su implementación de ACP es en realidad tomar esta cantidad y calcular lo que esto representa y esto le indicará cuál fue el porcentaje de varianza retenido y si reporta ese número, entonces las personas que están familiarizadas con ACP pueden usar esto para obtener una mejor comprensión de qué tan bien se está aproximando su representación de cien dimensiones a sus datos originales porque hay un 99% de la varianza retenida. Eso es realmente una medida de su error de construcción al cuadrado, esa relación de 0.01, sólo le da a la gente un buen sentido intuitivo sobre si su aplicación de ACP está encontrando una buena aproximación a su conjunto de datos original. Ojalá esto le de un procedimiento eficiente para la elección del número k, para elegir a qué dimensión reducir sus datos y si aplica ACP a grupos de datos de altas dimensiones, digamos datos de 1000 dimensiones, muy a menudo, sólo porque los conjuntos de datos tienden a tener variables altamente correlacionadas, esta es sólo una propiedad de la mayoría de los conjuntos de datos que se ven, a menudo encuentra que el ACP será capaz de retener el 99% de la varianza o digamos 95, 99, alguna fracción alta de la varianza, incluso durante la compresión de datos por un factor muy grande.