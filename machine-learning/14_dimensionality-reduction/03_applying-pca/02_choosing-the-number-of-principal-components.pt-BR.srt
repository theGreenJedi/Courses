1
00:00:00,090 --> 00:00:01,560
No algoritmo de PCA, nós usamos

2
00:00:01,980 --> 00:00:03,530
variáveis de dimensão "n" e as reduzimos

3
00:00:03,970 --> 00:00:06,260
a uma representação com "k" dimensões.

4
00:00:07,620 --> 00:00:09,090
O número "k" é um parâmetro

5
00:00:09,820 --> 00:00:10,800
do algoritmo de PCA.

6
00:00:11,810 --> 00:00:13,240
O número "k" é também chamado de

7
00:00:13,620 --> 00:00:15,080
número de componentes principais

8
00:00:15,830 --> 00:00:17,480
que mantemos.

9
00:00:18,530 --> 00:00:19,640
Neste vídeo, gostaria de

10
00:00:19,810 --> 00:00:20,850
dar algumas orientações,

11
00:00:21,730 --> 00:00:23,090
contar como as pessoas

12
00:00:23,430 --> 00:00:24,490
tendem a pensar sobre como

13
00:00:24,610 --> 00:00:26,740
escolher este parâmetro "k" para o PCA.

14
00:00:28,650 --> 00:00:29,670
Para escolher "k",

15
00:00:30,110 --> 00:00:30,990
isto é, para escolher o número

16
00:00:31,360 --> 00:00:34,110
de componentes principais, seguem alguns conceitos úteis.

17
00:00:36,430 --> 00:00:37,520
O que o PCA tenta fazer

18
00:00:37,760 --> 00:00:38,760
é tentar minimizar

19
00:00:40,070 --> 00:00:41,510
o erro médio da projeção quadrática.

20
00:00:42,030 --> 00:00:43,200
Ele tenta minimizar

21
00:00:43,430 --> 00:00:45,480
essa quantidade, que eu estou escrevendo,

22
00:00:46,410 --> 00:00:47,980
que é a diferença entre a

23
00:00:48,150 --> 00:00:50,010
o dados "x⁽ⁱ⁾" e a

24
00:00:50,690 --> 00:00:53,470
versão projetada "x⁽ⁱ⁾ₐₚₚᵣₒₓ",

25
00:00:53,620 --> 00:00:54,930
definida no último vídeo,

26
00:00:55,020 --> 00:00:55,900
tenta minimizar

27
00:00:56,160 --> 00:00:57,360
a distância quadrática entre "x⁽ⁱ⁾"

28
00:00:58,330 --> 00:00:59,750
e sua projeção na superfície de menor dimensão.

29
00:01:01,220 --> 00:01:02,990
Esse é o erro médio da projeção quadrática.

30
00:01:03,990 --> 00:01:05,320
Também vou definir a

31
00:01:05,440 --> 00:01:07,020
variação total nos

32
00:01:07,100 --> 00:01:08,730
dados como

33
00:01:09,020 --> 00:01:11,730
a média do comprimento ao quadrado

34
00:01:12,140 --> 00:01:14,130
desses exemplos "x⁽ⁱ⁾".

35
00:01:14,450 --> 00:01:16,010
Assim, a variação total nos dados

36
00:01:16,260 --> 00:01:17,930
será a média no

37
00:01:18,070 --> 00:01:19,250
conjunto de treinamento do

38
00:01:19,370 --> 00:01:21,640
comprimento de cada um dos meus exemplos.

39
00:01:22,190 --> 00:01:23,690
É mais ou menos dizer:

40
00:01:23,940 --> 00:01:24,850
"Na média, quão longe

41
00:01:25,690 --> 00:01:27,960
estão exemplos de do vetor, de serem todos nulos?"

42
00:01:28,770 --> 00:01:30,460
Quão longe, em média,

43
00:01:30,820 --> 00:01:32,820
estão meus exemplos de treinamento da origem?

44
00:01:33,510 --> 00:01:34,450
Quando tentamos

45
00:01:35,870 --> 00:01:37,210
escolher "k", um princípio básico

46
00:01:37,400 --> 00:01:38,620
para escolhê-lo, é escolher

47
00:01:38,800 --> 00:01:40,290
o menor valor de forma que

48
00:01:40,980 --> 00:01:43,810
esta razão seja menor que 0.01.

49
00:01:44,550 --> 00:01:45,540
Em outras palavras,

50
00:01:46,340 --> 00:01:47,370
um jeito comum de

51
00:01:47,510 --> 00:01:48,460
pensar sobre como escolher "k"

52
00:01:48,800 --> 00:01:51,180
é com o erro médio quadrático da projeção.

53
00:01:51,580 --> 00:01:54,700
Essa é a distância média

54
00:01:55,240 --> 00:01:56,340
entre "x⁽ⁱ⁾" e suas projeções,

55
00:01:57,570 --> 00:02:00,330
dividida pela variação total dos dados,

56
00:02:00,800 --> 00:02:01,870
o quanto os dados variam.

57
00:02:02,940 --> 00:02:04,060
Queremos que essa proporção seja

58
00:02:04,240 --> 00:02:06,760
menor que, digamos, 0.01.

59
00:02:06,830 --> 00:02:09,450
Ou que seja menor que 1%, é uma outra forma de pensar nisso.

60
00:02:10,860 --> 00:02:11,940
A forma que a maioria

61
00:02:12,150 --> 00:02:13,640
das pessoas pensa sobre a escolha de "k" é,

62
00:02:13,860 --> 00:02:15,660
ao invés de escolher "k" diretamente,

63
00:02:15,890 --> 00:02:17,110
a forma mais comum

64
00:02:17,480 --> 00:02:18,940
é discutir sobre este número,

65
00:02:19,160 --> 00:02:20,630
se é 0.01

66
00:02:20,740 --> 00:02:23,330
ou algum outro número.

67
00:02:23,720 --> 00:02:25,320
E se for 0.01, outro jeito

68
00:02:25,490 --> 00:02:27,020
de falar isso, para usar a linguagem

69
00:02:27,270 --> 00:02:30,120
do PCA, é que 99% da variação é mantida.

70
00:02:32,060 --> 00:02:33,480
Não se preocupem

71
00:02:33,850 --> 00:02:34,810
sobre o que essa frase

72
00:02:35,140 --> 00:02:36,920
realmente significa tecnicamente,

73
00:02:37,830 --> 00:02:39,170
mas a frase "99% da variação é mantida"

74
00:02:39,420 --> 00:02:41,710
quer dizer que essa quantidade à esquerda é menor que 0.01.

75
00:02:42,340 --> 00:02:43,910
Assim, se você

76
00:02:44,930 --> 00:02:46,510
está usando PCA e você quer

77
00:02:46,630 --> 00:02:47,730
dizer a alguém

78
00:02:48,220 --> 00:02:49,860
quantos componentes principais você

79
00:02:49,980 --> 00:02:51,080
reteve, seria mais comum dizer

80
00:02:51,140 --> 00:02:52,360
"Bem, eu escolhi k

81
00:02:52,450 --> 00:02:55,360
para que 99% da variação fosse mantida."

82
00:02:55,990 --> 00:02:56,960
É útil saber isso.

83
00:02:57,660 --> 00:02:58,530
Isso quer dizer que

84
00:02:58,620 --> 00:02:59,820
o erro quadrático médio da projeção

85
00:03:00,760 --> 00:03:01,720
dividido pela

86
00:03:01,900 --> 00:03:03,260
variação total é no máximo 1%.

87
00:03:03,820 --> 00:03:04,770
É uma coisa interessante para se pensar,

88
00:03:05,270 --> 00:03:06,790
pois se você

89
00:03:06,920 --> 00:03:08,420
disser a alguém

90
00:03:09,170 --> 00:03:10,710
"Eu tinha 100 componentes básicos"

91
00:03:10,890 --> 00:03:12,030
ou "k era igual a 100

92
00:03:12,720 --> 00:03:13,850
dados em mil dimensões"

93
00:03:14,220 --> 00:03:15,350
é um pouco complicado

94
00:03:15,420 --> 00:03:16,600
para as pessoas interpretarem.

95
00:03:19,100 --> 00:03:19,100
Você pode ver que as margens que eu tenho, estão especificadas aqui.

96
00:03:19,320 --> 00:03:22,220
Então, o número 0.01 é o que as pessoas costumam usar.

97
00:03:23,070 --> 00:03:25,380
Outro valor comum é 0.05,

98
00:03:26,840 --> 00:03:27,810
e isso seria 5%,

99
00:03:27,990 --> 00:03:28,870
e com isso

100
00:03:29,210 --> 00:03:30,390
você vai dizer "Bem, 95%

101
00:03:30,740 --> 00:03:32,320
da variância é mantida",

102
00:03:32,480 --> 00:03:34,280
e assim também

103
00:03:34,700 --> 00:03:36,710
com outros números, talvez 90%,

104
00:03:37,980 --> 00:03:40,030
no ponto mais baixo, talvez até 85%.

105
00:03:40,150 --> 00:03:42,410
90% corresponderia a 0.10

106
00:03:44,340 --> 00:03:46,950
ou 10%.

107
00:03:47,250 --> 00:03:49,160
Assim, a faixa de valores

108
00:03:49,900 --> 00:03:50,770
de 90%, 95%, 99%,

109
00:03:50,870 --> 00:03:51,470
talvez até 85% da

110
00:03:51,500 --> 00:03:55,100
variação mantida, seriam faixas típicas de valores.

111
00:03:55,780 --> 00:03:56,900
Talvez de 95% até 99%

112
00:03:57,690 --> 00:03:58,810
são, na realidade, as

113
00:03:59,020 --> 00:04:02,080
faixas de valores mais comuns.

114
00:04:02,130 --> 00:04:02,950
Você ficaria surpreso

115
00:04:03,010 --> 00:04:04,320
em muitos conjuntos de dados.

116
00:04:04,790 --> 00:04:06,590
Para manter 99% da variância, pode-se

117
00:04:06,790 --> 00:04:08,160
reduzir a dimensão dos

118
00:04:08,200 --> 00:04:11,810
dados significativamente
e ainda manter a maior parte da variância.

119
00:04:12,440 --> 00:04:13,410
A maioria conjuntos

120
00:04:13,560 --> 00:04:15,210
de dados da vida real

121
00:04:15,280 --> 00:04:17,060
são altamente correlacionadas,

122
00:04:17,310 --> 00:04:17,940
e, assim, torna-se possível

123
00:04:18,490 --> 00:04:19,540
comprimir muito os dados

124
00:04:19,610 --> 00:04:20,990
e ainda assim manter

125
00:04:21,360 --> 00:04:22,310
99% da variância

126
00:04:22,530 --> 00:04:26,260
ou 95% da variância.
Então, como você implementa isso?

127
00:04:26,810 --> 00:04:28,610
Bem, aqui está um algoritmo que você talvez use.

128
00:04:28,890 --> 00:04:30,360
Você pode iniciar, se você

129
00:04:30,540 --> 00:04:31,360
quiser escolher o valor de "k",

130
00:04:31,470 --> 00:04:33,510
podemos começar com "k = 1".

131
00:04:33,550 --> 00:04:34,670
Então nós executamos o PCA.

132
00:04:35,350 --> 00:04:36,440
Calculamos "U_reduce",

133
00:04:36,570 --> 00:04:38,880
"z⁽¹⁾", "z⁽²⁾" até "z⁽ᵐ⁾".

134
00:04:39,520 --> 00:04:40,790
Calculamos todos os "x⁽¹⁾ₐₚₚᵣₒₓ", "x⁽²⁾ₐₚₚᵣₒₓ"

135
00:04:41,090 --> 00:04:42,540
e assim por diante até "x⁽ᵐ⁾ₐₚₚᵣₒₓ".

136
00:04:43,200 --> 00:04:45,110
Então verificamos se 99% da variância foi mantida.

137
00:04:47,140 --> 00:04:48,890
Nesse caso, usamos "k = 1".

138
00:04:49,020 --> 00:04:51,960
Caso contrário, tentamos "k = 2".

139
00:04:52,620 --> 00:04:53,810
Novamente,

140
00:04:54,200 --> 00:04:56,070
executamos todo o procedimento e verificamos

141
00:04:56,170 --> 00:04:57,770
se a expressão foi satisfeita.

142
00:04:58,470 --> 00:05:00,980
Se isso for menor que 0.01.
Se não for, nós repetimos o processo.

143
00:05:01,220 --> 00:05:03,070
Vamos tentar "k = 3",

144
00:05:03,310 --> 00:05:04,910
tentar "k = 4",

145
00:05:04,970 --> 00:05:06,250
e assim sucessivamente até

146
00:05:06,630 --> 00:05:07,730
que talvez alcancemos "k = 17",

147
00:05:08,070 --> 00:05:09,040
e vemos que

148
00:05:09,090 --> 00:05:13,060
99% dos dados foram mantidos e então

149
00:05:14,120 --> 00:05:15,110
nós usamos "k = 17".

150
00:05:15,570 --> 00:05:17,160
Esta é uma forma

151
00:05:17,240 --> 00:05:18,790
de escolher o menor valor para "k"

152
00:05:19,130 --> 00:05:20,920
para que 99% da variância seja mantida.

153
00:05:22,380 --> 00:05:23,440
Mas, como você pode imaginar,

154
00:05:23,550 --> 00:05:25,140
esse procedimento parece horrivelmente ineficiente.

155
00:05:26,210 --> 00:05:28,120
Estamos tentando "k = 1", "k = 2",
fazendo todos esses cálculos.

156
00:05:29,580 --> 00:05:30,540
Felizmente, quando você

157
00:05:31,130 --> 00:05:33,510
implementa PCA, na realidade, nesse passo,

158
00:05:33,960 --> 00:05:35,530
ele realmente nos dá uma quantidade

159
00:05:35,910 --> 00:05:37,080
que torna

160
00:05:37,320 --> 00:05:40,160
muito mais fácil calcular essas coisas.

161
00:05:41,110 --> 00:05:42,160
Quando você está chamando "svd()"

162
00:05:42,820 --> 00:05:44,120
para conseguir essas matrizes,

163
00:05:44,340 --> 00:05:45,550
"U", "S" e "V",

164
00:05:45,610 --> 00:05:46,780
quando você chama "svd()" na

165
00:05:47,040 --> 00:05:48,560
matriz de covariância "Sigma",

166
00:05:48,860 --> 00:05:49,780
a função retorna

167
00:05:50,300 --> 00:05:52,170
a matriz "S",

168
00:05:52,360 --> 00:05:53,430
que é uma

169
00:05:53,520 --> 00:05:56,790
é uma matriz quadrada "n x n",

170
00:05:57,640 --> 00:05:58,090
diagonal.

171
00:05:58,290 --> 00:05:58,290
diagonal.

172
00:05:58,830 --> 00:06:00,380
Então, os elementos da diagonal,

173
00:06:00,540 --> 00:06:01,640
"s₁₁", "s₂₂",

174
00:06:01,980 --> 00:06:03,240
"s₃₃" até "sₙₙ",

175
00:06:03,590 --> 00:06:05,130
serão os

176
00:06:05,260 --> 00:06:07,010
únicos elementos não nulos

177
00:06:07,130 --> 00:06:08,880
desta matriz, e tudo fora

178
00:06:09,060 --> 00:06:11,470
da diagonal será zero.

179
00:06:11,590 --> 00:06:11,590
OK?

180
00:06:11,670 --> 00:06:12,530
Esses grandes "O"s

181
00:06:13,340 --> 00:06:14,260
que desenhei

182
00:06:14,740 --> 00:06:16,330
significam que todas as entradas

183
00:06:17,130 --> 00:06:18,220
fora da diagonal

184
00:06:18,480 --> 00:06:20,310
da matriz serão iguais a zero.

185
00:06:22,300 --> 00:06:23,790
Assim, é possível mostrar,

186
00:06:24,190 --> 00:06:25,250
mas não vou provar aqui,

187
00:06:25,480 --> 00:06:26,380
é que

188
00:06:26,620 --> 00:06:27,880
que para cada valor de "k",

189
00:06:27,980 --> 00:06:29,920
essa quantidade

190
00:06:30,590 --> 00:06:37,820
aqui pode ser calculada de forma mais simples.

191
00:06:38,800 --> 00:06:40,310
Essa quantidade pode ser reescrita

192
00:06:41,000 --> 00:06:42,900
como 1 menos a soma

193
00:06:43,130 --> 00:06:44,400
dos "sᵢᵢ",

194
00:06:44,610 --> 00:06:47,960
com "i" de 1 a "k", dividido pela

195
00:06:48,640 --> 00:06:50,050
soma dos "sᵢᵢ" com "i"

196
00:06:50,170 --> 00:06:52,010
de 1 a "n".

197
00:06:53,360 --> 00:06:54,820
Expressando em palavras,

198
00:06:55,000 --> 00:06:56,170
ou mostrando de outro

199
00:06:56,450 --> 00:06:57,330
ponto de vista,

200
00:06:57,960 --> 00:06:59,370
vamos supor que "k = 3".

201
00:07:00,810 --> 00:07:01,970
Para calcular

202
00:07:02,080 --> 00:07:03,200
o numerador, somamos

203
00:07:03,340 --> 00:07:04,680
"sᵢᵢ" com "i"

204
00:07:04,820 --> 00:07:05,830
de 1 a 3,

205
00:07:06,240 --> 00:07:08,170
são esses 3 primeiros elementos.

206
00:07:09,280 --> 00:07:09,710
Esse é o numerador.

207
00:07:10,980 --> 00:07:12,880
Para o denominador,

208
00:07:13,090 --> 00:07:14,970
somamos todas as entradas na diagonal.

209
00:07:16,210 --> 00:07:17,470
E 1 menos essa razão

210
00:07:17,660 --> 00:07:19,080
é igual a esta

211
00:07:19,300 --> 00:07:21,330
quantidade aqui, que eu

212
00:07:21,650 --> 00:07:23,440
circulei em azul.

213
00:07:23,650 --> 00:07:24,380
Agora, podemos

214
00:07:24,650 --> 00:07:26,000
testar se isso

215
00:07:26,430 --> 00:07:29,330
é menor ou igual a 0.01.

216
00:07:29,370 --> 00:07:30,460
Ou, de forma equivalente,

217
00:07:30,830 --> 00:07:31,960
podemos testar se a soma com "i"

218
00:07:32,180 --> 00:07:33,010
de 1 a "k" dos "sᵢᵢ"

219
00:07:33,970 --> 00:07:35,180
dividido pela soma com "i"

220
00:07:35,320 --> 00:07:37,090
de 1 a "n" dos "sᵢᵢ"

221
00:07:37,650 --> 00:07:38,580
é maior

222
00:07:38,770 --> 00:07:40,600
ou igual a 0.99, se você

223
00:07:40,720 --> 00:07:42,920
quer ter certeza que 99% da variância é mantida.

224
00:07:44,770 --> 00:07:45,650
Agora, você pode

225
00:07:45,940 --> 00:07:48,360
lentamente incrementar "k",

226
00:07:48,770 --> 00:07:49,820
por exemplo, "k = 1", "k = 2",

227
00:07:50,100 --> 00:07:51,290
"k = 3" e assim por diante,

228
00:07:52,140 --> 00:07:53,240
e apenas teste essa quantidade

229
00:07:54,720 --> 00:07:56,120
para ver qual é o menor

230
00:07:56,350 --> 00:07:58,820
valor de k que garante que 99%
da variância é mantida.

231
00:08:00,600 --> 00:08:01,810
Se você fizer isso,

232
00:08:02,000 --> 00:08:02,790
você precisa chamar

233
00:08:03,170 --> 00:08:04,660
a função "svd()" apenas uma vez,

234
00:08:04,970 --> 00:08:05,830
porque ela te dá

235
00:08:06,010 --> 00:08:07,060
a matriz "S", e uma vez que você

236
00:08:07,090 --> 00:08:08,350
possui a matriz S, você pode

237
00:08:08,490 --> 00:08:09,540
apenas continuar fazendo

238
00:08:09,770 --> 00:08:11,370
o cálculo incrementando

239
00:08:11,930 --> 00:08:12,910
o valor de "k"

240
00:08:13,070 --> 00:08:14,450
no numerador, e assim você

241
00:08:14,560 --> 00:08:16,290
não precisa ficar chamando "svd()"

242
00:08:16,540 --> 00:08:18,620
várias vezes para testar os diferentes valores de "k".

243
00:08:18,910 --> 00:08:20,030
Esse procedimento é muito

244
00:08:20,150 --> 00:08:21,700
mais eficiente, e isso permite

245
00:08:21,910 --> 00:08:24,020
que você selecione o

246
00:08:24,090 --> 00:08:25,890
valor de "k" sem precisar executar

247
00:08:26,260 --> 00:08:27,620
PCA desde o início repetidamente.

248
00:08:28,030 --> 00:08:30,650
Você só executa "svd()" uma vez,

249
00:08:30,850 --> 00:08:32,350
obtém todos esses números da diagonal,

250
00:08:32,780 --> 00:08:35,090
todos esses números, "s₁₁", "s₂₂" até "sₙₙ",

251
00:08:35,780 --> 00:08:36,820
e então você pode

252
00:08:36,920 --> 00:08:38,440
apenas variar "k"

253
00:08:38,730 --> 00:08:40,740
nessa expressão para encontrar

254
00:08:41,010 --> 00:08:42,250
o menor valor para "k" tal que

255
00:08:43,140 --> 00:08:44,030
99% da variância é mantida.

256
00:08:44,850 --> 00:08:45,870
Resumindo, a forma

257
00:08:46,050 --> 00:08:47,850
que eu costumo usar,

258
00:08:47,970 --> 00:08:49,050
a forma com que eu geralmente escolho "k"

259
00:08:49,420 --> 00:08:50,790
quando eu estou usando o PCA para compressão

260
00:08:51,120 --> 00:08:52,590
é chamar "svd()" uma vez

261
00:08:52,950 --> 00:08:54,480
na matriz de covariância, e então

262
00:08:54,540 --> 00:08:55,750
usaria esta fórmula para encontrar

263
00:08:56,030 --> 00:08:57,930
o menor valor para "k"

264
00:08:58,020 --> 00:09:00,390
para o qual esta expressão é satisfeita.

265
00:09:01,580 --> 00:09:02,560
Por falar nisso, mesmo que

266
00:09:02,650 --> 00:09:03,850
escolhesse outro valor para "k",

267
00:09:04,180 --> 00:09:04,960
mesmo se você

268
00:09:05,000 --> 00:09:05,920
escolhesse um valor para "k"

269
00:09:06,090 --> 00:09:07,250
manualmente, talvez você

270
00:09:07,300 --> 00:09:08,620
tenha dados com mil dimensões

271
00:09:09,540 --> 00:09:11,590
e queira escolher "k = 100".

272
00:09:12,430 --> 00:09:13,450
Então, se você quiser explicar

273
00:09:13,690 --> 00:09:14,760
para os outros o que você acabou de fazer,

274
00:09:15,230 --> 00:09:17,070
uma boa forma para mostrar o desempenho

275
00:09:17,750 --> 00:09:18,910
da sua implementação do PCA

276
00:09:19,220 --> 00:09:20,300
seria pegar essa

277
00:09:20,540 --> 00:09:21,670
quantidade e calcular

278
00:09:21,890 --> 00:09:23,000
isto, que vai dizer a você

279
00:09:23,110 --> 00:09:25,770
a porcentagem da variância mantida.

280
00:09:26,300 --> 00:09:28,070
Se você mostrar esse número,

281
00:09:28,340 --> 00:09:29,720
as pessoas que forem

282
00:09:30,100 --> 00:09:31,610
familiarizadas com PCA podem

283
00:09:31,880 --> 00:09:33,020
usar isso para obter

284
00:09:33,080 --> 00:09:34,560
um bom entendimento do quão bem

285
00:09:34,900 --> 00:09:37,340
sua representação de 100 dimensões está

286
00:09:37,690 --> 00:09:39,270
aproximando o seu conjunto de dados original,

287
00:09:39,580 --> 00:09:41,300
porque existe 99% de variância mantida.

288
00:09:41,990 --> 00:09:44,140
Isto é uma medida do seu

289
00:09:44,360 --> 00:09:45,860
erro de reconstrução ao quadrado,

290
00:09:46,240 --> 00:09:47,870
onde a razão de 0.01

291
00:09:48,430 --> 00:09:49,940
dá uma boa intuição sobre

292
00:09:50,430 --> 00:09:51,820
se a sua implementação

293
00:09:52,580 --> 00:09:53,840
do PCA está encontrando

294
00:09:54,000 --> 00:09:56,530
uma boa aproximação do seu conjunto de dados original.

295
00:09:58,440 --> 00:09:59,600
Espero que isso lhe dê

296
00:09:59,800 --> 00:10:01,260
um procedimento eficiente para escolher

297
00:10:01,850 --> 00:10:02,800
o número "k",

298
00:10:03,260 --> 00:10:04,940
a dimensão à qual reduzir

299
00:10:05,160 --> 00:10:06,630
seus dados, e se

300
00:10:06,750 --> 00:10:07,830
você aplicar o PCA em

301
00:10:07,970 --> 00:10:09,740
conjuntos de dados com muitas dimensões,

302
00:10:09,990 --> 00:10:11,570
por exemplo, dados com mil dimensões,

303
00:10:11,980 --> 00:10:13,340
Frequentemente, como os conjuntos

304
00:10:13,530 --> 00:10:14,720
de dados tendem a ter variáveis

305
00:10:15,070 --> 00:10:16,140
altamente correlacionadas, uma propriedade

306
00:10:16,280 --> 00:10:17,190
da maioria dos conjuntos de dados que se vê,

307
00:10:18,440 --> 00:10:19,420
você verá que o PCA

308
00:10:20,040 --> 00:10:21,610
poderá manter 99%

309
00:10:21,840 --> 00:10:22,940
da variância, ou digamos,

310
00:10:23,110 --> 00:10:24,440
95% ou 99%,

311
00:10:24,720 --> 00:10:25,910
alguma fração alta de variância,

312
00:10:26,360 --> 00:10:27,580
mesmo comprimindo os dados

313
00:10:28,560 --> 00:10:29,720
por um fator alto.