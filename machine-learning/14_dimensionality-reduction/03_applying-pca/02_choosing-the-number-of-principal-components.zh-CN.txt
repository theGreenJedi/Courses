在 PCA 算法中 我们把n维特征变量 降维到k维特征变量 这个数字k 是 PCA 算法的一个参数 这个数字k也被称作 主成分的数量 或者说是我们保留的主成分的数量 在这个视频中 我会给你们一些参考 告诉你们 人们是怎样思考如何选择 PCA 的参数k的 为了选择参数k 也就是要选择主成分的数量 这里有几个有用的概念 PCA 所做的是 尽量最小化 平均平方映射误差 (Average Squared Projection Error) 因此 PCA 就是要将这个量最小化 就是我正在写的这个 它是原始数据x 它是原始数据x 和映射值 x_approx(i) 之间的差 这个在上节视频中定义过 它就是要最小化 x和其在低维表面上的映射点 之间的距离的平方 这就是平均平方映射误差 我还要定义一下 数据的总变差 (Total Variation) 数据的总变差 (Total Variation) 它是这些样本x(i)的 长度的平方的均值 因此数据的总变差 就是我这些训练集中 每个训练样本长度的 平均值 它的意思是 “平均来看 我的训练样本 距离零向量多远？ 平均来看 我的训练样本距离原点多远？” 当我们去选择k值的时候 一个常见的 选择K值的经验法则是 选择能够使得它们之间的比例 小于等于0.01的最小的k值 换言之 一个非常常用的 选择k值的方法是 我们希望平均平方映射误差 就是x和其映射值之间的 平均距离 除以数据的总变差 就是数据的变化有多大 我们想要这个比值能够 小于 比如说 0.01 或者说是小于1% 这是另一种说法 大部分人在考虑 选择k的方法时 不是直接选择k值 大部分人所考虑的是 这个数应该是多少 它应该是0.01 还是其它的数 如果是0.01 换言之 用PCA的语言说就是 保留了99%的差异性 我其实并不想 不用担心这短话的学术意义 不用担心这短话的学术意义 这句“保留了99%的差异性”的意思是 左边这个数值小于0.01 因此 如果你使用PCA 并且你想要告诉别人 并且你想要告诉别人 你保留了多少个主成分 更为常见的 一种说法是 我选择了参数k 使得99%的差异性得以保留 了解这个事情是有用的 它的意思是 平均平方映射误差 除以总变差 至多是1% 这是一个可以去思考的 有见解的事情 然而如果你跟别人说 “我有100个主成分” 或者说“从1000维的数据中 得到的k等于100” 这就有点 让人难以理解 刚才我说的这些 这个数字0.01就是人们经常用的 另一个常用的值是0.05 那么这就会是5% 如果是这样的话 你可以说 95%的差异性被保留了 95%的差异性被保留了 还有其它数值 比如90%的差异性被保留了 还有低到85%的 因此90%对应的是0.10 也就是10% 取值范围是 90 95 99 也可能低到保留85%的差异性 这些都是比较典型的取值范围 可能从95到99 是人们最为 常用的取值范围 对于许多数据集 你可能会惊讶 为了保留99%的差异性 通常你可以大幅地降低数据的维度 却还能保留大部分的差异性 因为大部分现实中的数据 许多特征变量 都是高度相关的 所以实际上 大量压缩数据是可能的 而且仍然会保留 99%或95%的差异性 那么你该如何实现它呢？ 你可能会用到这个算法 你可以这样开始 比如你想选取k的值 我们可以从k=1开始 然后我们再进行主成分分析 我们算出 Ureduce z(1) z(2) 一直到 z(m) 算出所有那些 x_approx(1) 一直到 x_approx(m) 然后我们看一下99%的差异性是否被保留下来了 是的话就搞定了 我们就用 k=1 但如果不是 那么我们接下来尝试 k=2 然后我们要重新走一遍 这整个过程 检查是否满足这个表达式 这个式子的值是否小于0.01 如果不是 我们再重复一次 我们尝试 k=3 然后试 k=4 以此类推 一直试到 比如我们一直试到 k=17 然后发现99%的数据 都被保留了 我们就会用 k=17 这是一种用来选择 使得99%的差异性能够得以保留的 最小的k值的方法 但是可以想见 这个过程的效率相当地低 我们在尝试 k=1  k=2 时 做了所有这些计算 幸好 你在应用 PCA 时 实际上 在这一步 它已经给了我们 一个可以使计算变得 容易很多的量 特别是当你调用 svd 来计算这些 矩阵U S V时 当你对协方差的矩阵 Sigma 调用 svd 时 我们还会得到 这个矩阵S S是一个正方形矩阵 实际上是一个 n×n 的矩阵 它是一个 对角矩阵 对角线上的元素是 s11 s22 s33 一直到 snn S11 S22 S33 一直到 Snn 它们是矩阵中 仅有的非零元素 对角以外的其他元素 都是0 明白吗？ 所以我写的这些巨大的零 就代表我所说的 这个矩阵中对角以外的 其他元素 所有这些元素都等于0 可以证明的是 我不会在此证明 实际上 对于一个给定的k值 这边的这个量 可以更容易地算出来 那个数值可以通过这个式子计算出来 1减去 从 i=1 到 k 对 Sii 求和 从 i=1 到 k 对 Sii 求和 除以 从 i=1 到 n 对 Sii 求和 除以 从 i=1 到 n 对 Sii 求和 这个的意思是 或者说从另一个角度来解释它 或者说从另一个角度来解释它 假如说 k=3 我们接下来要 计算的分子是 从 i=1 到 3 对 Sii 求和 从 i=1 到 3 对 Sii 求和 就是算出这前三个元素的和 这就是分子 然后计算分母 分母是这些对角元素的总和 1减去这个比例 就是这里的这个值 就是这个我用 蓝色圈起来的数字 接下来我们要做的就是 看一下这个数字 是否小于等于0.01 或者等价的 我们可以查看 从 i=1 到 k 对 Sii 求和 从 i=1 到 k 对 Sii 求和 除以 从 i=1 到 n 对 Sii 求和 除以 从 i=1 到 n 对 Sii 求和 它是否大于等于0.99 它是否大于等于0.99 如果你想确保能够保留99%的差异性的话 所以你要做的 就是慢慢地增大k值 把k值设为1 k值设为2 把k值设为3 以此类推 并检验这个数值 找出能够确保99%的差异性被保留的最小的k值 找出能够确保99%的差异性被保留的最小的k值 如果这样做 那么你只需要调用 一次 svd 函数 因为它会给你S矩阵 一旦有了S矩阵 你便可以 通过增加分子上的k值 通过增加分子上的k值 来做这个计算 因此你就不用 一遍一遍地调用 svd 函数 来检验不同的k值 因此这一步非常高效 它能够让你 不用一遍一遍地 从头运行 PCA 就能选择k值 你只要运行一次 svd 函数 就能得到所有的对角数值 所有这些数值 S11 S22 一直到 Snn 然后你就可以 改变这个式子中的k值 来找到能够保留99%的差异性 来找到能够保留99%的差异性 的最小的k值 来总结一下 我在使用 PCA 进行压缩时 通常采用的决定k值的方法是 通常采用的决定k值的方法是 对协方差矩阵 调用一次 svd 函数 然后用这个公式 来找出满足这个表达式的 最小的k值 顺便说一下 即使你要挑选一些不同的k值 即使你要 手动挑选k值 也许你有 1000维的数据 我只想要 k=100 如果你想要向别人 解释你所做的 向他们解释 你实现的 PCA 的性能 的一个好方法 实际上是这个数值 把它算出来 它会告诉你百分之多少的差异性被保留了下来 如果你把这个数值展现出来 那么熟悉 PCA 的人们 就可以通过它 来更好地理解 你用来代表原始数据的 100维数据 近似得有多好 因为有99%的差异性被保留了 这就是一个平方投影误差的测量指标 这就是一个平方投影误差的测量指标 这个比值是0.01 就会让别人对你实现的 PCA 是否对原始数据做了一个好的近似 是否对原始数据做了一个好的近似 有一个很好的直观感受 希望对你来说 这是一个选择k值的 有效的过程 这就是如何选择 要将数据降低到什么维度 如果你将 PCA 应用于 很高维的数据集 常见的 比如一千维的数据 因为数据集的特征变量 通常有较高的相关性 这其实是常见的 大部分数据集的一个特性 你经常会发现 PCA 能够保留 99%的差异性 或者说95%的差异性 或者某些高百分比的差异性 即使在压缩很大比例的数据的情况下 也会看到这样的现象 【教育无边界字幕组】翻译：星星之火 校对：竹二个 审核：所罗门捷列夫