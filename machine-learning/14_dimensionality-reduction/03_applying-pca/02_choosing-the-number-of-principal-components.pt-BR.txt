No algoritmo de PCA, nós usamos variáveis de dimensão "n" e as reduzimos a uma representação com "k" dimensões. O número "k" é um parâmetro do algoritmo de PCA. O número "k" é também chamado de número de componentes principais que mantemos. Neste vídeo, gostaria de dar algumas orientações, contar como as pessoas tendem a pensar sobre como escolher este parâmetro "k" para o PCA. Para escolher "k", isto é, para escolher o número de componentes principais, seguem alguns conceitos úteis. O que o PCA tenta fazer é tentar minimizar o erro médio da projeção quadrática. Ele tenta minimizar essa quantidade, que eu estou escrevendo, que é a diferença entre a o dados "x⁽ⁱ⁾" e a versão projetada "x⁽ⁱ⁾ₐₚₚᵣₒₓ", definida no último vídeo, tenta minimizar a distância quadrática entre "x⁽ⁱ⁾" e sua projeção na superfície de menor dimensão. Esse é o erro médio da projeção quadrática. Também vou definir a variação total nos dados como a média do comprimento ao quadrado desses exemplos "x⁽ⁱ⁾". Assim, a variação total nos dados será a média no conjunto de treinamento do comprimento de cada um dos meus exemplos. É mais ou menos dizer: "Na média, quão longe estão exemplos de do vetor, de serem todos nulos?" Quão longe, em média, estão meus exemplos de treinamento da origem? Quando tentamos escolher "k", um princípio básico para escolhê-lo, é escolher o menor valor de forma que esta razão seja menor que 0.01. Em outras palavras, um jeito comum de pensar sobre como escolher "k" é com o erro médio quadrático da projeção. Essa é a distância média entre "x⁽ⁱ⁾" e suas projeções, dividida pela variação total dos dados, o quanto os dados variam. Queremos que essa proporção seja menor que, digamos, 0.01. Ou que seja menor que 1%, é uma outra forma de pensar nisso. A forma que a maioria das pessoas pensa sobre a escolha de "k" é, ao invés de escolher "k" diretamente, a forma mais comum é discutir sobre este número, se é 0.01 ou algum outro número. E se for 0.01, outro jeito de falar isso, para usar a linguagem do PCA, é que 99% da variação é mantida. Não se preocupem sobre o que essa frase realmente significa tecnicamente, mas a frase "99% da variação é mantida" quer dizer que essa quantidade à esquerda é menor que 0.01. Assim, se você está usando PCA e você quer dizer a alguém quantos componentes principais você reteve, seria mais comum dizer "Bem, eu escolhi k para que 99% da variação fosse mantida." É útil saber isso. Isso quer dizer que o erro quadrático médio da projeção dividido pela variação total é no máximo 1%. É uma coisa interessante para se pensar, pois se você disser a alguém "Eu tinha 100 componentes básicos" ou "k era igual a 100 dados em mil dimensões" é um pouco complicado para as pessoas interpretarem. Você pode ver que as margens que eu tenho, estão especificadas aqui. Então, o número 0.01 é o que as pessoas costumam usar. Outro valor comum é 0.05, e isso seria 5%, e com isso você vai dizer "Bem, 95% da variância é mantida", e assim também com outros números, talvez 90%, no ponto mais baixo, talvez até 85%. 90% corresponderia a 0.10 ou 10%. Assim, a faixa de valores de 90%, 95%, 99%, talvez até 85% da variação mantida, seriam faixas típicas de valores. Talvez de 95% até 99% são, na realidade, as faixas de valores mais comuns. Você ficaria surpreso em muitos conjuntos de dados. Para manter 99% da variância, pode-se reduzir a dimensão dos dados significativamente
e ainda manter a maior parte da variância. A maioria conjuntos de dados da vida real são altamente correlacionadas, e, assim, torna-se possível comprimir muito os dados e ainda assim manter 99% da variância ou 95% da variância.
Então, como você implementa isso? Bem, aqui está um algoritmo que você talvez use. Você pode iniciar, se você quiser escolher o valor de "k", podemos começar com "k = 1". Então nós executamos o PCA. Calculamos "U_reduce", "z⁽¹⁾", "z⁽²⁾" até "z⁽ᵐ⁾". Calculamos todos os "x⁽¹⁾ₐₚₚᵣₒₓ", "x⁽²⁾ₐₚₚᵣₒₓ" e assim por diante até "x⁽ᵐ⁾ₐₚₚᵣₒₓ". Então verificamos se 99% da variância foi mantida. Nesse caso, usamos "k = 1". Caso contrário, tentamos "k = 2". Novamente, executamos todo o procedimento e verificamos se a expressão foi satisfeita. Se isso for menor que 0.01.
Se não for, nós repetimos o processo. Vamos tentar "k = 3", tentar "k = 4", e assim sucessivamente até que talvez alcancemos "k = 17", e vemos que 99% dos dados foram mantidos e então nós usamos "k = 17". Esta é uma forma de escolher o menor valor para "k" para que 99% da variância seja mantida. Mas, como você pode imaginar, esse procedimento parece horrivelmente ineficiente. Estamos tentando "k = 1", "k = 2",
fazendo todos esses cálculos. Felizmente, quando você implementa PCA, na realidade, nesse passo, ele realmente nos dá uma quantidade que torna muito mais fácil calcular essas coisas. Quando você está chamando "svd()" para conseguir essas matrizes, "U", "S" e "V", quando você chama "svd()" na matriz de covariância "Sigma", a função retorna a matriz "S", que é uma é uma matriz quadrada "n x n", diagonal. diagonal. Então, os elementos da diagonal, "s₁₁", "s₂₂", "s₃₃" até "sₙₙ", serão os únicos elementos não nulos desta matriz, e tudo fora da diagonal será zero. OK? Esses grandes "O"s que desenhei significam que todas as entradas fora da diagonal da matriz serão iguais a zero. Assim, é possível mostrar, mas não vou provar aqui, é que que para cada valor de "k", essa quantidade aqui pode ser calculada de forma mais simples. Essa quantidade pode ser reescrita como 1 menos a soma dos "sᵢᵢ", com "i" de 1 a "k", dividido pela soma dos "sᵢᵢ" com "i" de 1 a "n". Expressando em palavras, ou mostrando de outro ponto de vista, vamos supor que "k = 3". Para calcular o numerador, somamos "sᵢᵢ" com "i" de 1 a 3, são esses 3 primeiros elementos. Esse é o numerador. Para o denominador, somamos todas as entradas na diagonal. E 1 menos essa razão é igual a esta quantidade aqui, que eu circulei em azul. Agora, podemos testar se isso é menor ou igual a 0.01. Ou, de forma equivalente, podemos testar se a soma com "i" de 1 a "k" dos "sᵢᵢ" dividido pela soma com "i" de 1 a "n" dos "sᵢᵢ" é maior ou igual a 0.99, se você quer ter certeza que 99% da variância é mantida. Agora, você pode lentamente incrementar "k", por exemplo, "k = 1", "k = 2", "k = 3" e assim por diante, e apenas teste essa quantidade para ver qual é o menor valor de k que garante que 99%
da variância é mantida. Se você fizer isso, você precisa chamar a função "svd()" apenas uma vez, porque ela te dá a matriz "S", e uma vez que você possui a matriz S, você pode apenas continuar fazendo o cálculo incrementando o valor de "k" no numerador, e assim você não precisa ficar chamando "svd()" várias vezes para testar os diferentes valores de "k". Esse procedimento é muito mais eficiente, e isso permite que você selecione o valor de "k" sem precisar executar PCA desde o início repetidamente. Você só executa "svd()" uma vez, obtém todos esses números da diagonal, todos esses números, "s₁₁", "s₂₂" até "sₙₙ", e então você pode apenas variar "k" nessa expressão para encontrar o menor valor para "k" tal que 99% da variância é mantida. Resumindo, a forma que eu costumo usar, a forma com que eu geralmente escolho "k" quando eu estou usando o PCA para compressão é chamar "svd()" uma vez na matriz de covariância, e então usaria esta fórmula para encontrar o menor valor para "k" para o qual esta expressão é satisfeita. Por falar nisso, mesmo que escolhesse outro valor para "k", mesmo se você escolhesse um valor para "k" manualmente, talvez você tenha dados com mil dimensões e queira escolher "k = 100". Então, se você quiser explicar para os outros o que você acabou de fazer, uma boa forma para mostrar o desempenho da sua implementação do PCA seria pegar essa quantidade e calcular isto, que vai dizer a você a porcentagem da variância mantida. Se você mostrar esse número, as pessoas que forem familiarizadas com PCA podem usar isso para obter um bom entendimento do quão bem sua representação de 100 dimensões está aproximando o seu conjunto de dados original, porque existe 99% de variância mantida. Isto é uma medida do seu erro de reconstrução ao quadrado, onde a razão de 0.01 dá uma boa intuição sobre se a sua implementação do PCA está encontrando uma boa aproximação do seu conjunto de dados original. Espero que isso lhe dê um procedimento eficiente para escolher o número "k", a dimensão à qual reduzir seus dados, e se você aplicar o PCA em conjuntos de dados com muitas dimensões, por exemplo, dados com mil dimensões, Frequentemente, como os conjuntos de dados tendem a ter variáveis altamente correlacionadas, uma propriedade da maioria dos conjuntos de dados que se vê, você verá que o PCA poderá manter 99% da variância, ou digamos, 95% ou 99%, alguma fração alta de variância, mesmo comprimindo os dados por um fator alto.