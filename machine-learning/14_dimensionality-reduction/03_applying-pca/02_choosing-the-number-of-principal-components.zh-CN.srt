1
00:00:00,090 --> 00:00:01,560
在 PCA 算法中

2
00:00:01,980 --> 00:00:03,530
我们把n维特征变量

3
00:00:03,970 --> 00:00:06,260
降维到k维特征变量

4
00:00:07,620 --> 00:00:09,090
这个数字k

5
00:00:09,820 --> 00:00:10,800
是 PCA 算法的一个参数

6
00:00:11,810 --> 00:00:13,240
这个数字k也被称作

7
00:00:13,620 --> 00:00:15,080
主成分的数量

8
00:00:15,830 --> 00:00:17,480
或者说是我们保留的主成分的数量

9
00:00:18,530 --> 00:00:19,640
在这个视频中

10
00:00:19,810 --> 00:00:20,850
我会给你们一些参考

11
00:00:21,730 --> 00:00:23,090
告诉你们

12
00:00:23,430 --> 00:00:24,490
人们是怎样思考如何选择

13
00:00:24,610 --> 00:00:26,740
PCA 的参数k的

14
00:00:28,650 --> 00:00:29,670
为了选择参数k

15
00:00:30,110 --> 00:00:30,990
也就是要选择主成分的数量

16
00:00:31,360 --> 00:00:34,110
这里有几个有用的概念

17
00:00:36,430 --> 00:00:37,520
PCA 所做的是

18
00:00:37,760 --> 00:00:38,760
尽量最小化

19
00:00:40,070 --> 00:00:41,510
平均平方映射误差 (Average Squared Projection Error)

20
00:00:42,030 --> 00:00:43,200
因此 PCA 就是要将这个量最小化

21
00:00:43,430 --> 00:00:45,480
就是我正在写的这个

22
00:00:46,410 --> 00:00:47,980
它是原始数据x

23
00:00:48,150 --> 00:00:50,010
它是原始数据x

24
00:00:50,690 --> 00:00:53,470
和映射值 x_approx(i) 之间的差

25
00:00:53,620 --> 00:00:54,930
这个在上节视频中定义过

26
00:00:55,020 --> 00:00:55,900
它就是要最小化

27
00:00:56,160 --> 00:00:57,360
x和其在低维表面上的映射点

28
00:00:58,330 --> 00:00:59,750
之间的距离的平方

29
00:01:01,220 --> 00:01:02,990
这就是平均平方映射误差

30
00:01:03,990 --> 00:01:05,320
我还要定义一下

31
00:01:05,440 --> 00:01:07,020
数据的总变差 (Total Variation)

32
00:01:07,100 --> 00:01:08,730
数据的总变差 (Total Variation)

33
00:01:09,020 --> 00:01:11,730
它是这些样本x(i)的

34
00:01:12,140 --> 00:01:14,130
长度的平方的均值

35
00:01:14,450 --> 00:01:16,010
因此数据的总变差

36
00:01:16,260 --> 00:01:17,930
就是我这些训练集中

37
00:01:18,070 --> 00:01:19,250
每个训练样本长度的

38
00:01:19,370 --> 00:01:21,640
平均值

39
00:01:22,190 --> 00:01:23,690
它的意思是

40
00:01:23,940 --> 00:01:24,850
“平均来看 我的训练样本

41
00:01:25,690 --> 00:01:27,960
距离零向量多远？

42
00:01:28,770 --> 00:01:30,460
平均来看

43
00:01:30,820 --> 00:01:32,820
我的训练样本距离原点多远？”

44
00:01:33,510 --> 00:01:34,450
当我们去选择k值的时候

45
00:01:35,870 --> 00:01:37,210
一个常见的

46
00:01:37,400 --> 00:01:38,620
选择K值的经验法则是

47
00:01:38,800 --> 00:01:40,290
选择能够使得它们之间的比例

48
00:01:40,980 --> 00:01:43,810
小于等于0.01的最小的k值

49
00:01:44,550 --> 00:01:45,540
换言之

50
00:01:46,340 --> 00:01:47,370
一个非常常用的

51
00:01:47,510 --> 00:01:48,460
选择k值的方法是

52
00:01:48,800 --> 00:01:51,180
我们希望平均平方映射误差

53
00:01:51,580 --> 00:01:54,700
就是x和其映射值之间的

54
00:01:55,240 --> 00:01:56,340
平均距离

55
00:01:57,570 --> 00:02:00,330
除以数据的总变差

56
00:02:00,800 --> 00:02:01,870
就是数据的变化有多大

57
00:02:02,940 --> 00:02:04,060
我们想要这个比值能够

58
00:02:04,240 --> 00:02:06,760
小于 比如说 0.01

59
00:02:06,830 --> 00:02:09,450
或者说是小于1% 这是另一种说法

60
00:02:10,860 --> 00:02:11,940
大部分人在考虑

61
00:02:12,150 --> 00:02:13,640
选择k的方法时

62
00:02:13,860 --> 00:02:15,660
不是直接选择k值

63
00:02:15,890 --> 00:02:17,110
大部分人所考虑的是

64
00:02:17,480 --> 00:02:18,940
这个数应该是多少

65
00:02:19,160 --> 00:02:20,630
它应该是0.01

66
00:02:20,740 --> 00:02:23,330
还是其它的数

67
00:02:23,720 --> 00:02:25,320
如果是0.01 换言之

68
00:02:25,490 --> 00:02:27,020
用PCA的语言说就是

69
00:02:27,270 --> 00:02:30,120
保留了99%的差异性

70
00:02:32,060 --> 00:02:33,480
我其实并不想

71
00:02:33,850 --> 00:02:34,810
不用担心这短话的学术意义

72
00:02:35,140 --> 00:02:36,920
不用担心这短话的学术意义

73
00:02:37,830 --> 00:02:39,170
这句“保留了99%的差异性”的意思是

74
00:02:39,420 --> 00:02:41,710
左边这个数值小于0.01

75
00:02:42,340 --> 00:02:43,910
因此 如果你使用PCA

76
00:02:44,930 --> 00:02:46,510
并且你想要告诉别人

77
00:02:46,630 --> 00:02:47,730
并且你想要告诉别人

78
00:02:48,220 --> 00:02:49,860
你保留了多少个主成分

79
00:02:49,980 --> 00:02:51,080
更为常见的

80
00:02:51,140 --> 00:02:52,360
一种说法是

81
00:02:52,450 --> 00:02:55,360
我选择了参数k 使得99%的差异性得以保留

82
00:02:55,990 --> 00:02:56,960
了解这个事情是有用的

83
00:02:57,660 --> 00:02:58,530
它的意思是

84
00:02:58,620 --> 00:02:59,820
平均平方映射误差

85
00:03:00,760 --> 00:03:01,720
除以总变差

86
00:03:01,900 --> 00:03:03,260
至多是1%

87
00:03:03,820 --> 00:03:04,770
这是一个可以去思考的

88
00:03:05,270 --> 00:03:06,790
有见解的事情

89
00:03:06,920 --> 00:03:08,420
然而如果你跟别人说

90
00:03:09,170 --> 00:03:10,710
“我有100个主成分”

91
00:03:10,890 --> 00:03:12,030
或者说“从1000维的数据中

92
00:03:12,720 --> 00:03:13,850
得到的k等于100”

93
00:03:14,220 --> 00:03:15,350
这就有点

94
00:03:15,420 --> 00:03:16,600
让人难以理解

95
00:03:19,100 --> 00:03:19,100
刚才我说的这些

96
00:03:19,320 --> 00:03:22,220
这个数字0.01就是人们经常用的

97
00:03:23,070 --> 00:03:25,380
另一个常用的值是0.05

98
00:03:26,840 --> 00:03:27,810
那么这就会是5%

99
00:03:27,990 --> 00:03:28,870
如果是这样的话

100
00:03:29,210 --> 00:03:30,390
你可以说

101
00:03:30,740 --> 00:03:32,320
95%的差异性被保留了

102
00:03:32,480 --> 00:03:34,280
95%的差异性被保留了

103
00:03:34,700 --> 00:03:36,710
还有其它数值 比如90%的差异性被保留了

104
00:03:37,980 --> 00:03:40,030
还有低到85%的

105
00:03:40,150 --> 00:03:42,410
因此90%对应的是0.10

106
00:03:44,340 --> 00:03:46,950
也就是10%

107
00:03:47,250 --> 00:03:49,160
取值范围是

108
00:03:49,900 --> 00:03:50,770
90 95 99

109
00:03:50,870 --> 00:03:51,470
也可能低到保留85%的差异性

110
00:03:51,500 --> 00:03:55,100
这些都是比较典型的取值范围

111
00:03:55,780 --> 00:03:56,900
可能从95到99

112
00:03:57,690 --> 00:03:58,810
是人们最为

113
00:03:59,020 --> 00:04:02,080
常用的取值范围

114
00:04:02,130 --> 00:04:02,950
对于许多数据集

115
00:04:03,010 --> 00:04:04,320
你可能会惊讶

116
00:04:04,790 --> 00:04:06,590
为了保留99%的差异性

117
00:04:06,790 --> 00:04:08,160
通常你可以大幅地降低数据的维度

118
00:04:08,200 --> 00:04:11,810
却还能保留大部分的差异性

119
00:04:12,440 --> 00:04:13,410
因为大部分现实中的数据

120
00:04:13,560 --> 00:04:15,210
许多特征变量

121
00:04:15,280 --> 00:04:17,060
都是高度相关的

122
00:04:17,310 --> 00:04:17,940
所以实际上

123
00:04:18,490 --> 00:04:19,540
大量压缩数据是可能的

124
00:04:19,610 --> 00:04:20,990
而且仍然会保留

125
00:04:21,360 --> 00:04:22,310
99%或95%的差异性

126
00:04:22,530 --> 00:04:26,260
那么你该如何实现它呢？

127
00:04:26,810 --> 00:04:28,610
你可能会用到这个算法

128
00:04:28,890 --> 00:04:30,360
你可以这样开始

129
00:04:30,540 --> 00:04:31,360
比如你想选取k的值

130
00:04:31,470 --> 00:04:33,510
我们可以从k=1开始

131
00:04:33,550 --> 00:04:34,670
然后我们再进行主成分分析

132
00:04:35,350 --> 00:04:36,440
我们算出

133
00:04:36,570 --> 00:04:38,880
Ureduce z(1) z(2) 一直到 z(m)

134
00:04:39,520 --> 00:04:40,790
算出所有那些

135
00:04:41,090 --> 00:04:42,540
x_approx(1) 一直到 x_approx(m)

136
00:04:43,200 --> 00:04:45,110
然后我们看一下99%的差异性是否被保留下来了

137
00:04:47,140 --> 00:04:48,890
是的话就搞定了 我们就用 k=1

138
00:04:49,020 --> 00:04:51,960
但如果不是 那么我们接下来尝试 k=2

139
00:04:52,620 --> 00:04:53,810
然后我们要重新走一遍

140
00:04:54,200 --> 00:04:56,070
这整个过程

141
00:04:56,170 --> 00:04:57,770
检查是否满足这个表达式

142
00:04:58,470 --> 00:05:00,980
这个式子的值是否小于0.01 如果不是 我们再重复一次

143
00:05:01,220 --> 00:05:03,070
我们尝试 k=3

144
00:05:03,310 --> 00:05:04,910
然后试 k=4 以此类推

145
00:05:04,970 --> 00:05:06,250
一直试到

146
00:05:06,630 --> 00:05:07,730
比如我们一直试到 k=17

147
00:05:08,070 --> 00:05:09,040
然后发现99%的数据

148
00:05:09,090 --> 00:05:13,060
都被保留了

149
00:05:14,120 --> 00:05:15,110
我们就会用 k=17

150
00:05:15,570 --> 00:05:17,160
这是一种用来选择

151
00:05:17,240 --> 00:05:18,790
使得99%的差异性能够得以保留的

152
00:05:19,130 --> 00:05:20,920
最小的k值的方法

153
00:05:22,380 --> 00:05:23,440
但是可以想见

154
00:05:23,550 --> 00:05:25,140
这个过程的效率相当地低

155
00:05:26,210 --> 00:05:28,120
我们在尝试 k=1  k=2 时 做了所有这些计算

156
00:05:29,580 --> 00:05:30,540
幸好 你在应用 PCA 时

157
00:05:31,130 --> 00:05:33,510
实际上 在这一步

158
00:05:33,960 --> 00:05:35,530
它已经给了我们

159
00:05:35,910 --> 00:05:37,080
一个可以使计算变得

160
00:05:37,320 --> 00:05:40,160
容易很多的量

161
00:05:41,110 --> 00:05:42,160
特别是当你调用

162
00:05:42,820 --> 00:05:44,120
svd 来计算这些

163
00:05:44,340 --> 00:05:45,550
矩阵U S V时 

164
00:05:45,610 --> 00:05:46,780
当你对协方差的矩阵 Sigma

165
00:05:47,040 --> 00:05:48,560
调用 svd 时

166
00:05:48,860 --> 00:05:49,780
我们还会得到

167
00:05:50,300 --> 00:05:52,170
这个矩阵S

168
00:05:52,360 --> 00:05:53,430
S是一个正方形矩阵

169
00:05:53,520 --> 00:05:56,790
实际上是一个 n×n 的矩阵

170
00:05:57,640 --> 00:05:58,090
它是一个

171
00:05:58,290 --> 00:05:58,290
对角矩阵

172
00:05:58,830 --> 00:06:00,380
对角线上的元素是

173
00:06:00,540 --> 00:06:01,640
s11 s22 s33 一直到 snn

174
00:06:01,980 --> 00:06:03,240
S11 S22 S33 一直到 Snn

175
00:06:03,590 --> 00:06:05,130
它们是矩阵中

176
00:06:05,260 --> 00:06:07,010
仅有的非零元素

177
00:06:07,130 --> 00:06:08,880
对角以外的其他元素

178
00:06:09,060 --> 00:06:11,470
都是0

179
00:06:11,590 --> 00:06:11,590
明白吗？

180
00:06:11,670 --> 00:06:12,530
所以我写的这些巨大的零

181
00:06:13,340 --> 00:06:14,260
就代表我所说的

182
00:06:14,740 --> 00:06:16,330
这个矩阵中对角以外的

183
00:06:17,130 --> 00:06:18,220
其他元素

184
00:06:18,480 --> 00:06:20,310
所有这些元素都等于0

185
00:06:22,300 --> 00:06:23,790
可以证明的是

186
00:06:24,190 --> 00:06:25,250
我不会在此证明

187
00:06:25,480 --> 00:06:26,380
实际上

188
00:06:26,620 --> 00:06:27,880
对于一个给定的k值

189
00:06:27,980 --> 00:06:29,920
这边的这个量

190
00:06:30,590 --> 00:06:37,820
可以更容易地算出来

191
00:06:38,800 --> 00:06:40,310
那个数值可以通过这个式子计算出来

192
00:06:41,000 --> 00:06:42,900
1减去

193
00:06:43,130 --> 00:06:44,400
从 i=1 到 k 对 Sii 求和

194
00:06:44,610 --> 00:06:47,960
从 i=1 到 k 对 Sii 求和

195
00:06:48,640 --> 00:06:50,050
除以 从 i=1 到 n 对 Sii 求和

196
00:06:50,170 --> 00:06:52,010
除以 从 i=1 到 n 对 Sii 求和

197
00:06:53,360 --> 00:06:54,820
这个的意思是

198
00:06:55,000 --> 00:06:56,170
或者说从另一个角度来解释它

199
00:06:56,450 --> 00:06:57,330
或者说从另一个角度来解释它

200
00:06:57,960 --> 00:06:59,370
假如说 k=3

201
00:07:00,810 --> 00:07:01,970
我们接下来要

202
00:07:02,080 --> 00:07:03,200
计算的分子是

203
00:07:03,340 --> 00:07:04,680
从 i=1 到 3 对 Sii 求和

204
00:07:04,820 --> 00:07:05,830
从 i=1 到 3 对 Sii 求和

205
00:07:06,240 --> 00:07:08,170
就是算出这前三个元素的和

206
00:07:09,280 --> 00:07:09,710
这就是分子

207
00:07:10,980 --> 00:07:12,880
然后计算分母

208
00:07:13,090 --> 00:07:14,970
分母是这些对角元素的总和

209
00:07:16,210 --> 00:07:17,470
1减去这个比例

210
00:07:17,660 --> 00:07:19,080
就是这里的这个值

211
00:07:19,300 --> 00:07:21,330
就是这个我用

212
00:07:21,650 --> 00:07:23,440
蓝色圈起来的数字

213
00:07:23,650 --> 00:07:24,380
接下来我们要做的就是

214
00:07:24,650 --> 00:07:26,000
看一下这个数字

215
00:07:26,430 --> 00:07:29,330
是否小于等于0.01

216
00:07:29,370 --> 00:07:30,460
或者等价的 我们可以查看

217
00:07:30,830 --> 00:07:31,960
从 i=1 到 k 对 Sii 求和

218
00:07:32,180 --> 00:07:33,010
从 i=1 到 k 对 Sii 求和

219
00:07:33,970 --> 00:07:35,180
除以 从 i=1 到 n 对 Sii 求和

220
00:07:35,320 --> 00:07:37,090
除以 从 i=1 到 n 对 Sii 求和

221
00:07:37,650 --> 00:07:38,580
它是否大于等于0.99

222
00:07:38,770 --> 00:07:40,600
它是否大于等于0.99

223
00:07:40,720 --> 00:07:42,920
如果你想确保能够保留99%的差异性的话

224
00:07:44,770 --> 00:07:45,650
所以你要做的

225
00:07:45,940 --> 00:07:48,360
就是慢慢地增大k值

226
00:07:48,770 --> 00:07:49,820
把k值设为1 k值设为2

227
00:07:50,100 --> 00:07:51,290
把k值设为3 以此类推

228
00:07:52,140 --> 00:07:53,240
并检验这个数值

229
00:07:54,720 --> 00:07:56,120
找出能够确保99%的差异性被保留的最小的k值

230
00:07:56,350 --> 00:07:58,820
找出能够确保99%的差异性被保留的最小的k值

231
00:08:00,600 --> 00:08:01,810
如果这样做

232
00:08:02,000 --> 00:08:02,790
那么你只需要调用

233
00:08:03,170 --> 00:08:04,660
一次 svd 函数

234
00:08:04,970 --> 00:08:05,830
因为它会给你S矩阵

235
00:08:06,010 --> 00:08:07,060
一旦有了S矩阵

236
00:08:07,090 --> 00:08:08,350
你便可以

237
00:08:08,490 --> 00:08:09,540
通过增加分子上的k值

238
00:08:09,770 --> 00:08:11,370
通过增加分子上的k值

239
00:08:11,930 --> 00:08:12,910
来做这个计算

240
00:08:13,070 --> 00:08:14,450
因此你就不用

241
00:08:14,560 --> 00:08:16,290
一遍一遍地调用 svd 函数

242
00:08:16,540 --> 00:08:18,620
来检验不同的k值

243
00:08:18,910 --> 00:08:20,030
因此这一步非常高效

244
00:08:20,150 --> 00:08:21,700
它能够让你

245
00:08:21,910 --> 00:08:24,020
不用一遍一遍地

246
00:08:24,090 --> 00:08:25,890
从头运行 PCA

247
00:08:26,260 --> 00:08:27,620
就能选择k值

248
00:08:28,030 --> 00:08:30,650
你只要运行一次 svd 函数

249
00:08:30,850 --> 00:08:32,350
就能得到所有的对角数值

250
00:08:32,780 --> 00:08:35,090
所有这些数值 S11 S22 一直到 Snn

251
00:08:35,780 --> 00:08:36,820
然后你就可以

252
00:08:36,920 --> 00:08:38,440
改变这个式子中的k值

253
00:08:38,730 --> 00:08:40,740
来找到能够保留99%的差异性

254
00:08:41,010 --> 00:08:42,250
来找到能够保留99%的差异性

255
00:08:43,140 --> 00:08:44,030
的最小的k值

256
00:08:44,850 --> 00:08:45,870
来总结一下 

257
00:08:46,050 --> 00:08:47,850
我在使用 PCA 进行压缩时

258
00:08:47,970 --> 00:08:49,050
通常采用的决定k值的方法是

259
00:08:49,420 --> 00:08:50,790
通常采用的决定k值的方法是

260
00:08:51,120 --> 00:08:52,590
对协方差矩阵

261
00:08:52,950 --> 00:08:54,480
调用一次 svd 函数

262
00:08:54,540 --> 00:08:55,750
然后用这个公式

263
00:08:56,030 --> 00:08:57,930
来找出满足这个表达式的

264
00:08:58,020 --> 00:09:00,390
最小的k值

265
00:09:01,580 --> 00:09:02,560
顺便说一下

266
00:09:02,650 --> 00:09:03,850
即使你要挑选一些不同的k值

267
00:09:04,180 --> 00:09:04,960
即使你要

268
00:09:05,000 --> 00:09:05,920
手动挑选k值

269
00:09:06,090 --> 00:09:07,250
也许你有

270
00:09:07,300 --> 00:09:08,620
1000维的数据

271
00:09:09,540 --> 00:09:11,590
我只想要 k=100

272
00:09:12,430 --> 00:09:13,450
如果你想要向别人

273
00:09:13,690 --> 00:09:14,760
解释你所做的

274
00:09:15,230 --> 00:09:17,070
向他们解释

275
00:09:17,750 --> 00:09:18,910
你实现的 PCA 的性能

276
00:09:19,220 --> 00:09:20,300
的一个好方法

277
00:09:20,540 --> 00:09:21,670
实际上是这个数值

278
00:09:21,890 --> 00:09:23,000
把它算出来

279
00:09:23,110 --> 00:09:25,770
它会告诉你百分之多少的差异性被保留了下来

280
00:09:26,300 --> 00:09:28,070
如果你把这个数值展现出来

281
00:09:28,340 --> 00:09:29,720
那么熟悉 PCA 的人们

282
00:09:30,100 --> 00:09:31,610
就可以通过它

283
00:09:31,880 --> 00:09:33,020
来更好地理解

284
00:09:33,080 --> 00:09:34,560
你用来代表原始数据的

285
00:09:34,900 --> 00:09:37,340
100维数据

286
00:09:37,690 --> 00:09:39,270
近似得有多好

287
00:09:39,580 --> 00:09:41,300
因为有99%的差异性被保留了

288
00:09:41,990 --> 00:09:44,140
这就是一个平方投影误差的测量指标

289
00:09:44,360 --> 00:09:45,860
这就是一个平方投影误差的测量指标

290
00:09:46,240 --> 00:09:47,870
这个比值是0.01

291
00:09:48,430 --> 00:09:49,940
就会让别人对你实现的 PCA

292
00:09:50,430 --> 00:09:51,820
是否对原始数据做了一个好的近似

293
00:09:52,580 --> 00:09:53,840
是否对原始数据做了一个好的近似

294
00:09:54,000 --> 00:09:56,530
有一个很好的直观感受

295
00:09:58,440 --> 00:09:59,600
希望对你来说

296
00:09:59,800 --> 00:10:01,260
这是一个选择k值的

297
00:10:01,850 --> 00:10:02,800
有效的过程

298
00:10:03,260 --> 00:10:04,940
这就是如何选择

299
00:10:05,160 --> 00:10:06,630
要将数据降低到什么维度

300
00:10:06,750 --> 00:10:07,830
如果你将 PCA 应用于

301
00:10:07,970 --> 00:10:09,740
很高维的数据集

302
00:10:09,990 --> 00:10:11,570
常见的 比如一千维的数据

303
00:10:11,980 --> 00:10:13,340
因为数据集的特征变量

304
00:10:13,530 --> 00:10:14,720
通常有较高的相关性

305
00:10:15,070 --> 00:10:16,140
这其实是常见的

306
00:10:16,280 --> 00:10:17,190
大部分数据集的一个特性

307
00:10:18,440 --> 00:10:19,420
你经常会发现

308
00:10:20,040 --> 00:10:21,610
PCA 能够保留

309
00:10:21,840 --> 00:10:22,940
99%的差异性

310
00:10:23,110 --> 00:10:24,440
或者说95%的差异性

311
00:10:24,720 --> 00:10:25,910
或者某些高百分比的差异性

312
00:10:26,360 --> 00:10:27,580
即使在压缩很大比例的数据的情况下

313
00:10:28,560 --> 00:10:29,720
也会看到这样的现象 【教育无边界字幕组】翻译：星星之火 校对：竹二个 审核：所罗门捷列夫