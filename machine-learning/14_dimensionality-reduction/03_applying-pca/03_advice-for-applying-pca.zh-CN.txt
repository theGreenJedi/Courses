在之前的视频中 我已经 提到过 PCA 有时可以 用来提高机器学习算法的速度 在本节课的视频中 讲解如何在实际操作中 来实现 同时列举 一些例子 只是希望能够 提供一些应用 PCA 的建议 首先我先介绍如何通过 PCA 来提高学习算法的速度 这种监督学习算法的提速 实际上也是 我个人经常通过使用 PCA 来实现的一种功能 比如说你遇到了一个监督学习问题 注意这个 监督学习算法问题有 输入 x 和 标签 y 假如说你的样本 x(i) 是非常高维的数据 比如说 x(i) 是 一个10,000维的向量 比如说其中的一个例子是 你在解决某个计算机 视觉的问题 在这里有 一张100 × 100的图片 那么 如果是100×100 那就是10000 像素 如果 x(i) 是 包含了 这10000像素 强度值的特征向量 那么 你就会有10000维特征向量 像这样有很高维 的特征向量 运行会比较慢 如果你输入10,000维 的特征向量到逻辑回归中 或者到一个神经网络、支持向量机中 或者任何别的算法中 由于这是一个很大量的数据集 有10,000个数据 因此这会使得学习算法运行得更慢 幸运的是 通过使用 PCA 我们 能够降低数据的维数 从而使得 算法能够更加高效地运行 这就是其中 的原理 首先我们需要 检查带标签的 的训练数据集并提取出 输入数据 我们只需要提取出 x 并暂时把 y 放在一边 这一步我们会得到 一组无标签的训练集 从 x(1) 到 x(m) 这可能会有 10,000维数据 也就是10,000维数据样本 所以 就是从数据组中 x(1) 到 x(m) 中提取出输入向量 然后我们应用PCA 从中我们 会得到一个降维的 数据 与刚才的 10,000维特征相比 我现在 就只有1000维特征向量 因此这就降低了10倍的维数 这就给了我们一个新的训练集 所以之前我有 这样一个样本 x(1), y(1) 这是我的第一个训练集的输入 现在用 z(1) 来表示 这样我们就有了 一个新的训练集样本 其中 z(1) 与 y(1) 是一对儿 同样地  z(2) 对应 y(2) 等等 一直到 z(m) 对 y(m) 因为现在的训练集 由这样一个 更加低维的数据集所代替 z(1),z(2) 一直到 z(m) 最后 我可以将这个 已经降维的数据集 输入到学习算法 或者是将其 放入到神经网络中 或者是 逻辑回归中 可以 学习出假设 h 把这个作为输入 这些低维的 z 作为输入 并作出预测 所以比如说在使用逻辑回归时 我应该训练得到 某一个假设函数 其输出是 1除以1加e的 负的θ次方 转置 乘以 z  这个式子将 z 向量作为 输入 并得出一个预测值 最后 如果你有 一个新的样本 比如说一个新的 测试样本 x 你所要做的是 将你的测试样本 x 通过同样的过程操作下来 通过 PCA 你会得到所对应的 z 然后这个 z 值 又可以输入到这个假设式子中 这个假设之后会对你输入的 x作出一个预测 最后要注意一点 PCA 定义了从 x到z的对应关系 这种从 x 到 z的对应关系只可以通过 在训练集上运行 PCA 定义出来 具体来讲 这种 PCA所学习出的对应关系 所做的就是计算出一系列的参数 这就是特征缩放和均值归一化 同时也计算出这样一个降维的矩阵Ureduce 但是降维矩阵 Ureduce 中的数据 就像一个 PCA所学习的参数一样 我们需要 使我们的参数唯一地适应 这些训练集 而不是 适应我们的交叉验证或者测试集 因此Ureduce矩阵中的数据 就应该 只通过对训练集运行PCA来获得 找出了降维矩阵Ureduce 或者找出了这些特征扩展的参数之后 均值均一化 并扩展可以 用分隔可以比较的规格 在训练集中找到了 所有这些参数后 就可以 将同样的对应关系应用到其他样本中了 可能 是交叉验证数集样本 或者 用在你的测试数据集中 总结一下 当你在 运行PCA的时候 只是在 训练集那一部分 来进行的 而不是 交叉验证的数据集 这就定义了从 x到z的映射 然后你就可以 将这个映射应用到 交叉验证数据集中和 测试数据集中 通过这个 例子中的这种方式 我们讨论了将数据从 上万维降到 千维 这实际上 并不切实际 因为对于大多数我们实际面对的数据降维问题 降维到原来的五分之一或者十分之一 依旧保持着原本维度数据的变化情况 改变并不会有多少影响 就分类的精确度而言 数据降维后对学习算法 几乎没有什么影响 如果我们将降维 用在低维数据上 我们的学习算法会运行得更快 总之 迄今为止我们讨论过的有关PCA的应用中 第一个是数据压缩 我们可以借此 减少内存或者磁盘空间的使用 以存取更多的数据 正如刚刚我们讨论过的 就是如何使用数据压缩以加快学习算法的例子 在这些应用中 为了选择一个k值 我们将会根据 保留方差的百分比 来确定k值 对于一个学习算法来说 加快应用将会保留99%的方差 在如何选择k值的问题上 这就是一个很典型的问题 也就是说k的选择是一个数据压缩的应用 然而对于可视化应用来说 我们通常知道 如何将二维 或者三维的数据进行可视化 所以对于可视化应用 我们选择的K值要么等于2 要么等于3 因为我们能画出二维和三维的数据集 所以 我们来总结一下PCA的主要应用 其实也就是 对于不同的应用来选择K值 我要提醒的是有一个频繁 被误用的PCA应用 你有时或许能听到其他人这么做 当然我们不希望这样 我只是想提醒你不要这么做 这是一个对PCA不好的应用方面 那就是使用它来避免过拟合 下面是原因 这并不是一个 合适的PCA应用 下面 我就来讲原因 如果我们有x(i) 或许x(i)是有n个特征的数据集 如果我们将数据进行压缩 并用压缩后的数据z(i)来代替原始数据 在降维过程中 我们从n个特征降维到k个 比先前的维度低 例如如果我们有 非常小的特征数目 假如k值为1000 n值为10000 如果我们有1000维度的数据 和我们用10000维度的数据比起来 对于同样是1000个特征来说 或许更不容易过拟合 所以有些人认为 PCA是一种避免过拟合的方法 但我这里要强调一下 PCA在过拟合问题上的应用是不合适的 并且我不建议这么做 不仅仅这个方法的效果很差 如果你想使用 PCA方法来对数据降维 以避免过拟合 PCA方法实际看起来是可以的 但是这并不是 一个用来解决 过拟合问题的算法 如果你比较担心过拟合问题 有更好的方法来解决 那就是使用正则化方法 代替PCA来对数据进行降维 原因是 如果你仔细想想PCA是如何工作的 它并不需要使用数据的标签 你只需要看好 输入数据x(i) 同时使用这个方法 来寻找更低维度的数据近似 那么 PCA做了什么呢 它把某些信息舍弃掉了 舍弃掉一些数据 并在你对数据标签y值毫不知情的情况下 对数据进行降维 所以 这或许是一个使用PCA方法 的可行之路 如果保留99% 的方差 即保留绝大部分的方差 那也就是舍弃掉某些有用信息 事实证明 当你在保留99% 或者95% 或者其它百分比的方差时 结果表明 就只使用正则化将会给你 一种避免过拟合 绝对好的方法 同时正则化 效果也会比PCA更好 因为当你使用线性回归或者逻辑回归 或其他的方法 配合正则化时 这个最小化问题 实际就变成了y值是什么 才不至于 将有用的信息舍弃掉 然而PCA不需要使用到 这些标签 更容易将有价值信息舍弃 总之 使用PCA的目的是 加速 学习算法的时候是好的 但是用它来避免过拟合 却并不是一个好的PCA应用 我们使用正则化的方法来代替PCA方法 是很多人 建议的 最后讲一下PCA的误用 我说PCA是一个非常有用的算法 我经常用它在可视化数据上进行数据压缩 但我有时候 会看到有些人 把PCA用在了不应当使用的地方 从中 我都看到一个共同点 如果某人正在设计机器学习系统 他们或许会写下像这样的计划 让我们设计一个学习系统 得到训练集然后 我要做的是 先运行PCA 然后训练逻辑回归之后在测试数据上进行测试 通常在 一个项目的初期 有些人便直接写出 项目计划而不是说 来试试PCA的这四步 在写下一个 使用PCA方法的项目计划前 一个非常好的 问题是 如果我们在整个项目中 不使用前后有怎样的差别 通常人们不会 去思考这个问题 尤其是当人们提出一个复杂的项目 其中使用了PCA或其它方法 有时 我经常建议大家 在你使用PCA之前 首先 我要建议你的是 你要知道自己做的是什么 也就是说你想要做什么 这也是你首先 需要在原始数据x(i)上考虑的问题 只有你没有想过你要的是什么的时候 那么在你使用z(i)前使用PCA 所以在使用PCA之前 确定是减少数据维度 我会将这个问题思考清楚 让我们抛弃PCA这一步 并思考 让我们就试试在学习算法上 使用原始数据 那么 我们就会使用原始数据输入x(i) 同时我也建议 一开始不要将 PCA方法就直接放到算法里 先使用原始数据x(i)看看效果 只有一个原因 让我们相信算法出现了问题 那就是 你的学习算法 收敛地非常缓慢 占用内存 或者硬盘空间非常大 所以你想来压缩 数据 只有当你的x(i)效果不好 只有当你有证据或者 充足的理由来确定 x(i)效果不好的时候 那么就考虑用PCA来进行压缩数据 因为我常常看到 某些人 在项目开始时便将PCA考虑进去 有时他们 并没有仔细思考 他们做了什么使得结果表现地好的 更没有考虑在不用PCA下的情景 试想 这种没有PCA的选择 在花费大量时间 在PCA方法上 去计算 k 值之前 这才是使用PCA的正确流程 尽管有这些需要注意的 PCA仍旧是 一种不可思议的有用的算法 当你把它用在合适的应用上面 PCA方法 我使用地比较频繁 大部分时候我都用它来 加快学习算法 但我认为 PCA通常都是 被用来 压缩数据的 以减少内存使用 或硬盘空间占用 或者用来可视化数据 同时 是最常用的方法之一 也是一种强有力的无监督学习算法 通过在本视频中学到的 希望 你有能力实现 PCA算法并用它 来实现你的目的 【教育无边界字幕组】翻译：Yuens 校对/审核：所罗门捷列夫