Neste vídeo, eu gostaria de como inicializar o K-means e o mais importante, Isso também levará a uma discussão de como fazer o K-means evitar uma otimização local. Aqui está o algoritmo de agrupamento de que falamos antes. Uma etapa das quais nós nunca falamos muito foi sobre a etapa de como iniciar aleatoriamente os centróides. Existem algumas maneiras diferentes que podemos imaginar na inicialização aleatória os centróides aleatoriamente. Porém,  observa-se que existe um método que é muito mais recomendado que as outras opções que conhecemos. Deixe-me falar sobre esta opção, que é a que funciona melhor na maioria das vezes. Eu costumo inicializar meus centróides dessa forma: Ao executar o K-means, você deve ter o número de centróides, K, definido para ser inferior ao número de exemplos de treinamento M. Seria muito estranho executar o K-means com um número de centróides igual ou maior que o número de exemplos que você tem, certo? Então, o modo que eu normalmente inicializo o K-means seria, escolher aleatoriamente K exemplos de treinamento, e então o que eu faria seria definir µ1 a µk iguais a estes k exemplos. Vou mostrar um exemplo concreto. Digamos que k é igual a 2 e então neste exemplo à direita, vamos dizer que eu quero encontrar dois agrupamentos. Então, eu vou para inicializar meus centróides será escolher aleatoriamente alguns exemplos, digamos, eu escolho um deste e escolho um daquele. E a forma que eu vou utilizar para inicializar meus centróides é simplesmentes colocar meus centróides bem em cima desses exemplos. Então, esse é o meu primeiro centróide e esse é o meu segundo centróide, e e essa é uma inicialização aleatória do K-means. O que eu desenhei parece particularmente bom. Às vezes eu poderia ter menos sorte e e talvez eu acabaria pegando este como meu primeiro exemplo aleatório, e este como o meu segundo exemplo. E aqui eu estou pegando dois exemplos, porque k é igual a 2. Nós escolhemos aleatoriamente dois exemplos de treinamento e se eu escolhi aqueles dois então eu vou acabar tendo o meu primeiro centróide aqui e aquele como minha posição inicial do  segundo centróide. Então, é assim que você pode iniciar os centróides aleatoriamente. E assim, na inicialização, o seu primeiro centróide μ₁ será igual a x (i) por algum valor aleatorio de (i) e μ₂ será igual a x(j) para algum valor escolhido aleatoriamente diferente de (j) e assim por diante, se tiver mais agrupamentos e mais centróides. Apenas um comentário. Devo dizer que no vídeo anterior, onde eu ilustrei primeiro o K-means com a animação, naquele conjunto de slides. apenas para fins de ilustração, eu usei um método diferente para a inicialização dos meus centróides. Porém o método descrito neste slide, é o caminho recomendado. E é a forma que provavelmente você deve usar, quando você implementar o K-means. Assim, como é sugerido talvez por estas duas figuras à direita. Eu creio que o K-means pode acabar convergindo para diferentes soluções, dependendo exatamente de como os agrupamentos foram inicializados, Portanto, dependendo da inicialização aleatória. o K-means pode acabar em diferentes soluções. Em particular, K-means pode realmente acabar em um ponto ótimo. Se você possui dados como estes. Parece que existem três grupos, então se você executar o K-means e ele acabar em um ponto ótimo este pode ser realmente um ponto ótimo global, você pode acabar com esse agrupamento. Mas se você teve uma inicialização particularmente infeliz, o K-means também pode ficar preso em diferentes pontos ótimos locais. Assim, neste exemplo à esquerda parece que este agrupamento azul captou uma série de pontos à esquerda. e os agrupamento s verde e vermelho, ficaram com um número pequeno de pontos cada. Sendo assim, isso corresponde a um ponto ótimo local porque tomou basicamente esses dois agrupamentos e os usou em um. Além disso, dividiu o segundo agrupamento em dois sub-agrupamentos separados como vemos e também tomou o segundo agrupamento e dividiu em dois sub-agrupamentos separados como vemos, portanto, ambos os exemplos no canto inferior direito correspondem a diferentes pontos ótimos locais no K-means e, de fato, neste exemplo aqui, o agrupamento vermelho captou apenas um único exemplar. E o termo ótimo local, a propósito, refere-se ao ponto ótimo local desta função distorção J, e o que essas soluções no canto inferior esquerdo, o que esses ótimos locais correspondem são soluções reais onde o K-means ficou preso ao ponto ótimo local e não está fazendo um trabalho muito bom minimizar esta função distorção J. Então se você está preocupado com a possibilidade do K-means ficar preso em pontos ótimos locais, se você deseja aumentar as chances do K-means encontrar o melhor agrupamento possível, como exibido aqui em cima, o que podemos fazer é tentar múltiplas inicializações aleatórias. Dessa forma, ao invés de apenas inicializar K-means uma vez na esperança de que isso funcione o que podemos fazer é, inicializar o K-means várias vezes e executar K-means muitas vezes, e usar isso para tentar se certificar de chegarmos a uma melhor solução, o ótimo global ou o melhor ótimo local possível. Objetivamente, aqui está como você pode fazer sobre isso. Vamos dizer, eu decidi executar o K-means uma centena de vezes, então eu vou executar este loop uma centena de vezes isto é um número de vezes bem típico, deve ser um valor próximo de 50 podendo chegar a 1000. Então, vamos dizer que você decidiu executar o K-means 100 vezes. Então, isso significa que vamos inicializar o K-means aleatoriamente. E para cada uma dessas cem inicializações aleatórias nos executamos o K-means o que nos trará um conjunto de agrupamentos, e um conjunto de centróides e então nós calculamos a distorção J, isto vai calcular a função custo do conjunto do agrupamento e centróides que recebemos. Finalmente, depois de ter feito esse procedimento uma centena de vezes. Você terá uma centena de agrupamentos com diferentes tipos de dados e, finalmente, o que você fará com todas estas centenas de formas que você encontrou de agrupamento dos dados. Basta escolher um, que nos dá o menor custo. Isso nos dá a menor distorção. E acontece que, se estiver executando o K-means com um número de agrupamentos relativamente pequeno, para que você saiba se o número de agrupamentos é algo entre dois ou dez fazer várias inicializações aleatórias pode te certificar que encontrou o melhor ponto ótimo. dar uma garantia que você encontrou o melhor agrupamento de dados Mas se K é muito maior, de modo que, K for maior que 10, certamente se K for grande, se você estava tentando encontrar centenas de agrupamento, então haverá múltiplas inicializações aleatórias e será menos provável que haja uma diferença enorme e haverá uma chance muito maior que sua primeira inicialização aleatória lhe traga uma solução decente já feita, fazer várias inicializações aleatórias provavelmente vai lhe trazer uma solução um pouco melhor, mas talvez não tanto assim. Mas é realmente no regime onde você tem um número relativamente pequeno de agrupamentos, especialmente se você tem, talvez 2 ou 3 ou 4 agrupamentos onde a inicialização aleatória poderia fazer uma enorme diferença em termos de certificar-se de um bom trabalho minimizando a distorção da função e dando-lhe um bom agrupamento. Então, este é o K-means com inicialização aleatória. Se você está tentando desenvolver um agrupamento com um número relativamente pequeno de grupos, 2, 3, 4, 5, talvez 6, 7, usando várias inicializações aleatórias pode, às vezes, ajudá-lo a encontrar de maneira muito melhor o agrupamento dos dados. Mas, mesmo se você está trabalhando com um grande número de agrupamentos, a inicialização, o método de inicialização aleatória que eu descrevo aqui. Aqui tivemos um ponto de partida ao K-means para à partir dele encontrarmos um bom conjunto de agrupamentos.