このビデオでは、 K-meansをどう初期化するのかと、 その延長としてより重要な、 K-meansでどう局所最適の問題を 回避するのかについての2つを話す。 ここに、以前に話した K-meansのクラスタリングアルゴリズムがある。 ここまでちゃんと話してなかった ステップの一つに クラスタ重心を、どうランダムに初期化するか、というのがある。 ランダムにクラスタ重心を初期化する、と 言われた時には、 いくつかの方法が考えられるが、 その中の一つが、 それ以外の思いつく選択肢の多くよりも 優れている、という ものがある。 だからその方法について議論しようと思う、 何故ならそれがしばしば一番良く機能する選択肢だからだ。 これが私が普段クラスタ重心を初期化するやり方だ。 K-meansを実行する時には クラスタ重心の数、Kを トレーニング手本の数mよりも 小さい数に設定しなくてはならない。 K-meansを、手本の数と同じか それより多い数のクラスタ重心に対して 実行する、というのは ヘンテコなことだってのは分かるでしょ？ で、私が普段 K-meansを初期化するのに使う方法は K個のトレーニング手本を ランダムに取り出す。 そして次にやるのは、 ミュー1からミューKに、これらをセットする、という事。 具体例を見てみよう。 Kをイコール2だと してみよう。つまり この右側の手本に対し、2つのクラスタを見つけたい、とする。 その場合、クラスタ重心を 初期化する為に 私がやるのは 2つの手本をランダムに選ぶ。 そして、例えば これとそれを選んだとしよう。 その場合私が クラスタ重心を初期化するやり方は それらの手本の真上に クラスタ重心を置く、という方法。 これが最初のクラスタ重心となり、 これが二番目のクラスタ重心となる。 これがK-meansをランダムに初期化する方法だ。 ここに描いたのは極めて良い物っぽいけど、 たまにはもっとついてなくて、 結局最初の奴として こんなのをランダムな最初の手本として、 そして二番目としてこんなのを選ぶはめになるかもしれない。 ここで2つの手本を選んだのはK=2としたから。 適当に2つのトレーニング手本を ランダムに2つ選んだ、 それをこれら2つとすると、 その時は これを最初のクラスタ重心に これを2つ目のクラスタ重心の 初期位置とする。 以上がランダムに クラスタ重心を初期化するやり方だ。 つまり初期化の時には クラスタ重心ミュー1は 適当に選んだ値iの元に x(i)と等しくなる。 ミュー2は、iとは別の異なるランダムに選ばれた数jを使って x(j)と表せる物と等しくなる、 などなどと、 もっと多くのクラスタとクラスタ重心があるなら続けていく。 ここでちょっと補足しておく。 以前の動画で 最初にK-meansをアニメーションで 例示した時の話だ。 そこでのスライドは、 例示の為だけの目的の物だった。 私は実際には、そこで書いていたのとは 違うクラスタ重心の初期化の仕方をする。 このスライドで説明している方法こそが 本当に推奨する方法だ。 そして恐らく、あなたがK-meansを実装する時にも使うべき方法と言える。 さて、この右側の二つの図で 例示出来ているかもしれない事として、 K-meansは実際に どう初期化されるかに応じて つまりどうランダム初期化されるかに応じて 異なる解に収束しうるという事が 想像出来るかもしれない。 K-meansは異なる解に収束しうる。 そして実際、K-meansは異なる局所最適解に落ち着きうる。 もしこんなデータセットを与えられたとすると うーん、見た所、ここには 三つのクラスタがあるように見える。 だからもしK-meansを実行したら もし良い局所最適解に 落ち着けば、これが極めて良い局所最適解に見える、 その時には、このクラスタの輪となるだろう。 だがあなたが特にアンラッキーで ランダム初期化によっては、K-meansは 異なる局所最適に スタックしてしまうかもしれない。 だから、この左側の例では、 青いクラスタがたくさんの点を 捕捉してしまい、緑と赤のクラスタは それぞれ相対的にはちょっとの点しか捕捉出来ていない。 つまりこれは、悪い局所最適解に 対応している。 何故ならそれは、これら二つのクラスタを 一個に取り出してしまっていて、 さらに、二番目のクラスタを 二つのサブクラスタに分割 してしまっている。 二番目のクラスタを 二つの別々のサブクラスタに 分割して しまっている。 つまり、これら右下の例は どちらも異なる 局所最適に 対応した例で、実際、 この例では このクラスタ、赤のクラスタは たった一つぼっちの手本に捕捉されてしまっている。 ところで局所最適という 用語は、この ディストーション関数Jの 局所最適を示している、 そしてこれらの左下の 解は、これらの 局所最適な解は K-meansが局所最適に スタックしてしまい、 このディストーション関数Jを 最小化するのに あんまり良い仕事はしていない事に対応する。 だから、もしK-meansが局所最適に スタックしてしまいそうと心配だったら、 もしK-meansが可能な中で ベストなクラスタリングを見つけるオッズを 高めたければ、この上に見せたように、 我らに取れる手段としては 複数のランダム初期化を試みる事だ。 つまりK-meansを一回だけ初期化して うまく行く事を祈る代わりに やれる事として、 K-meansをたくさん初期化して そしてK-meansをたくさん実行する事だ。 そしてそれを用いて、 可能な限り一番良い局所最適、 つまりはグローバル最適を 得ている事を確認するのだ。 具体的には、これが、そのやり方だ。 K-meansを100回 走らせる事に決めたとしよう。 つまりこのループを 100回実行する。そしてそれは かなりありがちな数で、 だいたい50から1000の間の あたりの回数だと思う。 さて、K-meansを100回実行しよう、と決めたとしよう。 それはどういう事かというと、 K-meansをランダム初期化するという事で そしてこれらの100回の ランダム初期化の時に、毎回 K-meansを走らせる。 するとクラスタとクラスタ重心の 集合が得られて、 そしてそれを用いて ディストーション関数Jを計算する。 つまりこの得られたクラスタ割り振りと クラスタ重心の集合に対して、 コスト関数をそれぞれ計算していく。 最後に、これらのプロセス全てを100回行ったら、 100個の異なる、 データをクラスタリングする方法が得られる事になる。 そこから最後に行うのは、 これら全ての100通りの 見つけたクラスタリングのデータから、 単純に一番コストが低い物を選ぶだけ。 それが一番ディストーションが小さい。 そして以下のような事も分かるだろう。 もしかなり少ない数の クラスター数に対してK-meansを走らせれば、 つまり、そうだなぁ、 クラスタの数がだいたい2から10とか その位の範囲なら、 複数回のランダム初期化を行う事は かなりしばしば より良い局所最適を見つけた事を確認出来る、 より良いクラスタリングのデータを見つけた事を。 でもKがとても大きい時は、 Kが10よりもずっと大きく、 そうだなぁ、だいたい もしあなたが何百もの クラスタを見つけようとしている時には、 その時には複数回のランダム初期化を行っても、 そんなに大きな違いは無いだろう。 そして最初のランダム初期化で すでにかなり良い 解が得られている 公算が高い。 そして複数回、ランダム初期化を 行ったら、たぶん、 ちょっとはマシな解は得られるだろうが、でもそんなには変わらない。 だが、相対的に 小さな数のクラスタ数の 範囲に居る時は、 とくに2とか3とか4個の クラスタの時には、 複数回ランダム初期化は、 ディストーション関数をちゃんと最小化し、 ひいては良いクラスタリングを与えてくれている、と 確認するのに、やるとやらないとでは大違いとなる。 さて、以上がK-meansの ランダム初期化だ。 もし相対的に小さな数の クラスタ、たとえば 2とか3とか4とか5とか、 うーん、6とか7でも、それらの数のクラスタで学習させたい時には 複数回ランダム初期化を用いる事で、 より良いデータのクラスタリングが得られる助けとなる事がある。 だが、大きな数のクラスタの時だって、 ここで説明したランダム初期化の方法は K-meansに良いクラスタを探させる リーズナブルなスタート地点と なるだろう。