En este video me gustaría hablar de cómo iniciar K-means y esto nos llevará a una discusión de cómo podemos evitar las óptimas locales en K-means. Aquí tenemos el algoritmo de agrupamiento K-means del que hablamos antes. Uno de los pasos del que no hablamos mucho es el en el que iniciamos aleatoriamente los centroides de los agrupamientos. Podemos imaginar maneras distintas de iniciar aleatoriamente los centroides del agrupamiento, Pero resulta que la que hay un método mucho más recomendable que la mayoría de las otras opciones que podemos tener en mente. Permítanme hablarles de esta opción, ya que seguido es la que funciona mejor. Así es como yo inicio mis centroides de agrupamientos. Cuando estamos ejecutando K-means debemos tener un número de centrodes de agrupamiento “K” no menor al número de ejemplos de entrenamiento “m”. Sería muy raro ejecutar K-means con un número de centroides de agrupamientos igual o mayor que el número de ejemplos que tenemos ¿verdad? La manera en la que iniciaré K-means es eligiendo al azar “k” ejemplos de entrenamiento. Lo que hago entonces es determinar “µ1” de ˝µK” igual a estos “k” ejemplos. Les mostraré un ejemplo concreto. Digamos que “k” es igual a 2 y en este ejemplo de la derecha digamos que encontramos dos agrupamientos. Entonces, lo que haré iniciar mis centroides de agrupamientos es elegir al azar un par de ejemplos. Digamos que elijo este y este otro de aquí. La manera en la que iniciaré los centroides de agrupamiento será iniciarlos justo sobre esos ejemplos. Este es mi primer centroide de agrupamiento y este es el segundo centroide y este es un inicio aleatorio de K- means. El que dibujé parece ser uno particularmente bueno. Algunas veces puedes resultar menos suertudo y terminarás eligiendo este como mi primer ejemplo inicial aleatorio y este como el segundo. Aquí estoy eligiendo dos ejemplos porque “K” es igual a 2. Ya elegimos aleatoriamente dos ejemplos de entrenamiento y si elijo entre estos dos, pondré este como mi primer centroide de agrupamiento y este como mi segunda ubicación inicial para el centroide de agrupamiento. Así es como puedes iniciar aleatoriamente los centroides del agrupamiento, Por lo tanto, al inicio, tu primer agrupamiento “µ1” será igual a “x(i)” para un valor aleatorio de “i” y “µ2” será igual a “x(j)” para un valor elegido al azar de “J”, y así sucesivamente si tienes más agrupamientos y más centroides. Como comentario al margen debo decir que en un video anterior con diapositivas en las que ilustramos K-means por primera vez con la animación para explicar, utilicé un método diferente de inicio para los centroides de mis agrupamientos. Pero el método que describo en esta diapositiva es el más recomendado y el que deberías utilizar cuando implementes K-means. Entonces, como sugieren estas figuras de la derecha, puedes pensar que K-means puede acabar convergiendo con soluciones diferentes, dependiendo de qué tan exactamente se iniciaron los agrupamientos en el inicio aleatorio. K-means puede arrojar soluciones diferentes. En particular, K-means puede terminar en una óptima local. Si tienes un conjunto de datos como este, puedes ver que hay tres agrupamientos. Si ejecutas el K-means y termina en una buena óptima local, que pudiera ser la óptima global, puedes terminar con esta división en agrupamientos. Pero si tuviste un inicio aleatorio desafortunado, K-means puede atorarse en óptimas locales diferentes. En este ejemplo de la izquierda parece que el agrupamiento azul ha capturado muchos puntos de la izquierda y los agrupamientos verde y rojo capturó un número relativamente pequeño de puntos. Esto corresponde a una mala óptima local porque básicamente toma estos dos agrupamientos y los une en 1. Además, divide el segundo agrupamiento en dos agrupamiento secundarios y tomó el segundo agrupamiento y lo dividió el agrupamiento en dos agrupamientos secundarios. Estos dos ejemplos de abajo a la derecha corresponden a las diferentes óptimas locales de K-means. De hecho, en este ejemplo, el agrupamiento rojo capturó un sólo ejemplo de óptima. El término óptima local, por cierto, se refiere a las óptimas locales en esta función de distorsión “J” y las soluciones de la izquierda inferior; es decir, las óptimas locales corresponden a soluciones reales en las que K-means se atoró en la óptima local y no realizó un buen trabajo minimizando de la función de distorsión “j”. Si te preocupa que K-means se atore en óptimas locales y quieres aumentar tus probabilidades de encontrar los mejores agrupamientos posibles como el que se muestra aquí arriba, puedes intentar multiplicar inicios aleatorizados de manera que, en vez de iniciar K-means sólo una vez esperando que funcione, puedes iniciar K-means muchas veces y ejecutar K-means muchas veces y utilizar esto para asegurarte de obtener una buena solución con la mejor óptima local o global. Aquí explico cómo puedes hacer eso. Digamos que decido ejecutar el algoritmo K-means cien veces. Ejecutaré este ciclo cien veces. El número usual de veces que se ejecuta K-means es de entre 50 y 1000. Digamos que decidiste ejecutar K-means 100 veces. Lo que esto significa es que iniciaríamos K-means aleatoriamente y para cada uno de estos cien inicios, ejecutaríamos K-means. De esto obtendríamos un conjunto de agrupamientos y un conjunto de centroides de agrupamientos. Entonces calcularíamos la distorsión “J”; es decir, la función de costos en el conjunto de asignación de agrupamientos y centroides. Finalmente, una vez terminado todo este proceso cien veces, tendrás cien maneras diferentes de agrupar datos. Cuando termines, lo que harás es tomar las 100 maneras que encontraste con los datos de agrupamiento y elegirás la que nos dé el costo más bajo. Esto también nos da la menor distorsión. Resulta que si usted está estás ejecutando K-means con un número relativamente pequeño de agrupamientos. Si el número de agrupamientos está entre 2 y 10, realizar un inicio múltiple aleatorio puede asegurarte que encontrarás la óptima local más adecuada. Cerciórate de encontrar los mejores datos de agrupamiento. Si “K” es muy grande o mucho mayor de 10, o si “K” fuera enorme y estuviera intentando encontrar cientos de agrupamientos, entonces, realizar el inicio aleatorio múltiple no será una gran ventaja sino que tendrás una probabilidad más alta de que tu inicio aleatorio te de una solución decente. En cambio, realizar inicios múltiples aleatorios te dará una solución ligeramente mejor, pero no mucho. En el régimen en el que tenemos una cantidad relativamente pequeña de agrupamientos, especialmente si tienes dos, tres o cuatro agrupamientos, el inicio aleatorio sí puede hacer una diferencia enorme en cuanto al trabajo que puedes realizar minimizando la función de distorsión y obteniendo un buen agrupamiento. Esto fue K-means con un inicio aleatorio. Si estás intentando aprender un agrupamiento con un número relativamente bajo de agrupamientos (2, 3, 4, 5, tal vez 6, o 7) utilizar inicios múltiples aleatorios puede ayudarte a encontrar datos de agrupamiento mucho mejores. Aún si estás aprendiendo un gran número de agrupamientos, el método de inicio aleatorio que describí aquí debería darle a K-means un punto de partida razonable para iniciar para buscar buenos conjuntos de agrupamientos.