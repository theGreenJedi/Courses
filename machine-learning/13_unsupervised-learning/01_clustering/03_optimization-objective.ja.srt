1
00:00:00,090 --> 00:00:01,540
これまで見て来た教師あり学習アルゴリズム、

2
00:00:01,690 --> 00:00:02,890
線形回帰や

3
00:00:03,130 --> 00:00:04,730
ロジスティック回帰などは、

4
00:00:04,930 --> 00:00:05,850
それらは全て、

5
00:00:06,300 --> 00:00:08,089
最適化の為の目的関数、

6
00:00:08,670 --> 00:00:10,920
またの名をコスト関数を持っていて、それを最小化しようとしていた。

7
00:00:11,920 --> 00:00:13,180
K-meansもまた、最適化の

8
00:00:13,770 --> 00:00:15,730
目的関数、またはコスト関数を

9
00:00:15,870 --> 00:00:18,720
持っていて、それを最小化しようとする。

10
00:00:19,630 --> 00:00:20,180
そしてこの動画では、最適化の目的関数が

11
00:00:20,230 --> 00:00:23,620
何かを説明する。

12
00:00:23,730 --> 00:00:24,420
これをやりたい理由としては、

13
00:00:24,750 --> 00:00:26,960
二つの目的にこれは有用だからだ。

14
00:00:28,020 --> 00:00:29,330
まず、K-meansの最適化の目的関数が

15
00:00:29,480 --> 00:00:30,890
なんなのかを知る事は、

16
00:00:31,150 --> 00:00:32,390
学習アルゴリズムを

17
00:00:32,690 --> 00:00:33,970
デバッグする助けとなる。

18
00:00:34,070 --> 00:00:35,080
K-meansがちゃんと走ってるか

19
00:00:35,300 --> 00:00:37,100
確認も出来る。
二番目に、

20
00:00:37,610 --> 00:00:39,290
そしてたぶんこっちの方が重要だが、

21
00:00:39,530 --> 00:00:41,290
後半のビデオで、

22
00:00:41,490 --> 00:00:42,580
これをどう用いて

23
00:00:42,730 --> 00:00:44,000
K-meansがより良いクラスタを見つける助けに出来るか、

24
00:00:44,070 --> 00:00:46,290
そしてどう局所最適を避ける事が出来るかを議論する。
でもそれはこのビデオの後に続くビデオでね。

25
00:00:46,410 --> 00:00:47,330
思い出してもらう為に言っておくと、K-meansを

26
00:00:49,680 --> 00:00:52,870
実行している間、我らは

27
00:00:54,450 --> 00:00:55,820
二つの種類の変数を管理していく。

28
00:00:56,430 --> 00:00:58,390
一つ目はc(i)。これは

29
00:00:58,700 --> 00:00:59,830
x(i)が現在

30
00:01:00,190 --> 00:01:01,600
どのクラスタに割り振られているかのインデックスを

31
00:01:02,730 --> 00:01:05,040
トラックする為の変数だ。

32
00:01:05,230 --> 00:01:05,960
そしてもう一方の管理する変数は

33
00:01:06,540 --> 00:01:07,580
ミューの下付き添字kだ。

34
00:01:08,120 --> 00:01:09,410
それはクラスター重心Kの

35
00:01:10,140 --> 00:01:12,110
場所を表す。

36
00:01:12,380 --> 00:01:13,750
もう一度言っておくと、K-meansでは

37
00:01:14,030 --> 00:01:17,230
大文字のKをクラスタの総数を表すのに使う。

38
00:01:17,890 --> 00:01:19,310
そしてこの小文字のkで

39
00:01:20,010 --> 00:01:20,910
クラスタ重心の

40
00:01:21,040 --> 00:01:22,650
インデックスを表す。

41
00:01:22,970 --> 00:01:23,930
つまり小文字のkは

42
00:01:24,030 --> 00:01:24,940
1からKまでの間の

43
00:01:25,140 --> 00:01:26,390
数字となる。

44
00:01:26,600 --> 00:01:29,630
さらにもう一つ

45
00:01:29,840 --> 00:01:31,040
追加の記法として、

46
00:01:31,270 --> 00:01:32,280
ミューの下付き添字 c(i)という物で

47
00:01:32,630 --> 00:01:34,560
これはクラスターの重心のうち、

48
00:01:34,970 --> 00:01:36,660
サンプルx(i)に

49
00:01:36,780 --> 00:01:38,400
割り振られている物を

50
00:01:38,880 --> 00:01:40,500
表す。

51
00:01:40,710 --> 00:01:42,030
この記法についてもうちょっと

52
00:01:42,450 --> 00:01:43,450
説明しよう。

53
00:01:43,660 --> 00:01:45,600
x(i)がクラスタ重心5に

54
00:01:45,740 --> 00:01:47,760
割り振られているとしよう。

55
00:01:48,880 --> 00:01:49,830
それの意味する所はc(i)、

56
00:01:50,850 --> 00:01:52,290
このiはx(i)のインデックスだが、

57
00:01:53,130 --> 00:01:54,300
c(i)はイコール5となる。

58
00:01:54,420 --> 00:01:57,640
でしょ？だってc(i)=5となるのが

59
00:01:57,800 --> 00:01:59,270
サンプルx(i)が

60
00:02:00,500 --> 00:02:01,720
クラスタナンバー5に割り振らたという

61
00:02:01,910 --> 00:02:03,440
事だから。

62
00:02:03,510 --> 00:02:05,700
だからミュー下付き添字の

63
00:02:06,290 --> 00:02:07,960
c(i)はイコール、

64
00:02:08,100 --> 00:02:09,630
ミュー 下付き添字の 5 となる。

65
00:02:10,080 --> 00:02:12,260
何故ならc(i)がイコール

66
00:02:13,700 --> 00:02:14,100
5だからだ。

67
00:02:15,100 --> 00:02:16,570
このミュー下付き添字c(i)は

68
00:02:16,660 --> 00:02:18,420
クラスターナンバー5の

69
00:02:18,730 --> 00:02:19,670
クラスタ重心で、それがサンプルx(i)が

70
00:02:20,120 --> 00:02:22,480
割り振られている物だ。

71
00:02:23,470 --> 00:02:24,730
この記法で、

72
00:02:24,960 --> 00:02:26,040
我らはK-meansの

73
00:02:26,200 --> 00:02:28,150
クラスタリングアルゴリズムの

74
00:02:28,290 --> 00:02:30,360
最適化の目的関数を書き下す、準備が出来た事になる。

75
00:02:30,760 --> 00:02:30,800
それはこうだ。

76
00:02:31,330 --> 00:02:32,940
K-meansが最小化する

77
00:02:33,040 --> 00:02:34,380
コスト関数は

78
00:02:34,570 --> 00:02:35,770
これらのパラメータ全ての関数Jだ、

79
00:02:35,880 --> 00:02:37,470
c1からcmまでと、

80
00:02:37,890 --> 00:02:39,610
ミュー1からミューKまでの。

81
00:02:39,790 --> 00:02:41,570
K-meansは実行していく過程でこれらを変更していく。

82
00:02:42,100 --> 00:02:43,930
そして最適化の目的関数は、

83
00:02:44,160 --> 00:02:45,520
右に示した物で、平均としての

84
00:02:45,610 --> 00:02:46,430
1/mの、和を取る事の

85
00:02:46,620 --> 00:02:48,730
i=1からmまでの、この項で、

86
00:02:50,400 --> 00:02:52,670
今赤の箱でくくったこれ。

87
00:02:52,870 --> 00:02:54,680
サンプルx(i)と

88
00:02:55,160 --> 00:02:57,540
x(i)に割り振られた

89
00:02:57,690 --> 00:02:58,740
クラスタ重心の

90
00:02:59,130 --> 00:03:00,210
位置との間の

91
00:03:01,320 --> 00:03:01,920
二乗距離。

92
00:03:03,240 --> 00:03:06,070
ちょっと描いて、これを説明しよう。

93
00:03:06,240 --> 00:03:07,800
これはトレーニングサンプルの

94
00:03:08,190 --> 00:03:09,780
x(i)の位置で、

95
00:03:10,410 --> 00:03:11,760
これがサンプルx(i)が割り振られた

96
00:03:11,970 --> 00:03:13,660
クラスタ重心の位置とする。

97
00:03:14,560 --> 00:03:17,080
これを図で説明する為に、x1とx2があって、

98
00:03:17,420 --> 00:03:19,540
この点、

99
00:03:19,760 --> 00:03:21,210
ここがサンブルx(i)とすると、

100
00:03:22,080 --> 00:03:23,060
つまりこれがサンプルx(i)と

101
00:03:23,110 --> 00:03:24,840
イコールだとする。

102
00:03:25,860 --> 00:03:27,000
そしてx(i)があるクラスタ重心に

103
00:03:27,240 --> 00:03:28,270
割り振られているとすると、

104
00:03:28,340 --> 00:03:30,240
ところでクラスタの重心は十字で表す事にする。

105
00:03:30,630 --> 00:03:32,130
つまり、それがミュー5の

106
00:03:32,300 --> 00:03:33,830
場所で、

107
00:03:34,370 --> 00:03:35,640
この例ではx(i)がクラスター重心5に

108
00:03:35,850 --> 00:03:37,960
割り振られてるとすると。

109
00:03:38,810 --> 00:03:40,660
すると、二乗距離、つまり、

110
00:03:40,940 --> 00:03:41,840
点x(i)とそれが割り振らている

111
00:03:43,810 --> 00:03:46,010
クラスタ重心の間の

112
00:03:46,220 --> 00:03:48,400
距離の二乗だ。

113
00:03:49,570 --> 00:03:50,720
そしてK-Meansがやる事は、

114
00:03:51,070 --> 00:03:52,540
つまり、

115
00:03:52,680 --> 00:03:54,480
パラメータであるc(i)と

116
00:03:55,270 --> 00:03:57,410
ミューi を探そうとする、

117
00:03:57,570 --> 00:03:58,840
cとミューで、コスト関数Jを

118
00:03:58,960 --> 00:04:00,450
最小化する物を探そうとする。

119
00:04:01,440 --> 00:04:03,180
このコスト関数はまた、

120
00:04:03,680 --> 00:04:06,770
ディストーション（歪み）コスト関数、

121
00:04:07,060 --> 00:04:10,030
またはK-meansアルゴリズムのディストーションと

122
00:04:10,240 --> 00:04:12,130
呼ばれる。

123
00:04:12,790 --> 00:04:13,360
もうちょっと詳細を見ると、

124
00:04:13,630 --> 00:04:15,750
これがK-meansのアルゴリズムだ。

125
00:04:15,820 --> 00:04:16,450
これは前のスライドにあったのと全く同じ物を

126
00:04:16,610 --> 00:04:17,960
実際の形にした物だ。

127
00:04:18,950 --> 00:04:20,200
そしてこのアルゴリズムの

128
00:04:21,030 --> 00:04:23,120
最初のステップは

129
00:04:23,830 --> 00:04:25,910
クラスター割り振りのステップで、

130
00:04:27,920 --> 00:04:29,850
そこで各点を

131
00:04:30,030 --> 00:04:32,910
クラスター重心に割り振る。

132
00:04:33,010 --> 00:04:34,830
クラスター割り振りのステップは

133
00:04:35,050 --> 00:04:36,210
実際に変数c1, c2、、、と

134
00:04:36,450 --> 00:04:38,560
c(m)までの観点から

135
00:04:40,770 --> 00:04:42,950
Jを最小化している、という事を

136
00:04:43,420 --> 00:04:45,900
数学的に証明する

137
00:04:46,380 --> 00:04:48,050
事が出来る。

138
00:04:48,170 --> 00:04:52,030
この間、もっとも近い

139
00:04:52,480 --> 00:04:54,240
重心である、ミュー1からミューkまでは

140
00:04:54,720 --> 00:04:57,000
固定しておく。

141
00:04:58,580 --> 00:04:59,640
で、最初の割り振りのステップで

142
00:04:59,900 --> 00:05:00,990
何をやるかというと、そのステップでは

143
00:05:01,240 --> 00:05:02,850
クラスタ重心は変更しない、その代わりに

144
00:05:02,960 --> 00:05:05,730
コスト関数、またはディストーション関数であるJを

145
00:05:07,790 --> 00:05:10,240
最小化するc1, c2,...cmの値を

146
00:05:10,500 --> 00:05:11,790
選ぶ、という事をする。

147
00:05:12,510 --> 00:05:14,440
そして数学的にも

148
00:05:14,670 --> 00:05:16,550
やろうと思えば証明出来るが、ここではやらんけど、

149
00:05:17,170 --> 00:05:18,210
直感的にも自然だと思うけど

150
00:05:18,610 --> 00:05:19,630
これらの点に対し

151
00:05:20,090 --> 00:05:21,040
もっとも近いクラスタ重心を

152
00:05:21,530 --> 00:05:22,820
割り振っていく。というのはそれが

153
00:05:23,120 --> 00:05:24,160
点と対応するクラスタ重心の間の二乗距離の和を

154
00:05:24,660 --> 00:05:26,860
最小化する割り振り方だから。

155
00:05:27,840 --> 00:05:29,090
そして残りのK-meansの部分、

156
00:05:29,790 --> 00:05:32,880
2番目のステップは、この二番目のステップとなるが、

157
00:05:33,960 --> 00:05:35,480
この二番目のステップは重心を移動する

158
00:05:35,690 --> 00:05:38,770
ステップで、

159
00:05:39,000 --> 00:05:40,020
ここでも、証明はしないが、

160
00:05:40,510 --> 00:05:41,250
数学的にも証明出来る事として、

161
00:05:41,520 --> 00:05:42,590
重心移動のステップでは、

162
00:05:43,140 --> 00:05:44,910
Jを最小化する

163
00:05:45,150 --> 00:05:46,740
ミューを

164
00:05:47,260 --> 00:05:49,370
選ぶ。

165
00:05:50,150 --> 00:05:51,270
つまりコスト関数Jを

166
00:05:51,650 --> 00:05:53,000
以下の観点から最小化する、

167
00:05:53,380 --> 00:05:54,710
ここでwrtはwith respect to（以下の観点から）の

168
00:05:54,920 --> 00:05:56,930
省略だ。

169
00:05:57,030 --> 00:05:58,380
で、Jをミュー1からミューKまでの

170
00:05:58,790 --> 00:06:01,930
位置に関して、最小化する。

171
00:06:02,040 --> 00:06:05,690
つまり、K-meansが実際にやっているのは、

172
00:06:05,790 --> 00:06:06,910
二つの種類の変数群を

173
00:06:07,010 --> 00:06:08,380
とり、そして

174
00:06:09,070 --> 00:06:11,210
それら二つを二つに分けて、

175
00:06:11,550 --> 00:06:14,490
まずcの変数群、次にミューの変数群として、

176
00:06:15,450 --> 00:06:15,990
そしてやるのは、まず、

177
00:06:16,560 --> 00:06:17,750
Jを変数cに関して

178
00:06:18,050 --> 00:06:19,350
最小化する、

179
00:06:19,700 --> 00:06:20,610
変数ミューに関して最小化する、

180
00:06:21,120 --> 00:06:22,590
そしてそれを繰り返し続ける。

181
00:06:25,180 --> 00:06:26,680
以上がK-meansのやる事の全てだ。

182
00:06:27,700 --> 00:06:28,570
そして今やK-meansを理解したので、

183
00:06:29,150 --> 00:06:30,870
コスト関数Jを

184
00:06:31,030 --> 00:06:32,190
最小化しよう。

185
00:06:32,430 --> 00:06:33,640
これを用いて、また我らは

186
00:06:33,800 --> 00:06:34,890
学習アルゴリズムのデバッグを試みる事も

187
00:06:35,090 --> 00:06:36,350
可能だ。また我らの

188
00:06:36,520 --> 00:06:37,980
K-meansの実装が正しく走っているかを

189
00:06:38,900 --> 00:06:39,950
確認するのにも使える。

190
00:06:41,220 --> 00:06:42,560
つまり、いまや我らは

191
00:06:43,070 --> 00:06:44,260
K-meansアルゴリズムを、コスト関数Jを

192
00:06:44,610 --> 00:06:45,960
最小化する物として理解した。

193
00:06:46,640 --> 00:06:48,790
コスト関数はディストーション関数とも呼ばれるんだった。

194
00:06:50,650 --> 00:06:51,600
その事実を用いて、K-meansをデバッグしたり、

195
00:06:52,090 --> 00:06:53,060
K-meansが収束しているのを見たり出来る。

196
00:06:53,130 --> 00:06:54,050
そしてそれが正しく

197
00:06:54,510 --> 00:06:56,160
実行されているかも。

198
00:06:56,240 --> 00:06:57,460
次のビデオでは、

199
00:06:57,690 --> 00:06:59,040
どうこれを用いて

200
00:06:59,120 --> 00:07:00,650
K-meansがより良いクラスタを見つける助けと出来るか、

201
00:07:01,300 --> 00:07:03,240
そしてどうK-meansが局所最適を避ける助けと出来るかを話していく。