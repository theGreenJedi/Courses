1
00:00:00,170 --> 00:00:01,340
このビデオでは、

2
00:00:01,450 --> 00:00:03,230
K-meansをどう初期化するのかと、

3
00:00:04,580 --> 00:00:05,970
その延長としてより重要な、

4
00:00:06,170 --> 00:00:07,240
K-meansでどう局所最適の問題を

5
00:00:07,550 --> 00:00:10,210
回避するのかについての2つを話す。

6
00:00:10,740 --> 00:00:12,390
ここに、以前に話した

7
00:00:12,950 --> 00:00:14,420
K-meansのクラスタリングアルゴリズムがある。

8
00:00:15,760 --> 00:00:16,760
ここまでちゃんと話してなかった

9
00:00:17,260 --> 00:00:18,350
ステップの一つに

10
00:00:18,820 --> 00:00:21,560
クラスタ重心を、どうランダムに初期化するか、というのがある。

11
00:00:22,390 --> 00:00:23,490
ランダムにクラスタ重心を初期化する、と

12
00:00:23,710 --> 00:00:25,350
言われた時には、

13
00:00:25,960 --> 00:00:26,860
いくつかの方法が考えられるが、

14
00:00:27,510 --> 00:00:28,580
その中の一つが、

15
00:00:28,720 --> 00:00:29,820
それ以外の思いつく選択肢の多くよりも

16
00:00:30,050 --> 00:00:31,700
優れている、という

17
00:00:32,080 --> 00:00:33,830
ものがある。

18
00:00:34,400 --> 00:00:35,250
だからその方法について議論しようと思う、

19
00:00:35,590 --> 00:00:38,160
何故ならそれがしばしば一番良く機能する選択肢だからだ。

20
00:00:39,550 --> 00:00:42,210
これが私が普段クラスタ重心を初期化するやり方だ。

21
00:00:43,300 --> 00:00:44,710
K-meansを実行する時には

22
00:00:45,140 --> 00:00:47,160
クラスタ重心の数、Kを

23
00:00:47,430 --> 00:00:48,520
トレーニング手本の数mよりも

24
00:00:48,590 --> 00:00:50,090
小さい数に設定しなくてはならない。

25
00:00:50,170 --> 00:00:51,210
K-meansを、手本の数と同じか

26
00:00:51,430 --> 00:00:52,600
それより多い数のクラスタ重心に対して

27
00:00:52,870 --> 00:00:54,270
実行する、というのは

28
00:00:54,520 --> 00:00:55,790
ヘンテコなことだってのは分かるでしょ？

29
00:00:58,080 --> 00:00:59,010
で、私が普段

30
00:00:59,150 --> 00:01:00,510
K-meansを初期化するのに使う方法は

31
00:01:00,770 --> 00:01:02,510
K個のトレーニング手本を

32
00:01:02,990 --> 00:01:05,170
ランダムに取り出す。

33
00:01:05,610 --> 00:01:06,730
そして次にやるのは、

34
00:01:06,850 --> 00:01:09,320
ミュー1からミューKに、これらをセットする、という事。

35
00:01:10,610 --> 00:01:11,470
具体例を見てみよう。

36
00:01:12,560 --> 00:01:14,190
Kをイコール2だと

37
00:01:14,470 --> 00:01:16,600
してみよう。つまり

38
00:01:17,070 --> 00:01:19,520
この右側の手本に対し、2つのクラスタを見つけたい、とする。

39
00:01:21,170 --> 00:01:22,060
その場合、クラスタ重心を

40
00:01:22,200 --> 00:01:23,350
初期化する為に

41
00:01:23,770 --> 00:01:25,340
私がやるのは

42
00:01:25,470 --> 00:01:27,320
2つの手本をランダムに選ぶ。

43
00:01:27,760 --> 00:01:28,960
そして、例えば

44
00:01:29,230 --> 00:01:31,060
これとそれを選んだとしよう。

45
00:01:31,230 --> 00:01:32,320
その場合私が

46
00:01:32,380 --> 00:01:34,100
クラスタ重心を初期化するやり方は

47
00:01:34,310 --> 00:01:35,190
それらの手本の真上に

48
00:01:36,200 --> 00:01:38,930
クラスタ重心を置く、という方法。

49
00:01:39,530 --> 00:01:40,430
これが最初のクラスタ重心となり、

50
00:01:41,410 --> 00:01:43,230
これが二番目のクラスタ重心となる。

51
00:01:43,390 --> 00:01:45,770
これがK-meansをランダムに初期化する方法だ。

52
00:01:48,540 --> 00:01:50,480
ここに描いたのは極めて良い物っぽいけど、

53
00:01:50,890 --> 00:01:51,810
たまにはもっとついてなくて、

54
00:01:52,040 --> 00:01:53,370
結局最初の奴として

55
00:01:53,510 --> 00:01:54,900
こんなのをランダムな最初の手本として、

56
00:01:55,330 --> 00:01:58,420
そして二番目としてこんなのを選ぶはめになるかもしれない。

57
00:01:59,050 --> 00:02:01,380
ここで2つの手本を選んだのはK=2としたから。

58
00:02:01,590 --> 00:02:03,590
適当に2つのトレーニング手本を

59
00:02:03,890 --> 00:02:05,030
ランダムに2つ選んだ、

60
00:02:05,100 --> 00:02:06,660
それをこれら2つとすると、

61
00:02:06,830 --> 00:02:08,040
その時は

62
00:02:08,250 --> 00:02:09,200
これを最初のクラスタ重心に

63
00:02:09,510 --> 00:02:10,980
これを2つ目のクラスタ重心の

64
00:02:11,140 --> 00:02:13,560
初期位置とする。

65
00:02:14,150 --> 00:02:15,690
以上がランダムに

66
00:02:16,070 --> 00:02:17,560
クラスタ重心を初期化するやり方だ。

67
00:02:17,810 --> 00:02:19,670
つまり初期化の時には

68
00:02:19,860 --> 00:02:21,110
クラスタ重心ミュー1は

69
00:02:21,270 --> 00:02:23,350
適当に選んだ値iの元に

70
00:02:23,520 --> 00:02:25,870
x(i)と等しくなる。

71
00:02:26,980 --> 00:02:27,660
ミュー2は、iとは別の異なるランダムに選ばれた数jを使って

72
00:02:29,240 --> 00:02:30,980
x(j)と表せる物と等しくなる、

73
00:02:31,380 --> 00:02:32,830
などなどと、

74
00:02:32,910 --> 00:02:34,440
もっと多くのクラスタとクラスタ重心があるなら続けていく。

75
00:02:35,680 --> 00:02:37,540
ここでちょっと補足しておく。

76
00:02:38,110 --> 00:02:39,240
以前の動画で

77
00:02:39,320 --> 00:02:40,840
最初にK-meansをアニメーションで

78
00:02:41,150 --> 00:02:43,030
例示した時の話だ。

79
00:02:44,310 --> 00:02:45,070
そこでのスライドは、

80
00:02:45,900 --> 00:02:46,890
例示の為だけの目的の物だった。

81
00:02:47,590 --> 00:02:48,690
私は実際には、そこで書いていたのとは

82
00:02:49,240 --> 00:02:51,750
違うクラスタ重心の初期化の仕方をする。

83
00:02:52,460 --> 00:02:53,790
このスライドで説明している方法こそが

84
00:02:53,900 --> 00:02:55,940
本当に推奨する方法だ。

85
00:02:56,430 --> 00:02:58,850
そして恐らく、あなたがK-meansを実装する時にも使うべき方法と言える。

86
00:03:00,090 --> 00:03:01,560
さて、この右側の二つの図で

87
00:03:02,070 --> 00:03:04,090
例示出来ているかもしれない事として、

88
00:03:04,930 --> 00:03:06,050
K-meansは実際に

89
00:03:06,530 --> 00:03:08,130
どう初期化されるかに応じて

90
00:03:08,260 --> 00:03:10,150
つまりどうランダム初期化されるかに応じて

91
00:03:10,860 --> 00:03:12,470
異なる解に収束しうるという事が

92
00:03:12,990 --> 00:03:15,170
想像出来るかもしれない。

93
00:03:16,280 --> 00:03:18,180
K-meansは異なる解に収束しうる。

94
00:03:18,930 --> 00:03:22,560
そして実際、K-meansは異なる局所最適解に落ち着きうる。

95
00:03:23,650 --> 00:03:24,920
もしこんなデータセットを与えられたとすると

96
00:03:25,400 --> 00:03:26,370
うーん、見た所、ここには

97
00:03:26,660 --> 00:03:28,340
三つのクラスタがあるように見える。

98
00:03:28,780 --> 00:03:30,090
だからもしK-meansを実行したら

99
00:03:30,150 --> 00:03:31,380
もし良い局所最適解に

100
00:03:31,820 --> 00:03:32,910
落ち着けば、これが極めて良い局所最適解に見える、

101
00:03:33,040 --> 00:03:35,830
その時には、このクラスタの輪となるだろう。

102
00:03:36,820 --> 00:03:38,440
だがあなたが特にアンラッキーで

103
00:03:39,110 --> 00:03:41,630
ランダム初期化によっては、K-meansは

104
00:03:42,100 --> 00:03:43,660
異なる局所最適に

105
00:03:44,180 --> 00:03:45,740
スタックしてしまうかもしれない。

106
00:03:45,850 --> 00:03:47,330
だから、この左側の例では、

107
00:03:47,620 --> 00:03:48,700
青いクラスタがたくさんの点を

108
00:03:49,470 --> 00:03:51,700
捕捉してしまい、緑と赤のクラスタは

109
00:03:52,050 --> 00:03:54,810
それぞれ相対的にはちょっとの点しか捕捉出来ていない。

110
00:03:55,020 --> 00:03:56,480
つまりこれは、悪い局所最適解に

111
00:03:56,640 --> 00:03:58,470
対応している。

112
00:03:58,530 --> 00:04:00,060
何故ならそれは、これら二つのクラスタを

113
00:04:00,470 --> 00:04:01,560
一個に取り出してしまっていて、

114
00:04:01,780 --> 00:04:03,440
さらに、二番目のクラスタを

115
00:04:04,150 --> 00:04:06,070
二つのサブクラスタに分割

116
00:04:06,580 --> 00:04:09,170
してしまっている。

117
00:04:09,380 --> 00:04:10,270
二番目のクラスタを

118
00:04:10,720 --> 00:04:12,280
二つの別々のサブクラスタに

119
00:04:12,540 --> 00:04:14,220
分割して

120
00:04:14,460 --> 00:04:16,630
しまっている。

121
00:04:16,760 --> 00:04:17,880
つまり、これら右下の例は

122
00:04:18,000 --> 00:04:18,970
どちらも異なる

123
00:04:19,220 --> 00:04:20,890
局所最適に

124
00:04:21,250 --> 00:04:22,440
対応した例で、実際、

125
00:04:22,890 --> 00:04:24,440
この例では

126
00:04:25,070 --> 00:04:26,150
このクラスタ、赤のクラスタは

127
00:04:26,550 --> 00:04:27,870
たった一つぼっちの手本に捕捉されてしまっている。

128
00:04:28,380 --> 00:04:29,810
ところで局所最適という

129
00:04:30,200 --> 00:04:31,000
用語は、この

130
00:04:31,490 --> 00:04:32,930
ディストーション関数Jの

131
00:04:33,190 --> 00:04:35,940
局所最適を示している、

132
00:04:36,320 --> 00:04:38,380
そしてこれらの左下の

133
00:04:38,590 --> 00:04:39,830
解は、これらの

134
00:04:40,120 --> 00:04:41,420
局所最適な解は

135
00:04:41,530 --> 00:04:42,880
K-meansが局所最適に

136
00:04:43,330 --> 00:04:44,050
スタックしてしまい、

137
00:04:44,600 --> 00:04:45,940
このディストーション関数Jを

138
00:04:46,170 --> 00:04:47,940
最小化するのに

139
00:04:48,110 --> 00:04:50,030
あんまり良い仕事はしていない事に対応する。

140
00:04:50,540 --> 00:04:52,250
だから、もしK-meansが局所最適に

141
00:04:52,540 --> 00:04:53,810
スタックしてしまいそうと心配だったら、

142
00:04:53,970 --> 00:04:55,110
もしK-meansが可能な中で

143
00:04:55,330 --> 00:04:56,950
ベストなクラスタリングを見つけるオッズを

144
00:04:57,230 --> 00:04:58,480
高めたければ、この上に見せたように、

145
00:04:58,730 --> 00:05:00,290
我らに取れる手段としては

146
00:05:00,350 --> 00:05:02,820
複数のランダム初期化を試みる事だ。

147
00:05:03,580 --> 00:05:04,820
つまりK-meansを一回だけ初期化して

148
00:05:05,430 --> 00:05:06,460
うまく行く事を祈る代わりに

149
00:05:06,670 --> 00:05:07,680
やれる事として、

150
00:05:08,040 --> 00:05:10,020
K-meansをたくさん初期化して

151
00:05:10,130 --> 00:05:10,990
そしてK-meansをたくさん実行する事だ。

152
00:05:11,890 --> 00:05:12,870
そしてそれを用いて、

153
00:05:12,950 --> 00:05:13,840
可能な限り一番良い局所最適、

154
00:05:14,110 --> 00:05:15,640
つまりはグローバル最適を

155
00:05:15,800 --> 00:05:18,380
得ている事を確認するのだ。

156
00:05:19,480 --> 00:05:22,460
具体的には、これが、そのやり方だ。

157
00:05:22,720 --> 00:05:23,500
K-meansを100回

158
00:05:23,700 --> 00:05:24,800
走らせる事に決めたとしよう。

159
00:05:25,160 --> 00:05:26,790
つまりこのループを

160
00:05:27,060 --> 00:05:28,900
100回実行する。そしてそれは

161
00:05:29,330 --> 00:05:30,830
かなりありがちな数で、

162
00:05:30,920 --> 00:05:31,910
だいたい50から1000の間の

163
00:05:32,160 --> 00:05:33,670
あたりの回数だと思う。

164
00:05:35,090 --> 00:05:36,730
さて、K-meansを100回実行しよう、と決めたとしよう。

165
00:05:38,220 --> 00:05:39,100
それはどういう事かというと、

166
00:05:39,170 --> 00:05:41,490
K-meansをランダム初期化するという事で

167
00:05:42,350 --> 00:05:43,250
そしてこれらの100回の

168
00:05:43,340 --> 00:05:44,710
ランダム初期化の時に、毎回

169
00:05:45,370 --> 00:05:47,040
K-meansを走らせる。

170
00:05:47,220 --> 00:05:48,200
するとクラスタとクラスタ重心の

171
00:05:48,430 --> 00:05:50,270
集合が得られて、

172
00:05:50,590 --> 00:05:51,940
そしてそれを用いて

173
00:05:52,040 --> 00:05:53,760
ディストーション関数Jを計算する。

174
00:05:54,500 --> 00:05:55,600
つまりこの得られたクラスタ割り振りと

175
00:05:56,910 --> 00:05:58,260
クラスタ重心の集合に対して、

176
00:05:58,720 --> 00:05:59,910
コスト関数をそれぞれ計算していく。

177
00:06:01,000 --> 00:06:03,470
最後に、これらのプロセス全てを100回行ったら、

178
00:06:04,450 --> 00:06:06,330
100個の異なる、

179
00:06:06,710 --> 00:06:08,990
データをクラスタリングする方法が得られる事になる。

180
00:06:09,240 --> 00:06:10,310
そこから最後に行うのは、

181
00:06:10,590 --> 00:06:11,510
これら全ての100通りの

182
00:06:11,820 --> 00:06:13,210
見つけたクラスタリングのデータから、

183
00:06:13,800 --> 00:06:16,050
単純に一番コストが低い物を選ぶだけ。

184
00:06:16,400 --> 00:06:18,480
それが一番ディストーションが小さい。

185
00:06:18,960 --> 00:06:20,610
そして以下のような事も分かるだろう。

186
00:06:21,170 --> 00:06:22,490
もしかなり少ない数の

187
00:06:22,670 --> 00:06:24,520
クラスター数に対してK-meansを走らせれば、

188
00:06:24,630 --> 00:06:25,260
つまり、そうだなぁ、

189
00:06:25,520 --> 00:06:26,700
クラスタの数がだいたい2から10とか

190
00:06:26,760 --> 00:06:28,180
その位の範囲なら、

191
00:06:28,980 --> 00:06:30,650
複数回のランダム初期化を行う事は

192
00:06:31,460 --> 00:06:32,880
かなりしばしば

193
00:06:32,990 --> 00:06:34,430
より良い局所最適を見つけた事を確認出来る、

194
00:06:34,690 --> 00:06:37,680
より良いクラスタリングのデータを見つけた事を。

195
00:06:37,870 --> 00:06:38,930
でもKがとても大きい時は、

196
00:06:39,080 --> 00:06:40,000
Kが10よりもずっと大きく、

197
00:06:40,160 --> 00:06:41,010
そうだなぁ、だいたい

198
00:06:41,080 --> 00:06:42,340
もしあなたが何百もの

199
00:06:42,400 --> 00:06:44,050
クラスタを見つけようとしている時には、

200
00:06:45,840 --> 00:06:47,310
その時には複数回のランダム初期化を行っても、

201
00:06:47,940 --> 00:06:49,220
そんなに大きな違いは無いだろう。

202
00:06:49,360 --> 00:06:50,400
そして最初のランダム初期化で

203
00:06:50,590 --> 00:06:51,910
すでにかなり良い

204
00:06:52,320 --> 00:06:53,610
解が得られている

205
00:06:53,730 --> 00:06:55,380
公算が高い。

206
00:06:56,590 --> 00:06:58,070
そして複数回、ランダム初期化を

207
00:06:58,680 --> 00:07:00,060
行ったら、たぶん、

208
00:07:00,260 --> 00:07:02,500
ちょっとはマシな解は得られるだろうが、でもそんなには変わらない。

209
00:07:02,780 --> 00:07:04,230
だが、相対的に

210
00:07:04,540 --> 00:07:05,810
小さな数のクラスタ数の

211
00:07:06,090 --> 00:07:07,740
範囲に居る時は、

212
00:07:08,040 --> 00:07:09,080
とくに2とか3とか4個の

213
00:07:09,150 --> 00:07:10,550
クラスタの時には、

214
00:07:11,140 --> 00:07:13,790
複数回ランダム初期化は、

215
00:07:14,190 --> 00:07:15,090
ディストーション関数をちゃんと最小化し、

216
00:07:15,170 --> 00:07:16,920
ひいては良いクラスタリングを与えてくれている、と

217
00:07:17,560 --> 00:07:18,730
確認するのに、やるとやらないとでは大違いとなる。

218
00:07:21,390 --> 00:07:22,560
さて、以上がK-meansの

219
00:07:22,640 --> 00:07:23,300
ランダム初期化だ。

220
00:07:24,350 --> 00:07:25,570
もし相対的に小さな数の

221
00:07:25,710 --> 00:07:26,950
クラスタ、たとえば

222
00:07:27,310 --> 00:07:28,250
2とか3とか4とか5とか、

223
00:07:28,400 --> 00:07:30,540
うーん、6とか7でも、それらの数のクラスタで学習させたい時には

224
00:07:31,660 --> 00:07:34,040
複数回ランダム初期化を用いる事で、

225
00:07:34,380 --> 00:07:36,830
より良いデータのクラスタリングが得られる助けとなる事がある。

226
00:07:37,680 --> 00:07:39,650
だが、大きな数のクラスタの時だって、

227
00:07:40,350 --> 00:07:43,280
ここで説明したランダム初期化の方法は

228
00:07:43,520 --> 00:07:45,110
K-meansに良いクラスタを探させる

229
00:07:45,370 --> 00:07:46,680
リーズナブルなスタート地点と

230
00:07:47,030 --> 00:07:48,580
なるだろう。