1
00:00:00,300 --> 00:00:02,220
クラスタリングの問題では、

2
00:00:02,360 --> 00:00:03,630
ラベル付けされていないデータセットが渡されて、

3
00:00:03,950 --> 00:00:05,040
アルゴリズムに自動で

4
00:00:05,200 --> 00:00:06,480
互いに密接なサブセット、または互いに密接なクラスタに

5
00:00:07,320 --> 00:00:08,700
グループ分け

6
00:00:09,340 --> 00:00:11,000
して欲しい。

7
00:00:12,380 --> 00:00:14,160
K-Means アルゴリズムは

8
00:00:14,310 --> 00:00:15,860
ずば抜けて人気のある、

9
00:00:16,090 --> 00:00:17,410
ずば抜けて広く使われているクラスタリングアルゴリズムだ。

10
00:00:17,780 --> 00:00:19,380
このビデオでは、

11
00:00:19,550 --> 00:00:20,320
K-Meansアルゴリズムとは何か、

12
00:00:20,570 --> 00:00:23,400
それがどう機能するかを話していきたい。

13
00:00:27,000 --> 00:00:29,310
K-Meansクラスタリングアルゴリズムは絵で表すのが一番。

14
00:00:29,960 --> 00:00:30,770
ここに見せたような

15
00:00:31,080 --> 00:00:32,330
ラベルの無いデータセットが

16
00:00:32,490 --> 00:00:34,040
あるとしよう。

17
00:00:34,100 --> 00:00:36,450
そしてこのデータを2つのクラスタにグループ分けしたい。

18
00:00:37,710 --> 00:00:38,740
K-Meansアルゴリズムを実行するとしたら

19
00:00:39,080 --> 00:00:41,560
これがやるべきことだ。

20
00:00:41,910 --> 00:00:44,190
最初の一歩はランダムに2つの点を選ぶ、

21
00:00:44,410 --> 00:00:45,920
これはクラスタの重心と呼ばれる。

22
00:00:46,700 --> 00:00:48,170
そう、これら2つのバッテンが、

23
00:00:49,010 --> 00:00:51,730
クラスタ重心と呼ばれる物だ。

24
00:00:53,270 --> 00:00:54,320
そしてそれが2つなのは、

25
00:00:55,100 --> 00:00:57,840
データを2つのクラスタにグループ分けしたいから。

26
00:00:59,130 --> 00:01:02,400
K-Meansはイテレーティブなアルゴリズムで、2つの事をする。

27
00:01:03,480 --> 00:01:04,790
最初はクラスタの割り付けステップ。

28
00:01:05,330 --> 00:01:07,800
二番目は重心移動ステップ。

29
00:01:08,360 --> 00:01:09,630
それらが何を意味するか解説していこう。

30
00:01:11,170 --> 00:01:12,520
K-Meansのループの中の2つのステップの内、

31
00:01:12,700 --> 00:01:14,930
最初の方は、クラスタ割り付けのステップだ。

32
00:01:15,840 --> 00:01:17,070
その意味する所は

33
00:01:17,220 --> 00:01:18,360
各手本を見ていって、

34
00:01:18,700 --> 00:01:19,880
ここで示したのだと、この緑のドットを

35
00:01:20,170 --> 00:01:22,120
見ていき、

36
00:01:22,580 --> 00:01:24,140
赤のクラスタ重心と青のクラスタ重心の

37
00:01:24,350 --> 00:01:25,530
どちらと近いかによって、

38
00:01:25,620 --> 00:01:27,390
各データポイントを

39
00:01:27,560 --> 00:01:28,570
2つのクラスタのうちのどちらかに

40
00:01:28,670 --> 00:01:30,670
割り振る。

41
00:01:32,040 --> 00:01:33,350
具体的にその意味する所を見ると、

42
00:01:33,460 --> 00:01:34,610
データセットを見ていって、

43
00:01:34,730 --> 00:01:36,930
各点を赤か青に

44
00:01:37,130 --> 00:01:38,510
色付けしていく、

45
00:01:38,810 --> 00:01:39,890
赤のクラスタ重心に近いか

46
00:01:40,160 --> 00:01:41,060
青のクラスタ重心に近いかに

47
00:01:41,170 --> 00:01:42,150
よって。

48
00:01:42,470 --> 00:01:45,210
この図ではそれを実際にやってみた。

49
00:01:46,930 --> 00:01:48,700
以上がクラスタ割り付けステップ。

50
00:01:49,780 --> 00:01:52,270
K-Meansのループ内を

51
00:01:52,410 --> 00:01:53,390
構成するもう一方は

52
00:01:53,590 --> 00:01:54,860
重心の移動ステップだ。

53
00:01:55,020 --> 00:01:55,730
我らがやるべきことは、

54
00:01:55,800 --> 00:01:56,890
2つのクラスタの重心を、

55
00:01:57,390 --> 00:01:58,550
つまり、赤のバッテンと

56
00:01:58,830 --> 00:02:00,270
青のバッテンを、

57
00:02:00,420 --> 00:02:01,420
同じ色で色付けされた点の平均へと

58
00:02:02,070 --> 00:02:03,900
移動する。

59
00:02:04,880 --> 00:02:05,700
つまり我らがやる事は、

60
00:02:05,730 --> 00:02:06,510
全ての赤の点を見て、

61
00:02:06,630 --> 00:02:07,810
平均を計算し、

62
00:02:08,240 --> 00:02:09,520
それは真に全ての赤い点の

63
00:02:10,080 --> 00:02:11,500
平均だが、

64
00:02:11,650 --> 00:02:13,690
赤のクラスタの重心をそこへ移動する。

65
00:02:14,190 --> 00:02:15,260
同じ事を青のクラスタ重心にも

66
00:02:15,460 --> 00:02:16,370
行う。青い点を

67
00:02:16,560 --> 00:02:17,720
全て見て、平均を計算し、

68
00:02:17,840 --> 00:02:19,710
青のクラスタ重心をそこへ移動する。

69
00:02:20,320 --> 00:02:20,880
ではやってみよう。

70
00:02:21,170 --> 00:02:22,990
クラスタ重心を以下のように動かし、

71
00:02:24,590 --> 00:02:27,350
それらは新しい平均へと移動した。

72
00:02:28,300 --> 00:02:29,760
赤いのはこんな感じで動き、

73
00:02:29,820 --> 00:02:31,350
青いのはこんな感じで動いた。

74
00:02:31,510 --> 00:02:34,460
そして赤いのはこんな感じで動いた。

75
00:02:34,620 --> 00:02:35,460
そして次にあらたなクラスタ割り当てステップに戻る。

76
00:02:35,910 --> 00:02:36,920
つまりまたラベルづけされていない

77
00:02:37,190 --> 00:02:38,090
手本を全て見ていって、

78
00:02:38,160 --> 00:02:39,670
青と赤のどちらの重心に

79
00:02:40,090 --> 00:02:42,840
近いかによって、

80
00:02:43,340 --> 00:02:45,150
それらを赤か青に色付けする。

81
00:02:45,640 --> 00:02:47,160
各点に2つのクラスタ重心のどちらかを

82
00:02:47,530 --> 00:02:48,550
割り振る、という事なので、やってみよう。

83
00:02:51,450 --> 00:02:52,260
幾つかの点の色は変わった。

84
00:02:53,400 --> 00:02:55,690
そしてまた、重心移動のステップに進む。

85
00:02:56,040 --> 00:02:56,810
つまり全ての青の点の

86
00:02:57,070 --> 00:02:57,880
平均を計算し、

87
00:02:58,110 --> 00:02:59,000
全ての赤の点の

88
00:02:59,040 --> 00:03:00,360
平均を計算し、

89
00:03:00,480 --> 00:03:03,770
クラスタ重心をこんな感じで移動する。

90
00:03:03,930 --> 00:03:05,650
やってみよう。

91
00:03:06,160 --> 00:03:07,810
クラスタ割り振りステップをもう一回やってみよう。

92
00:03:08,320 --> 00:03:09,450
各点を赤か青に色分けする、

93
00:03:09,620 --> 00:03:10,840
とちらに近いかに基づいて

94
00:03:11,170 --> 00:03:13,070
そして次に

95
00:03:13,310 --> 00:03:20,000
重心移動のステップを行う。行った。

96
00:03:20,350 --> 00:03:21,230
そして実の所、

97
00:03:21,290 --> 00:03:23,250
ここからさらにK-Meansのイテレーションを

98
00:03:23,500 --> 00:03:26,020
走らせ続けても、

99
00:03:26,160 --> 00:03:27,240
クラスタ重心はこれ以上

100
00:03:27,540 --> 00:03:28,770
移動しない。そして点の色も

101
00:03:28,830 --> 00:03:29,760
これ以上は変わらない。

102
00:03:29,940 --> 00:03:31,520
つまり、これが

103
00:03:31,810 --> 00:03:33,520
こここそが、K-Meansが

104
00:03:33,770 --> 00:03:35,290
収束する点だ。

105
00:03:35,400 --> 00:03:36,430
そしてそれは、このデータの2つのクラスタを

106
00:03:37,470 --> 00:03:38,750
探すには、かなり良い仕事をしている。

107
00:03:39,360 --> 00:03:40,310
ではK-Meansのアルゴリズムをよりフォーマルに記述しよう。

108
00:03:42,150 --> 00:03:43,930
K -Meansアルゴリズムは2つの入力を取る。

109
00:03:44,570 --> 00:03:46,200
一つ目はパラメータK、

110
00:03:46,450 --> 00:03:47,260
それはデータの中から見つけたい

111
00:03:47,830 --> 00:03:48,900
クラスタの数。

112
00:03:49,640 --> 00:03:50,820
あとでどうやってこのKを

113
00:03:51,170 --> 00:03:53,290
どうやって選んだらいいかの話をするつもりだが、

114
00:03:53,470 --> 00:03:54,600
今のところはあるクラスタの数を

115
00:03:55,110 --> 00:03:56,210
既に決めた、と

116
00:03:56,360 --> 00:03:57,600
しておこう。

117
00:03:57,690 --> 00:03:58,810
そして幾つのクラスタがデータセットにあるかを

118
00:03:59,040 --> 00:04:00,730
アルゴリズムに伝えるとする。

119
00:04:01,170 --> 00:04:02,120
そしてK-Meansはまた、

120
00:04:02,490 --> 00:04:03,430
このようなラベルの無いトレーニングセットを

121
00:04:03,880 --> 00:04:05,060
入力にとる。

122
00:04:05,250 --> 00:04:06,530
単なるxだけ。

123
00:04:06,710 --> 00:04:08,430
これは教師なし学習だから、

124
00:04:08,520 --> 00:04:10,690
もうラベルyは無い。

125
00:04:10,980 --> 00:04:12,470
そしてK-Meansの教師なし学習に対しては

126
00:04:12,740 --> 00:04:14,020
xiをRnのベクトルを

127
00:04:14,550 --> 00:04:16,160
表すというコンベンションを

128
00:04:16,420 --> 00:04:17,750
用いることにする。

129
00:04:18,280 --> 00:04:19,190
そんな訳でトレーニング手本はいまや、

130
00:04:19,750 --> 00:04:22,460
n+1次元では無くn次元のベクトルとなる。

131
00:04:24,340 --> 00:04:25,430
これがK-Meansアルゴリズムがやる事だ:

132
00:04:27,180 --> 00:04:28,630
最初のステップは

133
00:04:28,790 --> 00:04:31,170
ランダムにK個の重心を選ぶ、

134
00:04:31,570 --> 00:04:33,550
それをミュー1、ミュー2、、、

135
00:04:33,820 --> 00:04:34,610
ミューkと名付ける

136
00:04:34,840 --> 00:04:36,250
つまり、

137
00:04:36,650 --> 00:04:38,450
前の図だと、

138
00:04:38,550 --> 00:04:40,770
クラスタ重心は

139
00:04:41,060 --> 00:04:42,240
赤のバッテンと

140
00:04:42,660 --> 00:04:44,020
青のバッテンの場所に対応してた。

141
00:04:44,410 --> 00:04:45,640
つまりそれは、2つのクラスタ重心があったという事で、

142
00:04:45,960 --> 00:04:47,000
たとえば赤のバッテンが

143
00:04:47,170 --> 00:04:48,470
ミュー1で、

144
00:04:48,650 --> 00:04:49,940
青のバッテンがミュー2だった、という事。

145
00:04:50,300 --> 00:04:51,360
そして今度はより一般的に

146
00:04:51,820 --> 00:04:53,830
2つだけじゃなくて、K個のクラスタ重心を考えていく。

147
00:04:54,520 --> 00:04:56,240
するとK-Meansの内側のループは

148
00:04:56,520 --> 00:04:57,360
以下を実行する、

149
00:04:57,830 --> 00:04:59,020
つまり以下を繰り返し実行する事になる:

150
00:05:00,070 --> 00:05:01,950
まず、各トレーニング手本に対して

151
00:05:02,160 --> 00:05:03,920
この変数c(i)を

152
00:05:04,110 --> 00:05:05,950
xiに一番近いクラスタ重心のインデックスを

153
00:05:06,790 --> 00:05:07,960
セットする。

154
00:05:08,170 --> 00:05:10,520
値は重心のインデックスである、1からKまでの値。

155
00:05:11,170 --> 00:05:13,810
つまりこれは、クラスタ割り付けステップにあたる。

156
00:05:14,330 --> 00:05:16,870
そこでは各サンプルに対して

157
00:05:17,000 --> 00:05:18,680
それを赤か青か、

158
00:05:18,980 --> 00:05:20,740
そのどちらの重心に近いかに基づいて

159
00:05:21,050 --> 00:05:22,050
色分けしていく。

160
00:05:22,380 --> 00:05:23,940
色分けしていく。

161
00:05:24,140 --> 00:05:25,090
つまりc(i)は

162
00:05:25,280 --> 00:05:26,280
1からKまでの数で、

163
00:05:26,380 --> 00:05:27,680
その値は我らに

164
00:05:27,780 --> 00:05:28,760
それが赤のバッテンか

165
00:05:28,920 --> 00:05:29,820
青のバッテンか、

166
00:05:29,900 --> 00:05:31,170
どちらに近いかを教えてくれる。

167
00:05:32,200 --> 00:05:33,210
これの他の書き方としては、

168
00:05:33,580 --> 00:05:35,350
ciを計算するのに、、、

169
00:05:35,620 --> 00:05:37,820
i番目の手本

170
00:05:37,890 --> 00:05:39,120
xiを取ってきて

171
00:05:39,380 --> 00:05:41,170
それと個々の

172
00:05:41,360 --> 00:05:42,670
クラスタ重心との

173
00:05:43,900 --> 00:05:44,860
距離を測る。

174
00:05:45,410 --> 00:05:46,690
これがミューk、

175
00:05:47,060 --> 00:05:48,640
小文字のk。

176
00:05:48,890 --> 00:05:50,630
大文字のKを重心の

177
00:05:50,910 --> 00:05:51,900
総数を表すのに使い、

178
00:05:52,100 --> 00:05:53,160
小文字のkを

179
00:05:53,770 --> 00:05:55,140
重心を識別するインデックスに使う。

180
00:05:56,240 --> 00:05:58,470
そしてciを

181
00:05:58,550 --> 00:06:00,110
kの値に関して最小化する、

182
00:06:00,550 --> 00:06:01,930
そしてこのxiとクラスタ重心の

183
00:06:02,120 --> 00:06:03,650
距離を最小化する

184
00:06:03,900 --> 00:06:04,750
kを探す。

185
00:06:04,800 --> 00:06:06,130
そして、

186
00:06:06,340 --> 00:06:08,990
あー、

187
00:06:09,070 --> 00:06:10,350
これを最小化する値kを、

188
00:06:10,940 --> 00:06:12,160
そのkをciに代入する。

189
00:06:12,300 --> 00:06:14,100
以上がciとは何なのか？を

190
00:06:14,360 --> 00:06:16,470
記述するもう一つの書き方。

191
00:06:18,050 --> 00:06:19,150
xi マイナス ミューk のノルム、

192
00:06:19,270 --> 00:06:21,500
と書いた時は、

193
00:06:23,000 --> 00:06:24,120
これはi番目のトレーニング手本と

194
00:06:24,630 --> 00:06:26,040
クラスター重心の ミュー下付き添字k との

195
00:06:26,180 --> 00:06:27,350
距離を表す。

196
00:06:28,140 --> 00:06:30,280
これ、この

197
00:06:31,150 --> 00:06:32,830
これは小文字のk。つまり大文字のKは

198
00:06:33,320 --> 00:06:34,710
クラスタ重心の総数を

199
00:06:34,980 --> 00:06:36,210
表すのに

200
00:06:36,450 --> 00:06:38,020
使う。

201
00:06:38,770 --> 00:06:40,430
そしてこの小文字のkは

202
00:06:40,790 --> 00:06:41,840
1から大文字のKまでの間の数字で

203
00:06:41,960 --> 00:06:42,940
異なるクラスタ重心同士を

204
00:06:43,210 --> 00:06:44,450
識別するのに

205
00:06:44,930 --> 00:06:45,990
小文字のkを使う。

206
00:06:47,130 --> 00:06:49,020
それが小文字のk。

207
00:06:50,050 --> 00:06:51,330
これがサンプルとクラスタ重心の距離で、

208
00:06:51,490 --> 00:06:52,810
我らがやるべき事は

209
00:06:53,050 --> 00:06:54,330
これを最小化する

210
00:06:55,250 --> 00:06:56,390
k、このkは小文字のkだが、

211
00:06:56,710 --> 00:06:58,900
それを探す。

212
00:06:59,080 --> 00:07:00,320
そしてその最小化するkの値を

213
00:07:00,480 --> 00:07:02,100
ciに

214
00:07:02,280 --> 00:07:03,610
セットする。

215
00:07:04,000 --> 00:07:06,560
そしてコンベンションにより、

216
00:07:06,760 --> 00:07:07,850
xiとクラスタ重心の距離を

217
00:07:08,190 --> 00:07:09,430
実際には距離の二乗で

218
00:07:09,480 --> 00:07:11,310
人々は

219
00:07:11,820 --> 00:07:13,330
書いている。

220
00:07:13,780 --> 00:07:15,370
つまりciを、

221
00:07:15,660 --> 00:07:16,860
トレーニング手本xiとの二乗距離が

222
00:07:17,450 --> 00:07:20,110
最小になるクラスタ重心を選びとった物と考える事が出来る。

223
00:07:20,750 --> 00:07:22,080
だがもちろん、距離の二乗を最小化しようと、

224
00:07:22,500 --> 00:07:23,700
距離を最小化しようと、

225
00:07:23,880 --> 00:07:25,670
同じciの値になるはず。

226
00:07:25,830 --> 00:07:26,670
でも普通は二乗をつける。

227
00:07:26,750 --> 00:07:28,120
単なる慣例。

228
00:07:28,430 --> 00:07:31,020
K-Meansを使う時の。

229
00:07:31,170 --> 00:07:32,320
以上がクラスタ割り付けステップ。

230
00:07:33,480 --> 00:07:34,750
K-Meansのループの中の

231
00:07:34,980 --> 00:07:37,740
他の仕事は重心移動のステップだ。

232
00:07:40,540 --> 00:07:41,750
そこで行うのは、

233
00:07:42,160 --> 00:07:43,460
クラスタ重心に対し、

234
00:07:43,550 --> 00:07:44,740
つまり小文字のkを1から大文字のKまでの範囲で、

235
00:07:44,870 --> 00:07:46,190
ミューkにその重心に

236
00:07:46,710 --> 00:07:48,460
関連付けられた点たちの平均を代入する。

237
00:07:49,270 --> 00:07:50,720
具体例としては、

238
00:07:50,910 --> 00:07:52,100
クラスタ重心の一つ、

239
00:07:52,340 --> 00:07:53,420
クラスタ重心2としよう、

240
00:07:53,750 --> 00:07:55,030
それがトレーニング手本を持ってるとして、

241
00:07:55,820 --> 00:08:02,390
それは1、5、6、10に、それが割り振られているとする。

242
00:08:03,220 --> 00:08:04,270
その意味するところは、

243
00:08:04,470 --> 00:08:05,510
c1=c5=c6=...

244
00:08:06,560 --> 00:08:09,180
と、

245
00:08:10,690 --> 00:08:12,180
c10も同様に

246
00:08:12,300 --> 00:08:13,730
イコールだ。

247
00:08:14,970 --> 00:08:17,070
クラスタ割り振りのステップで

248
00:08:17,160 --> 00:08:18,940
それを得たとすると、

249
00:08:19,190 --> 00:08:21,250
それはつまり、手本1、5、6、10は

250
00:08:21,450 --> 00:08:22,960
クラスタ重心2が割り付けられている。

251
00:08:24,020 --> 00:08:25,210
次にこの重心移動のステップでは、

252
00:08:25,540 --> 00:08:26,580
そこでやるべきは単に

253
00:08:27,180 --> 00:08:29,290
これら4つの平均を取るという事。

254
00:08:31,340 --> 00:08:33,950
つまりx1+x5+x6+x10と。

255
00:08:34,270 --> 00:08:35,620
。

256
00:08:35,890 --> 00:08:37,190
そして今、それらを

257
00:08:37,380 --> 00:08:38,630
平均したいのだから、

258
00:08:38,920 --> 00:08:40,020
このクラスタには点が4つ

259
00:08:40,100 --> 00:08:41,700
割り振られているのだから、

260
00:08:42,280 --> 00:08:43,240
1/4を取る。

261
00:08:43,980 --> 00:08:45,890
するとミュー2は

262
00:08:46,100 --> 00:08:47,910
n次元ベクトルとなる。

263
00:08:48,420 --> 00:08:49,480
何故なら各手本、

264
00:08:49,700 --> 00:08:51,050
x1、x5、x6、x10は、

265
00:08:52,160 --> 00:08:53,170
どれもn次元ベクトルだったから。

266
00:08:53,700 --> 00:08:55,150
そしてこれらを

267
00:08:55,240 --> 00:08:56,270
足し合わせて、

268
00:08:56,550 --> 00:08:57,870
4で割ってる、だって

269
00:08:57,940 --> 00:08:59,320
このクラスタ重心には

270
00:08:59,490 --> 00:09:00,730
4つの点が割り振られているから。

271
00:09:01,280 --> 00:09:02,770
その結果がクラスタ重心ミュー2の

272
00:09:03,870 --> 00:09:05,260
重心移動のステップとなる。

273
00:09:05,870 --> 00:09:06,850
これはミュー2を

274
00:09:07,210 --> 00:09:08,950
ここに列挙した4つの点の

275
00:09:09,130 --> 00:09:10,620
平均となる。

276
00:09:12,430 --> 00:09:13,850
よく質問される事として、今、

277
00:09:14,080 --> 00:09:16,600
ミューkをクラスタに割り振られた点の平均にしよう、と言ったが、

278
00:09:17,500 --> 00:09:18,900
もし点を割り振られないクラスタ重心、

279
00:09:18,960 --> 00:09:21,310
点が0個しか割り振られない

280
00:09:21,690 --> 00:09:23,000
クラスタ重心があったら、どうしたらいい？

281
00:09:23,280 --> 00:09:24,300
その場合、一番普通の対応は

282
00:09:24,650 --> 00:09:25,720
たんにそのクラスタ重心を

283
00:09:26,140 --> 00:09:27,220
取り除く。

284
00:09:27,830 --> 00:09:28,630
そうすると、

285
00:09:28,840 --> 00:09:30,260
最終結果はK個のクラスタではなくて

286
00:09:31,350 --> 00:09:33,840
K-1個のクラスタとなる。

287
00:09:34,400 --> 00:09:35,620
時々、ほんとうにK個のクラスタが

288
00:09:35,830 --> 00:09:37,380
必要な場合もある。その場合にやる別の手段としては、

289
00:09:37,490 --> 00:09:38,220
もし点が割り振られない

290
00:09:38,290 --> 00:09:39,530
クラスタ重心が

291
00:09:39,740 --> 00:09:41,170
あったら、単にそのクラスタ重心を

292
00:09:41,260 --> 00:09:42,590
ランダムに再初期化する。

293
00:09:43,450 --> 00:09:44,870
でも単に取り除く方が

294
00:09:45,170 --> 00:09:46,590
普通だね。

295
00:09:46,670 --> 00:09:48,210
K-Meansの最中に

296
00:09:48,410 --> 00:09:49,690
クラスタ重心に

297
00:09:50,290 --> 00:09:52,020
点が割り振られなかったら。

298
00:09:52,140 --> 00:09:53,340
そしてそもそも実際には

299
00:09:53,820 --> 00:09:55,590
そういう事はめったに起こらない。

300
00:09:55,810 --> 00:09:57,280
以上がK-Meansアルゴリズム。

301
00:09:59,330 --> 00:10:00,220
このビデオのまとめに入る前に、

302
00:10:00,620 --> 00:10:01,290
もう一つの良くある

303
00:10:01,350 --> 00:10:02,710
K-Meansの応用を

304
00:10:03,350 --> 00:10:04,680
話しておきたい。

305
00:10:04,920 --> 00:10:06,840
それは、あまり綺麗に分かれていないクラスタの問題だ。

306
00:10:08,160 --> 00:10:08,640
それはこんな意味だ。

307
00:10:09,280 --> 00:10:10,320
ここまではK-Meansを

308
00:10:10,950 --> 00:10:12,090
こんな感じのデータ、

309
00:10:12,330 --> 00:10:13,520
3つの綺麗に分かれたクラスタの

310
00:10:14,150 --> 00:10:15,590
データに対して

311
00:10:15,900 --> 00:10:17,380
図解したり適用したりしてきた。

312
00:10:17,670 --> 00:10:19,930
そしてアルゴリズムに3つのクラスタを探させてきた。

313
00:10:20,750 --> 00:10:21,840
だがK-Meansは

314
00:10:21,980 --> 00:10:23,180
こんな感じのデータセットに対しても

315
00:10:23,600 --> 00:10:24,860
とても良く適用されている、

316
00:10:25,170 --> 00:10:26,240
そこでは幾つかのクラスタに

317
00:10:26,330 --> 00:10:28,300
綺麗に分けられるようには

318
00:10:28,550 --> 00:10:30,250
見えない。

319
00:10:30,830 --> 00:10:32,960
これはTシャツのサイズに関する適用の例だ。

320
00:10:34,070 --> 00:10:34,650
あなたはTシャツ作ってる会社だとしよう、

321
00:10:35,270 --> 00:10:37,360
あなたは自分たちが

322
00:10:38,030 --> 00:10:39,310
Tシャツを売りたい、と思っている

323
00:10:39,380 --> 00:10:40,520
母集団に対して、

324
00:10:40,800 --> 00:10:42,190
たくさんのサンプルの

325
00:10:42,580 --> 00:10:43,770
身長と体重の

326
00:10:44,270 --> 00:10:45,350
データを集めた、

327
00:10:45,680 --> 00:10:46,740
つまり、えー、

328
00:10:47,220 --> 00:10:48,280
たぶん身長と体重は

329
00:10:48,370 --> 00:10:50,310
正の相関があるだろうから、

330
00:10:50,540 --> 00:10:51,160
こんな感じのデータセットに

331
00:10:51,430 --> 00:10:52,590
なるだろう。

332
00:10:52,830 --> 00:10:53,910
様々な人で構成されたサンプルの

333
00:10:53,980 --> 00:10:56,000
人々の身長と体重。

334
00:10:56,530 --> 00:10:57,880
Tシャツのサイズを決めたいと思ってるとしよう。

335
00:10:58,570 --> 00:10:59,810
3つのサイズ、S、M、LのTシャツの

336
00:11:00,330 --> 00:11:01,480
デザインをして売りたい、

337
00:11:01,660 --> 00:11:03,970
としよう。

338
00:11:04,660 --> 00:11:06,420
ではその時、Sはどのくらいの大きさにすべきだろう？

339
00:11:06,550 --> 00:11:07,320
Mのサイズはどのくらいの大きさ？

340
00:11:07,690 --> 00:11:09,300
そしてLのTシャツは、どのくらいの大きさにすべきか？

341
00:11:10,370 --> 00:11:11,290
それを決める一つの方法としては、

342
00:11:11,410 --> 00:11:12,050
右に示したこのデータセットに対し

343
00:11:12,270 --> 00:11:13,570
K-Meansクラスタリングアルゴリズムを

344
00:11:13,830 --> 00:11:14,640
適用する、というのがある。

345
00:11:14,820 --> 00:11:16,570
するとたぶんK-Meansが

346
00:11:16,770 --> 00:11:18,270
行う事は、

347
00:11:18,600 --> 00:11:20,460
これら全部の点を

348
00:11:20,660 --> 00:11:22,120
一つのクラスタに、

349
00:11:22,340 --> 00:11:24,150
これらの点全部を

350
00:11:24,190 --> 00:11:25,530
二番目のクラスタに、

351
00:11:25,740 --> 00:11:28,080
そしてこれらの点全部を三番目のクラスタにグループ分けする、という事だ。

352
00:11:28,520 --> 00:11:29,870
つまり、もともとのデータセットが

353
00:11:30,020 --> 00:11:30,960
3つの異なるクラスタに

354
00:11:31,060 --> 00:11:31,990
分かれているようには見えない

355
00:11:32,050 --> 00:11:33,920
のにも関わらず、K-Meansは

356
00:11:34,050 --> 00:11:36,870
複数のクラスタに

357
00:11:37,320 --> 00:11:38,560
分けてくれるのだ。

358
00:11:39,270 --> 00:11:40,220
そこから可能な事としては、

359
00:11:40,420 --> 00:11:42,010
この最初の人々を

360
00:11:42,130 --> 00:11:44,340
彼らを見て、

361
00:11:44,500 --> 00:11:45,590
彼らの身長と体重を見て、

362
00:11:45,780 --> 00:11:46,810
そして

363
00:11:46,900 --> 00:11:47,900
SのTシャツをデザインする、という事。

364
00:11:48,350 --> 00:11:49,540
最初の

365
00:11:49,710 --> 00:11:51,160
人々にうまく

366
00:11:51,310 --> 00:11:52,830
合うように。

367
00:11:53,150 --> 00:11:55,800
そしてその後で、MのTシャツとLのTシャツをデザインする。

368
00:11:56,510 --> 00:11:57,860
これはつまり、マーケットセグメンテーションの

369
00:11:57,990 --> 00:11:59,740
例となっている、

370
00:12:01,140 --> 00:12:02,800
そこではK-Meansを使って、

371
00:12:02,940 --> 00:12:04,320
マーケットを3つのセグメントに分けている。

372
00:12:05,220 --> 00:12:06,500
つまりこれで、S、M、Lに分けて

373
00:12:07,000 --> 00:12:08,260
Tシャツをデザイン出来る、

374
00:12:09,880 --> 00:12:11,230
3つのサブ集団の

375
00:12:11,650 --> 00:12:12,770
ニーズに

376
00:12:12,920 --> 00:12:15,310
良く合うように。

377
00:12:16,220 --> 00:12:17,570
以上がK-Meansアルゴリズムです。

378
00:12:18,240 --> 00:12:19,080
ここまでで、もうK-Meansを

379
00:12:19,300 --> 00:12:20,500
どうやって実装するか、そして

380
00:12:20,630 --> 00:12:22,510
どんな問題に適用出来るかを理解したはずだ。

381
00:12:23,420 --> 00:12:24,380
だが次に続くいくつかのビデオで

382
00:12:24,860 --> 00:12:26,430
K-Meansの要点を

383
00:12:26,580 --> 00:12:27,690
より深く話していくのと

384
00:12:28,010 --> 00:12:29,210
実際にとてもうまくやる為に

385
00:12:29,510 --> 00:12:32,080
必要な事をちょろっと話していきたい。