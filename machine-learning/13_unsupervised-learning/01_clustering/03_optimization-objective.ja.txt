これまで見て来た教師あり学習アルゴリズム、 線形回帰や ロジスティック回帰などは、 それらは全て、 最適化の為の目的関数、 またの名をコスト関数を持っていて、それを最小化しようとしていた。 K-meansもまた、最適化の 目的関数、またはコスト関数を 持っていて、それを最小化しようとする。 そしてこの動画では、最適化の目的関数が 何かを説明する。 これをやりたい理由としては、 二つの目的にこれは有用だからだ。 まず、K-meansの最適化の目的関数が なんなのかを知る事は、 学習アルゴリズムを デバッグする助けとなる。 K-meansがちゃんと走ってるか 確認も出来る。
二番目に、 そしてたぶんこっちの方が重要だが、 後半のビデオで、 これをどう用いて K-meansがより良いクラスタを見つける助けに出来るか、 そしてどう局所最適を避ける事が出来るかを議論する。
でもそれはこのビデオの後に続くビデオでね。 思い出してもらう為に言っておくと、K-meansを 実行している間、我らは 二つの種類の変数を管理していく。 一つ目はc(i)。これは x(i)が現在 どのクラスタに割り振られているかのインデックスを トラックする為の変数だ。 そしてもう一方の管理する変数は ミューの下付き添字kだ。 それはクラスター重心Kの 場所を表す。 もう一度言っておくと、K-meansでは 大文字のKをクラスタの総数を表すのに使う。 そしてこの小文字のkで クラスタ重心の インデックスを表す。 つまり小文字のkは 1からKまでの間の 数字となる。 さらにもう一つ 追加の記法として、 ミューの下付き添字 c(i)という物で これはクラスターの重心のうち、 サンプルx(i)に 割り振られている物を 表す。 この記法についてもうちょっと 説明しよう。 x(i)がクラスタ重心5に 割り振られているとしよう。 それの意味する所はc(i)、 このiはx(i)のインデックスだが、 c(i)はイコール5となる。 でしょ？だってc(i)=5となるのが サンプルx(i)が クラスタナンバー5に割り振らたという 事だから。 だからミュー下付き添字の c(i)はイコール、 ミュー 下付き添字の 5 となる。 何故ならc(i)がイコール 5だからだ。 このミュー下付き添字c(i)は クラスターナンバー5の クラスタ重心で、それがサンプルx(i)が 割り振られている物だ。 この記法で、 我らはK-meansの クラスタリングアルゴリズムの 最適化の目的関数を書き下す、準備が出来た事になる。 それはこうだ。 K-meansが最小化する コスト関数は これらのパラメータ全ての関数Jだ、 c1からcmまでと、 ミュー1からミューKまでの。 K-meansは実行していく過程でこれらを変更していく。 そして最適化の目的関数は、 右に示した物で、平均としての 1/mの、和を取る事の i=1からmまでの、この項で、 今赤の箱でくくったこれ。 サンプルx(i)と x(i)に割り振られた クラスタ重心の 位置との間の 二乗距離。 ちょっと描いて、これを説明しよう。 これはトレーニングサンプルの x(i)の位置で、 これがサンプルx(i)が割り振られた クラスタ重心の位置とする。 これを図で説明する為に、x1とx2があって、 この点、 ここがサンブルx(i)とすると、 つまりこれがサンプルx(i)と イコールだとする。 そしてx(i)があるクラスタ重心に 割り振られているとすると、 ところでクラスタの重心は十字で表す事にする。 つまり、それがミュー5の 場所で、 この例ではx(i)がクラスター重心5に 割り振られてるとすると。 すると、二乗距離、つまり、 点x(i)とそれが割り振らている クラスタ重心の間の 距離の二乗だ。 そしてK-Meansがやる事は、 つまり、 パラメータであるc(i)と ミューi を探そうとする、 cとミューで、コスト関数Jを 最小化する物を探そうとする。 このコスト関数はまた、 ディストーション（歪み）コスト関数、 またはK-meansアルゴリズムのディストーションと 呼ばれる。 もうちょっと詳細を見ると、 これがK-meansのアルゴリズムだ。 これは前のスライドにあったのと全く同じ物を 実際の形にした物だ。 そしてこのアルゴリズムの 最初のステップは クラスター割り振りのステップで、 そこで各点を クラスター重心に割り振る。 クラスター割り振りのステップは 実際に変数c1, c2、、、と c(m)までの観点から Jを最小化している、という事を 数学的に証明する 事が出来る。 この間、もっとも近い 重心である、ミュー1からミューkまでは 固定しておく。 で、最初の割り振りのステップで 何をやるかというと、そのステップでは クラスタ重心は変更しない、その代わりに コスト関数、またはディストーション関数であるJを 最小化するc1, c2,...cmの値を 選ぶ、という事をする。 そして数学的にも やろうと思えば証明出来るが、ここではやらんけど、 直感的にも自然だと思うけど これらの点に対し もっとも近いクラスタ重心を 割り振っていく。というのはそれが 点と対応するクラスタ重心の間の二乗距離の和を 最小化する割り振り方だから。 そして残りのK-meansの部分、 2番目のステップは、この二番目のステップとなるが、 この二番目のステップは重心を移動する ステップで、 ここでも、証明はしないが、 数学的にも証明出来る事として、 重心移動のステップでは、 Jを最小化する ミューを 選ぶ。 つまりコスト関数Jを 以下の観点から最小化する、 ここでwrtはwith respect to（以下の観点から）の 省略だ。 で、Jをミュー1からミューKまでの 位置に関して、最小化する。 つまり、K-meansが実際にやっているのは、 二つの種類の変数群を とり、そして それら二つを二つに分けて、 まずcの変数群、次にミューの変数群として、 そしてやるのは、まず、 Jを変数cに関して 最小化する、 変数ミューに関して最小化する、 そしてそれを繰り返し続ける。 以上がK-meansのやる事の全てだ。 そして今やK-meansを理解したので、 コスト関数Jを 最小化しよう。 これを用いて、また我らは 学習アルゴリズムのデバッグを試みる事も 可能だ。また我らの K-meansの実装が正しく走っているかを 確認するのにも使える。 つまり、いまや我らは K-meansアルゴリズムを、コスト関数Jを 最小化する物として理解した。 コスト関数はディストーション関数とも呼ばれるんだった。 その事実を用いて、K-meansをデバッグしたり、 K-meansが収束しているのを見たり出来る。 そしてそれが正しく 実行されているかも。 次のビデオでは、 どうこれを用いて K-meansがより良いクラスタを見つける助けと出来るか、 そしてどうK-meansが局所最適を避ける助けと出来るかを話していく。