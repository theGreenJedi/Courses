クラスタリングの問題では、 ラベル付けされていないデータセットが渡されて、 アルゴリズムに自動で 互いに密接なサブセット、または互いに密接なクラスタに グループ分け して欲しい。 K-Means アルゴリズムは ずば抜けて人気のある、 ずば抜けて広く使われているクラスタリングアルゴリズムだ。 このビデオでは、 K-Meansアルゴリズムとは何か、 それがどう機能するかを話していきたい。 K-Meansクラスタリングアルゴリズムは絵で表すのが一番。 ここに見せたような ラベルの無いデータセットが あるとしよう。 そしてこのデータを2つのクラスタにグループ分けしたい。 K-Meansアルゴリズムを実行するとしたら これがやるべきことだ。 最初の一歩はランダムに2つの点を選ぶ、 これはクラスタの重心と呼ばれる。 そう、これら2つのバッテンが、 クラスタ重心と呼ばれる物だ。 そしてそれが2つなのは、 データを2つのクラスタにグループ分けしたいから。 K-Meansはイテレーティブなアルゴリズムで、2つの事をする。 最初はクラスタの割り付けステップ。 二番目は重心移動ステップ。 それらが何を意味するか解説していこう。 K-Meansのループの中の2つのステップの内、 最初の方は、クラスタ割り付けのステップだ。 その意味する所は 各手本を見ていって、 ここで示したのだと、この緑のドットを 見ていき、 赤のクラスタ重心と青のクラスタ重心の どちらと近いかによって、 各データポイントを 2つのクラスタのうちのどちらかに 割り振る。 具体的にその意味する所を見ると、 データセットを見ていって、 各点を赤か青に 色付けしていく、 赤のクラスタ重心に近いか 青のクラスタ重心に近いかに よって。 この図ではそれを実際にやってみた。 以上がクラスタ割り付けステップ。 K-Meansのループ内を 構成するもう一方は 重心の移動ステップだ。 我らがやるべきことは、 2つのクラスタの重心を、 つまり、赤のバッテンと 青のバッテンを、 同じ色で色付けされた点の平均へと 移動する。 つまり我らがやる事は、 全ての赤の点を見て、 平均を計算し、 それは真に全ての赤い点の 平均だが、 赤のクラスタの重心をそこへ移動する。 同じ事を青のクラスタ重心にも 行う。青い点を 全て見て、平均を計算し、 青のクラスタ重心をそこへ移動する。 ではやってみよう。 クラスタ重心を以下のように動かし、 それらは新しい平均へと移動した。 赤いのはこんな感じで動き、 青いのはこんな感じで動いた。 そして赤いのはこんな感じで動いた。 そして次にあらたなクラスタ割り当てステップに戻る。 つまりまたラベルづけされていない 手本を全て見ていって、 青と赤のどちらの重心に 近いかによって、 それらを赤か青に色付けする。 各点に2つのクラスタ重心のどちらかを 割り振る、という事なので、やってみよう。 幾つかの点の色は変わった。 そしてまた、重心移動のステップに進む。 つまり全ての青の点の 平均を計算し、 全ての赤の点の 平均を計算し、 クラスタ重心をこんな感じで移動する。 やってみよう。 クラスタ割り振りステップをもう一回やってみよう。 各点を赤か青に色分けする、 とちらに近いかに基づいて そして次に 重心移動のステップを行う。行った。 そして実の所、 ここからさらにK-Meansのイテレーションを 走らせ続けても、 クラスタ重心はこれ以上 移動しない。そして点の色も これ以上は変わらない。 つまり、これが こここそが、K-Meansが 収束する点だ。 そしてそれは、このデータの2つのクラスタを 探すには、かなり良い仕事をしている。 ではK-Meansのアルゴリズムをよりフォーマルに記述しよう。 K -Meansアルゴリズムは2つの入力を取る。 一つ目はパラメータK、 それはデータの中から見つけたい クラスタの数。 あとでどうやってこのKを どうやって選んだらいいかの話をするつもりだが、 今のところはあるクラスタの数を 既に決めた、と しておこう。 そして幾つのクラスタがデータセットにあるかを アルゴリズムに伝えるとする。 そしてK-Meansはまた、 このようなラベルの無いトレーニングセットを 入力にとる。 単なるxだけ。 これは教師なし学習だから、 もうラベルyは無い。 そしてK-Meansの教師なし学習に対しては xiをRnのベクトルを 表すというコンベンションを 用いることにする。 そんな訳でトレーニング手本はいまや、 n+1次元では無くn次元のベクトルとなる。 これがK-Meansアルゴリズムがやる事だ: 最初のステップは ランダムにK個の重心を選ぶ、 それをミュー1、ミュー2、、、 ミューkと名付ける つまり、 前の図だと、 クラスタ重心は 赤のバッテンと 青のバッテンの場所に対応してた。 つまりそれは、2つのクラスタ重心があったという事で、 たとえば赤のバッテンが ミュー1で、 青のバッテンがミュー2だった、という事。 そして今度はより一般的に 2つだけじゃなくて、K個のクラスタ重心を考えていく。 するとK-Meansの内側のループは 以下を実行する、 つまり以下を繰り返し実行する事になる: まず、各トレーニング手本に対して この変数c(i)を xiに一番近いクラスタ重心のインデックスを セットする。 値は重心のインデックスである、1からKまでの値。 つまりこれは、クラスタ割り付けステップにあたる。 そこでは各サンプルに対して それを赤か青か、 そのどちらの重心に近いかに基づいて 色分けしていく。 色分けしていく。 つまりc(i)は 1からKまでの数で、 その値は我らに それが赤のバッテンか 青のバッテンか、 どちらに近いかを教えてくれる。 これの他の書き方としては、 ciを計算するのに、、、 i番目の手本 xiを取ってきて それと個々の クラスタ重心との 距離を測る。 これがミューk、 小文字のk。 大文字のKを重心の 総数を表すのに使い、 小文字のkを 重心を識別するインデックスに使う。 そしてciを kの値に関して最小化する、 そしてこのxiとクラスタ重心の 距離を最小化する kを探す。 そして、 あー、 これを最小化する値kを、 そのkをciに代入する。 以上がciとは何なのか？を 記述するもう一つの書き方。 xi マイナス ミューk のノルム、 と書いた時は、 これはi番目のトレーニング手本と クラスター重心の ミュー下付き添字k との 距離を表す。 これ、この これは小文字のk。つまり大文字のKは クラスタ重心の総数を 表すのに 使う。 そしてこの小文字のkは 1から大文字のKまでの間の数字で 異なるクラスタ重心同士を 識別するのに 小文字のkを使う。 それが小文字のk。 これがサンプルとクラスタ重心の距離で、 我らがやるべき事は これを最小化する k、このkは小文字のkだが、 それを探す。 そしてその最小化するkの値を ciに セットする。 そしてコンベンションにより、 xiとクラスタ重心の距離を 実際には距離の二乗で 人々は 書いている。 つまりciを、 トレーニング手本xiとの二乗距離が 最小になるクラスタ重心を選びとった物と考える事が出来る。 だがもちろん、距離の二乗を最小化しようと、 距離を最小化しようと、 同じciの値になるはず。 でも普通は二乗をつける。 単なる慣例。 K-Meansを使う時の。 以上がクラスタ割り付けステップ。 K-Meansのループの中の 他の仕事は重心移動のステップだ。 そこで行うのは、 クラスタ重心に対し、 つまり小文字のkを1から大文字のKまでの範囲で、 ミューkにその重心に 関連付けられた点たちの平均を代入する。 具体例としては、 クラスタ重心の一つ、 クラスタ重心2としよう、 それがトレーニング手本を持ってるとして、 それは1、5、6、10に、それが割り振られているとする。 その意味するところは、 c1=c5=c6=... と、 c10も同様に イコールだ。 クラスタ割り振りのステップで それを得たとすると、 それはつまり、手本1、5、6、10は クラスタ重心2が割り付けられている。 次にこの重心移動のステップでは、 そこでやるべきは単に これら4つの平均を取るという事。 つまりx1+x5+x6+x10と。 。 そして今、それらを 平均したいのだから、 このクラスタには点が4つ 割り振られているのだから、 1/4を取る。 するとミュー2は n次元ベクトルとなる。 何故なら各手本、 x1、x5、x6、x10は、 どれもn次元ベクトルだったから。 そしてこれらを 足し合わせて、 4で割ってる、だって このクラスタ重心には 4つの点が割り振られているから。 その結果がクラスタ重心ミュー2の 重心移動のステップとなる。 これはミュー2を ここに列挙した4つの点の 平均となる。 よく質問される事として、今、 ミューkをクラスタに割り振られた点の平均にしよう、と言ったが、 もし点を割り振られないクラスタ重心、 点が0個しか割り振られない クラスタ重心があったら、どうしたらいい？ その場合、一番普通の対応は たんにそのクラスタ重心を 取り除く。 そうすると、 最終結果はK個のクラスタではなくて K-1個のクラスタとなる。 時々、ほんとうにK個のクラスタが 必要な場合もある。その場合にやる別の手段としては、 もし点が割り振られない クラスタ重心が あったら、単にそのクラスタ重心を ランダムに再初期化する。 でも単に取り除く方が 普通だね。 K-Meansの最中に クラスタ重心に 点が割り振られなかったら。 そしてそもそも実際には そういう事はめったに起こらない。 以上がK-Meansアルゴリズム。 このビデオのまとめに入る前に、 もう一つの良くある K-Meansの応用を 話しておきたい。 それは、あまり綺麗に分かれていないクラスタの問題だ。 それはこんな意味だ。 ここまではK-Meansを こんな感じのデータ、 3つの綺麗に分かれたクラスタの データに対して 図解したり適用したりしてきた。 そしてアルゴリズムに3つのクラスタを探させてきた。 だがK-Meansは こんな感じのデータセットに対しても とても良く適用されている、 そこでは幾つかのクラスタに 綺麗に分けられるようには 見えない。 これはTシャツのサイズに関する適用の例だ。 あなたはTシャツ作ってる会社だとしよう、 あなたは自分たちが Tシャツを売りたい、と思っている 母集団に対して、 たくさんのサンプルの 身長と体重の データを集めた、 つまり、えー、 たぶん身長と体重は 正の相関があるだろうから、 こんな感じのデータセットに なるだろう。 様々な人で構成されたサンプルの 人々の身長と体重。 Tシャツのサイズを決めたいと思ってるとしよう。 3つのサイズ、S、M、LのTシャツの デザインをして売りたい、 としよう。 ではその時、Sはどのくらいの大きさにすべきだろう？ Mのサイズはどのくらいの大きさ？ そしてLのTシャツは、どのくらいの大きさにすべきか？ それを決める一つの方法としては、 右に示したこのデータセットに対し K-Meansクラスタリングアルゴリズムを 適用する、というのがある。 するとたぶんK-Meansが 行う事は、 これら全部の点を 一つのクラスタに、 これらの点全部を 二番目のクラスタに、 そしてこれらの点全部を三番目のクラスタにグループ分けする、という事だ。 つまり、もともとのデータセットが 3つの異なるクラスタに 分かれているようには見えない のにも関わらず、K-Meansは 複数のクラスタに 分けてくれるのだ。 そこから可能な事としては、 この最初の人々を 彼らを見て、 彼らの身長と体重を見て、 そして SのTシャツをデザインする、という事。 最初の 人々にうまく 合うように。 そしてその後で、MのTシャツとLのTシャツをデザインする。 これはつまり、マーケットセグメンテーションの 例となっている、 そこではK-Meansを使って、 マーケットを3つのセグメントに分けている。 つまりこれで、S、M、Lに分けて Tシャツをデザイン出来る、 3つのサブ集団の ニーズに 良く合うように。 以上がK-Meansアルゴリズムです。 ここまでで、もうK-Meansを どうやって実装するか、そして どんな問題に適用出来るかを理解したはずだ。 だが次に続くいくつかのビデオで K-Meansの要点を より深く話していくのと 実際にとてもうまくやる為に 必要な事をちょろっと話していきたい。