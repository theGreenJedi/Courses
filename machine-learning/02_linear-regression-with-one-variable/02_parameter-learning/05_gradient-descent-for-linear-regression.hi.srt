1
00:00:00,520 --> 00:00:04,480
पिछले वीडियो में, हमने बात की ग्रेडीयंट डिसेंट अल्गोरिद्म की और

2
00:00:04,480 --> 00:00:09,540
हमने बात की लिनीअर रेग्रेशन मॉडल की और स्क्वेर्ड एरर कॉस्ट फ़ंक्शन की.

3
00:00:09,540 --> 00:00:14,280
इस वीडियो में हम एक साथ रखेंगे ग्रेडीयंट डिसेंट और हमारा कॉस्ट फ़ंक्शन,

4
00:00:14,280 --> 00:00:17,400
और वह देगा हमें एक अल्गोरिद्म लिनीअर रेग्रेशन के लिए या

5
00:00:17,400 --> 00:00:18,730
डालना / रखना एक सीधी रेखा हमारे डेटा में.

6
00:00:20,800 --> 00:00:24,950
तो यह है जो हम पिछले वीडियो में किया था.

7
00:00:24,950 --> 00:00:28,920
यह ग्रेडीयंट डिसेंट अल्गोरिद्म जिससे आप परिचित होने चाहिए और

8
00:00:28,920 --> 00:00:34,210
यहाँ है लिनीअर रेग्रेशन मॉडल हमारी लिनीअर हायपॉथिसस के साथ और

9
00:00:34,210 --> 00:00:36,540
हमारा स्क्वेर्ड एरर कोस्ट फ़ंक्शन.

10
00:00:36,540 --> 00:00:42,312
हम क्या करेंगे कि अप्लाई करेंगे ग्रेडीयंट डिसेंट

11
00:00:42,312 --> 00:00:47,820
न्यूनतम करने के लिए हमारा स्क्वेर्ड एरर कोस्ट फ़ंक्शन.

12
00:00:47,820 --> 00:00:51,275
अब अप्लाई करने के लिए ग्रेडीयंट डिसेंट, आप जानते हैं

13
00:00:51,275 --> 00:00:59,810
लिखने के लिए यह कोड का हिस्सा, कीई टर्म जो हमें चाहिए वह है यह डेरिवेटिव टर्म यहाँ.

14
00:00:59,810 --> 00:01:04,060
तो आपको यह समझना है कि क्या है यह पर्शियल डेरिवेटिव टर्म और

15
00:01:04,060 --> 00:01:07,710
प्लग करना है इसे परिभाषा में कोस्ट फ़ंक्शन जे की.

16
00:01:07,710 --> 00:01:11,670
परिणाम यह निकलता है.

17
00:01:13,020 --> 00:01:15,550
सम वाय बराबर एक से एम तक.

18
00:01:15,550 --> 00:01:21,400
इस स्क्वेर्ड एरर कॉस्ट फ़ंक्शन टर्म की.

19
00:01:21,400 --> 00:01:23,520
और मैंने सिर्फ़ यहाँ क्या किया कि मैंने सिर्फ़

20
00:01:23,520 --> 00:01:26,190
आप जानते हैं प्लग किया परिभाषा में कॉस्ट फ़ंक्शन की वहाँ.

21
00:01:27,290 --> 00:01:34,820
और थोड़ा और अधिक सरल बनाने के लिए, यह निकलता है इसके बराबर.

22
00:01:34,820 --> 00:01:43,280
सिग्मा आइ बराबर एक से एम तक थीटा 0 प्लस थीटा वन एक्स 1 माईनस वाय आइ स्क्वेर्ड.

23
00:01:43,280 --> 00:01:47,830
मैंने सिर्फ़ क्या किया कि ली परिभाषा मेरी हायपॉथिसस की और

24
00:01:47,830 --> 00:01:50,782
प्लग किया इसे वहाँ.

25
00:01:50,782 --> 00:01:53,190
और ऐसा है कि हमें समझना है कि क्या है यह पर्शियल डेरिवेटिव

26
00:01:53,190 --> 00:01:56,570
दो केस के लिए J बराबर 0 और जे बराबर 1.

27
00:01:56,570 --> 00:02:00,310
तो हम चाहते हैं समझना कि क्या है यह पर्शियल डेरिवेटिव

28
00:02:00,310 --> 00:02:04,170
दोनो थीटा 0 केस के लिए और थीटा 1 केस के लिए.

29
00:02:04,170 --> 00:02:06,940
और अब मैं सिर्फ़ लिखूँगा उत्तर.

30
00:02:06,940 --> 00:02:12,064
यह पहली टर्म है, सरलीकृत 1/M

31
00:02:12,064 --> 00:02:18,354
सम मेरे ट्रेनिंग सेट पर केवल X(i)-Y(i) और

32
00:02:18,354 --> 00:02:24,294
इस पर्शियल डेरिवेटिव टर्म के लिए चलो लिखते हैं थीटा 1,

33
00:02:24,294 --> 00:02:27,114
मुझे मिलती है यह टर्म.

34
00:02:27,114 --> 00:02:34,008
माइनस Y (i) टाइम्ज़ X(i).

35
00:02:34,008 --> 00:02:37,440
ठीक है और

36
00:02:37,440 --> 00:02:41,720
कम्प्यूट करने के लिए ये पर्शियल डेरिवेटिव, तो हम जा रहे हैं इस इक्वेज़न से.

37
00:02:41,720 --> 00:02:46,000
ठीक है जा रहे हैं इस इक्वेज़न से इनमें से एक इक्वेज़न पर नीचे वहाँ.

38
00:02:46,000 --> 00:02:51,020
कम्प्यूट करने के लिए वे पर्शियल डेरिवेटिव आवश्यकता है कुछ मल्टी-वेरियेट कैल्क्युलस की.

39
00:02:51,020 --> 00:02:54,930
यदि आप जानते हैं कैल्क्युलस, आप अपने आप कर सकते हैं डेरिवेशनज़ और

40
00:02:54,930 --> 00:02:59,510
चेक कर सकते हैं कि यदि आप लेते हैं डेरिवेटिव्स, आपको वास्तव में मिलते हैं वही जवाब जो मुझे मिले हैं.

41
00:02:59,510 --> 00:03:04,050
लेकिन यदि आप कम परिचित है कैल्क्युलस से, चिंता न करें उसकी और

42
00:03:04,050 --> 00:03:08,100
यह ठीक है ले लेना इन इक्वेज़न्स को जो हल की गई हैं और आपको नहीं होगी

43
00:03:08,100 --> 00:03:11,350
आवश्यकता जानने की कैल्क्युलस या वैसा कुछ, करने के लिए होमवर्क तो

44
00:03:11,350 --> 00:03:13,390
चलो इम्प्लमेंट करते हैं ग्रेडीयंट डिसेंट और काम पर लगते हैं.

45
00:03:14,750 --> 00:03:18,490
तो इन परिभाषाओं के साथ या जो हमने निकाले

46
00:03:18,490 --> 00:03:22,310
डेरिवेटिव्स जो है वास्तव में सिर्फ़ स्लोप कॉस्ट फ़ंक्शन जे की

47
00:03:23,310 --> 00:03:27,160
हम अब प्लग कर सकते हैं वापिस उन्हें हमारे ग्रेडीयंट डिसेंट अल्गोरिद्म में.

48
00:03:27,160 --> 00:03:28,640
तो यहाँ है ग्रेडीयंट डिसेंट

49
00:03:28,640 --> 00:03:32,728
लिनीअर रेग्रेशन के लिए जो बार बार चलेगा जब तक कन्वर्जेन्स नहीं हो जाता, थीटा 0 और

50
00:03:32,728 --> 00:03:38,380
थीटा 1 होंगे अपडेट जैसे आप जानते हैं यह माइनस अल्फ़ा टाइम्ज़ डेरिवेटिव टर्म.

51
00:03:39,390 --> 00:03:41,070
तो यह टर्म यहाँ.

52
00:03:43,080 --> 00:03:46,050
तो यहाँ है हमारा लिनीअर रेग्रेशन अल्गोरिद्म.

53
00:03:47,160 --> 00:03:48,628
यह पहली टर्म यहाँ.

54
00:03:52,529 --> 00:03:56,804
वह टर्म है निश्चय ही सिर्फ़ पर्शियल डेरिवेटिव विद रिस्पेक्ट टु थीटा ज़ीरो,

55
00:03:56,804 --> 00:03:59,790
जो हमने पिछली स्लाइड में किया था.

56
00:03:59,790 --> 00:04:05,730
और यह दूसरी टर्म यहाँ, वह टर्म है सिर्फ़ पर्शियल डेरिवेटिव विद

57
00:04:05,730 --> 00:04:11,420
रिस्पेक्ट टु थीटा 1, जो हमने पिछली स्लाइड में किया था.

58
00:04:11,420 --> 00:04:15,230
और सिर्फ़ एक छोटी चेतावनी, आप, जब कर रहे हैं इम्प्लमेंट ग्रेडीयंट डिसेंट.

59
00:04:15,230 --> 00:04:19,265
यह विस्तृत जानकारी कि आपको इम्प्लमेंट करना चाहिए इसे ताकि

60
00:04:19,265 --> 00:04:22,250
थीटा 0 और थीटा 1 अपडेट हों एक साथ.

61
00:04:24,290 --> 00:04:25,570
अतः.

62
00:04:25,570 --> 00:04:28,120
तो चलिए देखते हैं ग्रेडीयंट डिसेंट कैसे काम करता है.

63
00:04:28,120 --> 00:04:31,862
एक मुद्दा जो हमने ग्रेडीयंट डिसेंट के साथ देखा कि यह संवेदनशील हो सकता है

64
00:04:31,862 --> 00:04:32,700
लोकल ऑप्टिमा को.

65
00:04:32,700 --> 00:04:36,780
तो जब मैंने पहली बार ग्रेडीयंट डिसेंट समझाया मैंने दिखाई इस की यह तस्वीर आपको

66
00:04:36,780 --> 00:04:40,900
नीचे जाते हुए सतह पर, और हमने देखा कि कैसे निर्भर करते हुए कि कहाँ आप ईनिशीयलाइज करते हैं इसे,

67
00:04:40,900 --> 00:04:43,014
आप पहुँच सकते हो भिन्न लोकल ऑप्टिमा पर.

68
00:04:43,014 --> 00:04:45,480
आप पहुँच सकते हैं यहाँ या यहाँ.

69
00:04:45,480 --> 00:04:50,390
लेकिन, ऐसा होता है कि वह कॉस्ट फ़ंक्शन

70
00:04:50,390 --> 00:04:55,220
लिनीअर रेग्रेशन का हमेशा होगा एक धनुष के आकार का फ़ंक्शन ऐसा.

71
00:04:55,220 --> 00:05:00,190
इस के लिए तकनीकी शब्द है कि इसे कहते हैं एक कान्वेक्स फ़ंक्शन,

72
00:05:03,230 --> 00:05:07,800
और मैं नहीं दूँगा विधिवत परिभाषा कि क्या है कान्वेक्स फ़ंक्शन,

73
00:05:07,800 --> 00:05:09,490
सी,ओ,एन,वी,ई,एक्स.

74
00:05:09,490 --> 00:05:16,620
लेकिन अनौपचारिक रूप से एक कान्वेक्स फ़ंक्शन का मतलब है एक धनुष के आकार का फ़ंक्शन और इसलिए

75
00:05:16,620 --> 00:05:22,295
इस फ़ंक्शन का नहीं है कोई लोकल ऑप्टिमा एक ग्लोबल ऑप्टिमा के अतिरिक्त.

76
00:05:22,295 --> 00:05:26,465
और ग्रेडीयंट डिसेंट इस तरह के कॉस्ट फ़ंक्शन पर जो आपको मिलता है जब भी

77
00:05:26,465 --> 00:05:30,445
आप करते हैं लिनीअर रेग्रेशन यह होगा हमेश कन्वर्ज ग्लोबल ओप्टिमम पर.

78
00:05:30,445 --> 00:05:33,155
क्योंकि वहाँ कोई लोकल ओप्टिमम नहीं है, सिर्फ़ ग्लोबल ओप्टिमम है.

79
00:05:33,155 --> 00:05:36,615
तो अब देखते हैं यह अल्गोरिद्म काम करते हुए.

80
00:05:38,250 --> 00:05:45,910
हमेशा की तरह, यहां हैं प्लॉट्स हायपॉथिसस फ़ंक्शन के और मेरे कॉस्ट फ़ंक्शन जे के.

81
00:05:45,910 --> 00:05:50,020
और इसलिए मान लो मैंने ईनिशीयलाइज किए हैं मेरे पेरमिटर्स इस वैल्यू पर.

82
00:05:50,020 --> 00:05:54,220
मान लो, आम तौर पर आप ईनिशीयलाइज करते हैं आपके पेरमिटर्स ज़ीरो, ज़ीरो पर.

83
00:05:54,220 --> 00:05:56,370
थीटा ज़ीरो और थीटा वन बराबर हैं ज़ीरो.

84
00:05:56,370 --> 00:06:01,354
लेकिन समझाने के लिए, इस केस में

85
00:06:01,354 --> 00:06:07,619
मैंने ईनिशीयलाइज किया है, आप जानते हैं, थीटा ज़ीरो को 900 और थीटा एक को -0.1 ठीक है.

86
00:06:07,619 --> 00:06:12,644
और इसलिए यह बनता है एच(एक्स)=900-0.1 एक्स,

87
00:06:12,644 --> 00:06:16,547
है यह लाइन, यहाँ कॉस्ट फ़ंक्शन पर.

88
00:06:16,547 --> 00:06:21,060
अब, यदि मैं लेता हूँ एक स्टेप ग्रेडीयंट डिसेंट में,

89
00:06:21,060 --> 00:06:26,845
मैं पहुँचूँगा इस पोईँट यहाँ पर से नीचे और

90
00:06:26,845 --> 00:06:31,510
बाईं तरफ़, उस दूसरे पोईँट पर

91
00:06:31,510 --> 00:06:35,450
और आप ध्यान दें कि मेरी लाइन बदल गई थोड़ी, और

92
00:06:35,450 --> 00:06:39,780
जैसे मैं लेता हूँ एक और स्टेप ग्रेडीयंट डिसेंट में, मेरी लाइन बाईं तरफ़ की बदल जाएगी.

93
00:06:41,230 --> 00:06:42,380
ठीक?

94
00:06:42,380 --> 00:06:46,370
और मैं पहुँच गया एक नए पोईँट पर मेरे कॉस्ट फ़ंक्शन में.

95
00:06:47,670 --> 00:06:52,760
जैसे मैं लेता हूँ और स्टेप्स ग्रेडीयंट डिसेंट में, मैं जाता हूँ नीचे कॉस्ट में.

96
00:06:52,760 --> 00:06:56,190
तो मेरे पेरमिटर्स ले रहे हैं यह रास्ता.

97
00:06:57,340 --> 00:07:02,430
और अगर आप बाईं तरफ देखें, यह कॉरेस्पॉंड करता है हायपॉथिसस से.

98
00:07:02,430 --> 00:07:06,520
वह प्रतीत होता है बेहतर और बेहतर फ़िट होता हुआ डेटा को

99
00:07:08,200 --> 00:07:14,660
जब तक अंत में में पहुँचता हूँ ग्लोबल मिनिमम पास यह ग्लोबल मिनिमम

100
00:07:14,660 --> 00:07:20,090
कॉरेस्पॉंड करता है इस हायपॉथिसस को जो देती है मुझे एक अच्छा फ़िट डेटा को.

101
00:07:21,400 --> 00:07:25,800
और इसलिए वह है ग्रेडीयंट डिसेंट, और हम सिर्फ़ इसे रन करते हैं और

102
00:07:25,800 --> 00:07:31,230
पाते हैं एक अच्छा फ़िट मेरे डेटा सेट के लिए घरों की क़ीमतों के.

103
00:07:31,230 --> 00:07:34,490
और अब आप इसे उपयोग कर सकते हैं प्रिडिक्ट करने के लिए, आप जानते हैं,

104
00:07:34,490 --> 00:07:38,900
यदि अपने दोस्त के पास है एक घर जिसका साइज़ है 1250 वर्ग फुट,

105
00:07:38,900 --> 00:07:43,350
अब आप पढ़ सकते हैं वैल्यू और बता सकते हैं उन्हें कि मैं नहीं जानता शायद उन्हें

106
00:07:43,350 --> 00:07:48,720
मिल सकते हैं $ 250,000 उनके घर के लिए.

107
00:07:48,720 --> 00:07:52,620
अंत में सिर्फ़ देने के लिए इसे एक और नाम ऐसा होता है कि अल्गोरिद्म

108
00:07:52,620 --> 00:07:57,510
जो हमने अभी समझा कभी कभी कहलाता है बैच ग्रेडीयंट डिसेंट.

109
00:07:57,510 --> 00:08:00,730
और ऐसा होता है मशीन लर्निंग में मैं नहीं जानता मुझे लगता है हम मशीन लर्निंग के

110
00:08:00,730 --> 00:08:04,310
लोग नहीं थे हमेशा महान नाम देने में अल्गोरिद्म्स को.

111
00:08:04,310 --> 00:08:08,880
लेकिन टर्म बैच ग्रेडीयंट डिसेंट सम्बोधित करती है तथ्य को कि

112
00:08:08,880 --> 00:08:13,850
ग्रेडीयंट डिसेंट के प्रत्येक स्टेप में, हम देख रहे हैं सारे ट्रेनिंग इग्ज़ाम्पल्ज़.

113
00:08:13,850 --> 00:08:17,760
तो ग्रेडीयंट डिसेंट में, जब कम्प्यूट करते हैं डेरिवेटिव्स,

114
00:08:17,760 --> 00:08:21,400
हम कम्प्यूट कर रहे हैं सम [ सुनाई नहीं दिया].

115
00:08:21,400 --> 00:08:25,660
तो ग्रेडीयंट डिसेंट के प्रत्येक स्टेप में हम करते हैं कम्प्यूट कुछ ऐसे कि सम करें

116
00:08:25,660 --> 00:08:30,620
हमारे एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर और इसलिए टर्म बैच ग्रेडीयंट डिसेंट बताती है

117
00:08:30,620 --> 00:08:34,175
कि हम देख रहे हैं पूरे बैच पर ट्रेनिंग इग्ज़ाम्पल्ज़ के.

118
00:08:34,175 --> 00:08:36,365
और फिर, यह वास्तव में नहीं है एक बढ़िया नाम, लेकिन

119
00:08:36,365 --> 00:08:39,585
यह है जो मशीन लर्निंग के लोग कहते हैं इसे.

120
00:08:39,585 --> 00:08:43,715
और यह होता है, कि कभी कभी हैं दूसरे वर्ज़न ग्रेडीयंट डिसेंट के जो

121
00:08:43,715 --> 00:08:46,247
नहीं है बैच वर्ज़न, लेकिन वे हैं इसके बजाय.

122
00:08:46,247 --> 00:08:48,837
वे नहीं देखते पूरे ट्रेनिंग सेट बल्कि

123
00:08:48,837 --> 00:08:51,247
देखते हैं एक छोटे सब सेट को ट्रेनिंग सेट के एक समय में.

124
00:08:51,247 --> 00:08:55,207
और हम बात करेंगे उन वर्ज़न्स की भी बाद में इस कोर्स में.

125
00:08:55,207 --> 00:08:58,357
लेकिन अभी के लिए इस्तेमला करेंगे अल्गोरिद्म जो हमने अभी सीखा या इस्तेमाल करेंगे बैच ग्रेडीयंट

126
00:08:58,357 --> 00:09:03,497
डिसेंट आप अब जानते हैं कैसे इम्प्लमेंट करना है ग्रेडीयंट डिसेंट लिनीअर रेग्रेशन के लिए.

127
00:09:05,980 --> 00:09:09,550
तो वह है लिनीअर रेग्रेशन ग्रेडीयंट डिसेंट के साथ.

128
00:09:09,550 --> 00:09:12,260
यदि अपने देखा है एडवान्सड लिनीअर ऐल्जेब्रा पहले, तो

129
00:09:12,260 --> 00:09:15,510
आप में से कुछ ने शायद ली हो एक क्लास एडवांसड लिनीअर ऐल्जेब्रा की.

130
00:09:15,510 --> 00:09:19,410
आप शायद जानते होंगे कि एक हल मौजूद है नूमेरिक्ली निकालने के लिए

131
00:09:19,410 --> 00:09:22,270
न्यूनतम कॉस्ट फ़ंक्शन जे का बिना

132
00:09:22,270 --> 00:09:25,870
इस्तेमाल किए एक आइटरेटिव अल्गोरिद्म ग्रेडीयंट डिसेंट जैसा.

133
00:09:25,870 --> 00:09:29,730
बाद में इस कोर्स में हम बात करेंगे उस विधि की भी जो सिर्फ़ हल करता है

134
00:09:29,730 --> 00:09:33,020
न्यूनतम के लिए कॉस्ट फ़ंक्शन जे के, बिना करे ये बहुत स्टेप

135
00:09:33,020 --> 00:09:34,520
ग्रेडीयंट डिसेंट के.

136
00:09:34,520 --> 00:09:37,020
उस दूसरी विधि को कहते हैं नोर्मल इक्वेज़नज़ विधि.

137
00:09:37,020 --> 00:09:41,000
लेकिन यदि अपने सुना है पहले उस विधि के बारे में तो ऐसा होता है कि ग्रेडीयंट

138
00:09:41,000 --> 00:09:46,420
डिसेंट बेहतर काम करता है बड़े डेटा सेट्स के लिए तुलना में नोर्मल इक्वेज़न विधि के.

139
00:09:46,420 --> 00:09:50,140
और अब जब हम जानते हैं ग्रेडीयंट डिसेंट के बारे में हम इस्तेमाल कर पाएँगे इसे बहुत से

140
00:09:50,140 --> 00:09:51,400
विभिन्न संदर्भों में और

141
00:09:51,400 --> 00:09:53,910
हम इस्तेमाल करेंगे इसे बहुत सी मशीन लर्निंग प्राब्लम्ज़ में भी.

142
00:09:55,340 --> 00:10:00,430
तो आपको पहला मशीन लर्निंग एल्गोरिद्म के बारे में सीखने पर बधाई.

143
00:10:00,430 --> 00:10:04,990
बाद में हमारे पास अभ्यास है जिसमें हम आपको ग्रेडीयंट डिसेंट करने के लिए कहेंगे और

144
00:10:04,990 --> 00:10:07,480
उम्मीद है कि देख पाएँगे इन अल्गोरिद्म्स को अपने आप.

145
00:10:07,480 --> 00:10:11,460
लेकिन उससे पहले मैं पहले बताना चाहता हूँ आपको अगले सेट के वीडियोंज़ में.

146
00:10:11,460 --> 00:10:14,510
पहला जो आपको बताना चाहता हूँ एक जनरलाइज़ेशन के बारे में

147
00:10:14,510 --> 00:10:17,900
ग्रेडीयंट डिसेंट के जो बनाएगा इसे और ताक़तवर.

148
00:10:17,900 --> 00:10:20,420
और मैं सोचता हूँ मैं बताऊँगा उसके बारे में अगले वीडियो में.