En el video anterior, hablamos del algoritmo de gradiente de descenso y del modelo de regresión lineal y la función de costos de error al cuadrado. En este vídeo, vamos a unir el gradiente de descenso con nuestra función de costos, y eso nos dará un algoritmo para la regresión lineal para ajustar una línea recta a nuestro datos. Esto es en lo que trabajamos en los videos anteriores. Ese es nuestro algoritmo de gradiente de descenso, con el que ya debes estar familiarizado, y verás el modelo de regresión lineal con nuestra hipótesis lineal y nuestra función de costos de error al cuadrado. Lo que haremos es aplicar el gradiente de descenso para minimizar nuestra función de costos del error al cuadrado. Ahora, para aplicar el gradiente de descenso, para escribir este fragmento de código, el término clave que necesitamos es el término derivado de aquí. Entonces, necesitamos descubrir cuál es el término de derivada parcial, y relacionar la definición de la función de costos J, esto resulta ser igual a suma 1 a través de m de este término de la función de costos del error al cuadrado, y lo que hice fue relacionar la definición de la función de costos y simplificar un poco más, lo que resultó ser igual a Suma es igual a 1 a través de m de tetha cero más tetha uno, x(i) menos y(i) al cuadrado. Y todo lo que hice fue tomar la definición para mi hipótesis y relacionarla ahí. Y resulta que necesitamos descubrir cuál es la derivada parcial de dos casos para J igual a 0 y para J igual a 1, por lo que queremos averiguar cuál es la derivada parcial del caso tetha 0 y el caso tetha 1. Sólo voy a escribir las respuestas. Resulta que este primer término se simplifica a 1/M, suma sobre mi conjunto de entrenamiento de sólo ese, x(i)-y(i). Y para este término, la derivada parcial con respecto a tetha1, resulta que obtengo este término: x(i)-y(i) por x(i)</i> Ok. Y el cálculo de estas derivadas parciales, que van desde esta ecuación a cualquiera de estas ecuaciones de abajo, el cálculo de estos términos de derivadas parciales requiere cálculos multivariados. Si sabes de cálculo, no dudes en trabajar con las derivaciones por ti mismo, y comprueba, si tomas las derivadas realmente obtienes las respuesta que tengo. Pero si no estás tan familiarizado con cálculo, no te preocupes, está bien sólo tomar estas ecuaciones que se manejan, y no debes saber cálculo o algo como eso para hacer la tarea, para implementar la gradiente de descenso que tendrías que trabajar. Pero después de estas definiciones o después de lo que hemos visto que son las derivadas, que realmente es sólo la pendiente de la función de costos J, ahora podemos relacionarlas con nuestro algoritmo de gradiente de descenso. Entonces, este es el gradiente de descenso para la regresión lineal, que se repetirá hasta la convergencia, tetha 0 y tetha uno se actualizan como el mismo menos alfa veces el término de derivada. Bien, el término de aquí. Este es nuestro algoritmo de regresión lineal. El primer término de aquí es, por supuesto, sólo la derivada parcial de la respectiva tetha cero con la que trabajamos en la diapositiva anterior. Y el segundo término de aquí es sólo una derivada parcial con respecto a tetha uno con la que trabajamos en la línea anterior. Sólo como breve recordatorio, al utilizar el gradiente de descenso, realmente existe un detalle que debes implementar para la actualización de tetha cero y tetha uno de forma simultánea. Veamos cómo funciona el gradiente de descenso. Uno de los problemas que resolvimos del gradiente de descenso es que puede ser susceptible al óptimo local. Cuando expliqué por primera vez el gradiente de descenso, te mostré una imagen que iba en descenso en la superficie y vimos cómo, dependiendo de dónde inicies, puedes terminas con un óptimo local diferente. Puedes terminar aquí o aquí. Pero resulta que la función de costos para la función de costo-gradientes para la regresión lineal siempre será una función en forma de arco como esta. El término técnico para esto es función convexa. No daré la definición formal de qué es una función convexa, c-o-n-v-e-x-a, pero de forma informal una función convexa significa una función con forma curva, como una forma de arco. Y entonces, esta función no tiene un óptimo local, excepto por el óptimo global. Y también el gradiente de descenso en este tipo de función de costos que obtienes siempre que usas la regresión lineal, siempre se convierte a óptimo global debido a que no hay otro óptimo local que no sea el óptimo global. Ahora veamos este algoritmo en acción. Como de costumbre, aquí están las gráficas de la función de hipótesis y de mi función de costos J. Veamos cómo iniciar mis parámetros con este valor. Digamos, que generalmente inicias tus parámetros con cero, cero, tetha cero y tetha igual a cero. Para ilustrar en esta aplicación específica del gradiente de descenso, inicié tetha cero con aproximadamente 900, y tetha uno con aproximadamente menos 0.1, ¿ok? Así que esta corresponde con h sobre x, igual a menos 900, menos 0.1 x es esta línea, fuera de aquí en la función de costos. Ahora, si tomamos un paso de gradiente de descenso, terminamos yendo desde este punto de aquí, un poco a la izquierda inferior hasta el segundo punto de allá. Y puedes darte cuenta que mi línea cambió un poco. Y si tomo otro paso en el gradiente de descenso, mi línea de la izquierda cambiará. ¿Correcto? Y también me moví a otro punto nuevo de mi función de costos. Y si tomo otros pasos en el gradiente de descenso, reduciré los costos, entonces mi parámetro sigue esta trayectoria y, si veo a la izquierda, esto corresponde con la hipótesis que parece estar mejor y ajustarse mejor a los datos, hasta que finalmente cierro con el mínimo global. Y este mínimo global corresponde con la hipótesis, que me da una buena opción para los datos. Y eso es el gradiente de descenso, acabamos de realizarlo y obtuvimos una buena opción para mi conjunto de datos de precios de casas. Ahora puedes usarlo para predecir. Si tu amigo tiene una casa con un tamaño de 1250 pies cuadrados, ahora puedes visualizar el valor y decirle que, tal vez, podría obtener $350,000 por su casa. Finalmente, sólo para darle otro nombre, resulta que el algoritmo que obtuvimos, a veces se llama gradiente de descenso por lotes. Y resulta que en el aprendizaje electrónico, siento que, nosotros la gente del aprendizaje automático, no siempre somos buenos dando nombres a los algoritmos. Pero el término gradiente de descenso por lotes se refiere al hecho de que en cada paso del gradiente de descenso, vemos todos los ejemplos de entrenamiento. Entonces, en el gradiente de descenso, al calcular las derivadas, estamos calculando estas sumas. En cada gradiente de descenso separado, terminamos calculando algo como esto, que suma sobre nuestros ejemplos de entrenamiento de m. Entonces, el término gradiente de descenso por lotes se refiere al hecho de que vemos todo el lote de los ejemplos de entrenamiento y, una vez más, realmente no es un buen nombre, pero esto es como lo llama la gente de aprendizaje automático. Y resulta que, a veces, otras versiones de gradiente de descenso que no son versiones por lotes, y no ven todo el conjunto de entrenamiento pero sí los pequeños subconjuntos de los conjuntos entrenamiento en el momento. Más adelante también hablaremos sobre estas versiones. Pero por ahora, con el algoritmo que acabas de aprender, ahora que usamos un gradiente de descenso por lotes, sabrás cómo aplicar el gradiente de descenso o la regresión lineal. Eso es la regresión lineal con gradiente de descenso. Si ya has visto álgebra lineal avanzada, es posible que hayas tomado una clase de álgebra lineal avanzada, es posible que sepas que existe una solución para resolver de forma numérica el mínimo de la función de costos J, sin necesidad de usar un algoritmo iterativo como el gradiente de descenso. Más adelante en este curso también hablaremos sobre ese método que sólo despeja la función de costos J del mínimo sin necesidad de múltiples pasos de gradiente de descenso. El otro método se llama método de ecuaciones normales. Y en caso de que hayas escuchado sobre ese método, resulta que el gradiente de descenso es mejor para los conjuntos de datos más grandes que con el método de ecuaciones normales y, ahora que sabemos sobre el gradiente de descenso, podremos usarlo en muchos contextos diferentes, y también lo usaremos en muchos problemas de aprendizaje automático. Así que felicidades por aprender tu primer algoritmo de aprendizaje automático. Más adelante tendremos ejercicios en los que te pediré que apliques el gradiente de descenso y espero que estos algoritmos sean útiles. Pero primero quiero decirte que en en la siguiente serie de videos, en el primero quiero hablarte sobre una generalización del algoritmo de gradiente de descenso que la hará mucho más potente. Supongo que te hablaré de eso en el siguiente video.