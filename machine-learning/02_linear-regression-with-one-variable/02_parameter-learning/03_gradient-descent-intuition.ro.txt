In filmul anterior, am dat o definitie matematica a optimizarii pe baza de gradient. Hai sa studiem mai multe in acest video si sa intelegem mai bine ce face algoritmul si de ce au sens pasii algoritmului. Iata algoritmul de optimizare pe baza de gradient pe care l-am vazut data trecuta. Ca sa va amintiti, parametrul sau termenul alpha se numeste rata de invatare. El controleaza cat de mare este pasul pe care il face algoritmul cand actualizeaza parametrul theta. Al doilea termen de aici este termenul derivativ. Ceea ce imi propun in acest video este sa va dau o explicatie intuitiva despre ce face fiecare dintre acesti termeni si de ce, atunci cand sunt pusi laolalta, intregul proces de actualizare functioneaza. Pentru a va ajuta sa intelegeti, vreau sa folosesc un exemplu ceva mai simplu, in care dorim sa minimizam o functie cu un singur parametru. Deci, sa presupunem ca avem o functie cost J cu un singur parametru, theta1, cum am avut intr-unul din filmele anterioare. Theta1 este un numar real, da? Asta ca sa avem grafice unidimensionale, care sunt mai usor de interpretat. Hai sa incercam sa intelegem ce face optimizarea pe baza de gradient in cazul acestei functii. Sa zicem ca asta e functia. J(theta1) unde theta1 este un numar real. Bun, acum sa zicem ca am initializat algoritmul de optimizare pe baza de gradient cu theta1 aflat in aceasta locatie. Imaginati-va ca pornim din acest punct al functiei. Ce face algoritmul de gradient este sa se actualizeze. Theta1 este actualizat dupa relatia Theta1=Theta1 - alpha  dJ/dTheta1. Oh, si ca sa stiti, in dreapta este tot termenul derivativ, dar poate va intrebati de ce am schimbat acele simboluri de derivate partiale. Daca nu stiti care este diferenta intre simbolurile de derivate partiale si dJ/dTtheta, nu va faceti griji. Teoretic, in matematica, numim asta derivata partiala, sau o numim doar derivata, in functie de numarul de parametri ai functiei J, dar asta este numai o conventie matematica. Pentru scopul acestui curs, ganditi-va ca simbolurile de derivata partiala si dJ.dTheta sunt acelasi lucru. Nu va ingrijorati in ceea ce priveste diferentele care ar putea exista. Eu voi incerca sa folosesc notatiile matematice conventionale. Dar pentru scopurile noastre, aceste notatii sunt echivalente. Deci, hai sa vedem ce face aceasta ecuatie. Vom incerca sa calculam aceasta derivata - nu stiu daca va sunt cunoscute derivatele de la algebra. Dar ce face o derivata, in acest context, este sa "spuna" - "Hai sa luam tangenta in acest punct", adica linia asta dreapta, cea rosie, care atinge graficul functiei, si hai sa vedem care este panta dreptei rosii. Asta este semnificatia derivatei. Ea spune cat este panta dreptei (liniei) care e tangenta la graficul functiei, ok? Iar panta unei drepte este - stiti - inaltimea impartita la chestia asta orizontala. Dreapta asta are panta pozitiva, deci derivata este pozitiva. Si deci actualizarea lui Theta va fi Theta1 = Theta1 - alpha  (un numar pozitiv). Ok. Rata de invatare (alpha) este intotdeauna un numar pozitiv. Deci o sa am Theta1 = Theta1 minus ceva, ceea ce inseamna ca Theta1 se va muta catre stanga. Theta1 va scadea si observ ca este exact ce trebuie sa se intample, pentru ca de fapt am avansat in directia care ma duce catre minimul de aici. Deci pana acum, algoritmul de gradient pare sa faca ceea ce trebuie. Hai sa luam un alt exemplu. Sa consideram aceeasi functie J. Desenez aceeasi functie J(Theta1). Si acum sa zicem ca am initializat parametrul meu in alta parte, acolo, in stanga. Deci Theta1 este aici. Voi adauga acel punct pe suprafata. Acum, termenul derivativ dJ/dTheta1 cand este evaluat in acest punct, va indica directia catre dreapta. Panta acelei drepte. Termenul derivativ este panta acestei drepte. Dar dreapta aceasta este orientata in jos, adica functia are derivata negative, ceea ce inseamna panta negative in acel punct. Deci asta este  < = 0. Cand actualizez Theta, care este calculat ca Theta - alpha  (un numar negativ), de fapt voi creste Theta, corect? Pentru ca am minus un numar negativ, ceea ce inseamna ca adun ceva la Theta deci voi creste Theta. Si in felul asta vom incepe sa crestem Theta, ceea ce pare ca este exact lucrul pe care trebuia sa-l facem ca sa ne apropiem de minim. Sper ca asta explica intuitiv ce face termenul derivativ. Hai sa ne uitam acum la rata de invatare, alpha, si sa incercam sa ne dam seama care este rolul ei. Deci, asta este relatia de optimizare (coborare) prin gradient. Hai sa vedem ce se intampla daca alpha este fie prea mic, fie prea mare. In acest prim exemplu, vom vedea ce se intampla daca alpha este prea mic. 
Iata functia J(Theta). Hai sa incepem de aici. Daca alpha este prea mic, atunci ce voi face este sa inmultesc termenul derivativ cu un numar foarte mic. Si voi sfarsi prin a face pasi extrem de mici, cam asa. Ok, deci asta este un pas minuscul. Apoi, din acest nou punct, vom face un alt pas minuscul pentru ca alpha este prea mic. Hai sa facem un alt pas mititel. Si asa, daca rata de invatare este prea mica, o sa ajung sa fac acesti pasi mici, mititei, de copil mic, incercand sa ajung la minim. Si o sa am nevoie de foarte multi pasi ca sa ating minimul, astfle incat, daca alpha este prea mic, algoritmul va fi lent, pentru ca va face pasi foarte micuti. Va avea nevoie de foarte multi pasi inainte ca macar sa se apropie de minimul global. Acum, [...] Iata functia mea. Din moment ce alpha este prea mare, algoritmul de gradient poate depasi minimul si ar putea chiar sa nu fie convergent, sau sa fie divergent. [...] Deci daca alpha este prea mare, algoritmul face un pas urias, asa,