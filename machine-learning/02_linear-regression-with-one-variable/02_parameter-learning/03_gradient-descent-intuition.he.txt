בסרטון הקודם, נתנו הגדרה מתמטית לירידה בכיוון הגרדיאנט. בואו נצלול בסרט הזה עמוק יותר,  בכדי לקבל אינטואיציה טובה יותר על מה עושה האלגוריתם ומדוע הצעדים שמבצע האלגוריתם הם הגיוניים. הנה אלגוריתם הירידה בכיוון הגרדיאנט, כפי שראינו בפעם שעברה, ורק להזכיר, הפרמטר הזה או הגורם אלפא נקרא קצב הלימוד. והוא שולט בגודל הצעד שאנו עושים בכל עדכון של הפרמטר תטא-j. והגורם השני כאן הוא הנגזרת ומה שאני רוצה לעשות בהרצאה זו  הוא לתת לכם אינטואיציה לגבי מה עושה כל אחד משני הגורמים האלה, ומדוע כאשר מרכיבים אותם, כל העדכון הגיוני. כדי להעביר את האינטואיציות האלה, מה שאני רוצה לעשות הוא להשתמש בדוגמא קצת יותר פשוטה, בה אנחנו רוצים למזער פונקציה של פרמטר אחד בלבד. נניח שיש לנו פונקצית מחיר, j של פרמטר אחד בלבד, תטא-1, כמו שעשינו לפני כמה קטעי הרצאות, ותטא-1 הוא מספר ממשי. אז יש לנו גרף דו-ממדי, שקצת יותר פשוט לשרטט אותו. בואו ננסה להבין מה תעשה הירידה בכיוון הגרדיאנט לפונקציה הזו. אז בואו נאמר שזו הפונקציה שלי, J של תטא-1. זו הפונקציה שלי. ותטא-1 הוא מספר ממשי. בסדר? עתה, בואו נשים בשקופית את הירידה בכיוון הגרדיאנט עם תטא-1 כאן. דמיינו שאנחנו מתחילים בנקודה הזו על הפונקציה שלי. מה שהירידה בכיוון הגרדיאנט תעשה הוא לעדכן את תטא-1 , שייתעדכן לערך לתטא-1 פחות אלפא כפול נגרזת לפי תטא-1 של J של תטא-1, נכון? ואגב, הגורם הזה, הנגזרת, אם אתם תוהים מדוע שיניתי את הסמל הקודם של נגזרת חלקית לזה. אם אינכם יודעים מה ההבדל בין הסימון של נגזרת ונגזרת חלקית, תתעלמו מזה. מבחינה טכנית במתמטיקה קוראים לאחד נגזרת חלקית ולשני נגזרת, בהתאם למספר הפרמטרים בפונקציה j. אבל זה עניין טכני מתמטי. לצורך הרצאה זו חישבו על הסמלים של נגזרת חלקית ושל נגזרת כאילו הם אותו דבר בדיוק. ואל תדאגו לגבי ההבדל ביניהם. אני אנסה להשתמש בסימון מתמטי מדויק, אבל לענייננו שני הסימונים האלה הם בעצם אותו הדבר. אז בואו נראה מה תעשה המשוואה הזאת. עכשיו נחשב את הנגזרת הזו, אינני יודע כמה אתם מכירים נגזרות, אבל בעצם הנגזרת היא פשוט הישר המשיק לאותה נקודה, קו ישר כזה, הקו האדום, שבדיוק נוגע בפונקציה הזו, בואו נראה מה השיפוע של הישר האדום הזה. זהו מה שהנגזרת אומרת, מה השיפוע של הישר הזה שבדיוק משיק לפונקציה. אוקיי, השיפוע של ישר הוא הגובה הזה חלקי האורך האופקי הזה. עכשיו, לישר הזה יש שיפוע חיובי, ולכן הנגזרת היא חיובית. אז העדכון של תטא יהיה תטא-1 מקבל את תטא-1 פחות אלפא כפול איזשהו מספר חיובי. בסדר. אלפא, המשתנה שמגדיר את קצב הלמידה, הוא תמיד מספר חיובי. אז מה שיוצא זה שאנחנו נעדכן את תטא-1 לערך תטא-1 מינוס משהו חיובי. אז בסופו של דבר תטא-1 נע שמאלה. תטא-1 פוחת והולך, ואנחנו יכולים לראות שזה הדבר הנכון לעשות כי אנחנו רוצים לנוע בכיוון הזה. אתם רואים, להתקרב למינימום שנמצא שם. אז עד עכשיו, לפי הירידה בכיוון הגדריאנט אנחנו עושים את הדבר הנכון. בואו נראה עוד דוגמא. אז בואו ניקח את אותה פונקציה j, בואו ננסה להשתמש באותה פונקציה, J של תטא-1. ועכשיו, נניח שאתחלנו את הפרמטר שם משמאל. אז תטא-1 הוא כאן. אני מתבונן בנקודה הזו על המשטח. עכשיו הנגזרת לפי תטא-1 של J של תטא-1 כשמחשבים אותה בנקודה הזו, אנחנו מסתכלים על השיפוע של הישר הזה, אז הנגזרת היא השיפוע של הקו הזה. אבל הישר הזה הוא יורד, ולכן לישר הזה יש שיפוע שלילי. בסדר. או במילים אחרות זה אומר שלפונקציה הזו יש נגזרת שלילית, זו המשמעות של שיפוע שלילי בנקודה זו. אז החלק הזה קטן  או שווה ל-0, וכאשר נעדכן את תטא-1, יהיה לנו... תטא-1 פחות אלפא כפול מספר שלילי. את התוצאה היא תטא-1 פחות מספר שלילי כלומר בעצם תטא-1 תגדל, כי חיסור מספר שלילי פירושו תוספת של משהו חיובי לתטא-1. ופירוש הדבר הוא שאנחנו מגדילים את תטא-1 עד שהוא זז הנה, וזה אכן שוב נראה כמו הדבר הנכון לעשות כדי לנסות ולהתקרב למינימום. אז זו כל התיאוריה הזאת או האינטואיציה מאחורי מה שעושה גורם הנגזרת. עכשיו בואו נעיף מבט על אלפא, קצב הלימוד, ונראה מה הוא עושה. אז הנה הירידה בכיוון הגרדיאנט עם המשוואה הזו. בואו נסתכל מה קורה אם אלפא הוא או קטן מדי או גדול מדי. אז קודם כל מה קורה אם אלפא קטן מדי? אז הנה הפונקציה J שלי, J של תטא-1. בואו נתחיל כאן. אם אלפא קטן מדי, אז מה שנעשה הוא להכפיל את הנגזרת במספר קטן, מה שיגרום שתטא-1 ישתנה בכמות מאד קטנה. אוקיי, אז זה היה הצעד הראשון. מהנקודה החדשה הזה, נצטרך לעשות צעד נוסף. אבל אם אלפא קטן מדי, גם הצעד הבא יהיה קטן. אז אם קצב הלימוד הוא קטן מדי אנחנו פוסעים בצעדי תינוק קטנטנים בדרך למינימום. ונצטרך הרבה שלבים כדי להגיע למינימום ולכן אם אלפא הוא קטן מדי אז הירידה במדרון תהיה מאד איטית כי אנחנו צועדים צעדי תינוק קטנטנים ולכן נצטרך הרבה שלבים לפני שנגיע לאיזה שהוא מקום קרוב למינימום הגלובלי. מצד שני, מה קורה אם אלפא הוא גדול מדי? אז הנה הפונקציה j של תטא-1, ואם אלפא מדי גדול, אז הירידה במדרון עלולה להחטיא את המינימום ועלולה אפילו לא להצליח להתכנס, ואולי היא אפילו תתבדר, הנה מה שאני מתכוון. נניח שהנקודה הנוכחית שלנו היא שם, קרובה למדי למינימום. אז הנגזרת פונה ימינה, אבל אם אלפא הוא גדול מדי, אנחנו נעשה צעד ענק. נעשה צעד ענק כזה. אז אנחנו נעשה צעד ענק, ועכשיו הפונקציה שלנו מתחילה להתבדר. כי היא התחילה עם הערך הזה, ועכשיו, הערכים שלה מתחילים לעלות. עכשיו הנגזרת מצביעה שמאלה, מה שאומר שאני צריך להקטין את תטא-1. אבל אם אלפא, הלמידה, גדולה מדי, אנחנו עלולים לעשות צעד ענק שיוצא מכאן ועולה שמאלה הרבה. אז הגענו לכאן, נכון? ואם אלפא קצת גדול מדי, אנחנו יכולים לעשות עוד צעד ענק ולעלות עוד יותר לגובה ולהחטיא את המטרה שוב ושוב, עד שאתם כבר שמים לב שאני בעצם מתרחק יותר ויותר מהמינימום. אז אם אלפא גדולה מדי, התהליך עלול לא להתכנס או אפילו להתבדר. עכשיו, יש לי עוד שאלה בשבילכם. זה קצת מסובך וכשלמדתי את זה לראשונה באמת זה לקח לי הרבה זמן להבין את זה. מה אם הפרמטר תטא-1 כבר נמצא במינימום מקומי, מה אתה חושב שיעשה הצעד הבא של הירידה במדרון? בואו נניח שאתחלנו את תטא-1 למינימום מקומי. נניח שהערך הראשוני של תטא-1 הוא כבר באופטימום המקומי או במינימום המקומי. אז מה שקורה הוא שבאופטימום המקומי, הנגזרת תהיה שווה לאפס. אז בשביל המדרון הזה, זו נקודת המשיק, ולכן השיפוע של הקו הזה יהיה שווה לאפס ולכן הנגזרת הזו שווה לאפס. אז שלב העדכון בירידה בכיוון הגרדיאנט תשאיר את תטא-1 כמו שהוא כי אנחנו מפחיתים ממנו אלפא כפול אפס. אז מה שזה אומר הוא שאם אתה כבר נמצא במינימום המקומי תטא-1 יישאר ללא שינוי בשלב העדכון, תטא-1 שווה תטא-1. אז אם הפרמטרים שלך כבר במינימום מקומי אז הירידה בכיוון הגרדיאנט לא תעשה דבר לפרמטרים וזה מה שאנחנו רוצים, כי הוא משאיר את הפתרון במינימום המקומי. וזה גם מסביר מדוע הירידה בכיוון הגרדיאנט מתכנסת למינימום המקומי למרות שאלפא, קצב הלימוד, קבוע. הנה מה שאני מתכוון, בואו נסתכל בדוגמא. אז הנה פונקצית העלות j של תטא שאותה אני רוצה למזער ובואו נניח שאתחלתי את האלגוריתם שלי, אלגוריתם הירידה בכיוון הגרדיאנט, שם בנקודה בצבע מגנטה. אם אני עושה צעד אחד של ירידה בכיוון הגרדיאנט, אולי זה יביא אותי לנקודה הזו, כי הנגזרת באזור הזה כאן היא די תלולה. נכון? עכשיו, אני בנקודה הירוקה הזו, ואם אני עושה עוד צעד בירידה בכיוון הגרדיאנט, אתם יכולים לראות שהנגזרת, או המדרון, הוא פחות תלול בנקודה הירוקה לעומת הנקודה החיצונית בסגול. ככל שאני מתקרב למינימום, הנגזרת מתקרבת יותר ויותר לאפס, כשאני מתקרב למינימום. אז אחרי צעד אחד במדרון, הנגזרת החדשה שלי קצת יותר קטנה. וכשאעשה את הצעד הבא, הצעד שאעשה יהיה קצת יותר קטן מהקודם. עכשיו יש לנו עוד נקודה, נקודה אדומה, קרוב אפילו יותר למינימום הגלובלי והנגזרת כאן תהיה עוד יותר קטנה מאשר בנקודה הירוקה. אז בצעד הבא בירידה בכיוון הגרדיאנט הנגזרת עוד יותר קטנה ולכן גודל העדכון של תטא-1 יהיה עוד יותר קטן, ולכן הצעד יהיה יותר קטן. ותוך כדי התהליך של הירידה בכיוון הגרדיאנט, הצעדים באופן אוטומטי יהפכו לצעדים קטנים יותר. עד שבסופו של דבר הצעדים יהיו קטנים מאוד ובסוף התהליך יתכנס למינימום המקומי. אז לסיכום, בירידה בכיוון הגרדיאנט, ככל שאנו מתקרבים למינימום מקומי, הירידה תתקדם בצעדים קטנים יותר באופן אוטומטי. וזה משום שככל שאנו מתקרבים למינימום המקומי, ההגדרה של המינימום המקומי היא שהנגזרת בו משתווה לאפס. ככל שאנו מתקרבים למינימום מקומי, הביטוי הזה של הנגזרת ילך ויקטן באופן אוטומטי, ולכן הירידה תיעשה בצעדים קטנים יותר. כך שאין צורך להקטין את אלפא כל הזמן. אז זהו אלגוריתם הירידה בכיוון הגרדיאנט, ואפשר להשתמש בו כדי לנסות ולמזער כל פונקציית עלות J, לאו דווקא פונקצית העלות J שהגדרנו עבור רגרסיה ליניארית. בסרטון הבא, אנחנו הולכים לחזור לפונקצית העלות j שבה השתמשנו כפונקצית העלות של רגרסיה ליניארית, פונקצית העלות של ריבועי השגיאות בה השתמשנו קודם. ניקח את הירידה בכיוון הגרדיאנט ואת פונקצית העלות הריבועית הזו ונשלב אותן. וזה ייתן לנו את אלגוריתם הלמידה הראשון שלנו, אלגוריתם הרגרסיה הליניארית.