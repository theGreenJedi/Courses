1
00:00:00,520 --> 00:00:04,480
在以前的视频中我们谈到

2
00:00:04,480 --> 00:00:09,540
关于梯度下降算法

3
00:00:09,540 --> 00:00:14,280
梯度下降是很常用的算法 它不仅被用在线性回归上

4
00:00:14,280 --> 00:00:17,400
和线性回归模型、平方误差代价函数

5
00:00:17,400 --> 00:00:18,730
在这段视频中  我们要

6
00:00:20,800 --> 00:00:24,950
将梯度下降

7
00:00:24,950 --> 00:00:28,920
和代价函数结合

8
00:00:28,920 --> 00:00:34,210
在后面的视频中 我们将用到此算法 并将其应用于

9
00:00:34,210 --> 00:00:36,540
具体的拟合直线的线性回归算法里

10
00:00:36,540 --> 00:00:42,312
这就是

11
00:00:42,312 --> 00:00:47,820
我们在之前的课程里所做的工作

12
00:00:47,820 --> 00:00:51,275
这是梯度下降法

13
00:00:51,275 --> 00:00:59,810
这个算法你应该很熟悉

14
00:00:59,810 --> 00:01:04,060
这是线性回归模型

15
00:01:04,060 --> 00:01:07,710
还有线性假设和平方误差代价函数

16
00:01:07,710 --> 00:01:11,670
我们将要做的就是

17
00:01:13,020 --> 00:01:15,550
用梯度下降的方法

18
00:01:15,550 --> 00:01:21,400
来最小化平方误差代价函数

19
00:01:21,400 --> 00:01:23,520
为了

20
00:01:23,520 --> 00:01:26,190
使梯度下降 为了

21
00:01:27,290 --> 00:01:34,820
写这段代码

22
00:01:34,820 --> 00:01:43,280
我们需要的关键项

23
00:01:43,280 --> 00:01:47,830
是这里这个微分项

24
00:01:47,830 --> 00:01:50,782
所以.我们需要弄清楚

25
00:01:50,782 --> 00:01:53,190
这个偏导数项是什么

26
00:01:53,190 --> 00:01:56,570
并结合这里的

27
00:01:56,570 --> 00:02:00,310
代价函数J

28
00:02:00,310 --> 00:02:04,170
的定义

29
00:02:04,170 --> 00:02:06,940
就是这样

30
00:02:06,940 --> 00:02:12,064
一个求和项

31
00:02:12,064 --> 00:02:18,354
代价函数就是

32
00:02:18,354 --> 00:02:24,294
这个误差平方项

33
00:02:24,294 --> 00:02:27,114
我这样做 只是

34
00:02:27,114 --> 00:02:34,008
把定义好的代价函数

35
00:02:34,008 --> 00:02:37,440
插入了这个微分式

36
00:02:37,440 --> 00:02:41,720
再简化一下

37
00:02:41,720 --> 00:02:46,000
这等于是

38
00:02:46,000 --> 00:02:51,020
这一个求和项

39
00:02:51,020 --> 00:02:54,930
θ0 + θ1x(1) - y(i)

40
00:02:54,930 --> 00:02:59,510
θ0 + θ1x(1) - y(i)

41
00:02:59,510 --> 00:03:04,050
这一项其实就是

42
00:03:04,050 --> 00:03:08,100
我的假设的定义

43
00:03:08,100 --> 00:03:11,350
然后把这一项放进去

44
00:03:11,350 --> 00:03:13,390
实际上我们需要

45
00:03:14,750 --> 00:03:18,490
弄清楚这两个

46
00:03:18,490 --> 00:03:22,310
偏导数项是什么

47
00:03:23,310 --> 00:03:27,160
这两项分别是 j=0

48
00:03:27,160 --> 00:03:28,640
和j=1的情况

49
00:03:28,640 --> 00:03:32,728
因此我们要弄清楚

50
00:03:32,728 --> 00:03:38,380
θ0 和 θ1 对应的

51
00:03:39,390 --> 00:03:41,070
偏导数项是什么

52
00:03:43,080 --> 00:03:46,050
我只把答案写出来

53
00:03:47,160 --> 00:03:48,628
事实上 第一项可简化为

54
00:03:52,529 --> 00:03:56,804
1 / m 乘以求和式

55
00:03:56,804 --> 00:03:59,790
对所有训练样本求和

56
00:03:59,790 --> 00:04:05,730
求和项是 h(x(i))-y(i)

57
00:04:05,730 --> 00:04:11,420
而这一项

58
00:04:11,420 --> 00:04:15,230
对θ(1)的微分项

59
00:04:15,230 --> 00:04:19,265
得到的是这样一项

60
00:04:19,265 --> 00:04:22,250
对吧

61
00:04:24,290 --> 00:04:25,570
所以

62
00:04:25,570 --> 00:04:28,120
偏导数项

63
00:04:28,120 --> 00:04:31,862
从这个等式

64
00:04:31,862 --> 00:04:32,700
到下面的等式

65
00:04:32,700 --> 00:04:36,780
计算这些偏导数项需要一些多元微积分

66
00:04:36,780 --> 00:04:40,900
如果你掌握了微积分

67
00:04:40,900 --> 00:04:43,014
你可以随便自己推导这些

68
00:04:43,014 --> 00:04:45,480
然后你检查你的微分

69
00:04:45,480 --> 00:04:50,390
你实际上会得到我给出的答案

70
00:04:50,390 --> 00:04:55,220
但如果你

71
00:04:55,220 --> 00:05:00,190
不太熟悉微积分

72
00:05:03,230 --> 00:05:07,800
别担心

73
00:05:07,800 --> 00:05:09,490
你可以直接用这些

74
00:05:09,490 --> 00:05:16,620
已经算出来的结果

75
00:05:16,620 --> 00:05:22,295
你不需要掌握微积分

76
00:05:22,295 --> 00:05:26,465
或者别的东西

77
00:05:26,465 --> 00:05:30,445
来完成作业 你只需要会用梯度下降就可以

78
00:05:30,445 --> 00:05:33,155
在定义这些以后

79
00:05:33,155 --> 00:05:36,615
在我们算出

80
00:05:38,250 --> 00:05:45,910
这些微分项以后

81
00:05:45,910 --> 00:05:50,020
这些微分项

82
00:05:50,020 --> 00:05:54,220
实际上就是代价函数J的斜率

83
00:05:54,220 --> 00:05:56,370
现在可以将它们放回

84
00:05:56,370 --> 00:06:01,354
我们的梯度下降算法

85
00:06:01,354 --> 00:06:07,619
所以这就是专用于

86
00:06:07,619 --> 00:06:12,644
线性回归的梯度下降

87
00:06:12,644 --> 00:06:16,547
反复执行括号中的式子直到收敛

88
00:06:16,547 --> 00:06:21,060
θ0和θ1不断被更新

89
00:06:21,060 --> 00:06:26,845
都是加上一个-α/m

90
00:06:26,845 --> 00:06:31,510
乘上后面的求和项

91
00:06:31,510 --> 00:06:35,450
所以这里这一项

92
00:06:35,450 --> 00:06:39,780
所以这就是我们的线性回归算法

93
00:06:41,230 --> 00:06:42,380
对吧?

94
00:06:42,380 --> 00:06:46,370
当然

95
00:06:47,670 --> 00:06:52,760
这一项就是关于θ0的偏导数

96
00:06:52,760 --> 00:06:56,190
在上一张幻灯片中推出的

97
00:06:57,340 --> 00:07:02,430
而第二项

98
00:07:02,430 --> 00:07:06,520
这一项是刚刚的推导出的

99
00:07:08,200 --> 00:07:14,660
关于θ1的

100
00:07:14,660 --> 00:07:20,090
偏导数项

101
00:07:21,400 --> 00:07:25,800
提醒一下

102
00:07:25,800 --> 00:07:31,230
执行梯度下降时

103
00:07:31,230 --> 00:07:34,490
有一个细节要注意

104
00:07:34,490 --> 00:07:38,900
就是必须要

105
00:07:38,900 --> 00:07:43,350
同时更新θ0和θ1

106
00:07:43,350 --> 00:07:48,720
所以 让我们来看看梯度下降是如何工作的

107
00:07:48,720 --> 00:07:52,620
我们用梯度下降解决问题的

108
00:07:52,620 --> 00:07:57,510
一个原因是 它更容易得到局部最优值

109
00:07:57,510 --> 00:08:00,730
当我第一次解释梯度下降时

110
00:08:00,730 --> 00:08:04,310
我展示过这幅图

111
00:08:04,310 --> 00:08:08,880
在表面上

112
00:08:08,880 --> 00:08:13,850
不断下降

113
00:08:13,850 --> 00:08:17,760
并且我们知道了

114
00:08:17,760 --> 00:08:21,400
根据你的初始化 你会得到不同的局部最优解

115
00:08:21,400 --> 00:08:25,660
你知道.你可以结束了.在这里或这里。

116
00:08:25,660 --> 00:08:30,620
但是 事实证明

117
00:08:30,620 --> 00:08:34,175
用于线性回归的

118
00:08:34,175 --> 00:08:36,365
代价函数

119
00:08:36,365 --> 00:08:39,585
总是这样一个

120
00:08:39,585 --> 00:08:43,715
弓形的样子

121
00:08:43,715 --> 00:08:46,247
这个函数的专业术语是

122
00:08:46,247 --> 00:08:48,837
这是一个凸函数

123
00:08:48,837 --> 00:08:51,247
我不打算在这门课中

124
00:08:51,247 --> 00:08:55,207
给出凸函数的定义

125
00:08:55,207 --> 00:08:58,357
凸函数(convex function)

126
00:08:58,357 --> 00:09:03,497
但不正式的说法是

127
00:09:05,980 --> 00:09:09,550
它就是一个弓形的函数

128
00:09:09,550 --> 00:09:12,260
因此 这个函数

129
00:09:12,260 --> 00:09:15,510
没有任何局部最优解

130
00:09:15,510 --> 00:09:19,410
只有一个全局最优解

131
00:09:19,410 --> 00:09:22,270
并且无论什么时候

132
00:09:22,270 --> 00:09:25,870
你对这种代价函数

133
00:09:25,870 --> 00:09:29,730
使用线性回归

134
00:09:29,730 --> 00:09:33,020
梯度下降法得到的结果

135
00:09:33,020 --> 00:09:34,520
总是收敛到全局最优值 因为没有全局最优以外的其他局部最优点

136
00:09:34,520 --> 00:09:37,020
现在 让我们来看看这个算法的执行过程

137
00:09:37,020 --> 00:09:41,000
像往常一样

138
00:09:41,000 --> 00:09:46,420
这是假设函数的图

139
00:09:46,420 --> 00:09:50,140
还有代价函数J的图

140
00:09:50,140 --> 00:09:51,400
让我们来看看如何

141
00:09:51,400 --> 00:09:53,910
初始化参数的值

142
00:09:55,340 --> 00:10:00,430
通常来说

143
00:10:00,430 --> 00:10:04,990
初始化参数为零

144
00:10:04,990 --> 00:10:07,480
θ0和θ1都在零

145
00:10:07,480 --> 00:10:11,460
但为了展示需要

146
00:10:11,460 --> 00:10:14,510
在这个梯度下降的实现中

147
00:10:14,510 --> 00:10:17,900
我把θ0初始化为-900

148
00:10:17,900 --> 00:10:20,420
θ1初始化为-0.1