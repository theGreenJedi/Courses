1
00:00:00,520 --> 00:00:04,480
В предыдущих видео мы говорили об алгоритме градиентного

2
00:00:04,480 --> 00:00:09,540
спуска, о модели линейной

3
00:00:09,540 --> 00:00:14,280
регрессии и о функции затрат, определенной через сумму

4
00:00:14,280 --> 00:00:17,400
среднеквадратических отклонений.

5
00:00:17,400 --> 00:00:18,730
Теперь мы сведем вместе градиентный

6
00:00:20,800 --> 00:00:24,950
спуск и нашу

7
00:00:24,950 --> 00:00:28,920
функции стоимости, что даст

8
00:00:28,920 --> 00:00:34,210
нам алгоритм линейной регрессии

9
00:00:34,210 --> 00:00:36,540
для аппроксимации данных прямой линией.

10
00:00:36,540 --> 00:00:42,312
Напомню, к чему

11
00:00:42,312 --> 00:00:47,820
мы пришли в предыдущих видео.

12
00:00:47,820 --> 00:00:51,275
Это хорошо знакомый нам алгоритм градиентного

13
00:00:51,275 --> 00:00:59,810
спуска, а это наша модель линейной регрессии:

14
00:00:59,810 --> 00:01:04,060
линейная гипотеза и усредненная сумма квадратов отклонений, наша функция

15
00:01:04,060 --> 00:01:07,710
затрат.

16
00:01:07,710 --> 00:01:11,670
Я собираюсь применить

17
00:01:13,020 --> 00:01:15,550
градиентный спуск к нашей

18
00:01:15,550 --> 00:01:21,400
функции стоимости.

19
00:01:21,400 --> 00:01:23,520
Чтобы применить алгоритм

20
00:01:23,520 --> 00:01:26,190
и написать

21
00:01:27,290 --> 00:01:34,820
программу, в первую

22
00:01:34,820 --> 00:01:43,280
очередь нам нужно

23
00:01:43,280 --> 00:01:47,830
получить эту производную.

24
00:01:47,830 --> 00:01:50,782
Давайте посчитаем эту

25
00:01:50,782 --> 00:01:53,190
частную производную...

26
00:01:53,190 --> 00:01:56,570
подставим функцию J... то есть коэффициент... сумма

27
00:01:56,570 --> 00:02:00,310
от 1 до m квадрата ошибки...

28
00:02:00,310 --> 00:02:04,170
Пока я просто переписал

29
00:02:04,170 --> 00:02:06,940
сюда определение

30
00:02:06,940 --> 00:02:12,064
функции

31
00:02:12,064 --> 00:02:18,354
затрат,

32
00:02:18,354 --> 00:02:24,294
упростим

33
00:02:24,294 --> 00:02:27,114
еще немного...

34
00:02:27,114 --> 00:02:34,008
сумма от 1 до

35
00:02:34,008 --> 00:02:37,440
m...

36
00:02:37,440 --> 00:02:41,720
тета нулевое

37
00:02:41,720 --> 00:02:46,000
плюс тета первое

38
00:02:46,000 --> 00:02:51,020
на x(i) минус

39
00:02:51,020 --> 00:02:54,930
y(i) и все это в

40
00:02:54,930 --> 00:02:59,510
квадрате.

41
00:02:59,510 --> 00:03:04,050
Теперь я просто подставил

42
00:03:04,050 --> 00:03:08,100
формулу

43
00:03:08,100 --> 00:03:11,350
функции-гипотезы.

44
00:03:11,350 --> 00:03:13,390
И, собственно говоря,

45
00:03:14,750 --> 00:03:18,490
нам нужно получить

46
00:03:18,490 --> 00:03:22,310
эти частные производные для двух случаев: для j,

47
00:03:23,310 --> 00:03:27,160
равного 0, и для j, равного 1.

48
00:03:27,160 --> 00:03:28,640
То есть взять ее

49
00:03:28,640 --> 00:03:32,728
относительно тета нулевого

50
00:03:32,728 --> 00:03:38,380
и тета

51
00:03:39,390 --> 00:03:41,070
первого.

52
00:03:43,080 --> 00:03:46,050
Я просто напишу, чему они равны.

53
00:03:47,160 --> 00:03:48,628
В первом случае получится 1/m умножить на сумму по

54
00:03:52,529 --> 00:03:56,804
обучающему набору...

55
00:03:56,804 --> 00:03:59,790
h от x(i)

56
00:03:59,790 --> 00:04:05,730
минус y(i).

57
00:04:05,730 --> 00:04:11,420
А во втором частная

58
00:04:11,420 --> 00:04:15,230
производная по тета первому

59
00:04:15,230 --> 00:04:19,265
получится равна... то же самое, умножить на x(i).

60
00:04:19,265 --> 00:04:22,250
Отлично.

61
00:04:24,290 --> 00:04:25,570
Расчет этих частных

62
00:04:25,570 --> 00:04:28,120
производных, то есть получение этих

63
00:04:28,120 --> 00:04:31,862
выражений из этого, требует

64
00:04:31,862 --> 00:04:32,700
представления об анализе функций многих

65
00:04:32,700 --> 00:04:36,780
переменных.

66
00:04:36,780 --> 00:04:40,900
Если вы знакомы с

67
00:04:40,900 --> 00:04:43,014
многомерным анализом, можете сами провести выкладки и убедиться,

68
00:04:43,014 --> 00:04:45,480
что частные производные действительно получатся такими, как у

69
00:04:45,480 --> 00:04:50,390
меня.

70
00:04:50,390 --> 00:04:55,220
А если не знакомы, ничего

71
00:04:55,220 --> 00:05:00,190
страшного,

72
00:05:03,230 --> 00:05:07,800
можете просто использовать

73
00:05:07,800 --> 00:05:09,490
выведенные мной выражения.

74
00:05:09,490 --> 00:05:16,620
В домашнем задании анализ

75
00:05:16,620 --> 00:05:22,295
вам тоже не понадобится,

76
00:05:22,295 --> 00:05:26,465
для реализации градиентного спуска достаточно

77
00:05:26,465 --> 00:05:30,445
готовых производных.

78
00:05:30,445 --> 00:05:33,155
Итак, получив эти

79
00:05:33,155 --> 00:05:36,615
выражения, эти производные,

80
00:05:38,250 --> 00:05:45,910
соответствующие уклону графика

81
00:05:45,910 --> 00:05:50,020
функции стоимости J,

82
00:05:50,020 --> 00:05:54,220
мы можем

83
00:05:54,220 --> 00:05:56,370
подставить их в

84
00:05:56,370 --> 00:06:01,354
формулы алгоритма градиентного спуска.

85
00:06:01,354 --> 00:06:07,619
Вот формулы шага

86
00:06:07,619 --> 00:06:12,644
градиентного спуска для линейной

87
00:06:12,644 --> 00:06:16,547
регрессии, которые мы будем применять до схождения. Новое

88
00:06:16,547 --> 00:06:21,060
значение для тета нулевого

89
00:06:21,060 --> 00:06:26,845
и тета первого получаем, вычитая из старого

90
00:06:26,845 --> 00:06:31,510
производную, умноженную на альфа.

91
00:06:31,510 --> 00:06:35,450
Вот она.

92
00:06:35,450 --> 00:06:39,780
Итак, это алгоритм линейной регрессии.

93
00:06:41,230 --> 00:06:42,380
Верно?

94
00:06:42,380 --> 00:06:46,370
равен, соответственно,

95
00:06:47,670 --> 00:06:52,760
частной производной по тета нулевому,

96
00:06:52,760 --> 00:06:56,190
которую мы получили на предыдущем слайде.

97
00:06:57,340 --> 00:07:02,430
А во втором — частной

98
00:07:02,430 --> 00:07:06,520
производной по тета первому,

99
00:07:08,200 --> 00:07:14,660
тоже полученной на предыдущем

100
00:07:14,660 --> 00:07:20,090
слайде.

101
00:07:21,400 --> 00:07:25,800
На всякий случай напомню

102
00:07:25,800 --> 00:07:31,230
одну тонкость

103
00:07:31,230 --> 00:07:34,490
реализации градиентного спуска: обновлять

104
00:07:34,490 --> 00:07:38,900
тета нулевое и тета первое вам нужно

105
00:07:38,900 --> 00:07:43,350
одновременно.

106
00:07:43,350 --> 00:07:48,720
Посмотрим, как градиентный спуск работает.

107
00:07:48,720 --> 00:07:52,620
Если помните, у градиентного спуска была одна проблема: он

108
00:07:52,620 --> 00:07:57,510
может «застрять» в локальном экстремуме.

109
00:07:57,510 --> 00:08:00,730
Когда я показывал вам градиентный спуск, я пользовался этим

110
00:08:00,730 --> 00:08:04,310
графиком, по которому

111
00:08:04,310 --> 00:08:08,880
мы спускались, как с холма, и оказалось, что

112
00:08:08,880 --> 00:08:13,850
мы можем прийти в разные локальные

113
00:08:13,850 --> 00:08:17,760
экстремумы в

114
00:08:17,760 --> 00:08:21,400
зависимости от того, откуда начали.

115
00:08:21,400 --> 00:08:25,660
Вы можете прийти сюда или сюда.

116
00:08:25,660 --> 00:08:30,620
Но, как выясняется,

117
00:08:30,620 --> 00:08:34,175
функция стоимости для

118
00:08:34,175 --> 00:08:36,365
линейной регрессии всегда будет чашеобразной,

119
00:08:36,365 --> 00:08:39,585
как на этом

120
00:08:39,585 --> 00:08:43,715
графике.

121
00:08:43,715 --> 00:08:46,247
Математический термин для

122
00:08:46,247 --> 00:08:48,837
этого — выпуклая функция.

123
00:08:48,837 --> 00:08:51,247
Я не буду давать строгого определения выпуклой

124
00:08:51,247 --> 00:08:55,207
функции, говоря простым языком,

125
00:08:55,207 --> 00:08:58,357
выпуклая функция и

126
00:08:58,357 --> 00:09:03,497
функция с чашеобразным графиком, ну, условно

127
00:09:05,980 --> 00:09:09,550
чашеобразным, — это одно и то же.

128
00:09:09,550 --> 00:09:12,260
У такой функции нет

129
00:09:12,260 --> 00:09:15,510
никаких локальных экстремумов,

130
00:09:15,510 --> 00:09:19,410
кроме одного глобального.

131
00:09:19,410 --> 00:09:22,270
Таким образом, применив

132
00:09:22,270 --> 00:09:25,870
градиентный спуск к выпуклой функции, а при линейной

133
00:09:25,870 --> 00:09:29,730
регрессии она всегда выпуклая, вы всегда

134
00:09:29,730 --> 00:09:33,020
окажетесь в глобальном экстремуме,

135
00:09:33,020 --> 00:09:34,520
потому что других локальных экстремумов нет.

136
00:09:34,520 --> 00:09:37,020
Теперь посмотрим на алгоритм в действии.

137
00:09:37,020 --> 00:09:41,000
Как обычно, здесь у меня

138
00:09:41,000 --> 00:09:46,420
графики функции-гипотезы и функции

139
00:09:46,420 --> 00:09:50,140
стоимости J. Пусть

140
00:09:50,140 --> 00:09:51,400
начальные значения моих параметров соответствуют

141
00:09:51,400 --> 00:09:53,910
этой точке.

142
00:09:55,340 --> 00:10:00,430
Обычно мы задаем в

143
00:10:00,430 --> 00:10:04,990
качестве начальных

144
00:10:04,990 --> 00:10:07,480
значений нули.

145
00:10:07,480 --> 00:10:11,460
Но для иллюстрации этого

146
00:10:11,460 --> 00:10:14,510
случая я положил тета нулевое

147
00:10:14,510 --> 00:10:17,900
равным примерно

148
00:10:17,900 --> 00:10:20,420
900, а тета первое — примерно −0,1.