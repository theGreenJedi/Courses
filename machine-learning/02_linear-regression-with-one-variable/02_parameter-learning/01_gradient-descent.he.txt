הגדרנו קודם את פונקציית העלות j. בסרטון הזה, אני רוצה לספר לכם על אלגוריתם שנקרא ירידה בכיווןו הגרדיאנט, למזעור פונקצית העלות j. ירידה בכיוון הגרדיאנט הוא אלגוריתם כללי יותר, ומשמש לא רק ברגרסיה ליניארית. הוא בעצם נמצא בשימוש במקומות שונים בלמידה חישובית. ומאוחר יותר בכיתה נשתמש בו למזער גם פונקציות אחרות בנוסף לפונקצית העלות של רגרסיה ליניארית. אז בסרט הזה נדבר על הירידה בכיוון הגרדיאנט למזעור אילו שהן פונקציות j שרירותיות ובסרטונים יותר מאוחרים ניקח את האלגוריתם הזה ונחיל אותו באופן ספציפי על פונקצית העלות j שהגדרנו עבור רגרסיה ליניארית. אז הנה הצגת הבעיה. נניח שיש לנו איזו פונקציה (תטא-1, תטא-0)j, אולי פונקצית עלות של רגרסיה ליניארית או אולי איזו שהיא פונקציה אחרת שאנחנו רוצים למזער. ואנחנו רוצים להגדיר אלגוריתם שימזער את ערך הפונקציה כפונקציה של תטא-0 ותטא-1. אני אומר שוב, אלגוריתם הירידה בכיוון הגרדיאנט עובד על פונקציות בצורה יותר כללית. אז דמיינו שיש לנו פונקציה וזו פונקציה בצורה (... ,תטא-2 ,תטא-1, תטא-0)j עד נניח תטא-n, ואנחנו רוצים למזער את הפונקציה הזו (תטא-n ... ,תטא-0)j מעל תטא-0 עד תטא-n. וכאמור, מסתבר שהירידה בכיוון הגדדיאנט שלנו היא אלגוריתם לפתרון הבעיה הזו הכללית יותר. אבל למען הקיצור, למען התמציתיות של הכתיבה, אני אעסוק בפונקציה רק עם שני פרמטרים בכל שאר הוידאו הזה. הנה הרעיון של הירידה בכיוון הגרדיאנט. מה שאנחנו נעשה הוא להתחיל עם איזה שהוא ניחוש ראשוני עבור תטא-0 ותטא-1. לא ממש משנה מה הם, אבל בחירה מקובלת תהיה אפס גם עבור תטא-0 וגם עבור תטא-1, פשוט נאתחל אותם ב-0. ומה שנעשה בירידה בכיוון הגרדיאנט הוא שנשנה בצעדים קטנים את תטא-0 ותטא-1 כדי לנסות ולהקטין את ערך הפונקציה (תטא-1, תטא-0)j, עד ש(בתקווה) נתכנס למינימום או לפחות למינימום מקומי. אז בואו נמחיש מה עושה ירידה בכיוון הגרדיאנט. נניח שמנסים למזער את הפונקציה הזו. שימו לב לצירים, תטא-0 ותטא-1 על הצירים האופקיים וj על הציר האנכי ולכן גובהו של המשטח הוא ערך הפונקציה j ואת זה אנחנו רוצים למזער. אז אנחנו מתחילים עם תטא-0 ותטא-1 עם ערכים כלשהם. אז תחשבו שבחרנו ערכים כלשהם לתטא-0 ותטא-1 וזה מקביל לבחירת נקודה מסוימת על המשטח של הפונקציה. הערכים של תטא-0 ותטא-1 מגדירים איזו נקודה כאן. אתחלתי אותם ל(0,0) אבל לפעמים מאתחלים אותם גם לערכים אחרים. עכשיו, אני רוצה שתדמיינו שהתמונה הזו מראה נוף אמיתי. דמיינו את זה כנוף של איזו גינה עם דשא ושתי גבעות, ודמיינו שאנחנו עומדים פיזית בנקודה הזו על הגבעה, על הגבעה האדומה הקטנה הזאת בתוך הפארק. בירידה בכיוון הגרדיאנט, מה שאנחנו עושים הוא לעשות סיבוב של 360 מעלות, להסתכל סביב, ולשאול את עצמנו, לו עשיתי צעד קטן קטן בכיוון מסוים, בהנחה שאני רוצה לרדת במורד הגבעה כמה שיותר מהר, מהו הכיוון שבו אני צריך לעשות את הצעד הקטן הזה? אם אני רוצה לרדת, אני רוצה לרדת פיזית במורד הגבעה הזאת במהירות האפשרית. אז אם אתה עומד עכשיו על הגבעה, אתה מסתכל מסביב ומוצא שהכיוון הטוב ביותר לעשות צעד קטן במדרון הוא בערך לשם. אוקיי, ועכשיו אתה בנקודה החדשה הזו על הגבעה. אז אתה צריך שוב להסתכל מסביב ולהחליט מה הכיוון שעלי לפנות אליו עכשיו כדי לעשות עוד צעד קטן במורד הגבעה? ואם אתה עושה את זה ועושה עוד צעד, אתה עושה צעד נוסף בכיוון הנכון. ואז אתה ממשיך הלאה. מהנקודה החדשה הזו אתה מסתכל סביב סביב כדי להחליט איזה כיוון ייקח אותך כלפי מטה במדרון הכי מהר. קח צעד נוסף ועוד אחד וכן הלאה עד שאתה מתכנס למינימום המקומי הזה כאן למטה. לירידה בכיוון הגרדיאנט יש תכונה מעניינת. בפעם הראשונה שהרצנו את הירידה בכיוון הגרדיאנט, התחלנו בנקודה הזו כאן, נכון? התחלנו כאן. עכשיו דמיינו שהיינו מתחילים את הירידה כמה צעדים ימינה. תארו לעצמכם שהיינו מתחילים את הירידה בנקודה בפינה הימנית העליונה. לו חזרנו על התהליך, מתחילים מהנקודה, מסתכלים מסביב, עושים צעד קטן בכיוון הירידה התלולה ביותר, היינו עושים את זה. ואז מסתכלים מסביב, עושים עוד צעד, וכן הלאה. ולו התחלנו רק כמה צעדים ימינה, הירידה בכיוון הגרדיאנט היתה מביאה אותנו לאופטימום המקומי האחר הזה פה מימין. אז אם התחלנו בנקודה הזו, היינו יורדים במדרון לנקודת האופטימום המקומית הזו, אבל לו התחלנו במקום קצת שונה, היינו מגיעים לאופטימום מקומי שונה מאוד. וזה מאפיין של הירידה במדרון שנדבר עליו עוד קצת אחר כך. אז זו האינטואיציה בתמונות. בואו נסתכל על המתמטיקה. זוהי ההגדרה של אלגוריתם הירידה בכיוון הגרדיאנט. אנחנו נחזור ונבצע את זה עד להתכנסות, אנחנו נעדכן את הפרמטר תטא-j על ידי כך שניקח אותו ונחסר ממנו אלפא כפול הביטוי זה שכאן, בסדר? אז בואו נראה, יש במשוואה הזאת הרבה פרטים אז בואו נסביר חלקים ממנה. ראשית, הסימון זה כאן, =:, אנחנו נשתמש בו לסמן השמה או הקצאה, זהו אופרטור ההשמה. אז בקצרה, אם אני כותב a := b, מה שזה אומר למחשב זה לקחת את הערך של b ולשים אותו בתוך התא a תוך דריסת ערכו המקורי של a. זה אומר לגרום לa להיות שווה לערך של b, זה הפרוש של השמה. ואני גם יכול לכתוב a := a+1. המשמעות של זה היא להגדיל את הערך של a באחת, לקדם את a. לעומת זאת, אם אני משתמש רק בסימן השוויון וכותב a = b, משמעותו היא שאני טוען שזו האמת. בסדר?
אם אני כותב a = b, אז אני טוען שהערך של a שווה לערך של b, בסדר? אז בצד שמאל, זו הפעולה של השמה במחשב, שבה אנו משנים את הערך של a לערך חדש. בצד ימין זו טענה, אני טוען שהערכים של a ו-b הם שווים, אז אפשר לכתוב a := a+1, שפירושו הוסף 1 ל-a, אבל אף פעם לא נכתוב a = a+1 כי זה פשוט לא נכון. ל-a ול-a+1 לא יכול להיות אף פעם אותו ערך. בסדר? אז זהו החלק הראשון של ההגדרה. אלפא כאן הוא מספר שנקרא קצב הלימוד. אלפא בעצם שולט בגודל הצעד שאנו עושים בזמן הירידה במדרון. אלפא גדול מאוד מתאים לירידה בשיפוע מאוד אגרסיבי שבו אנו מנסים לצעוד צעדים ענקיים במדרון, ואלפא קטן מאוד משמעותו שאנו עושים צעדים מאוד קטנים במורד הגבעה. ואני אחזור לזה יותר מאוחר ואדבר על זה אחר כך, על איך מגדירים את אלפא וכן הלאה. ולבסוף, הביטוי הזה כאן הוא נגזרת. אני לא רוצה לדבר על זה עכשיו, אבל אחר כך אני אגדיר את הנגזרת הזו ואומר לכם בדיוק מהי, בסדר? חלק מכם מכירים חשבון דיפרנציאלי יותר מאחרים, אבל גם אם אינכם מכירים חשבון דיפרנציאלי, אל תדאגו. אני כבר אסביר לכם כל מה שאתם צריכים לדעת על הביטוי הזה. עכשיו, יש עוד פרט קטן בקשר בכיוון הגרדיאנט שהוא שבירידה בכיוון הגרדיאנט אנחנו מעדכנים את תטא-0 ותטא-1, נכון? העדכון הזה מתבצע עבור j = 0 ו-j = 1, אז אנחנו מעדכנים את תטא-0 וגם את תטא-1. והנקודה העדינה היא - כיצד מיישמים את הירידה בכיוון הגרדיאנט בביטוי זה, עבור משוואת העדכון הזו, אנו רוצים לעדכן בו זמנית את תטא-0 ואת תטא-1. מה שאני מתכוון הוא שבמשוואה הזאת, אנחנו מציבים לתוך תטא-0: תטא-0 מינוס משהו =: תטא-0 , וגם משנים את תטא-1:
תטא-1 מינוס משהו =: תטא-1. ואנחנו מחשבים את הביטוי בצד ימין, נכון? קודם כל צריך לחשב את הביטוי שתלוי בתטא-0 ובתטא-1, ואז יש לעדכן בו זמנית את תטא-0 ותטא-1. בסדר? אז אני אסביר מה שאני מתכוון בזה. זהו יישום נכון של ירידה בכיוון הגרדיאנט, שמבצע עדכון סימולטני. אנחנו מכניסים את הביטוי שעומד להיכנס לתוך תטא-0, קודם כל לתוך temp0  ו-temp1 כדי לא לשנות את תטא-0 ותטא-1 לפני החישוב, ואז לאחר שחישבנו את צד ימין ושמרנו את הערכים במשתנים temp0 ו-temp1, עכשיו אנחנו מעדכנים את תטא-0 ותטא-1 בו זמנית, זה יישום נכון. בניגוד לכך הנה יישום שגוי שלא עושה עדכון סימולטני. אז ביישום השגוי הזה, אנו מחשבים את temp0, ואז מעדכנים את תטא-0, ואז אנו מחשבים את temp1, ואז מעדכנים את תטא-1. וההבדל בין המימושים בצד ימין ובצד שמאל הוא שאם מסתכלים כאן למטה, כאן בשלב הזה, אם בזמן הפעולה הזו כבר עדכנו את תטא-0, אז משתמשים בערך החדש של תטא-0 כדי לחשב את הנגזרת הזו. אז זה נותן ערך אחר של temp1, לעומת זה שבצד שמאל, נכון? כי עכשיו משתמשים בערך החדש של תטא-0 במשוואה הזו. ולכן היישום בצד ימין הוא יישום שגוי של הירידה בכיוון הגרדיאנט. בסדר? אז אני לא רוצה להרחיב לגבי למה צריכים לעשות את העדכונים סימולטנית. פשוט מתברר שבדרך שבה מיישמים את הירידה בכיוון הגרדיאנט בדרך כלל, שעליה אדבר עוד מאוחר יותר, זה נראה יותר טבעי ליישם עדכון סימולטני. וכאשר אנשים מדברים על ירידה בכיוון הגרדיאנט, הם תמיד מתכוונים לעדכון סימולטני. אם מיישמים את הירידה בכיוון הגרדיאנט עם עדכון בלתי-סימולטני, גם הוא מן-הסתם יעבוד. אבל האלגוריתם הזה לא היה נכון. הוא לא האלגוריתם שאליו מתייחסים כירידה בכיוון הגרדיאנט, זה אלגוריתם אחר עם מאפיינים שונים. ומסיבות שונות הוא מתנהג בצורות קצת מוזרות יותר, ולכן מה שצריך לעשות באמת זה ליישם את הירידה בכיוון הגרדיאנט עם עדכון סימולטני. אז, אלה הקווים הכלליים של אלגוריתם הירידה בכיוון הגרדיאנט. בסרטון הבא, אנחנו ניכנס לפרטים של גורם הנגזרת, שכתבתי למעלה אבל לא באמת הגדרתי. ואם למדתם חשבון דיפרנציאלי ואם אתם מכירים נגזרות חלקיות, אז זהו בדיוק הגורם הזה, אבל אם אינכם יודעים את החומר הזה, אל דאגה. הסרטון הבא ייתן לכם את כל האינטואיציות וילמד אתכם כל מה שאתם צריכים לדעת כדי לחשב את הנגזרת, גם אם לא למדתם נגזרות, ואפילו אם לא ראיתם מעולם נגזרות חלקיות. ועם הסרטון הבא, אני מקווה שנוכל לתת לך את כל האינטואיציות שאתם צריכים כדי ליישם ירידה בכיוון הגרדיאנט.