1
00:00:00,000 --> 00:00:04,353
В предыдущем видео мы дали
математическое определение

2
00:00:04,353 --> 00:00:09,464
градиентного спуска.Теперь давайте
рассмотрим алгоритм подробнее, чтобы

3
00:00:09,464 --> 00:00:14,701
лучше понять, как он
работает и почему каждый шаг градиентного

4
00:00:14,701 --> 00:00:20,639
спуска имеет смысл.Вот определение
алгоритма градиентного спуска, которое мы дали в прошлый раз.Напомню, что вот

5
00:00:20,639 --> 00:00:26,427
этот коэффициент, альфа — это
скорость обучения.

6
00:00:26,427 --> 00:00:32,444
Он задает размер изменения параметра тета-j на
каждом шаге. Второй

7
00:00:32,444 --> 00:00:41,360
множитель — это
частная производная.В этом видео я хочу дать

8
00:00:41,360 --> 00:00:47,360
вам более глубокое понимание того,
как работают эти элементы и какой смысл они вместе придают

9
00:00:47,360 --> 00:00:53,077
изменению параметра.Чтобы
передать вам это понимание, я хочу немного

10
00:00:53,077 --> 00:00:58,460
упростить задачу
и снова минимизировать

11
00:00:58,460 --> 00:01:03,022
функцию одного переменного.То есть
наша функция J будет зависеть только от

12
00:01:03,022 --> 00:01:07,294
одного параметра, тета первого, так же,
как в одном из предыдущих видео.Где тета

13
00:01:07,294 --> 00:01:11,913
первое — действительное число.Тогда у нас будут плоские

14
00:01:11,913 --> 00:01:16,416
графики, которые проще воспринимать.И давайте
попробуем понять, как градиентный спуск работает с

15
00:01:16,416 --> 00:01:23,940
этой функцией.
[клик]Положим, моя функция выглядит так.

16
00:01:24,660 --> 00:01:31,696
J от тета первого, и тета первое — действительное
число.Допустим,

17
00:01:31,696 --> 00:01:39,202
я начал градиентный спуск вот с этого значения
тета первого.

18
00:01:39,202 --> 00:01:46,989
То есть мы начинаем идти с
этой точки на графике моей функции.Градиентный спуск

19
00:01:46,989 --> 00:01:56,935
будет изменятьзначение тета
первого на тета первое минус альфа умножить на d по d тета первому от функции J от

20
00:01:56,935 --> 00:02:04,694
тета первого, так?Краткое
отступление: видите это обозначение производной,

21
00:02:04,694 --> 00:02:11,636
да?Если вы
не поняли, почему я обозначаю ее по-другому по сравнению с прошлым

22
00:02:11,636 --> 00:02:16,132
разом, не как частную производную,если вы не
знаете разницы между частной производной и

23
00:02:16,132 --> 00:02:20,523
вот этим «d по d тета», ничего
страшного.Формально

24
00:02:20,523 --> 00:02:24,491
говоря, математики называют это
частной производной, а это — производной из-за

25
00:02:24,491 --> 00:02:28,299
того, что эти функции зависят
от разного числа переменных.

26
00:02:28,299 --> 00:02:32,428
Но для нас сейчас эта техническая
подробность несущественна,

27
00:02:32,428 --> 00:02:36,768
считайте эти
символы частной производной и «d по d тета» одним и тем же.Так что об отличиях

28
00:02:36,768 --> 00:02:41,056
можете сейчас не задумываться.
Я буду использовать строгое математическое

29
00:02:41,056 --> 00:02:45,190
обозначение,но для наших учебных
целей это одно и то же.

30
00:02:45,360 --> 00:02:49,627
Обратимся к тому, что делает
это присваивание.Нам нужно вычислить

31
00:02:49,627 --> 00:02:54,293
эту производную... Не знаю,
приходилось ли вам иметь дело с производными,но

32
00:02:54,293 --> 00:02:58,666
производная в этой точке, это, по сути, вот
что:Проведите

33
00:02:58,666 --> 00:03:02,877
касательную  в этой точке, просто
прямую красную

34
00:03:02,877 --> 00:03:06,976
линию. Посмотрим на наклон этой
линии.Он как раз

35
00:03:06,976 --> 00:03:11,352
соответствует производной.То есть
производная это наклон касательной, так, а

36
00:03:11,352 --> 00:03:15,563
наклон прямой это,
конечно же, вот

37
00:03:15,563 --> 00:03:20,789
эта высота, деленная на
длину этого горизонтального отрезка.Так.Наклон этой

38
00:03:20,789 --> 00:03:28,378
прямой положителен, значит,
производная в этой точке положительна.Значит, вычисляя новое

39
00:03:28,378 --> 00:03:36,258
значение тета первого, я вычту из тета
первого некоторое

40
00:03:36,258 --> 00:03:43,103
положительное число, умноженное на альфа.Верно?А альфа, параметр
скорости обучения, всегда положительное число.Так что я

41
00:03:43,103 --> 00:03:47,932
возьму тета первое и присвою новое значение, тета
первое минус что-то.В результате тета первое

42
00:03:47,932 --> 00:03:52,644
сдвинется влево.Я
уменьшил тета первое, и, как видно, это правильно,

43
00:03:52,644 --> 00:03:57,473
потому что я сделал шаг в
этом направлении,

44
00:03:57,473 --> 00:04:02,582
то есть в направлении минимума
функции.Так что, похоже,

45
00:04:02,582 --> 00:04:08,115
градиентный спуск делает что-то разумное.Разберем еще
один пример.Пусть у меня есть та же

46
00:04:08,115 --> 00:04:13,787
функция J.Я попробую нарисовать
такой же график для J от тета первого.И теперь предположим,

47
00:04:13,787 --> 00:04:19,181
что начальное значение моего
параметра вот здесь, слева.В этой

48
00:04:19,181 --> 00:04:24,161
точке.Ей соответствует вот
эта точка на поверхности.Теперь, чтобы взять

49
00:04:24,161 --> 00:04:29,567
производную, d по d тета первое от J, в этой
точке, мне нужно посмотретьна наклон вот

50
00:04:29,567 --> 00:04:35,035
этой прямой.Производная в этой
точке — это наклон этой прямой.Но эта прямая

51
00:04:35,035 --> 00:04:42,745
уходит вниз, а значит, ее наклон
отрицателен.Согласны?Или, другими

52
00:04:42,745 --> 00:04:48,718
словами, ее производная в этой точке
отрицательна, это

53
00:04:48,718 --> 00:04:54,770
то же самое, что отрицательный наклон касательной.То есть этот член меньше
или равен нулю,и когда я буду вычислять

54
00:04:54,770 --> 00:05:02,840
новое значение тета первого, я вычту альфа, умноженное
на некоторое отрицательное число.Вычитая из тета

55
00:05:02,840 --> 00:05:07,881
первого отрицательное число я
в результате увеличу значение параметра,

56
00:05:07,881 --> 00:05:13,106
верно?Потому что вычитая
отрицательное число я на самом

57
00:05:13,106 --> 00:05:17,900
деле добавляю что-то положительное, то
есть увеличиваю тета первое.Так что, начав

58
00:05:17,900 --> 00:05:23,002
отсюда, мы увеличим значение тета первого, что,
похоже, снова приближает

59
00:05:23,002 --> 00:05:28,335
меня к минимуму функции.Надеюсь, это
дало вам некоторое понимание сути вот этой

60
00:05:28,335 --> 00:05:33,874
производной.Теперь
давайте разберемся с коэффициентом альфа,

61
00:05:33,874 --> 00:05:39,956
скоростью обучения, и посмотрим,
как он работает.Вот шаг нашего алгоритма,

62
00:05:39,956 --> 00:05:46,641
правило изменения параметра.Давайте
посмотрим, что будет, если выбрать альфа слишком

63
00:05:46,641 --> 00:05:52,845
маленьким или слишком
большим.Для первого примера

64
00:05:52,845 --> 00:05:59,583
возьмем очень маленькое значение альфа.Вот график моей
функции J от тета первого.Начнем, к

65
00:05:59,583 --> 00:06:04,230
примеру, здесь.Если альфа мало,
значит, я буду домножать

66
00:06:04,230 --> 00:06:09,322
корректирующую величину на какое-то маленькое число.И шаг,
который я сделаю,

67
00:06:09,322 --> 00:06:13,841
будет крохотным.Да? Один шаг
вот сюда.Из этой точки я

68
00:06:13,841 --> 00:06:18,870
сделаю еще один шаг.Но, поскольку
альфа мало, то шаг снова будет

69
00:06:18,870 --> 00:06:25,342
крохотным.То есть, если скорость
обучения слишком маленькая,мне придется

70
00:06:25,342 --> 00:06:30,589
делать вот эти крошечные сдвиги в
сторону минимума, и может

71
00:06:30,589 --> 00:06:35,837
понадобиться очень много шагов,
прежде чем я дойду.При маленьком

72
00:06:35,837 --> 00:06:41,019
альфа спуск будет слишком медленным, потому что шаги будут слишком
маленькими.И потребуется

73
00:06:41,019 --> 00:06:45,829
сделать много шагов, чтобы оказаться хотя бы
примерно в районе глобального минимума.Теперь

74
00:06:45,829 --> 00:06:52,236
рассмотрим случай, когда альфа слишком большое.
Снова нарисую график функции J от тета.

75
00:06:52,236 --> 00:06:57,590
Оказывается, при слишком большом альфа, градиентный
спуск может проскочить точку минимума.

76
00:06:57,590 --> 00:07:03,362
Он может никогда не остановиться и может даже разойтись.Вот что я имею в виду.Положим, я начну с
вот этой точки, довольно близко к

77
00:07:03,362 --> 00:07:08,647
минимуму.Производная указывает направо, но при слишком большом значении альфа я сделаю слишком

78
00:07:08,686 --> 00:07:14,140
большой шаг.Например, вот такой огромный шаг.Согласны?Я сделал огромный шаг,

79
00:07:14,140 --> 00:07:20,051
и теперь значение моей функции затрат возросло.Я начинал с этого значения, а сейчас стоимость стала выше.Теперь моя

80
00:07:20,051 --> 00:07:25,190
производная указывает влево, так что мы уменьшим тета.Но поскольку скорость

81
00:07:25,190 --> 00:07:29,792
обучения велика, я в итоге снова сделаю огромный шаг,
и попадувот

82
00:07:29,792 --> 00:07:35,372
сюда.Так?При такой
большой скорости обучения на следующей

83
00:07:35,372 --> 00:07:41,034
итерации я тоже сделаю огромный шаг и снова проскочу минимум, и так
далее... и уже заметно, что я вообще-то

84
00:07:41,034 --> 00:07:46,765
все больше удаляюсь от минимального
значения.Так что, если альфа

85
00:07:46,765 --> 00:07:51,905
слишком велико, алгоритм может никогда не остановиться или
вообще разойтись.Теперь я хочу задать

86
00:07:51,905 --> 00:07:56,057
вам вопрос.Внимание, вопрос с подвохом.Когда я сам изучал
эту тему, мне потребовалось

87
00:07:56,057 --> 00:08:00,005
много времени, чтобы это понять.Что если
ваше начальное значение тета первого уже соответствует локальному

88
00:08:00,005 --> 00:08:04,106
минимуму?Что
тогда сделает шаг

89
00:08:04,106 --> 00:08:10,857
градиентного спуска?Итак, предположим, начальное
значение тета первого попало в локальный минимум.То есть,

90
00:08:10,857 --> 00:08:16,713
смотрите, пусть вот это ваше начальное значение
тета первого, и ему соответствует этот

91
00:08:16,713 --> 00:08:22,718
локальный экстремум, локальный минимум.В этом случае,
в локальном экстремуме, производная

92
00:08:22,718 --> 00:08:28,796
будет равна нулю.Поскольку
касательная в этой точке горизонтальна, ее

93
00:08:28,796 --> 00:08:35,528
наклон равен нулю, значит, этот член
выраженияравен

94
00:08:35,528 --> 00:08:40,941
нулю.Таким образом,
на шаге градиентного спуска, вы присваиваете тета первому величину тета первого

95
00:08:40,941 --> 00:08:46,284
минус альфа умножить на ноль.
То есть, если вы уже в локальном экстремуме, ваш

96
00:08:46,284 --> 00:08:51,222
параметр останется неизменным, вы
присвоите тета первому

97
00:08:51,222 --> 00:08:56,132
значение тета первого.
То есть в этом случае шаг

98
00:08:56,132 --> 00:09:00,694
алгоритма
градиентного спуска не делает ничего.Он не меняет

99
00:09:00,694 --> 00:09:05,257
значение параметра, хотя, по нашей задумке,
должен.Оставляет его в точке

100
00:09:05,257 --> 00:09:09,706
локального экстремума.Это также объясняет,
как градиентный спуск может остановиться в локальном минимуме даже

101
00:09:09,706 --> 00:09:14,326
при фиксированном коэффициенте скорости
обучения.Сейчас я поясню, что имею в виду.Посмотрим

102
00:09:14,326 --> 00:09:21,550
на такой пример.Вот функция
J от тета,которую я хочу

103
00:09:21,550 --> 00:09:26,811
минимизировать, и допустим, я инициализирую алгоритм градиентного
спуска вот в этой точке, отмеченной

104
00:09:26,811 --> 00:09:32,080
пурпурным.Один шаг
алгоритма приведет меня, скажем, сюда, потому

105
00:09:32,080 --> 00:09:36,941
что здесь моя производная
имеет довольно большой уклон, так?

106
00:09:36,941 --> 00:09:42,051
Теперь я нахожусь в этой зеленой точке,
и на следующем шаге

107
00:09:42,051 --> 00:09:47,036
спуска моя производная,
то есть наклон касательной,

108
00:09:47,036 --> 00:09:51,959
будет меньше, чем в пурпурной точке,
верно?Поскольку по мере

109
00:09:51,959 --> 00:09:56,883
приближения к минимуму значение
производной приближается к нулю.

110
00:09:56,883 --> 00:10:01,794
Так что я сделал шаг градиентного спуска, и теперь моя производная
стала меньше.

111
00:10:01,794 --> 00:10:06,635
Сделаю еще один
шаг.Из зеленой точки

112
00:10:06,635 --> 00:10:11,598
он будет немного меньше, чем тот, что я
сделал из пурпурной.Теперь я

113
00:10:11,598 --> 00:10:16,038
нахожусь в этой точке, красной,
еще ближе к глобальному минимуму,

114
00:10:16,038 --> 00:10:21,229
и производная здесь еще меньше, чем была
в зеленой точке.И на следующем

115
00:10:21,229 --> 00:10:26,420
шаге градиентного спуска из-за
значения производной величина изменения

116
00:10:26,420 --> 00:10:31,360
тета первого будет еще меньше,
то есть мы сделаем вот такой

117
00:10:31,360 --> 00:10:39,145
небольшой шаг. И по мере работы
алгоритмамы будем автоматически

118
00:10:39,145 --> 00:10:46,343
делать все меньшие шаги,
до тех пор пока

119
00:10:46,343 --> 00:10:52,737
не окажемся в точке
минимума.Вкратце повторю, о чем мы говорили.Алгоритм

120
00:10:52,737 --> 00:10:57,716
градиентного спуска по мере приближения к
локальному минимуму будет делат

121
00:10:57,716 --> 00:11:02,634
ь все меньшие
шаги, поскольку, чем ближе

122
00:11:02,634 --> 00:11:07,122
к локальному минимуму, тем... по
определению, в локальном минимуме значение производной равно нулю...Так

123
00:11:07,122 --> 00:11:12,408
вот, по мере приближения к локальному
минимуму этот член выражения будет

124
00:11:12,408 --> 00:11:16,957
становиться все меньше, и градиентный спуск на каждом шаге будет все
меньше изменять параметр.Таким образом,

125
00:11:16,957 --> 00:11:21,506
нет нужды уменьшать альфа
по мере работы

126
00:11:21,506 --> 00:11:26,258
алгоритма.Итак, это алгоритм
градиентного спуска, с помощью

127
00:11:26,258 --> 00:11:30,713
которого можно минимизировать любую
функцию затрат, не только функцию затрат J, которая

128
00:11:30,713 --> 00:11:34,738
возникла в задаче линейной регрессии.В следующем
видео мы вернем

129
00:11:34,738 --> 00:11:38,549
функции J вид функции затрат из задачи
линейной регрессии,функции

130
00:11:38,549 --> 00:11:43,057
среднеквадратических отклонений, построенной
ранее,и применим к ней

131
00:11:43,057 --> 00:11:47,351
алгоритм
градиентного спуска.В результате мы

132
00:11:47,351 --> 00:11:50,948
получим наш первый обучающий алгоритм,
алгоритм линейной регрессии.