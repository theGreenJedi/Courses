1
00:00:00,000 --> 00:00:04,353
在之前的视频中  我们给出了一个数学上关于梯度

2
00:00:04,353 --> 00:00:09,464
下降的定义 本次视频我们更深入研究一下 更直观地感受一下这个

3
00:00:09,464 --> 00:00:14,701
算法是做什么的 以及梯度下降算法的更新过程有什么意义

4
00:00:14,701 --> 00:00:20,639
这是我们上次视频中看到的梯度下降算法

5
00:00:20,639 --> 00:00:26,427
提醒一下 这个参数 α  术语称为学习速率

6
00:00:26,427 --> 00:00:32,444
它控制我们以多大的幅度更新这个参数θj.

7
00:00:32,444 --> 00:00:41,360
第二部分是导数项   而我在这个视频中要做的就是

8
00:00:41,360 --> 00:00:47,360
给你一个更直观的认识 这两部分有什么用 以及 为什么当把

9
00:00:47,360 --> 00:00:53,077
这两部分放一起时 整个更新过程是有意义的  为了更好地让你明白

10
00:00:53,077 --> 00:00:58,460
我要做是用一个稍微简单的例子 比如我们想最小化的那个

11
00:00:58,460 --> 00:01:03,022
函数只有一个参数的情形 所以 假如我们有一个代价函数J

12
00:01:03,022 --> 00:01:07,294
只有一个参数 θ1  就像我们前几次视频中讲的

13
00:01:07,294 --> 00:01:11,913
θ1是一个实数 对吧？那么我们可以画出一维的曲线

14
00:01:11,913 --> 00:01:16,416
看起来很简单 让我们试着去理解 为什么梯度下降法

15
00:01:16,416 --> 00:01:23,940
会在这个函数上起作用 所以 假如这是我的函数

16
00:01:24,660 --> 00:01:31,696
关于θ1的函数J θ1是一个实数 对吧？

17
00:01:31,696 --> 00:01:39,202
现在我们已经对这个点上用于梯度下降法的θ1 进行了初始化

18
00:01:39,202 --> 00:01:46,989
想象一下在我的函数图像上 从那个点出发 那么梯度下降

19
00:01:46,989 --> 00:01:56,935
要做的事情是不断更新 θ1等于θ1减α倍的

20
00:01:56,935 --> 00:02:04,694
d/dθ1J(θ1)这个项 对吧？哦 顺便插一句 你知道

21
00:02:04,694 --> 00:02:11,636
这个微分项是吧？可能你想问为什么我改变了符号

22
00:02:11,636 --> 00:02:16,132
之前用的是偏导数的符号 如果你不知道偏导数的符号

23
00:02:16,132 --> 00:02:20,523
和d/dθ之间的区别是什么 不用担心 从技术上讲

24
00:02:20,523 --> 00:02:24,491
在数学中 我们称这是一个偏导数 这是一个导数

25
00:02:24,491 --> 00:02:28,299
这取决于函数J的参数数量 但是这是一个

26
00:02:28,299 --> 00:02:32,428
数学上的区别 就本课的目标而言 可以默认为

27
00:02:32,428 --> 00:02:36,768
这些偏导数符号 和d/dθ1是完全一样的东西 不用担心

28
00:02:36,768 --> 00:02:41,056
是否存在任何差异 我会尽量使用数学上的

29
00:02:41,056 --> 00:02:45,190
精确的符号 但就我们的目的而言 这些符号是没有区别的

30
00:02:45,360 --> 00:02:49,627
好的 那么我们来看这个方程 我们要计算

31
00:02:49,627 --> 00:02:54,293
这个导数 我不确定之前你是否在微积分中学过导数

32
00:02:54,293 --> 00:02:58,666
但对于这个问题 求导的目的 基本上可以说

33
00:02:58,666 --> 00:03:02,877
取这一点的切线 就是这样一条红色的直线

34
00:03:02,877 --> 00:03:06,976
刚好与函数相切于这一点 让我们看看这条红色直线的斜率

35
00:03:06,976 --> 00:03:11,352
其实这就是导数 也就是说 直线的斜率 也就是这条

36
00:03:11,352 --> 00:03:15,563
刚好与函数曲线相切的这条直线 这条直线的斜率正好是

37
00:03:15,563 --> 00:03:20,789
这个高度除以这个水平长度 现在 这条线有

38
00:03:20,789 --> 00:03:28,378
一个正斜率 也就是说它有正导数 因此 我得到的新的θ

39
00:03:28,378 --> 00:03:36,258
θ1更新后等于θ1减去一个正数乘以α.

40
00:03:36,258 --> 00:03:43,103
α 也就是学习速率也是一个正数 所以

41
00:03:43,103 --> 00:03:47,932
我要使θ1减去一个东西

42
00:03:47,932 --> 00:03:52,644
所以相当于我将θ1向左移 使θ1变小了 我们可以看到

43
00:03:52,644 --> 00:03:57,473
这么做是对的 因为实际上我往这个方向移动

44
00:03:57,473 --> 00:04:02,582
确实让我更接近那边的最低点 所以 梯度下降到目前为止似乎

45
00:04:02,582 --> 00:04:08,115
是在做正确的事 让我们来看看另一个例子 让我们用同样的函数J

46
00:04:08,115 --> 00:04:13,787
同样再画出函数J(θ1)的图像 而这次

47
00:04:13,787 --> 00:04:19,181
我们把参数初始化到左边这点 所以θ1在这里

48
00:04:19,181 --> 00:04:24,161
同样把这点对应到曲线上 现在 导数项d/dθ1J(θ1)

49
00:04:24,161 --> 00:04:29,567
在这点上计算时 看上去会是这样

50
00:04:29,567 --> 00:04:35,035
这条线的斜率 这个导数是这条线的斜率

51
00:04:35,035 --> 00:04:42,745
但是这条线向下倾斜 所以这条线具有负斜率 对吧？

52
00:04:42,745 --> 00:04:48,718
或者说 这个函数有负导数 也就意味着在那一点上有负斜率

53
00:04:48,718 --> 00:04:54,770
因此 这个导数项小于等于零  所以 当我更新θ时

54
00:04:54,770 --> 00:05:02,840
θ被更新为θ减去α乘以一个负数 因此我是在用

55
00:05:02,840 --> 00:05:07,881
θ1减去一个负数 这意味着我实际上是在增加θ1

56
00:05:07,881 --> 00:05:13,106
对不对？因为这是减去一个负数 意味着给θ加上一个数

57
00:05:13,106 --> 00:05:17,900
这就意味着最后我实际上增加了θ的值 因此 我们将

58
00:05:17,900 --> 00:05:23,002
从这里开始 增加θ 似乎这也是我希望得到的 也就是

59
00:05:23,002 --> 00:05:28,335
让我更接近最小值了 所以 我希望这样很直观地给你解释了

60
00:05:28,335 --> 00:05:33,874
导数项的意义 让我们接下来再看一看学习速率α

61
00:05:33,874 --> 00:05:39,956
我们来研究一下它有什么用 这就是我梯度下降法的

62
00:05:39,956 --> 00:05:46,641
更新规则 就是这个等式 让我们来看看如果α 太小或 α 太大 

63
00:05:46,641 --> 00:05:52,845
会出现什么情况 这第一个例子

64
00:05:52,845 --> 00:05:59,583
α太小会发生什么呢 这是我的函数J(θ)

65
00:05:59,583 --> 00:06:04,230
就从这里开始 如果α太小了 那么我要做的是要去

66
00:06:04,230 --> 00:06:09,322
用一个比较小的数乘以更新的值 所以最终 它就像一个小宝宝的步伐

67
00:06:09,322 --> 00:06:13,841
这是一步  然后从这个新的起点开始

68
00:06:13,841 --> 00:06:18,870
迈出另一步 但是由于α 太小 因此只能迈出另一个

69
00:06:18,870 --> 00:06:25,342
小碎步 所以如果我的学习速率太小 结果就是

70
00:06:25,342 --> 00:06:30,589
只能这样像小宝宝一样一点点地挪动 去努力接近最低点

71
00:06:30,589 --> 00:06:35,837
这样就需要很多步才能到达最低点 所以如果α 太小的话

72
00:06:35,837 --> 00:06:41,019
可能会很慢 因为它会一点点挪动 它会需要

73
00:06:41,019 --> 00:06:45,829
很多步才能到达全局最低点

74
00:06:45,829 --> 00:06:52,236
那么如果α 太大又会怎样呢 这是我的函数J(θ)

75
00:06:52,236 --> 00:06:57,590
如果α 太大 那么梯度下降法可能会越过最低点

76
00:06:57,590 --> 00:07:03,362
甚至可能无法收敛 我的意思是 比如我们从这个点开始

77
00:07:03,362 --> 00:07:08,647
实际上这个点已经接近最低点  因此导数指向右侧 但如果α 太大的话

78
00:07:08,686 --> 00:07:14,140
我会迈出很大一步 也许像这样巨大的一步 对吧？所以我最终迈出了一大步

79
00:07:14,140 --> 00:07:20,051
现在 我的代价函数变得更糟 因为离这个最低点越来越远

80
00:07:20,051 --> 00:07:25,190
现在我的导数指向左侧 实际上在减小θ 但是你看 如果我的学习速率过大

81
00:07:25,190 --> 00:07:29,792
我会移动一大步 从这点一下子又到那点了

82
00:07:29,792 --> 00:07:35,372
对吗？如果我的学习率太大 下一次迭代

83
00:07:35,372 --> 00:07:41,034
又移动了一大步 越过一次 又越过一次 一次次越过最低点 直到你发现

84
00:07:41,034 --> 00:07:46,765
实际上 离最低点越来越远 所以 如果α太大

85
00:07:46,765 --> 00:07:51,905
它会导致无法收敛 甚至发散 现在 我还有一个问题

86
00:07:51,905 --> 00:07:56,057
这问题挺狡猾的 当我第一次学习这个地方时

87
00:07:56,057 --> 00:08:00,005
我花了很长一段时间才理解这个问题 如果我们预先把θ1

88
00:08:00,005 --> 00:08:04,106
放在一个局部的最低点 你认为下一步梯度下降法会怎样工作？

89
00:08:04,106 --> 00:08:10,857
所以假设你将θ1初始化在局部最低点

90
00:08:10,857 --> 00:08:16,713
假设这是你的θ1的初始值 在这儿 它已经在一个局部的

91
00:08:16,713 --> 00:08:22,718
最优处或局部最低点 结果是局部最优点的导数

92
00:08:22,718 --> 00:08:28,796
将等于零 因为它是那条切线的斜率

93
00:08:28,796 --> 00:08:35,528
而这条线的斜率将等于零 因此 此导数项等于0

94
00:08:35,528 --> 00:08:40,941
因此 在你的梯度下降更新过程中 你有一个θ1

95
00:08:40,941 --> 00:08:46,284
然后用θ1 减α 乘以0来更新θ1  所以这意味着什么

96
00:08:46,284 --> 00:08:51,222
这意味着你已经在局部最优点 它使得θ1不再改变

97
00:08:51,222 --> 00:08:56,132
也就是新的θ1等于原来的θ1  因此 如果你的参数已经处于

98
00:08:56,132 --> 00:09:00,694
局部最低点 那么梯度下降法更新其实什么都没做 它不会改变参数的值

99
00:09:00,694 --> 00:09:05,257
这也正是你想要的 因为它使你的解始终保持在

100
00:09:05,257 --> 00:09:09,706
局部最优点 这也解释了为什么即使学习速率α 保持不变时

101
00:09:09,706 --> 00:09:14,326
梯度下降也可以收敛到局部最低点 我想说的是这个意思

102
00:09:14,326 --> 00:09:21,550
我们来看一个例子 这是代价函数J(θ)

103
00:09:21,550 --> 00:09:26,811
我想找到它的最小值 首先初始化我的梯度下降算法

104
00:09:26,811 --> 00:09:32,080
在那个品红色的点初始化 如果我更新一步梯度下降

105
00:09:32,080 --> 00:09:36,941
也许它会带我到这个点 因为这个点的导数是相当陡的

106
00:09:36,941 --> 00:09:42,051
现在 在这个绿色的点 如果我再更新一步

107
00:09:42,051 --> 00:09:47,036
你会发现我的导数 也即斜率 是没那么陡的

108
00:09:47,036 --> 00:09:51,959
相比于在品红点 对吧？因为随着我接近最低点

109
00:09:51,959 --> 00:09:56,883
我的导数越来越接近零

110
00:09:56,883 --> 00:10:01,794
所以 梯度下降一步后 新的导数会变小一点点

111
00:10:01,794 --> 00:10:06,635
然后我想再梯度下降一步 在这个绿点我自然会用一个稍微

112
00:10:06,635 --> 00:10:11,598
跟刚才在那个品红点时比 再小一点的一步

113
00:10:11,598 --> 00:10:16,038
 现在到了新的点 红色点 更接近全局最低点了

114
00:10:16,038 --> 00:10:21,229
因此这点的导数会比在绿点时更小 所以

115
00:10:21,229 --> 00:10:26,420
 我再进行一步梯度下降时 我的导数项是更小的

116
00:10:26,420 --> 00:10:31,360
θ1更新的幅度就会更小

117
00:10:31,360 --> 00:10:39,145
所以你会移动更小的一步 像这样 随着梯度下降法的运行

118
00:10:39,145 --> 00:10:46,343
 你移动的幅度会自动变得越来越小 直到最终移动幅度非常小

119
00:10:46,343 --> 00:10:52,737
你会发现 已经收敛到局部极小值 所以回顾一下

120
00:10:52,737 --> 00:10:57,716
在梯度下降法中 当我们接近局部最低点时 梯度下降法会自动采取

121
00:10:57,716 --> 00:11:02,634
更小的幅度 这是因为当我们接近局部最低点时

122
00:11:02,634 --> 00:11:07,122
很显然在局部最低时导数等于零 所以当我们

123
00:11:07,122 --> 00:11:12,408
接近局部最低时 导数值会自动变得越来越小

124
00:11:12,408 --> 00:11:16,957
所以梯度下降将自动采取较小的幅度

125
00:11:16,957 --> 00:11:21,506
这就是梯度下降的做法 所以实际上没有必要再另外减小α

126
00:11:21,506 --> 00:11:26,258
这就是梯度下降算法 你可以用它来最小化

127
00:11:26,258 --> 00:11:30,713
最小化任何代价函数J 不只是线性回归中的代价函数J

128
00:11:30,713 --> 00:11:34,738
在接下来的视频中 我们要用代价函数J

129
00:11:34,738 --> 00:11:38,549
回到它的本质 线性回归中的代价函数

130
00:11:38,549 --> 00:11:43,057
也就是我们前面得出的平方误差函数 结合梯度下降法

131
00:11:43,057 --> 00:11:47,351
以及平方代价函数 我们会得出第一个机器学习算法

132
00:11:47,351 --> 00:11:50,948
即线性回归算法
【教育无边界字幕组】翻译：10号少年  校对：Femtoyue  审核：所罗门捷列夫