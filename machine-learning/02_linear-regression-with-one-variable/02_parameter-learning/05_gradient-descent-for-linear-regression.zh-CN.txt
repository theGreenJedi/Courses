在以前的视频中我们谈到 关于梯度下降算法 梯度下降是很常用的算法 它不仅被用在线性回归上 和线性回归模型、平方误差代价函数 在这段视频中  我们要 将梯度下降 和代价函数结合 在后面的视频中 我们将用到此算法 并将其应用于 具体的拟合直线的线性回归算法里 这就是 我们在之前的课程里所做的工作 这是梯度下降法 这个算法你应该很熟悉 这是线性回归模型 还有线性假设和平方误差代价函数 我们将要做的就是 用梯度下降的方法 来最小化平方误差代价函数 为了 使梯度下降 为了 写这段代码 我们需要的关键项 是这里这个微分项 所以.我们需要弄清楚 这个偏导数项是什么 并结合这里的 代价函数J 的定义 就是这样 一个求和项 代价函数就是 这个误差平方项 我这样做 只是 把定义好的代价函数 插入了这个微分式 再简化一下 这等于是 这一个求和项 θ0 + θ1x(1) - y(i) θ0 + θ1x(1) - y(i) 这一项其实就是 我的假设的定义 然后把这一项放进去 实际上我们需要 弄清楚这两个 偏导数项是什么 这两项分别是 j=0 和j=1的情况 因此我们要弄清楚 θ0 和 θ1 对应的 偏导数项是什么 我只把答案写出来 事实上 第一项可简化为 1 / m 乘以求和式 对所有训练样本求和 求和项是 h(x(i))-y(i) 而这一项 对θ(1)的微分项 得到的是这样一项 对吧 所以 偏导数项 从这个等式 到下面的等式 计算这些偏导数项需要一些多元微积分 如果你掌握了微积分 你可以随便自己推导这些 然后你检查你的微分 你实际上会得到我给出的答案 但如果你 不太熟悉微积分 别担心 你可以直接用这些 已经算出来的结果 你不需要掌握微积分 或者别的东西 来完成作业 你只需要会用梯度下降就可以 在定义这些以后 在我们算出 这些微分项以后 这些微分项 实际上就是代价函数J的斜率 现在可以将它们放回 我们的梯度下降算法 所以这就是专用于 线性回归的梯度下降 反复执行括号中的式子直到收敛 θ0和θ1不断被更新 都是加上一个-α/m 乘上后面的求和项 所以这里这一项 所以这就是我们的线性回归算法 对吧? 当然 这一项就是关于θ0的偏导数 在上一张幻灯片中推出的 而第二项 这一项是刚刚的推导出的 关于θ1的 偏导数项 提醒一下 执行梯度下降时 有一个细节要注意 就是必须要 同时更新θ0和θ1 所以 让我们来看看梯度下降是如何工作的 我们用梯度下降解决问题的 一个原因是 它更容易得到局部最优值 当我第一次解释梯度下降时 我展示过这幅图 在表面上 不断下降 并且我们知道了 根据你的初始化 你会得到不同的局部最优解 你知道.你可以结束了.在这里或这里。 但是 事实证明 用于线性回归的 代价函数 总是这样一个 弓形的样子 这个函数的专业术语是 这是一个凸函数 我不打算在这门课中 给出凸函数的定义 凸函数(convex function) 但不正式的说法是 它就是一个弓形的函数 因此 这个函数 没有任何局部最优解 只有一个全局最优解 并且无论什么时候 你对这种代价函数 使用线性回归 梯度下降法得到的结果 总是收敛到全局最优值 因为没有全局最优以外的其他局部最优点 现在 让我们来看看这个算法的执行过程 像往常一样 这是假设函数的图 还有代价函数J的图 让我们来看看如何 初始化参数的值 通常来说 初始化参数为零 θ0和θ1都在零 但为了展示需要 在这个梯度下降的实现中 我把θ0初始化为-900 θ1初始化为-0.1