前のビデオでは、最急降下法の数学的定義をしました。 もっと掘り下げて行きましょう。このビデオでは、アルゴリズムの挙動、 アルゴリズムの手順の意味について直感的理解を深めて行きます。 ここに前回見た最急降下法アルゴリズムがあります。思い返して頂きたいのは、 このパラメータ、この項、alpha は学習率といいます。 そしてそれは パラメータ theta j を更新する際のステップの大きさを制御します。そして この二つ目の項は、導関数項です。このビデオでは 二点について直感的理解を深めます。この二つの項が何をしていりるのか。なぜこの二つを 組み合わせるとこの更新が全体として意味をなすのか。こうした直感的理解を お伝えするために、少し簡素化した例を使いたいと思います。最小化する 関数のパラメータを一つだけにします。ですから目的関数 J の パラメータは一つ、theta 1 だけになります。ちょうど少し前のビデオでやったように。そして theta 1 は実数です。これで一次元のプロットになりますので、 見て理解するのが少し容易になります。そして最急降下法がこの関数に対して何を行うか 理解します。さて、これが関数 J(theta 1) だとします。そして theta 1 は実数です。 では、theta 1 をこの位置にして最急降下法を初期化したとします。 ですから、関数上のこの点から開始すると想像してください。最急降下法が行うのは、 更新です。 theta 1 は theta 1 - alpha 掛ける dd theta 1 J(theta 1) として更新されます。余談ですが、この 導関数項について、もしなぜ私が表記方法を変えて こうした偏微分の記号を使わないのかと疑問をお持ちの方。もしこの記号の違いの意味が分からない場合、 偏微分の記号と dd theta の違いは気にしないでください。厳密には数学上、 これは偏微分、そしてこれは常微分といいます。違いは 関数 J のパラメータの数によります。しかしこれは数学上の 独特の表現ですので、この授業の目的としては、こうした 偏微分の記号と dd theta の表記は全く同じものと考えて頂いて構いませんので、心配しないでください。 何が違うのかなどと。授業の中では、数学的に 正確な表記を使っていきますが、実際の目的上は、こうした表記は実際には同じことです。 では、この式が何をするか見てみましょう。まず、計算するのは、 この微分の導関数です。皆さんが微分積分の微分を見たことがあるか定かではありませんが、 微分というのは、ここでは、基本的にこういうことです。この 点に対する接線を取ります。この直線、赤い線、ちょうど この関数に接していますが、その赤い線の勾配を見てみようということ、 それが微分です。つまり、関数に対する接線の勾配は何かということです。 そして線の勾配はもちろん単に 高さを幅で割ったものです。さて、この線は 正の勾配となっていますので、正の微分となります。ですから、theta の更新は theta 1 := theta 1 - alpha 掛ける何が正の 値。ちなみに、alpha、学習率、は常に正の値です。ですから theta 1 に対し、それを (theta 1 - 何か) で更新するわけです。ということは 結果的に theta 1 を左に移動することになります。theta 1 を減少すると、ご覧のように これが正しい動作ですよね、この方向に移動して ここの最小値に近づいたわけですから。最急降下法はここまでは 正しく動作しているようです。では、別の例を見てみましう。同じ関数 J を使います。 同じ関数 J(theta 1) を書きます。では、仮に 今回はパラメータをこの左ところで初期化したとします。theta 1 はここになります。 表面のここに点を追加します。では、導関数項、dd theta 1 J(theta 1) をこの点の値で評価すると、 この線の勾配になります。この導関数項はこの線の勾配となります。しかしこの 線は下に傾斜していますので、この線は負の勾配です。言い換えると、 この関数は負の導関数を持つということです。つまりその点で負の勾配となっていることを意味します。 ですから、これは 0 以下となります。そして theta を更新すると、theta が (theta - alpha 掛ける負の値) で更新されます。つまり (theta 1 - 負の値) となりますから、これは theta の値を増大するということを意味します。 そうですよね。負の値の引き算ですから、実際には theta に加算することになり、 それが意味するのは、結果的に theta の値が増加するということです。この 開始点から theta を増加させるというのは、これも意図通りの動作のようです。 最小値に近づくことになりますので。以上、これにより 導関数項が何をしているのかご理解いただけたと思います。では次に学習率 alpha を見て、それが何をしているのか理解しましょう。さて、ここに最急降下法の 更新ルールがあります。この式です。さて、どうなるでしょか。 もし alpha が小さすぎり、あるいは alpha が大きすぎた場合。この最初の例は、 alpha が小さすぎた場合にどうなるかです。これが関数 J、J(theta) 。 ではここから始めましょう。alpha が小さすぎる場合、どうなるかというと、 更新がある小さな値で乗算されることになります。すると結果的にこのような小さなステップ となります。これが一つのステップです。次にこの新しい点から もう一歩進みますが、alpha が小さすぎますのでまた 小さなステップとなります。ですから、学習率が小さすぎる場合、結果的に こうした小さな小さなステップで最小値に進むことになりますので、 最小値に辿り着くには、非常に多くのステップが必要になります。alpha が小さすぎると、 小さな小さなステップで進みますので最急降下法の実行に時間がかかります。 さらに、大域的最小点に近づくには非常に多くのステップが必要になります。 では、alpha が大きすぎる場合はどうでしょうか。これが関数 J(theta) です。実は alpha が大きすぎる場合、最急降下法が最小値を通り越して 収束しなかったり、さらには発散してしまうことがあります。つまり、こういうことです。ここから始めるとします。これは既に最小値に近いです。 この場合、微分は右向きになります。しかし alpha が大きすぎると、大きなステップで進むので、 このように大きなステップで進むので、結果的に目的関数の値が悪化します。なぜなら、 この値で始めたのに、結果的に値が悪くなるからです。今度は微分が 左を向きますので、theta の値は減少します。しかし学習率が大きすぎると、 大きなステップでここからそこまで飛び移りここに落ち着きます。 そして学習率が大きすぎると、また大きなステップで 次回も更新されて、行き過ぎ、行き過ぎ、を繰り返すことになり、ご覧のように 実際には最小値からどんどん遠ざかっていきます。ですから、alpha が大きすぎると、 収束しなかったり、さらには発散してしまうことがあります。ではもう一つ皆さんに質問があります。 これはちょっと難問です。私が最初にこれを学んだ時、 実はこれを理解するのにかなり時間がかかりました。もし最初に設定した theta 1 が既に局所的最小値だったら どうなるでしょうか。最急降下法の一ステップで何が起きるでしょうか。 では、theta 1 を局所的最小値で初期化したとします。ですから theta 1 の初期値がここで、それは既に局所的 最適値、局所的最小値です。実は局所的最適値では微分は ゼロとなります。これが勾配、ここが接点ですので、この線の勾配は ゼロと等しくなり、この導関数項もゼロとなります。 ですから、最急降下法の更新では、theta 1 := theta 1 - alpha 掛けるゼロ。ですから、この意味は、もし 既に局所的最小値にある場合は、theta 1 は変わらないということです。なぜなら、更新は theta 1 := theta 1 となりますので。パラメータが既に局所的 最小値の場合、最急降下法の一ステップは全く何も変えません。つまり パラメータは変わりません。そしてこれは望ましいことです。なぜなら、解が 局所的最適値に留まるからです。これは、なぜ最急降下法が収束して 学習率が固定されてても局所的最小値に落ち着くのかの説明にもなります。そのわけはこうです。では 例を見てみましょう。これが目的関数 J(theta) です。これを 最小化したいとします。そしてアルゴリズムを、最急降下法のアルゴリズムをここで初期化したとします。 このマジェンタ色の点です。最急降下法で更新を一ステップ実行すると、例えば、 この点に移動するとします。なぜならそこの微分はかなり急だからです。さて、 今はこの緑の点にあり、最急降下法でまた一ステップ更新したとします。 お気づきの通り、微分、つまり勾配は、緑の点ではそれほど急ではありません。 あそこのマジェンタ色の点と比較して。なぜなら、 最小値に近づくにつれ、微分がゼロにどんどん近づくからです。 さて、最急降下法で一ステップ更新した後、新しい微分はやや小さくなりました。 最急降下法でまた一ステップ更新したいと思います。当然、いくぶん マジェンタ色の点からの場合よりさらに小さなステップでこの緑の点から進みます。さて、新しい 点、赤い点、は大域的最小値にさらに近くなっています。ですから ここの微分は緑の点の時よりさらに小さくなります。そして、最急降下法で また一ステップ更新すると、導関数項はさらに小さくなるので theta 1 の更新の大きさもさらに小さくなりますので、ステップのまたこのように小さくなり、 このように最急降下法が進むにつれ、自動的にステップが どんどん小さくなり、やがて非常に小さなステップで進むことになります。そして 最後には、局所的最小値に収束します。さて、おさらいをします。最急降下法では 局所的最小値に近づくにつれ、最急降下法は自動的に 小さなステップを取るようになり、それは局所的最小値に近づくにつれ、定義により 局所的最小値での微分はゼロに等しくなるからです。よって、 局所的最小値に近づくにつれ、この導関数項も自動的に小さくなり、 このため最急降下法のステップも自動的に小さくなります。これが 最急降下法の仕組みですので、経過的にalpha を減少させる実際の必要はないのです。 さて、これが最急降下法アルゴリズムです。そしてこれを使って どのような目的関数 J でも最小化を試みることが出来ます。単に線形回帰で定義された目的関数に 限定されません。次のビデオでは、関数 J を 元通りの線形回帰の目的関数に戻します。 前に定義した二乗誤差の目的関数です。そして最急降下法と 二乗誤差の目的関数を組み合わせます。これによって最初の 学習アルゴリズム、線形回帰アルゴリズムが構築できます。