이전 강의에서 우리는 기울기 하강 알고리즘에 대해 이야기했습니다. 그리고 선형회귀, 비용함수의 차의 제곱도 이야기 했었습니다. 이번 수업에서는 비용함수와 기울기 하강을 함께 사용해서 선형회귀를 위한 알고리즘을 구하거나 우리의 자료의 일차함수를 구해보도록 하겠습니다. 그래서 우리가 이전 수업에서 배웠던 것이, 익숙하실, 이 기울기 하강 알고리즘입니다. 이쪽에는 선형 가설을 위한 선형 회귀 모형이 있습니다. 그리고 비용함수의 차의 제곱도 있네요. 우리가 이제부터 하려는 것은,
기울기 하강을 사용해서 비용함수 차의 제곱을 최소화 하려는 것입니다. 기울기 하강을 적용하기 위해서, 아시다시피 미분계수를 의미하는 이 기호를 사용해야합니다. 그래서 알아야 할것은 이 부분 미분계수와 함수 j에 끼치는 영향을 알아야합니다. 이런 공식이 됩니다. y값이 1 부터 m까지인 비용함수의 차의 제곱을 더해줍니다. 그래서 제가 지금까지 한 것은, 비용함수의 정의를 적었습니다. 더 간략하게 적어봅시다.
이런식으로 적을 수 있습니다. i 값이 1일때부터 m일때 까지의 시그마값에 
Θ0 + Θ1x - y i 의 제곱값 이 가설값이 의미하는 바는 여기에 쓴 이 값입니다. 그리고 부분 미분계수의 값을 구해보면, j 가 0일때와 1일때, 2가지 경우가 있습니다. 부분 미분계수를 구하기 위해서 Θ0과 Θ1 두가지 경우를 사용해 봅시다. 여기에 답을 적어보겠습니다. 공식을 간소화해보겠습니다.
1/m을 곱하고, X(i)- Y(i)의 합계를 더하고 부분 미분계수를 구합니다.
Θ1인 경우를 구해봅시다. 이런식으로 공식을 쓸 수 있습니다. Y(i)를 빼고, X(i)를 곱해줍니다. 좋습니다, 그리고 부분 미분계수를 계산해봅시다.
우리는 여기 이 공식과 밑에 있는 이 공식을 같게 해줄겁니다. 이 부분 미분계수를 계산하기 위해서는, 
다변수의 미적분학이 필요합니다. 미적분학을 아신다면,
혼자서 이 공식을 풀어보시고, 미분계수를 구해보시기 바랍니다.
실제로 제가 구한 답을 구하실 수 있을겁니다. 하지만 미적분학을 잘 아시지 못한다해도,
걱정하지마세요. 이 공식을 외우시고, 그냥 풀어보시면 됩니다. 미적분학에 대해 아실 필요도 없습니다.
숙제로 남겨드리겠습니다. 기울기 하강으로 돌아가봅시다. 이 정의를 가지고, 비용함수 j의 기울기값인 미분계수를 구해봅시다. 그 값을 하강 기울기에 넣어봅시다. 이게 선형회귀에서 쓰이는 
하강 기울기 알고리즘입니다. 여기서도 우리는 Θ0의 경우와 Θ1의 경우로 나눠서 생각해봅시다.
아시다시피 미분계수는 -α에 곱해줘야합니다. 여기에 공식이 있습니다. 우리의 선형회귀 알고리즘입니다. 여기가 첫번째 공식이고요, 부분 미분계수의 공식은 d/dΘ0를 곱한 값입니다. 이전 슬라이드에서 말씀드린 내용입니다. 여기에 있는 두번째 공식은 아까 썼던 공식에서 Θ1의 값을 사용합니다. 빠르게 상기시켜드리자면, 
기울기 하강을 사용하셔야합니다. 이런식으로 여러분이 지켜주셔야 할 것은 Θ0 값과 Θ1값을 둘다 구해주셔야 한다는 것입니다. 그래서, 기울기 하강이 어떻게 변하는지 보겠습니다. 중요한 사실은 기울기 하강이 지역 최적값에 민감하다는 것입니다. 그래서 제가 처음에 기울기 하강에 대해 설명할 때, 이 언덕의 표면에서 내려가는 그림을 보셨을겁니다. 
어디서 시작하는지에 따라 지역 최적값이 다르다는 것을 설명드렸습니다. 이 점에서 끝나게 되거나,
이 점에서 끝나게 될 것입니다. 그러나 선형회귀의 비용함수는 항상 이런 활 모양의 함수가 된다고 말씀드렸습니다. 기술적인 용어로는 볼록 함수라고 부릅니다. 그러나 저는 볼록함수의 
일반적인 정의에 대해 말씀드리자 않을겁니다. C, O, N, V, E, X. - 볼록 그러나 의미적으로는 볼록함수의 의미는
볼록한 모양의 함수라는 뜻이고, 전역적 최적값만 가지고 있을 뿐,
지역적 최적값은 없습니다. 그리고 선형회귀에서 사용되는 비용함수의 기울기 하강은 항상 전역적 최적값을 향하게됩니다. 왜냐면 다른 전역적 최적값이 없기 때문입니다. 이제 이 알고리즘을 활용해봅시다. 여기에 가설 그래프와 비용함수 j의 그래프가 있습니다. 이 값들로 파라메터를 초기화해봅시다. 주로 파라메터를 0과 0으로 초기화합니다. Θ0과 Θ1을 0과 같다고 둡니다. 하지만 여기서 증명하기 위해서, 여기서는 Θ0를 900으로 초기화하고,
Θ1을 -0,1로 초기화하겠습니다. 공식은  h(x)=-900-0.1x 가 됩니다. 이 선에서 비용함수를 구하는 공식입니다. 기울기 하강을 한번 해본다면, 이 점에서 아래로 내려가고, 왼쪽에 위치한 이 점으로 가게됩니다. 제 일차함수가 약간 바뀐 것을 아실 수 있으실겁니다. 기울기가 하강했기때문에, 
일차함수도 변하게됩니다. 그렇지 않습니까? 비용함수 점을 또 옮겨봅시다. 비용이 내려가게 되면서,
기울기 하강이 변하게됩니다. 이런 궤도로 바뀌게됩니다. 왼쪽을 보시면 제 가설과 일치하게됩니다. 자료와 더 잘 맞아 보입니다. 결국 이 전역적 최소값이 가설과 일치할 때, 자료와 잘 일치하게됩니다. 이것이 기울기 하강이고, 우리가 계속 했던 주택가격의 자료와 잘 일치하게 됩니다. 이제 이 것을 예측하는데 사용할 수 있습니다. 만약 당신의 친구의 집크기가 1250이라면 당신은 이제 친구에게 집 가격이 $250,000이라고 말해줄 수 있습니다. 마지막으로, 이 알고리즘의 다른 이름을 봅시다. 집단 기울기 하강이라고 부릅니다. 기계학습에서 사용되는 용어긴 하지만, 저는 기계학습에 있는 사람들이 알고리즘의
이름을 짓는 것을 잘 못한다고 생각합니다. 그렇지만 집단 기울기 하강이라는 용어는 훈련예제에서 모든 기울기 하강의 
단계를 포함한것을 의미합니다. 그래서 기울기 하강에서 미분계수를 계산할때, 우리는 합계를 계산합니다. [무음] 기울기 하강의 단계를 지나다 보면, m개의 훈련 예제들을 값을 더하게 됩니다.
그런 이유로 집단 기울기 하강이라는 말은 모든 훈련예제들의 전체 집단을 
나타내는 의미입니다. 다시 말씀드리자면, 저는 좋은 이름은 아니라고 생각합니다. 그렇지만 기계학습 분야의 사람들이
칭하는 이름입니다. 가끔 몇몇의 하강 기울기는 집단의 값과 다를 때도 있습니다. 전체적인 훈련집합을 보지마시고, 훈련집합의 작은 부분집합을 보세요. 이 코스의 다른 강의에서도 설명할 예정입니다. 하지만 우리가 지금 알고리즘을 배우는 이유는 선형회귀를 위한 하강 기울기 알고리즘에 대해
알기 위해서 입니다. 지금까지가 선형회귀와 관련된
하강 기울기에 관련된 내용이였습니다. 만약 이전에 고급 선형대수에 대해 배웠거나 고급 선형대수에 관련된 
수업을 들으신 분이 있을 겁니다. 비용 함수 j의 해답을 얻기 위해서는 하강 기울기 알고리즘같은 반복적 알고리즘을 
사용해야 한다는 것을 아셨을겁니다. 이 코스의 나중 쯤, 
우리는 하강 기울기의 반복 없이 비용함수 j의 최소값을 구하는 방법에 대해 배울 것입니다. 반복 최소 이승법이라고 불리는 방법입니다. 자료의 범위나 크기가 너무 거대해서,
하강 기울기 알고리즘을 사용할 수 없을때 반복최소이승법을 사용합니다. 우리는 이제 다양한 예제에서 하강 기울기 알고리즘을 사용 할 수 있게 됐습니다. 다양한 기계 학습 문제에서도 사용하게 될 것입니다. 첫번째 기계학습 알고리즘을 
배운 것을 축하합시다. 하강 기울기에 대한 다양한 예제들을 직접 풀어보시길 바랍니다. 다음 비디오를 시작하기 전에 해보시길 바랍니다. 하강 기울기 알고리즘이 더 강력한 
알고리즘이라는 것을 말씀드리고 싶었습니다. 더 강력한 하강 기울기 알고리즘에 대해
설명드릴겁니다. 다음 비디오에서  설명드리겠습니다.