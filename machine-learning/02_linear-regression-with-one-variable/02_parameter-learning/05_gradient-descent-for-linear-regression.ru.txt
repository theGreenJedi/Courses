В предыдущих видео мы говорили об алгоритме градиентного спуска, о модели линейной регрессии и о функции затрат, определенной через сумму среднеквадратических отклонений. Теперь мы сведем вместе градиентный спуск и нашу функции стоимости, что даст нам алгоритм линейной регрессии для аппроксимации данных прямой линией. Напомню, к чему мы пришли в предыдущих видео. Это хорошо знакомый нам алгоритм градиентного спуска, а это наша модель линейной регрессии: линейная гипотеза и усредненная сумма квадратов отклонений, наша функция затрат. Я собираюсь применить градиентный спуск к нашей функции стоимости. Чтобы применить алгоритм и написать программу, в первую очередь нам нужно получить эту производную. Давайте посчитаем эту частную производную... подставим функцию J... то есть коэффициент... сумма от 1 до m квадрата ошибки... Пока я просто переписал сюда определение функции затрат, упростим еще немного... сумма от 1 до m... тета нулевое плюс тета первое на x(i) минус y(i) и все это в квадрате. Теперь я просто подставил формулу функции-гипотезы. И, собственно говоря, нам нужно получить эти частные производные для двух случаев: для j, равного 0, и для j, равного 1. То есть взять ее относительно тета нулевого и тета первого. Я просто напишу, чему они равны. В первом случае получится 1/m умножить на сумму по обучающему набору... h от x(i) минус y(i). А во втором частная производная по тета первому получится равна... то же самое, умножить на x(i). Отлично. Расчет этих частных производных, то есть получение этих выражений из этого, требует представления об анализе функций многих переменных. Если вы знакомы с многомерным анализом, можете сами провести выкладки и убедиться, что частные производные действительно получатся такими, как у меня. А если не знакомы, ничего страшного, можете просто использовать выведенные мной выражения. В домашнем задании анализ вам тоже не понадобится, для реализации градиентного спуска достаточно готовых производных. Итак, получив эти выражения, эти производные, соответствующие уклону графика функции стоимости J, мы можем подставить их в формулы алгоритма градиентного спуска. Вот формулы шага градиентного спуска для линейной регрессии, которые мы будем применять до схождения. Новое значение для тета нулевого и тета первого получаем, вычитая из старого производную, умноженную на альфа. Вот она. Итак, это алгоритм линейной регрессии. Верно? равен, соответственно, частной производной по тета нулевому, которую мы получили на предыдущем слайде. А во втором — частной производной по тета первому, тоже полученной на предыдущем слайде. На всякий случай напомню одну тонкость реализации градиентного спуска: обновлять тета нулевое и тета первое вам нужно одновременно. Посмотрим, как градиентный спуск работает. Если помните, у градиентного спуска была одна проблема: он может «застрять» в локальном экстремуме. Когда я показывал вам градиентный спуск, я пользовался этим графиком, по которому мы спускались, как с холма, и оказалось, что мы можем прийти в разные локальные экстремумы в зависимости от того, откуда начали. Вы можете прийти сюда или сюда. Но, как выясняется, функция стоимости для линейной регрессии всегда будет чашеобразной, как на этом графике. Математический термин для этого — выпуклая функция. Я не буду давать строгого определения выпуклой функции, говоря простым языком, выпуклая функция и функция с чашеобразным графиком, ну, условно чашеобразным, — это одно и то же. У такой функции нет никаких локальных экстремумов, кроме одного глобального. Таким образом, применив градиентный спуск к выпуклой функции, а при линейной регрессии она всегда выпуклая, вы всегда окажетесь в глобальном экстремуме, потому что других локальных экстремумов нет. Теперь посмотрим на алгоритм в действии. Как обычно, здесь у меня графики функции-гипотезы и функции стоимости J. Пусть начальные значения моих параметров соответствуют этой точке. Обычно мы задаем в качестве начальных значений нули. Но для иллюстрации этого случая я положил тета нулевое равным примерно 900, а тета первое — примерно −0,1.