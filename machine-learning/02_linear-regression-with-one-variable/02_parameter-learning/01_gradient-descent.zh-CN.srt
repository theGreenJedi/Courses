1
00:00:00,000 --> 00:00:04,934
我们已经定义了代价函数J 而在这段视频中

2
00:00:04,934 --> 00:00:09,634
我想向你们介绍梯度下降这种算法 这种算法可以将代价函数J最小化

3
00:00:09,634 --> 00:00:14,275
梯度下降是很常用的算法 它不仅被用在线性回归上

4
00:00:14,275 --> 00:00:18,916
它实际上被广泛的应用于机器学习领域中的众多领域

5
00:00:18,916 --> 00:00:23,791
在后面课程中 为了解决其他线性回归问题 我们也将使用梯度下降法

6
00:00:23,791 --> 00:00:27,845
最小化其他函数 而不仅仅是只用在本节课的代价函数J

7
00:00:27,845 --> 00:00:32,558
因此在这个视频中 我将讲解用梯度下降算法最小化函数 J

8
00:00:32,558 --> 00:00:37,406
在后面的视频中 我们还会将此算法应用于具体的

9
00:00:37,406 --> 00:00:43,332
代价函数J中来解决线性回归问题 下面是问题概述

10
00:00:43,332 --> 00:00:48,112
在这里 我们有一个函数J(θ0, θ1)

11
00:00:48,112 --> 00:00:52,773
也许这是一个线性回归的代价函数 也许是一些其他函数

12
00:00:52,773 --> 00:00:56,801
要使其最小化 我们需要用一个算法

13
00:00:56,801 --> 00:01:01,174
来最小化函数J(θ0, θ1) 就像刚才说的

14
00:01:01,174 --> 00:01:05,793
事实证明 梯度下降算法可应用于

15
00:01:05,793 --> 00:01:10,994
多种多样的函数求解 所以想象一下如果你有一个函数

16
00:01:10,994 --> 00:01:16,194
J(θ0, θ1, θ2, ...,θn )

17
00:01:16,405 --> 00:01:21,795
你希望可以通过最小化 θ0到θn 来最小化此代价函数J(θ0 到θn)

18
00:01:21,795 --> 00:01:26,580
用n个θ是为了证明梯度下降算法可以解决更一般的问题

19
00:01:26,580 --> 00:01:31,368
但为了简洁起见 为了简化符号

20
00:01:31,368 --> 00:01:35,935
在接下来的视频中 我只用两个参数

21
00:01:36,113 --> 00:01:41,097
下面就是关于梯度下降的构想

22
00:01:41,097 --> 00:01:45,882
我们要做的是 我们要开始对θ0和θ1 进行一些初步猜测

23
00:01:45,882 --> 00:01:50,788
它们到底是什么其实并不重要 但通常的选择是将 θ0设为0

24
00:01:50,788 --> 00:01:55,452
将θ1也设为0 将它们都初始化为0

25
00:01:55,452 --> 00:02:00,322
我们在梯度下降算法中要做的

26
00:02:00,322 --> 00:02:05,258
就是不停地一点点地改变 θ0和θ1 试图通过这种改变使得J(θ0, θ1)变小

27
00:02:05,258 --> 00:02:10,571
直到我们找到 J 的最小值 或许是局部最小值

28
00:02:10,796 --> 00:02:16,106
让我们通过一些图片来看看梯度下降法是如何工作的

29
00:02:16,106 --> 00:02:20,849
我在试图让这个函数值最小 注意坐标轴 θ0和θ1在水平轴上

30
00:02:20,849 --> 00:02:25,774
而函数 J在垂直坐标轴上 图形表面高度则是 J的值

31
00:02:25,774 --> 00:02:30,582
我们希望最小化这个函数 所以我们从 θ0和θ1的某个值出发

32
00:02:30,582 --> 00:02:35,375
所以想象一下 对 θ0和θ1赋以某个初值

33
00:02:35,375 --> 00:02:39,934
也就是对应于从这个函数表面上的某个起始点出发 对吧

34
00:02:39,934 --> 00:02:44,201
所以不管 θ0和θ1的取值是多少

35
00:02:44,201 --> 00:02:48,819
我将它们初始化为0 但有时你也可把它初始化为其他值

36
00:02:48,819 --> 00:02:53,942
现在我希望大家把这个图像想象为一座山

37
00:02:53,942 --> 00:02:59,178
想像类似这样的景色 公园中有两座山

38
00:02:59,178 --> 00:03:04,618
想象一下你正站立在山的这一点上

39
00:03:04,618 --> 00:03:09,990
站立在你想象的公园这座红色山上 在梯度下降算法中

40
00:03:09,990 --> 00:03:15,770
我们要做的就是旋转360度 看看我们的周围 并问自己

41
00:03:15,770 --> 00:03:20,423
我要在某个方向上 用小碎步尽快下山

42
00:03:20,423 --> 00:03:25,320
如果我想要下山 如果我想尽快走下山

43
00:03:25,320 --> 00:03:29,686
这些小碎步需要朝什么方向?

44
00:03:29,686 --> 00:03:34,465
如果我们站在山坡上的这一点

45
00:03:34,465 --> 00:03:39,185
你看一下周围 ​​你会发现最佳的下山方向

46
00:03:39,185 --> 00:03:44,035
大约是那个方向 好的 现在你在山上的新起点上

47
00:03:44,035 --> 00:03:49,430
你再看看周围 然后再一次想想

48
00:03:49,430 --> 00:03:54,695
我应该从什么方向迈着小碎步下山? 然后你按照自己的判断又迈出一步

49
00:03:54,695 --> 00:03:59,700
往那个方向走了一步 然后重复上面的步骤

50
00:03:59,700 --> 00:04:04,835
从这个新的点 你环顾四周 并决定从什么方向将会最快下山

51
00:04:04,835 --> 00:04:09,775
然后又迈进了一小步 又是一小步
并依此类推 直到你接近这里

52
00:04:09,970 --> 00:04:15,059
直到局部最低点的位置 此外 这种下降有一个有趣的特点

53
00:04:15,059 --> 00:04:19,682
第一次我们是从这个点开始进行梯度下降算法的 是吧

54
00:04:19,682 --> 00:04:24,183
在这一点上从这里开始 现在想象一下

55
00:04:24,183 --> 00:04:29,232
我们在刚才的右边一些的位置 对梯度下降进行初始化

56
00:04:29,232 --> 00:04:34,159
想象我们在右边高一些的这个点 开始使用梯度下降 如果你重复上述步骤

57
00:04:34,159 --> 00:04:39,207
停留在该点 并环顾四周 往下降最快的方向迈出一小步

58
00:04:39,207 --> 00:04:44,772
然后环顾四周 又迈出一步 然后如此往复

59
00:04:44,772 --> 00:04:50,570
如果你从右边不远处开始 梯度下降算法将会带你来到

60
00:04:50,570 --> 00:04:56,236
这个右边的第二个局部最优处 如果从刚才的第一个点出发

61
00:04:56,236 --> 00:05:01,602
你会得到这个局部最优解 但如果你的起始点偏移了一些

62
00:05:01,602 --> 00:05:06,762
起始点的位置略有不同 你会得到一个

63
00:05:06,762 --> 00:05:12,197
非常不同的局部最优解 这就是梯度下降算法的一个特点

64
00:05:12,197 --> 00:05:17,425
我们会在之后继续探讨这个问题 好的 这是我们从图中得到的直观感受

65
00:05:17,425 --> 00:05:22,929
看看这个图 这是梯度下降算法的定义

66
00:05:22,929 --> 00:05:28,240
我们将会反复做这些 直到收敛 我们要更新参数 θj

67
00:05:28,240 --> 00:05:33,543
方法是 用 θj 减去 α乘以这一部分  

68
00:05:33,543 --> 00:05:39,129
让我们来看看 这个公式有很多细节问题

69
00:05:39,129 --> 00:05:45,030
我来详细讲解一下 首先 注意这个符号 :=

70
00:05:45,030 --> 00:05:51,643
我们使用 := 表示赋值 这是一个赋值运算符

71
00:05:51,643 --> 00:05:57,790
具体地说 如果我写 a:= b

72
00:05:57,790 --> 00:06:02,878
在计算机专业内 这意味着不管 a的值是什么

73
00:06:02,878 --> 00:06:08,517
取 b的值 并将其赋给a 这意味着我们让 a等于b的值

74
00:06:08,517 --> 00:06:13,674
这就是赋值 我也可以做 a:= a+1

75
00:06:13,674 --> 00:06:18,969
这意味着 取出a值 并将其增加1 与此不同的是

76
00:06:18,969 --> 00:06:26,067
如果我使用等号 = 并且写出a=b 那么这是一个判断为真的声明

77
00:06:26,067 --> 00:06:31,006
如果我写 a=b 就是在断言 a的值是等于 b的值的

78
00:06:31,006 --> 00:06:36,331
在左边这里 这是计算机运算 将一个值赋给 a

79
00:06:36,331 --> 00:06:41,399
而在右边这里 这是声明 声明 a的值 与b的值相同

80
00:06:41,399 --> 00:06:46,274
因此 我可以写 a:=a+1 这意味着 将 a的值再加上1

81
00:06:46,274 --> 00:06:50,764
但我不会写 a=a+1 因为这本来就是错误的

82
00:06:50,764 --> 00:06:55,704
a 和 a+1 永远不会是同一个值 这是这个定义的第一个部分

83
00:06:55,704 --> 00:07:05,733
这里的α 是一个数字 被称为学习速率

84
00:07:05,733 --> 00:07:12,360
什么是α呢?  在梯度下降算法中 它控制了

85
00:07:12,360 --> 00:07:17,113
我们下山时会迈出多大的步子 因此如果 α值很大

86
00:07:17,113 --> 00:07:21,925
那么相应的梯度下降过程中 我们会试图用大步子下山

87
00:07:21,925 --> 00:07:26,322
如果α值很小 那么我们会迈着很小的小碎步下山

88
00:07:26,322 --> 00:07:31,194
关于如何设置 α的值等内容 在之后的课程中

89
00:07:31,194 --> 00:07:35,660
我会回到这里并且详细说明 最后 是公式的这一部分

90
00:07:35,660 --> 00:07:40,582
这是一个微分项 我现在不想谈论它

91
00:07:40,582 --> 00:07:45,564
但我会推导出这个微分项 并告诉你到底这要如何计算

92
00:07:45,564 --> 00:07:50,547
你们中有人大概比较熟悉微积分 但即使你不熟悉微积分

93
00:07:50,547 --> 00:07:55,469
也不用担心 我会告诉你 对这一项 你最后需要做什么

94
00:07:55,469 --> 00:08:00,580
现在 在梯度下降算法中 还有一个更微妙的问题

95
00:08:00,580 --> 00:08:05,837
在梯度下降中 我们要更新 θ0和θ1

96
00:08:05,837 --> 00:08:10,699
当 j=0 和 j=1 时 会产生更新 所以你将更新 J θ0还有θ1

97
00:08:10,699 --> 00:08:15,955
实现梯度下降算法的微妙之处是

98
00:08:15,955 --> 00:08:21,562
在这个表达式中 如果你要更新这个等式

99
00:08:21,562 --> 00:08:31,384
你需要同时更新 θ0和θ1

100
00:08:31,384 --> 00:08:36,432
我的意思是在这个等式中 我们要这样更新

101
00:08:36,432 --> 00:08:40,975
θ0:=θ0 - 一些东西 并更新 θ1:=θ1 - 一些东西

102
00:08:40,975 --> 00:08:45,834
实现方法是 你应该计算公式右边的部分

103
00:08:45,834 --> 00:08:52,677
通过那一部分计算出θ0和θ1的值

104
00:08:52,677 --> 00:08:57,469
然后同时更新 θ0和θ1 让我进一步阐述这个过程

105
00:08:57,469 --> 00:09:02,024
在梯度下降算法中 这是正确实现同时更新的方法

106
00:09:02,024 --> 00:09:06,461
我要设 temp0等于这些 设temp1等于那些

107
00:09:06,461 --> 00:09:11,430
所以首先计算出公式右边这一部分 然后将计算出的结果

108
00:09:11,430 --> 00:09:15,926
一起存入 temp0和 temp1 之中 然后同时更新 θ0和θ1

109
00:09:15,926 --> 00:09:20,245
因为这才是正确的实现方法

110
00:09:20,245 --> 00:09:25,533
与此相反 下面是不正确的实现方法 因为它没有做到同步更新

111
00:09:25,533 --> 00:09:31,666
在这种不正确的实现方法中 我们计算 temp0 然后我们更新θ0

112
00:09:31,666 --> 00:09:36,644
然后我们计算 temp1 然后我们将 temp1 赋给θ1

113
00:09:36,644 --> 00:09:41,877
右边的方法和左边的区别是 让我们看这里

114
00:09:41,877 --> 00:09:46,791
就是这一步 如果这个时候你已经更新了θ0

115
00:09:46,791 --> 00:09:51,897
那么你会使用 θ0的新的值来计算这个微分项

116
00:09:51,897 --> 00:09:57,340
所以由于你已经在这个公式中使用了新的 θ0的值

117
00:09:57,340 --> 00:10:01,565
那么这会产生一个与左边不同的 temp1的值

118
00:10:01,565 --> 00:10:05,852
所以右边并不是正确地实现梯度下降的做法

119
00:10:05,852 --> 00:10:09,916
我不打算解释为什么你需要同时更新

120
00:10:09,916 --> 00:10:14,617
同时更新是梯度下降中的一种常用方法

121
00:10:14,617 --> 00:10:18,735
我们之后会讲到

122
00:10:18,735 --> 00:10:22,496
实际上同步更新是更自然的实现方法

123
00:10:22,496 --> 00:10:26,665
当人们谈到梯度下降时 他们的意思就是同步更新

124
00:10:26,665 --> 00:10:30,630
如果用非同步更新去实现算法 代码可能也会正确工作

125
00:10:30,630 --> 00:10:34,747
但是右边的方法并不是人们所指的那个梯度下降算法

126
00:10:34,747 --> 00:10:38,356
而是具有不同性质的其他算法

127
00:10:38,356 --> 00:10:42,220
由于各种原因 这其中会表现出微小的差别

128
00:10:42,220 --> 00:10:46,626
你应该做的是 在梯度下降中真正实现同时更新

129
00:10:46,626 --> 00:10:52,313
这些就是梯度下降算法的梗概

130
00:10:52,313 --> 00:10:56,998
在接下来的视频中 我们要进入这个微分项的细节之中

131
00:10:56,998 --> 00:11:01,799
我已经写了出来但没有真正定义 如果你已经修过微积分课程

132
00:11:01,799 --> 00:11:06,367
如果你熟悉偏导数和导数 这其实就是这个微分项

133
00:11:06,367 --> 00:11:11,425
如果你不熟悉微积分 不用担心

134
00:11:11,425 --> 00:11:15,680
即使你之前没有看过微积分

135
00:11:15,680 --> 00:11:19,883
或者没有接触过偏导数 在接下来的视频中

136
00:11:19,883 --> 00:11:24,296
你会得到一切你需要知道的 如何计算这个微分项的知识

137
00:11:24,296 --> 00:11:28,288
下一个视频中 希望我们能够给出

138
00:11:28,288 --> 00:11:30,180
实现梯度下降算法的所有知识 【教育无边界字幕组】翻译: 10号少年  校对: 小白_远游 审核: 所罗门捷列夫