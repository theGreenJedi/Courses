1
00:00:00,000 --> 00:00:04,934
Anteriormente definimos la función de costos
J. En este video quiero hablarte sobre

2
00:00:04,934 --> 00:00:09,634
un algoritmo llamado gradiente de descenso para
minimizar la función de costos J. Resulta

3
00:00:09,634 --> 00:00:14,275
que el gradiente de descenso es un algoritmo
más general y se utiliza no sólo en la

4
00:00:14,275 --> 00:00:18,916
regresión lineal. De hecho, se utiliza en todo
el aprendizaje automático. Y más adelante

5
00:00:18,916 --> 00:00:23,791
en esta clase, también usaremos el gradiente
de descenso para minimizar otras funciones, no sólo

6
00:00:23,791 --> 00:00:27,845
la función de costos J, para la regresión lineal.
En este video, hablaré

7
00:00:27,845 --> 00:00:32,558
sobre el gradiente de descenso para minimizar
alguna función arbitraria J. Y, después,

8
00:00:32,558 --> 00:00:37,406
en otros videos, usaré este algoritmo
y lo aplicaré específicamente para la

9
00:00:37,406 --> 00:00:43,332
función de costos J que teníamos que encontrar para la
regresión lineal. Este es el planteamiento

10
00:00:43,332 --> 00:00:48,112
del problema. Veremos que tenemos
alguna función J de (theta 0, theta 1).

11
00:00:48,112 --> 00:00:52,773
Tal vez es una función de costos de la
regresión lineal. Tal vez es otra función que

12
00:00:52,773 --> 00:00:56,801
queremos minimizar. Y queremos
obtener un algoritmo para

13
00:00:56,801 --> 00:01:01,174
minimizarla como una función de J de
(theta 0, theta 1). Como nota,

14
00:01:01,174 --> 00:01:05,793
resulta que esa gradiente de descenso
en realidad aplica para funciones

15
00:01:05,793 --> 00:01:10,994
más generales. Imagina que tienes una
función que es una función de

16
00:01:10,994 --> 00:01:16,194
J de (theta 0, theta 1, theta 2, hasta
theta n), y quieres

17
00:01:16,405 --> 00:01:21,795
minimizar (theta 0 hasta theta n)
de esta J de (theta 0 hasta theta n).

18
00:01:21,795 --> 00:01:26,580
Resulta que la gradiente de descenso
es un algoritmo para resolver

19
00:01:26,580 --> 00:01:31,368
este problema más general, pero
para ser breve, por el bien de

20
00:01:31,368 --> 00:01:35,935
la concisión de la notación, sólo
presentaré dos parámetros

21
00:01:36,113 --> 00:01:41,097
durante el resto de este video. Esta es
la idea para el gradiente de descenso. Lo que

22
00:01:41,097 --> 00:01:45,882
haremos es que iniciaremos con algunas
aproximaciones iniciales para theta 0 y

23
00:01:45,882 --> 00:01:50,788
theta 1. Realmente no importa lo que son,
pero una opción común sería si

24
00:01:50,788 --> 00:01:55,452
establecemos theta 0 a theta 0, y
establecemos theta 1 a 0. Sólo inicia

25
00:01:55,452 --> 00:02:00,322
a 0. Lo que haremos en el
gradiente de descenso es mantener

26
00:02:00,322 --> 00:02:05,258
theta 0 y theta 1 con pequeños cambios para
tratar de reducir J de (theta 0, theta 1)

27
00:02:05,258 --> 00:02:10,571
hasta que, con suerte, obtengamos un mínimo o
tal vez un mínimo local. Veamos

28
00:02:10,796 --> 00:02:16,106
imágenes de lo que hace el gradiente
de descenso. Digamos que trato de minimizar esta

29
00:02:16,106 --> 00:02:20,849
función. Observa los ejes. Este es
(theta 0, theta 1) en el eje

30
00:02:20,849 --> 00:02:25,774
horizontal y J es un eje vertical. Y, entonces, la
altura de la superficie muestra J, y

31
00:02:25,774 --> 00:02:30,582
queremos minimizar esta función. Así que
iniciaremos con (theta 0, theta 1)

32
00:02:30,582 --> 00:02:35,375
en algún punto. Imagina que escoges algún
valor para (theta 0 y theta 1), y ese

33
00:02:35,375 --> 00:02:39,934
corresponde a iniciar en algún punto en
la superficie de esta función. ¿Ok? Entonces,

34
00:02:39,934 --> 00:02:44,201
cualquier valor de (theta 0, theta 1)
te dará algún punto de aquí. Sí las

35
00:02:44,201 --> 00:02:48,819
inicié en 0,0 pero,
algunas veces, también las inicias

36
00:02:48,819 --> 00:02:53,942
con otros valores. Ahora, quiero que imagines
que esta imagen muestra una colina. Imagina

37
00:02:53,942 --> 00:02:59,178
que esto es un paisaje de algún parque
con zonas verdes y dos colinas como estas.

38
00:02:59,178 --> 00:03:04,618
Y quiero que imagines que estás
físicamente parado en ese punto de la

39
00:03:04,618 --> 00:03:09,990
colina, en esta pequeña colina roja del parque.
En el gradiente de descenso, lo que

40
00:03:09,990 --> 00:03:15,770
haremos es girar 360 grados y
ver todo nuestro alrededor y preguntarnos: “Si fuera

41
00:03:15,770 --> 00:03:20,423
a dar un pequeño paso en alguna
dirección y quisiera bajar la colina lo

42
00:03:20,423 --> 00:03:25,320
más rápido posible, ¿qué dirección
tomaría ese pequeño paso si quiero ir

43
00:03:25,320 --> 00:03:29,686
abajo, si quiero bajar físicamente 
de esta colina tan rápido como sea

44
00:03:29,686 --> 00:03:34,465
posible?” Resulta que si estuvieras parado
en es punto de la colina, verías todo

45
00:03:34,465 --> 00:03:39,185
el alrededor, descubrirías que la mejor
dirección para un pequeño paso hacia abajo

46
00:03:39,185 --> 00:03:44,035
es más o menos esa dirección. ¿Ok? Ahora
estás en un nuevo punto sobre tu colina.

47
00:03:44,035 --> 00:03:49,430
Una vez más, verás a tu alrededor y dirás:
“¿Qué dirección debo tomar para

48
00:03:49,430 --> 00:03:54,695
dar un pequeño paso cuesta abajo?” Y
si haces eso y das otro paso,

49
00:03:54,695 --> 00:03:59,700
darás un paso en esa dirección, y
después continuarás. Desde este nuevo

50
00:03:59,700 --> 00:04:04,835
punto, verás a tu alrededor, decidirás
qué dirección te llevará cuesta abajo

51
00:04:04,835 --> 00:04:09,775
más rápido, darás otro paso, otro paso,
y así sucesivamente, hasta converger al

52
00:04:09,970 --> 00:04:15,059
mínimo local de aquí. El mayor descenso
tiene una propiedad interesante. Esta primera

53
00:04:15,059 --> 00:04:19,682
vez que usamos el gradiente de descenso,
iniciamos en este punto, ¿cierto?

54
00:04:19,682 --> 00:04:24,183
Iniciamos en este punto de aquí. Ahora,
imagina que iniciamos el gradiente de

55
00:04:24,183 --> 00:04:29,232
descenso unos cuantos pasos a la derecha.
Imagina que iniciamos el gradiente de descenso con

56
00:04:29,232 --> 00:04:34,159
es punto en la parte superior derecha. Si fuera a
repetir este proceso, y te pararas en este

57
00:04:34,159 --> 00:04:39,207
punto, y vieras tu alrededor, darías un paso
pequeño en la dirección del mayor descenso.

58
00:04:39,207 --> 00:04:44,772
Harías eso. Después verías a tu alrededor,
darías otro paso, y así sucesivamente. Y si iniciaras

59
00:04:44,772 --> 00:04:50,570
unos cuantos pasos a la derecha, el
gradiente de descenso te habría llevado

60
00:04:50,570 --> 00:04:56,236
al segundo óptimo local a la
derecha. Si hubieras iniciado en este primer

61
00:04:56,236 --> 00:05:01,602
punto, hubieras obtenido este
óptimo local. Pero si iniciaras tan sólo un

62
00:05:01,602 --> 00:05:06,762
poco, en una ubicación ligeramente diferente,
hubieras obtenido un óptimo local

63
00:05:06,762 --> 00:05:12,197
muy diferente. Y esta es una
propiedad del gradiente de descenso de

64
00:05:12,197 --> 00:05:17,425
la que hablaremos más adelante. Así que,
esa es la intuición en imágenes. Veamos

65
00:05:17,425 --> 00:05:22,929
el mapa. Esta es la definición del
algoritmo de gradiente de descenso. Haremos

66
00:05:22,929 --> 00:05:28,240
esto en varias ocasiones hasta la
convergencia. Vamos a actualizar mi

67
00:05:28,240 --> 00:05:33,543
parámetro theta J al tomar
theta J y restarlo de alfa

68
00:05:33,543 --> 00:05:39,129
por este término de aquí. Entonces,
veamos. Hay muchos detalles en esta

69
00:05:39,129 --> 00:05:45,030
ecuación, voy a desentrañar algunos.
Primero, la notación de aquí, dos puntos, igual a.

70
00:05:45,030 --> 00:05:51,643
Usaremos ":=" para denotar
asignación, así que es el operador

71
00:05:51,643 --> 00:05:57,790
de asignación. En concreto, si
escribo "a:=b", lo que significa en

72
00:05:57,790 --> 00:06:02,878
una computadora es tomar el
valor en b y usarlo para sobrescribir

73
00:06:02,878 --> 00:06:08,517
cualquiera que sea el valor a. Significa
que estableceremos “a” para que sea igual al valor de b.

74
00:06:08,517 --> 00:06:13,674
Bien, eso es asignación. También
puedo usar "a:=a+1". Esto significa

75
00:06:13,674 --> 00:06:18,969
tomar “a” y aumentar su valor en uno.
Mientras que por el contrario, si uso el

76
00:06:18,969 --> 00:06:26,067
signo de igual y escribo "a=b", será una
afirmación verdadera. Entonces si escribo a=b,

77
00:06:26,067 --> 00:06:31,006
estoy afirmando que el valor de a
es igual al valor de b. Así que en el

78
00:06:31,006 --> 00:06:36,331
lado izquierdo, esa es una operación de computadora,
donde estableces cuál será el valor "a". El

79
00:06:36,331 --> 00:06:41,399
lado derecho es una afirmación, sólo
afirmo que los valores de a

80
00:06:41,399 --> 00:06:46,274
y b son los mismos. Y puedo escribir
"a:=a+1" que significa aumentar a en

81
00:06:46,274 --> 00:06:50,764
1. Espero nunca escribir "a=a+1",
porque eso está mal.

82
00:06:50,764 --> 00:06:55,704
a y a+1 nunca pueden ser iguales a
los mismos valores. Bueno, esa es la primera

83
00:06:55,704 --> 00:07:05,733
parte de la definición. Alfa
es un número que se llama

84
00:07:05,733 --> 00:07:12,360
índice de aprendizaje. Y lo que hace alfa es,
básicamente, controlar qué tan grande es un paso

85
00:07:12,360 --> 00:07:17,113
que damos cuesta abajo con el gradiente de descenso. Si
alfa es muy grande, entonces corresponde

86
00:07:17,113 --> 00:07:21,925
a un procedimiento de gradiente de
descenso muy agresivo, donde tratamos de dar

87
00:07:21,925 --> 00:07:26,322
pasos grandes cuesta abajo. Y si alfa es
pequeño, entonces damos pasos muy,

88
00:07:26,322 --> 00:07:31,194
muy pequeños cuesta abajo. Después
hablaré más de esto.

89
00:07:31,194 --> 00:07:35,660
Sobre cómo establecer alfa entre otras cosas.
Y, finalmente, este término de aquí. Ese es el

90
00:07:35,660 --> 00:07:40,582
término de derivada. Por ahora no
quiero hablar de él, pero derivaré este

91
00:07:40,582 --> 00:07:45,564
término y te diré en qué se basa
exactamente. Algunos de ustedes

92
00:07:45,564 --> 00:07:50,547
estarán más familiarizados que otros
con cálculo, pero incluso si no estás

93
00:07:50,547 --> 00:07:55,469
familiarizado, no te preocupes, te diré
lo que necesitas saber sobre este

94
00:07:55,469 --> 00:08:00,580
término. Hay una sutileza más
sobre el gradiente de descenso que es,

95
00:08:00,580 --> 00:08:05,837
en gradiente de descenso, vamos a
actualizar theta 0 y theta 1. Entonces,

96
00:08:05,837 --> 00:08:10,699
esta actualización ocurre cuando j=0, y
j=1. Así que vamos a actualizar j, theta 0,

97
00:08:10,699 --> 00:08:15,955
y a actualizar theta 1. Y la sutileza de
cómo aplicas gradiente de descenso,

98
00:08:15,955 --> 00:08:21,562
para esta expresión, para esta
ecuación de actualización, es que quieres

99
00:08:21,562 --> 00:08:31,384
actualizar de forma simultánea theta 0 y
theta 1. Con esto me refiero a

100
00:08:31,384 --> 00:08:36,432
que en esta ecuación,
vamos a actualizar

101
00:08:36,432 --> 00:08:40,975
theta 0:=theta 0 menos algo, y a actualizar
theta 1:= theta 1 menos algo.

102
00:08:40,975 --> 00:08:45,834
Y la forma de aplicarlo es
que debes calcular el lado

103
00:08:45,834 --> 00:08:52,677
derecho. Calcular eso para theta 0
y theta 1, y después al mismo

104
00:08:52,677 --> 00:08:57,469
tiempo actualizar theta 0 y
theta 1. Voy a explicar a lo que me

105
00:08:57,469 --> 00:09:02,024
refiero. Es una aplicación correcta
del gradiente de descenso que se refiere a las actualizaciones

106
00:09:02,024 --> 00:09:06,461
simultáneas. Voy a establecer temp0 igual a esto,
y temp1 igual a esto. Así que, básicamente,

107
00:09:06,461 --> 00:09:11,430
calcular los lados derechos. Y después de haber
calculado los lados derechos y almacenarlos

108
00:09:11,430 --> 00:09:15,926
juntos en temp0 y temp1,
actualizaré theta0 y theta 1

109
00:09:15,926 --> 00:09:20,245
de forma simultánea, porque esa
es la aplicación correcta. Por el contrario,

110
00:09:20,245 --> 00:09:25,533
esta es una aplicación incorrecta que
no se actualiza de forma simultánea. Así que en

111
00:09:25,533 --> 00:09:31,666
esta aplicación incorrecta, calculamos
temp0, y después actualizamos theta0

112
00:09:31,666 --> 00:09:36,644
y luego calculamos temp1. Posteriormente,
actualizamos temp 1. Y la diferencia entre

113
00:09:36,644 --> 00:09:41,877
las aplicaciones del lado derecho
y el izquierdo es que si vemos aquí,

114
00:09:41,877 --> 00:09:46,791
verás este paso, si para este
momento ya actualizaste theta 0,

115
00:09:46,791 --> 00:09:51,897
entonces estarías utilizando el nuevo
valor de theta 0 para calcular este

116
00:09:51,897 --> 00:09:57,340
término de derivada y esto te da un
valor diferente de temp 1 al del

117
00:09:57,340 --> 00:10:01,565
lado izquierdo, ya que ahora
ya introdujiste el nuevo valor de theta 0

118
00:10:01,565 --> 00:10:05,852
en esta ecuación. Y esto del lado
derecho no es una aplicación correcta del

119
00:10:05,852 --> 00:10:09,916
gradiente de descenso. Así que no
quiero decir por qué necesitas hacer las

120
00:10:09,916 --> 00:10:14,617
actualizaciones simultáneas, resulta
que la forma usual en la que se aplica

121
00:10:14,617 --> 00:10:18,735
la gradiente de descenso, más adelante
diré más, realmente es más

122
00:10:18,735 --> 00:10:22,496
natural implementar la actualización
simultánea. Y cuando la gente habla

123
00:10:22,496 --> 00:10:26,665
sobre gradiente de descenso, siempre se refieren a la
actualización simultánea. Si aplicas la

124
00:10:26,665 --> 00:10:30,630
actualización no simultánea, resulta que
es probable que de todos modos funcione,

125
00:10:30,630 --> 00:10:34,747
pero el algoritmo de la derecha no es a lo
que se refieren las personas como gradiente de descenso

126
00:10:34,747 --> 00:10:38,356
y es otro algoritmo con
diferentes propiedades. Y por varias

127
00:10:38,356 --> 00:10:42,220
razones, esto puede comportarse
de forma un poco extraña. Y

128
00:10:42,220 --> 00:10:46,626
lo que debes hacer es realmente
implementar las actualizaciones simultáneas

129
00:10:46,626 --> 00:10:52,313
del gradiente de descenso. Eso es el esquema del
algoritmo de gradiente de descenso. En el siguiente video,

130
00:10:52,313 --> 00:10:56,998
vamos a detallar el término de
derivada, que escribí pero

131
00:10:56,998 --> 00:11:01,799
realmente no lo definí. Si ya has tomado
una clase de cálculo y estás

132
00:11:01,799 --> 00:11:06,367
familiarizado con las derivadas parciales y las
derivadas, resulta que eso es

133
00:11:06,367 --> 00:11:11,425
exactamente el término de derivada. Pero en caso
de que no estés familiarizado, no te

134
00:11:11,425 --> 00:11:15,680
preocupes. El siguiente video te dará
todas los conceptos y te dirá

135
00:11:15,680 --> 00:11:19,883
todo lo que debes saber para calcular
el término de derivada, incluso si

136
00:11:19,883 --> 00:11:24,296
no has visto cálculo, o si no has visto
derivadas parciales anteriormente. Y con

137
00:11:24,296 --> 00:11:28,288
eso, con el siguiente video, con suerte
podremos darte todos los conceptos

138
00:11:28,288 --> 00:11:30,180
necesarios para aplicar el gradiente de descenso.