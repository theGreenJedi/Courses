Pada video sebelumnya, kita bicara tentang algoritma gradient descent dan bicara tentang model regresi linier dan ketidak-telitian kwadrat fungsi harga. Dalam video ini, kita akan menggunakan gradient descent bersamaan dengan fungsi harga kita, dan itu akan memberikan kita algoritma untuk regresi linier untuk mencocokkan garis lurus ke data kita. Jadi ini apa yang kita kerjakan di video sebelumnya. Itu algoritma gradient descent kita, yang seharusnya akrab, dan Anda melihat model regresi linier dengan hipotesis linier dan ketidak-telitian
kwadrat fungsi harga kita. Apa yang akan kita lakukan adalah menerapkan gradient descent untuk memperkecil ketidak-telitian kwadrat fungsi harga kita. Sekarang, untuk menerapkan gradient descent, untuk menulis potongan kode ini, syarat utamanya kita perlu bagian derivatif di sebelah ini. Jadi, kita perlu tahu apa derivatif parsial ini, dan menyambungkan definisi fungsi harga J, jelas ini menjadi ini sama dengan penjumlahan 1 sampai m dari ketidak-telitian kwadrat fungsi harga J ini, dan semua yang saya lakukan di sini hanya menyambungkan definisi fungsi harga di sana, dan sedikit lebih menyederhanakannya, ini jelas menjadi sama dengan, ini sama dengan penjumlahan 1 sampai m theta nol tambah theta satu, x(i) kurang y(i) kwadrat. Dan semua yang saya lakukan di sana, mengambil definisi untuk hipotesis saya dan menyambungkan itu di sana. Dan jelas kita perlu tahu apa derivatif parsial dari dua kasus, untuk J = 0 dan untuk J = 1, ingin tahu apa derivatif parsial untuk kedua kasus theta(0) dan theta(1). Dan saya akan tulis jawabannya. Jelas bagian pertama ini disederhanakan menjadi 1/m, penjumlahan pada set latihan saya dari x(i) - y(i). Dan untuk bagian ini, derivatif parsial berkenaan dengan theta(1), jelas saya mendapat bagian ini: -y(i) * x(i). Oke Dan menghitung derivatif parsial ini, jadi berangkat dari persamaan ini ke salah satu persamaan di bawah sana, menghitung derivatif parsial itu perlu kalkulus multivariat. Jika Anda tahu kalkulus, silahkan kerjakan derivatif itu sendiri dan cek derivatif itu Anda mendapatkan jawaban yang saya dapatkan. Tapi jika Anda kurang akrab dengan kalkulus, Anda jangan khawatirkan itu, dan tidak mengapa hanya mengambil dan menggunakan persamaan ini, dan Anda tidak perlu tahu kalkulus atau apapun yang seperti itu untuk mengerjakan PR. Jadi untuk mengimplementasikan
gradient descent, Anda akan dapatkan itu bekerja. Namun demikian, sesudah definisi ini, atau sesudah apa yang telah kita pecahkan menjadi derivatif itu, benar-benar hanya kemiringan dari fungsi harga J. Sekarang kita dapat menyambungkan mereka kembali ke algoritma gradient descent kita. Jadi ini gradient descent atau regresi, yang akan mengulang hingga konvergen, theta 0 dan theta satu diperbarui sebagai minus alpha yang sama kali bagian derivatifnya. Jadi, bagian ini di sini. Jadi, ini algoritma regresi linier kita. Betul? adalah, tentu saja, hanya derivatif parsial dari theta nol, yang kita kerjakan di slide sebelumya. Dan bagian kedua di sini, bagian itu hanya derivatif parsial dari theta satu yang kita kerjakan pada baris sebelumnya. Dan hanya mengingatkan, Anda harus, saat mengimplementasikan gradient descent, ada rincian yang Anda harus implementasikan sehingga theta nol dan theta satu diperbarui serentak. Mari kita lihat bagaimana gradient descent bekerja. Satu persoalan yang kita pecahkan gradient descent bahwa itu bisa mudah mencapai local optima. Jadi, ketika saya menjelaskan gradient descent pertama kali, saya menunjukkan Anda gambar ini, menuruni bukit permukaan ini dan kita lihat bagaimana, tergantung dimana Anda memulai, Anda bisa mendapatkan local optima berbeda. Anda tahu, Anda bisa berakhir di sini atau sini. Tapi, jelas bahwa fungsi harga untuk gradient fungsi harga untuk regresi linier selalu akan menjadi fungsi lengkungan seperti ini. Istilah teknis untuk ini adalah fungsi cembung. Dan saya tidak akan memberi definisi formil akan apa itu fungsi cembung, tapi secara tidak resmi fungsi cembung artinya fungsi lengkungan, seperti melengkung. Jadi, fungsi ini tidak punya local optima, kecuali global optimum. Dan gradient descent fungsi harga jenis ini yang Anda dapatkan kapanpun Anda menggunakan regresi linier, itu akan selalu mengkonversi ke global optimum, karena tidak ada local optima lain
selain global optimum. Jadi sekarang, mari kita lihat kerja algoritma ini. Seperti biasa, ini plot fungsi hipotesis dan fungsi harga J saya. Jadi, mari lihat bagaimana menginisialisasi parameter saya pada nilai ini. Katakanlah, biasanya Anda menginisialisasi parameter Anda pada nol untuk nol, theta nol dan nol. Untuk ilustrasi pada presentasi khusus ini, saya telah menginisialisasi theta nol di sekitar 900, dan theta satu di sekitar -0.1, okey?