1
00:00:00,000 --> 00:00:04,934
Ранее мы определили функцию затрат J.
В этом видео я хотел бы рассказать вам о методе, называемом

2
00:00:04,934 --> 00:00:09,634
градиентным спуском, который
применяется для минимизации функции

3
00:00:09,634 --> 00:00:14,275
стоимости J. Градиентный спуск
оказывается более общим алгоритмом и применяется не только для линейной

4
00:00:14,275 --> 00:00:18,916
регрессии.На самом деле, он
повсеместно используется и в машинном обучении.Далее в

5
00:00:18,916 --> 00:00:23,791
рамках курса мы будем применять метод
градиентного спуска для минимизации и других

6
00:00:23,791 --> 00:00:27,845
функций, не только функции стоимости J для линейной регрессии.
Итак, в этом видео я собираюсь рассказать вам о методе

7
00:00:27,845 --> 00:00:32,558
градиентного спуска минимизации некой
произвольной функции J. В

8
00:00:32,558 --> 00:00:37,406
последующих видео мы возьмем этот алгоритм и применим его к
той конкретной функции стоимости J,

9
00:00:37,406 --> 00:00:43,332
которую мы сформировали для
линейной регрессии.Итак, сформулируем

10
00:00:43,332 --> 00:00:48,112
задачу.Пусть имеется некоторая функция J, зависящая от
параметров тета нулевое и тета первое.

11
00:00:48,112 --> 00:00:52,773
Может, это функция затрат
в задаче линейной регрессии.Может быть, это какая-то

12
00:00:52,773 --> 00:00:56,801
другая функция, минимум которой мы хотим найти.И мы хотим
придумать алгоритм, минимизирующий

13
00:00:56,801 --> 00:01:01,174
эту функцию от J по параметрам
тета нулевое и тета первое.Собственно говоря,

14
00:01:01,174 --> 00:01:05,793
метод градиентного спуска
применим к более широкому классу

15
00:01:05,793 --> 00:01:10,994
функций.Представьте, что у вас
есть функция J от параметров тета

16
00:01:10,994 --> 00:01:16,194
нулевое, тета первое, тета
второе и так далее, до тета энного.И вы хотите

17
00:01:16,405 --> 00:01:21,795
минимизировать ее значение по
всем этим параметрам.

18
00:01:21,795 --> 00:01:26,580
Алгоритм градиентного спуска
решает и эту, более

19
00:01:26,580 --> 00:01:31,368
общую задачу, но ради краткости и
простоты обозначений в течение

20
00:01:31,368 --> 00:01:35,935
этого видео я буду
использовать только

21
00:01:36,113 --> 00:01:41,097
два параметра.Вот в чем
идея градиентного спуска.Сперва мы

22
00:01:41,097 --> 00:01:45,882
наугад возьмем некоторые начальные значения
для тета нулевого и тета

23
00:01:45,882 --> 00:01:50,788
первого.Большой разницы нет, но
полагают тета нулевое равным

24
00:01:50,788 --> 00:01:55,452
нулю и тета первое тоже равным
нулю.Просто зададим

25
00:01:55,452 --> 00:02:00,322
им начальное значение 0.В ходе работы
градиентного спуска мы будем понемногу менять

26
00:02:00,322 --> 00:02:05,258
их значения, пытаясь
уменьшить значение J, рассчитывая на

27
00:02:05,258 --> 00:02:10,571
то, чтобы в конце концов оказаться в точке минимума или локального
минимума.Давайте

28
00:02:10,796 --> 00:02:16,106
посмотрим, как работает градиентный спуск, на
графике.Допустим, я пытаюсь найти

29
00:02:16,106 --> 00:02:20,849
минимум этой функции.Обратите внимание на оси.Тета нулевое
и тета первое по горизонтальным

30
00:02:20,849 --> 00:02:25,774
осям, J по вертикальной оси.То есть высота
поверхности соответствует

31
00:02:25,774 --> 00:02:30,582
значению функции J, и мы хотим найти ее минимум.Начнем с некоторой
точки, соответствующей паре (тета нулевое, тета

32
00:02:30,582 --> 00:02:35,375
первое).Представьте, что вы
взяли какие-то значения для тета

33
00:02:35,375 --> 00:02:39,934
нулевого и тета первого, и нашли
соответствующую им точку поверхности.Так?Допустим,

34
00:02:39,934 --> 00:02:44,201
ваша пара (тета нулевое, тета первое) дает вам вот
эту точку.Мои

35
00:02:44,201 --> 00:02:48,819
начальные значения равны
нулю, хотя иногда начина

36
00:02:48,819 --> 00:02:53,942
ют и с других значений.Теперь.Представьте себе, что
эта поверхность - изображение холмистой местности.Например,

37
00:02:53,942 --> 00:02:59,178
полного растительности парка, с двумя
холмами... Нечто в этом духе.

38
00:02:59,178 --> 00:03:04,618
И я хочу, чтобы вы вообразили, что в
самом деле стоите в этой точке на

39
00:03:04,618 --> 00:03:09,990
склоне холма в вашем парке.Метод
градиентного спуска предполагает, что

40
00:03:09,990 --> 00:03:15,770
мы оборачиваемся вокруг себя на
360 градусов,

41
00:03:15,770 --> 00:03:20,423
осматриваем окрестности, и задаем себе вопрос: "Если я
собираюсь сделать

42
00:03:20,423 --> 00:03:25,320
маленький шаг в каком-нибудь
направлении, и я хочу спуститься вниз

43
00:03:25,320 --> 00:03:29,686
как можно быстрее, в каком направлении
мне следует сделать этот мой шаг?

44
00:03:29,686 --> 00:03:34,465
Если мне нужно вниз, если я в некотором
роде хочу физически спуститься

45
00:03:34,465 --> 00:03:39,185
вниз по этому холму как можно
быстрее?" Оказывается, что

46
00:03:39,185 --> 00:03:44,035
если мы стоим в этой точке на холме, то, осмотревшись вокруг, мы обнаружим, что лучше всего сделать свой шаг примерно в этом направлении.Так?И теперь вы
уже в этой точке холма.

47
00:03:44,035 --> 00:03:49,430
Вы снова осматриваетесь и спрашиваете
себя, в каком направлении сделать

48
00:03:49,430 --> 00:03:54,695
маленький шаг, чтобы
оказаться еще чуточку

49
00:03:54,695 --> 00:03:59,700
ниже. Определившись, вы
делаете шаг сюда и продолжаете идти.Вы знаете, что

50
00:03:59,700 --> 00:04:04,835
делать на новом месте: осмотреться,
выбрать направление, которое

51
00:04:04,835 --> 00:04:09,775
позволит вам поскорее спуститься,
сделать шаг, затем еще шаг, и так далее, пока вы, наконец, не окажетесь

52
00:04:09,970 --> 00:04:15,059
в этом локальном минимуме.Градиентный спуск
обладает интересным свойством.В первый раз

53
00:04:15,059 --> 00:04:19,682
мы начинали градиентный спуск
с вот этой точки, так?

54
00:04:19,682 --> 00:04:24,183
Вот с этой начальной точки.Теперь
представьте, что мы начали градиентный спуск

55
00:04:24,183 --> 00:04:29,232
всего на пару шагов правее.
То есть начальной точкой алгоритма стала вот эта, чуть выше

56
00:04:29,232 --> 00:04:34,159
и правее.Если повторить
процесс: встать здесь,

57
00:04:34,159 --> 00:04:39,207
осмотреться,сделать шаг в
направлении самого крутого уклона,

58
00:04:39,207 --> 00:04:44,772
вот сюда,снова осмотреться,
сделать еще шаг, и так далее,окажется, что, если

59
00:04:44,772 --> 00:04:50,570
начать всего в двух шагах правее,
градиентный спуск приведет нас во

60
00:04:50,570 --> 00:04:56,236
второй локальный минимум,
справа.То есть, если вы начнете в

61
00:04:56,236 --> 00:05:01,602
первой точке, вы окажетесь в этом
локальном минимуме,но если вы чуть-чуть

62
00:05:01,602 --> 00:05:06,762
измените начальное
положение, то окажетесь совсем в другом

63
00:05:06,762 --> 00:05:12,197
локальном минимуме.Об этом
свойстве алгоритма градиентного спуска

64
00:05:12,197 --> 00:05:17,425
мы немного поговорим позже.Итак, по графику
мы получили интуитивное представление об алгоритме.Давайте разберемся

65
00:05:17,425 --> 00:05:22,929
с математической стороной дела.Это определение
алгоритма градиентного спуска.Мы просто

66
00:05:22,929 --> 00:05:28,240
будем повторять этот шаг.Пока алгоритм
не сойдется в одной точке.Мы будем менять

67
00:05:28,240 --> 00:05:33,543
значение параметра тета с индексом j, для j
равного 0 и 1, вычитая из него вот это выражение с коэффициентом

68
00:05:33,543 --> 00:05:39,129
альфа.Давайте
разберемся.В этом выражении есть

69
00:05:39,129 --> 00:05:45,030
несколько деталей, которые я хочу прояснить.
Во-первых, вот это обозначение: двоеточие и знак равенства.

70
00:05:45,030 --> 00:05:51,643
Мы будем использовать «:=»
для обозначения присваивания, то

71
00:05:51,643 --> 00:05:57,790
есть это оператор присваивания.То есть, если я
пишу «a := b», с точки зрения

72
00:05:57,790 --> 00:06:02,878
компьютера это означает «взять
значение переменной b

73
00:06:02,878 --> 00:06:08,517
и заменить на него значение переменной a».
Тем самым, значение a станет равно значению b. Это

74
00:06:08,517 --> 00:06:13,674
присваивание.Я могу
написать и «a := a + 1».Это означает

75
00:06:13,674 --> 00:06:18,969
«взять значение переменной a и увеличить его на единицу».
Если же я использую только знак равенства

76
00:06:18,969 --> 00:06:26,067
и напишу «a = b», это будет утверждение
факта.Если я

77
00:06:26,067 --> 00:06:31,006
пишу «a = b», это означает, что я утверждаю,
что значение a равно значению b. То есть слева

78
00:06:31,006 --> 00:06:36,331
у меня здесь — компьютерная операция, устанавливающая
значение переменной а,

79
00:06:36,331 --> 00:06:41,399
а справа — утверждение:
я утверждаю,

80
00:06:41,399 --> 00:06:46,274
что значения a и b совпадают.Таким образом, хоть я
и могу написать «a := a + 1», что означает «увеличить значение a

81
00:06:46,274 --> 00:06:50,764
на 1»,надеюсь, я никогда не напишу «a = a + 1».
Потому что это просто неверно.

82
00:06:50,764 --> 00:06:55,704
a и a + 1 никогда не могут
быть равны.Это была первая

83
00:06:55,704 --> 00:07:05,733
часть определения.Числовой
коэффициент альфа называется

84
00:07:05,733 --> 00:07:12,360
скоростью обучения.Он определяет, насколько
большой шаг мы делаем на каждой

85
00:07:12,360 --> 00:07:17,113
итерации градиентного спуска.Так, большое
значение альфа соответствует

86
00:07:17,113 --> 00:07:21,925
решительному спуску: мы будем стараться
делать огромные шаги вниз по

87
00:07:21,925 --> 00:07:26,322
склону.При очень маленьком
значении альфа мы будем спускаться

88
00:07:26,322 --> 00:07:31,194
крохотными шажками.Я еще вернусь к
обсуждению этого коэффициента

89
00:07:31,194 --> 00:07:35,660
и расскажу о том, как выбирать значение альфа.
И, наконец, вот этот элемент.Это производная.

90
00:07:35,660 --> 00:07:40,582
Сейчас я не буду говорить
о ней подробно, но позже я выведу ее и

91
00:07:40,582 --> 00:07:45,564
объясню, откуда
она взялась.Некоторые из

92
00:07:45,564 --> 00:07:50,547
вас знакомы с дифференциальным исчислением лучше других, но
даже если вы не знаете, что

93
00:07:50,547 --> 00:07:55,469
это, не беспокойтесь, я расскажу об этом выражении
все, что вам будет

94
00:07:55,469 --> 00:08:00,580
необходимо.Осталась лишь одна
тонкость в устройстве алгоритма градиентного спуска.

95
00:08:00,580 --> 00:08:05,837
На каждом шаге мы будем изменять тета нулевое и тета
первое.Это

96
00:08:05,837 --> 00:08:10,699
присваивание относится к тета нулевому и тета
первому.Мы изменяем значение

97
00:08:10,699 --> 00:08:15,955
обоих параметров.И тонкость
состоит в том, что, применяя градиентный

98
00:08:15,955 --> 00:08:21,562
спуск, выполняя это присваивание,
вы должны обновить

99
00:08:21,562 --> 00:08:31,384
значения тета нулевого и тета первого
одновременно.Вот что я имею в

100
00:08:31,384 --> 00:08:36,432
виду. Это присваивание
означает: вычесть из тета нулевого некую величину и

101
00:08:36,432 --> 00:08:40,975
вычесть из тета
первого некую другую величину.

102
00:08:40,975 --> 00:08:45,834
И это должно быть сделано так,
чтобы вы сначала вычисляли

103
00:08:45,834 --> 00:08:52,677
правую часть,вычисляли эту некую
величину и для тета нулевого, и для тета первого, а затем уже обновляли

104
00:08:52,677 --> 00:08:57,469
значения тета нулевого и тета
первого.Еще раз продемонстрирую, что я

105
00:08:57,469 --> 00:09:02,024
имею в виду.Здесь приведено
корректное описание работы алгоритма, учитывающее необходимость

106
00:09:02,024 --> 00:09:06,461
одновременного изменения.Сначала я присвою это
переменной temp0, а это — переменной temp1.То есть, фактически,

107
00:09:06,461 --> 00:09:11,430
вычислю правые части.А затем, уже
после того, как я вычислил правые

108
00:09:11,430 --> 00:09:15,926
части и сохранил их величины в переменных
temp0 и temp1, я обновляю и значение тета нулевого, и значение

109
00:09:15,926 --> 00:09:20,245
тета первого, потому что это необходимо для правильной
работы алгоритма.Сопоставьте

110
00:09:20,245 --> 00:09:25,533
это с неверной реализацией алгоритма, в которой параметры обновляются
не одновременно.Здесь, в

111
00:09:25,533 --> 00:09:31,666
некорректной реализации, мы вычисляем temp0, затем
заменяем этим новым значением

112
00:09:31,666 --> 00:09:36,644
тета нулевое,Затем
обновляем тета_1.И разница

113
00:09:36,644 --> 00:09:41,877
между описаниями алгоритма, приведенными
слева и справа, в том, что — вглядитесь

114
00:09:41,877 --> 00:09:46,791
в этот шаг, — здесь мы уже
изменили тета нулевое, и при

115
00:09:46,791 --> 00:09:51,897
вычислении этой производной мы будем
использовать уже новое

116
00:09:51,897 --> 00:09:57,340
значение тета нулевого, то есть получим
другую величину для temp1, не такую, как

117
00:09:57,340 --> 00:10:01,565
в описании слева. Потому что мы уже
заменили старое значение тета

118
00:10:01,565 --> 00:10:05,852
нулевого.Таким образом,
реализация градиентного спуска,

119
00:10:05,852 --> 00:10:09,916
приведенная справа, некорректна.Я не хочу
сейчас разбираться, почему одновременное

120
00:10:09,916 --> 00:10:14,617
обновление необходимо. Оказывается,
что обычно при реализации алгоритма,

121
00:10:14,617 --> 00:10:18,735
мы еще поговорим об этом,
использовать одновременное

122
00:10:18,735 --> 00:10:22,496
обновление — более естественный
подход.И при

123
00:10:22,496 --> 00:10:26,665
обсуждении градиентного спуска всегда подразумевается одновременное
обновление параметров.Если вы

124
00:10:26,665 --> 00:10:30,630
будете использовать
последовательное обновление, возможно,

125
00:10:30,630 --> 00:10:34,747
ваш алгоритм все равно сработает.
Но описание, приведенное справа, это

126
00:10:34,747 --> 00:10:38,356
просто не то, что называют
алгоритмом градиентного спуска, это другой алгоритм с другими свойствами.

127
00:10:38,356 --> 00:10:42,220
Который по некоторым причинам может вести себя немного
более странно.Так

128
00:10:42,220 --> 00:10:46,626
что для градиентного
спуска вам следует использовать

129
00:10:46,626 --> 00:10:52,313
одновременное обновление.Итак, мы в общих
чертах разобрали алгоритм градиентного спуска.В следующем

130
00:10:52,313 --> 00:10:56,998
видео мы подробно разберемся с
производной, которую я написал, но никак пока

131
00:10:56,998 --> 00:11:01,799
не объяснил.Если вы
проходили курс дифференциального

132
00:11:01,799 --> 00:11:06,367
исчисления и знакомы с
частными производными, — это, собственно,

133
00:11:06,367 --> 00:11:11,425
она и есть, частная производная.Если вы не
знакомы с дифференциальным

134
00:11:11,425 --> 00:11:15,680
исчислением, ничего страшного.Следующее
видео даст вам интуитивное

135
00:11:15,680 --> 00:11:19,883
представление о ней, и все остальное, что
необходимо для ее вычисления, даже если

136
00:11:19,883 --> 00:11:24,296
вы не проходили производные или не
знакомы с частными производными.И таким

137
00:11:24,296 --> 00:11:28,288
образом, я надеюсь, со следующим видео у вас появится понимание того, как
применять градиентный

138
00:11:28,288 --> 00:11:30,180
спуск.