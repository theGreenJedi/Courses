Im vorherigem Video haben wir Gradient Descent mathematisch definiert. Lasst uns nun tiefer einsteigen und dieses Video soll ein besseres Verständnis über die Vorgänge im Algorithmus geben, und erklären warum die Schritte des Gradient Descent Algorithmusses Sinn machen. Hier ist der Gradient Descent Algorithmus den wir das letzte mal gesehen haben. Zur Erinerung, dieser Parameter, oder dieser Term, Alpha, wird Lernrate genannt. Und er kontroliert die Schrittgröße beim updaten vom Parameter theta J. Und der zweite Term hier ist der abgeleitete Term. Und was ich in diesem Video machen möchte, ist euch ein besseres Verständnis über diese beiden Terme geben, was sie tuen und warum die beiden Terme zusammengesetzt beim updaten Sinn machen. Um das Verständnis zu vermitteln, möchte ich ein vereinfachtes Beispiel nehmen, wo wir die Funktion mit nur einem Parameter minimieren. So, also wir haben eine, sagen wir eine, Funktion J mit nur einem Paramter, theta Eins, wie wir hatten, du weißt, ein paar Videos bevor. Wo theta Eins ist eine reelle Zahl, okay? Das ermöglicht uns ein-dimensionale plots zu erzeugen, die leichter anzuschauen sind. Und lasst uns versuchen zu verstehen warum einstufen und absinken ihre Aufgabe an der Funktion erledigen. So, sagen wir, das ist unsere Funktion. J von theta Eins, wo theta eine reele Zahl ist. Sagen wir, dass ich den Gradient Descent mit theta Eins an dieser Stelle initializiert habe.