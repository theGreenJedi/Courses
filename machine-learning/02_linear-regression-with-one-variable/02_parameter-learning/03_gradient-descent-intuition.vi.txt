Trong video trước đó, chúng tôi đã cung cấp một định nghĩa toán học của chuyển màu gốc. Chúng ta hãy nghiên cứu kỹ sâu hơn, và trong video này, có được trực giác tốt hơn về những gì các thuật toán thực hiện, và tại sao các bước của thuật toán gradient descent có thể làm cho ý thức. Dưới đây là các thuật toán gradient descent mà chúng tôi thấy thời gian qua. Và, chỉ để nhắc nhở bạn, tham số này, hoặc này thuật ngữ, alpha, được gọi là tỷ lệ học tập. Và nó kiểm soát như thế nào lớn một bước chúng tôi mất khi Cập Nhật của tôi theta tham số J. và thuật ngữ này lần thứ hai ở đây là khâu. Và những gì tôi muốn làm trong video này là cung cấp cho bạn tốt hơn trực giác về những gì mỗi trong số này hai điều khoản đang làm và tại sao, khi đặt cùng với nhau, Cập Nhật toàn bộ này làm cho tinh thần.
Để chuyển tải những intuitions, những gì Tôi muốn làm là sử dụng một ví dụ đơn giản hơn một chút nơi chúng tôi muốn giảm thiểu các chức năng của chỉ là một tham số. Như vậy, vì vậy chúng tôi có một, nói rằng chúng tôi đã gây ra chức năng j của chỉ cần một tham số, theta một, như chúng tôi đã làm, bạn đã biết, một vài video trở lại. Nơi Theta một trong những là một số thực, okay? Chỉ cần để chúng tôi có thể có 1 D lô, mà là một chút đơn giản hơn để xem xét. Hãy thử để hiểu tại sao lớp và gốc sẽ làm vào chức năng này.
[âm thanh]. Vì vậy, hãy nói rằng đây là chức năng của tôi. J theta một, và vì vậy đó của tôi, và nơi theta một trong những là một số thực. Quyền, bây giờ chúng ta hãy nói rằng tôi đã khởi gradient descent với theta một tại vị trí này. Rất hình ảnh mà chúng tôi bắt đầu vào thời điểm đó về chức năng của tôi. Những gì gradient descent sẽ làm, là nó sẽ Cập Nhật. Một trong được Cập Nhật như Theta một trừ Alpha Theta lần DD Theta một J L. Theta một ngay và oh một chỉ là một sang một bên, bạn biết điều này, điều này khâu nếu đúng, nếu bạn tự hỏi tại sao tôi đã thay đổi các ký hiệu từ các biểu tượng này bắt nguồn từ một phần. Nếu bạn không biết những gì là sự khác biệt giữa các biểu tượng này bắt nguồn từ một phần và dd theta Đừng lo lắng về nó. Về mặt kỹ thuật trong toán học chúng tôi gọi đây là bắt nguồn từ một phần, chúng tôi gọi đây là một sản phẩm phái sinh, tùy thuộc vào số lượng, các tham số trong các chức năng J, nhưng đó là một tiêu chuẩn kỹ thuật toán học, vì vậy, bạn biết cho mục đích của bài giảng này, hãy suy nghĩ của các biểu tượng một phần, và DD theta một trong như chính xác cùng một điều. Và đừng lo lắng về việc liệu có bất kỳ sự khác biệt.
I 'm gonna cố gắng sử dụng các toán học ký hiệu chính xác. Nhưng cho các mục đích của chúng tôi, các tả thực sự là điều tương tự. Vì vậy, hãy xem phương trình này, này sẽ làm gì. Và vì vậy chúng tôi sẽ tính toán này bắt nguồn từ của, I 'm không chắc chắn nếu bạn đã nhìn thấy phái sinh trong tính toán trước. Nhưng những gì một sản phẩm phái sinh, vào thời điểm này, không, là về cơ bản nói rằng, bạn đã biết, chúng ta hãy. Mất ốp như điểm, đường thẳng như thế, dòng màu đỏ, chỉ cần, chỉ cần touching chức năng này và hãy xem xét độ dốc của dòng màu đỏ này. Đó là Where đạo hàm 's. Nó nói gì là độ dốc của đường chỉ ốp chức năng, okay và độ dốc của đường là tất nhiên chỉ đúng, bạn biết chỉ chiều cao chia cho điều này ngang. Bây giờ. Dòng này có một tích cực dốc, vì vậy nó có một đạo hàm tích cực. Và như vậy, Cập Nhật của tôi để theta là sẽ có, theta nhất cho dữ liệu [unintelligible] một trong trừ alpha lần một số tích cực số. >> Được rồi. Tuy nhiên, tỷ lệ học tập là luôn luôn một số dương. Và như vậy I 'm gonna phải theta một, bản cập nhật này như theta một trừ đi một cái gì đó. Do đó, I 'm gonna kết thúc di chuyển theta một bên trái. I 'm gonna giảm theta ai và chúng tôi có thể nhìn thấy Đây là điều phải làm vì tôi đã thực sự đi trước theo hướng này bạn biết để làm cho tôi gần hơn đến tối thiểu trên đó. Vì vậy, gradient descent cho đến nay có vẻ sẽ thực hiện đúng. Hãy xem xét một ví dụ khác. Vì vậy, chúng ta hãy của tôi như vậy chức năng j. Chỉ cần cố gắng để vẽ cùng một chức năng j của theta một trong. Và bây giờ chúng ta hãy nói Tôi thay vào đó có khởi tạo tham số của tôi trên đó ở bên trái. Vì vậy, theta một là Ở đây. Tôi đang gonna thêm đó điểm trên bề mặt. Bây giờ, tôi khâu, d, d Theta một j của theta một, khi đánh giá tại thời điểm này, gonna tìm ở bên phải. Các độ dốc của đường đó. Vì vậy, thuật ngữ này bắt nguồn từ là một dốc của dòng này. Nhưng điều này dòng slanting xuống, do đó, dòng này có độ dốc tiêu cực. Quyền? Hoặc cách khác tôi nói rằng chức năng này có đạo hàm tiêu cực, chỉ có nghĩa là tiêu cực độ dốc lúc thời điểm đó. Vì vậy, điều này là ít hơn bình đẳng bằng không. Vì vậy, khi tôi Cập Nhật theta, sau đó nếu Theta Cập Nhật theta trừ alpha đôi khi một số tiêu cực. Và vì vậy tôi cần Theta một trừ đi một số tiêu cực có nghĩa là tôi thực sự sẽ tăng dữ liệu, quyền? Bởi vì đây là trừ đi của một phương tiện số tiêu cực tôi thêm một cái gì đó để theta và những gì mà có nghĩa là rằng tôi sẽ kết thúc tăng theta. Và vì vậy chúng tôi sẽ bắt đầu ở đây và tăng theta, một lần nữa có vẻ như điều tôi muốn làm để thử để làm cho tôi gần hơn đến tối thiểu. Vì vậy, hy vọng rằng điều này giải thích trực giác phía sau khâu là làm những gì. Hãy [unintelligible] xem xét việc học ngày Alpha, và cố gắng tìm ra những gì mà là làm. Vì vậy, đây là người gốc lớn hơn của tôi Cập nhật các quy tắc. Quyền, đó là phương trình này và hãy xem xét những gì có thể xảy ra, nếu Alpha là hoặc là quá nhỏ, hoặc nếu Alpha là quá lớn. Như vậy ví dụ đầu tiên này, những gì sẽ xảy ra nếu Alpha quá nhỏ. Vì vậy, ở đây là của tôi theta [unintelligible] j. chức năng. Cho phép chỉ cần bắt đầu ở đây. Nếu alpha quá nhỏ thì những gì tôi sẽ làm là gonna nhân [unintelligible] bởi một số lượng nhỏ. Vì vậy, sẽ bước [unintelligible] Giống như vậy. Rồi, do đó, đó là một bước [unintelligible]. Sau đó từ thời điểm này mới chúng tôi đang gonna mất một bước [unintelligible] alpha là quá nhỏ cho phép đi khác ít em bé bước. Và vì vậy nếu và vì vậy, nếu tỷ lệ học tập của tôi là quá nhỏ. Tôi đang gonna cuối cùng lên, bạn biết. Dùng những bước nhỏ, nhỏ bé. Để thử để có được tối thiểu và tôi gonna cần. Rất nhiều bước để nhận được tối thiểu và như vậy. Nếu alpha của quá nhỏ, có thể được làm chậm vì nó gonna mất những em bé nhỏ, nhỏ bước. Và nó gonna cần rất nhiều bước trước khi nó được bất kỳ nơi gần với tối thiểu toàn cầu. Bây giờ, [unintelligible] để nộp. Vì vậy, ở đây là các chức năng của dữ liệu [unintelligible] của tôi. Kể từ f là quá lớn, sau đó chấm điểm ý thức có thể vượt qua tối thiểu và có thể thậm chí chưa hội tụ hay thậm chí phân ra. [unintelligible] như vậy tối thiểu ireful vì vậy hội đồng thành phố phái sinh ngay rằng nếu văn phòng quá lớn mất một bước tiến lớn, phải mất một bước rất lớn như thế [unintelligible], và mất một bước rất lớn và bây giờ các chức năng chéo là mạnh nhất, bắt đầu với giá trị này, nhưng bây giờ giá trị của tôi đã đi xuống. Bây giờ là của tôi dẫn xuất bạn biết điểm bên trái đánh giá các dữ liệu yếu. Nhưng nếu khu vực học tập của tôi là để lớn tôi có thể mất một vài gashes đi từ đây tất cả các cách trên mạng vì vậy tôi sẽ chỉ. Đang được tất cả có. Quyền? Và nếu học tập của tôi đã to lớn tôi có thể mất một bước rất lớn trên các tiếp theo tăng tốc và loại vượt qua và vượt qua vv cho đến khi bạn nhận thấy Tôi thực sự nhận được tiếp tục và tiếp tục đi từ tối thiểu. Và vì vậy, nếu alpha là để lớn nó có thể không hội tụ hay thậm chí phân ra. Bây giờ. Tôi có một câu hỏi cho bạn. Vì vậy, đây là một trong những rối rắm. Và khi tôi lần đầu tiên học tập công cụ này, nó thực sự tôi có phải mất một thời gian dài để con số này ra ngoài.
Điều gì nếu bạn theta pre-emptive một trong là đã có tại một tối thiểu địa phương? Bạn nghĩ gì đi một bước của lớp và gốc sẽ làm gì? Vì vậy, hãy giả sử bạn khởi tạo dữ liệu một tối thiểu địa phương. Vì vậy, bạn biết cho rằng đây là giá trị ban đầu của bạn của 01 trên đây và nó đã được tại một địa phương tối ưu và tối thiểu địa phương. Nó sẽ gửi ra rằng tại địa phương tối ưu phái sinh của bạn sẽ được bằng không. Vì nó là dốc đó là thời điểm ốp vì vậy các độ dốc của dòng này sẽ tương đương với số không và vì thế này khâu. Bằng Zero. Và vì vậy, trong của bạn Cập Nhật lớp và gốc, bạn có theta một, [unintelligible] này một, theta trừ alpha lần zero.
Và vì vậy, điều này có nghĩa là, nếu bạn đã lúc một tối ưu địa phương, nó lá theta một trong không thay đổi. ?Nguyên nhân, bạn đã biết, [unintelligible] theta một trong. Bằng theta một trong.
Vì vậy, nếu tham số của bạn đã tại một địa phương tối thiểu, đi một bước của lớp và dòng máu hiện hoàn toàn không có gì. Nó không thay đổi tham số là, đó là những gì bạn muốn. Cuz nó giữ giải pháp của bạn tại các địa phương tối ưu. Điều này cũng giải thích tại sao lớp và gốc có thể trò chuyện các địa phương tối thiểu, ngay cả với tỷ lệ học tập Alpha cố định. Đây là những gì tôi có nghĩa là bởi đó. Chúng ta hãy xem xét một ví dụ. Vì vậy, đây là một chức năng chi phí J. Với dữ liệu. Rằng có lẽ tôi muốn để giảm thiểu và giả sử tôi khởi tạo của tôi giải thuật [unintelligible] thuật toán bạn biết trên mạng tại điểm đó đỏ tươi. Nếu tôi đi một bước của gradient descent, bạn biết, có thể có tôi sẽ đưa tôi đến đó điểm cuz của tôi dẫn xuất khá dốc lên đó ngay. Bây giờ tôi đang ở thời điểm màu xanh lá cây này và nếu tôi đi một bước tại [unintelligible] gốc bạn thông báo rằng đạo hàm của tôi có nghĩa là độ dốc là ít dốc tại điểm màu xanh lá cây trong so với lúc [unintelligible] chỉ ra có ngay. Bởi vì khi tôi tiếp cận các tối thiểu đạo hàm của tôi được gần hơn và gần bằng không như tôi tiếp cận tối thiểu. So. Sau khi một bước của lớp và gốc, phái sinh mới của tôi là một chút nhỏ hơn. Vì vậy, tôi muốn có một bước của lớp và dòng máu. Tôi tự nhiên sẽ đưa một phần nào nhỏ hơn bước từ màu xanh lá cây điểm hơn tôi đã làm từ điểm đỏ tươi. Bây giờ bởi mới điểm, đỏ điểm và sau đó bây giờ thậm chí gần hơn đến toàn cầu tối thiểu, do đó bắt nguồn từ đây sẽ nhỏ hơn so với nó tại điểm màu xanh lá cây. Vì vậy, khi tôi đi một bước của [unintelligible], bạn đã biết, bây giờ tôi khâu là nhỏ hơn, và do đó các độ lớn của bản Cập Nhật vào theta [unintelligible] là nhỏ hơn, vì bạn có thể bước nhỏ như vậy, và là lớn hơn xuống chạy. Bạn sẽ tự động lấy nhỏ hơn và các bước nhỏ hơn cho đến khi cuối cùng bạn đang dùng các bước rất nhỏ, bạn đã biết, và bạn tìm converge để các tối thiểu địa phương. Vì vậy, chỉ để recap. Trong chuyển màu gốc khi chúng tôi tiếp cận tối thiểu địa phương, chấm điểm gốc sẽ tự động đưa các bước nhỏ hơn và đó là bởi vì khi chúng tôi tiếp cận tối thiểu địa phương, theo định nghĩa tối thiểu địa phương là khi bạn có này bắt nguồn từ bằng 0. Vì vậy, như chúng tôi tiếp cận tối thiểu địa phương này định lý sản sẽ tự động nhận được nhỏ hơn và Vì vậy, gradient descent sẽ tự động bước nhỏ. Vì vậy, đây là những gì [unintelligible] giống như, và như vậy trên thực tế không có cần phải giảm alpha làm thêm giờ. Vì vậy, đó là thuật toán lớp và gốc, và bạn có thể sử dụng nó để giảm thiểu, để cố gắng giảm thiểu bất kỳ chức năng nguyên nhân J.
Không gây ra chức j được định nghĩa cho hồi quy tuyến tính. Trong video tiếp theo, chúng ta sẽ mất chức năng J, và thiết lập mà trở lại là chính xác hồi quy tuyến tính gây ra chức năng. Các, các nguyên nhân vuông chức năng mà chúng tôi nghĩ ra trước đó. Và lấy lớp và gốc, và Quảng trường gây ra chức năng, và đặt chúng lại với nhau. Mà sẽ cung cấp cho chúng tôi đầu tiên của chúng tôi học thuật toán, mà sẽ cung cấp cho chúng tôi các thuật toán hồi quy tuyến tính của chúng tôi.