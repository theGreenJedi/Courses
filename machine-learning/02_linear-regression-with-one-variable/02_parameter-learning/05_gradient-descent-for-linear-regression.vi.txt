Trong video trước đó, chúng tôi nói chuyện về các thuật toán gradient descent và nói chuyện về các tuyến tính hồi quy mô hình và chức năng bình phương lỗi chi phí. Trong video này, chúng tôi sẽ đặt cùng gradient descent với chức năng chi phí của chúng tôi, và rằng sẽ cung cấp cho chúng tôi một thuật toán hồi quy tuyến tính cho lắp một đường thẳng vào dữ liệu của chúng tôi. Vì vậy, đây là những gì chúng tôi làm ra trong các video trước đó. Đó là thuật toán gradient descent của chúng tôi, mà nên quen thuộc, và bạn xem các mô hình tuyến tính hồi qui tuyến tính với giả thuyết tuyến tính của chúng tôi và chức năng bình phương lỗi chi phí của chúng tôi. Những gì chúng tôi sẽ làm là áp dụng Gradient descent để giảm thiểu lỗi bình phương của chúng tôi chi phí chức năng. Bây giờ, để áp dụng Gradient descent, theo thứ tự để viết này mảnh Mã, thuật ngữ quan trọng chúng ta cần là khâu này trên đây. Vì vậy, chúng ta cần phải tìm ra những gì là thuật ngữ bắt nguồn từ phần này, và cắm vào các định nghĩa của chi phí chức năng J, biến này ra được điều này "unintelligible" bằng 1-3 M Sửa lỗi bình phương thuật ngữ hàm chi phí, và tất cả Tôi đã làm ở đây là tôi chỉ bạn biết cắm vào định nghĩa của chức năng chi phí ở đó, và simplifying chút nhiều hơn nữa, điều này sẽ chuyển thành ra phải tương đương, điều này "unintelligible" bằng 1-3 M tất nhiên là một trong hai XI một, trừ YI bình phương. Và tất cả tôi đã có được lấy định nghĩa cho giả thuyết của tôi và cắm đó ở đó. Và nó chỉ ra chúng tôi cần để tìm hiểu những gì là các phái sinh một phần 2 trường hợp cho j bằng 0 và cho j bằng 1 muốn để tìm hiểu những gì là điều này Partial derivative cho cả các trường hợp Theta(0) và theta(1) case. Và tôi chỉ cần đi để viết ra những câu trả lời. Nó chỉ ra firstterm này đơn giản hoá -1/M, số tiền hơn đào tạo của tôi đặt của chỉ đó, X(i)-Y(i). Và cho thuật ngữ này, partial derivative Đối với theta(1), nó chỉ trong tôi nhận được từ này:-Y(i)<i>X(i).</i> Ok. Andcomputing phần phái sinh, do đó đi từ phương trình này cho một trong hai của các phương trình xuống đó, máy tính những thuật ngữ partial derivative đòi hỏi một số giải tích đa biến. Nếu bạn biết tính toán, cảm thấy tự do làm việc thông qua các từ tiếng Anh cho mình và kiểm tra tận các dẫn xuất bạn thực sự có được câu trả lời tôi nhận. Nhưng nếu bạn có ít bạn không quen thuộc với giải tích lo lắng về nó, và nó là tốt để có những phương trình đã làm việc ra, và bạn sẽ không cần phải biết tính toán hoặc bất cứ điều gì tương tự như để làm bài tập ở nhà, do đó, để thực hiện gradient descent, bạn sẽ phải làm việc. Nhưng như vậy, sau khi các định nghĩa này, hoặc sau những gì chúng tôi đã làm việc ra để là các dẫn xuất, mà là thực sự chỉ có độ dốc của cos hoạt j.  Chúng tôi bây giờ có thể cắm chúng trở lại thuật toán gradient descent của chúng tôi. Vì vậy, ở đây là gradient descent, hoặc hồi qui sẽ lặp lại cho đến khi hội tụ, theta 0 và theta ai nhận được Cập Nhật như, bạn đã biết, giống trừ alpha lần khâu. Vì vậy, điều này hạn ở đây. Vì vậy, đây là thuật toán hồi quy tuyến tính của chúng tôi. Điều này lần đầu tiên hạn ở đây mà thuật ngữ là, tất nhiên, chỉ cần bắt nguồn posh từ tương ứng Theta 0, mà chúng tôi làm việc trên trong trình bày trước đó. Và nhiệm kỳ thứ hai này ở đây, cụm từ đó là chỉ bắt nguồn từ một phần với Theta một mà chúng tôi đã làm việc ra vào trước dòng. Và chỉ cần như một lời nhắc nhở nhanh chóng, bạn phải, khi thực hiện gradient descent, có thực sự có là chi tiết, bạn biết, bạn nên thực hiện nó để Cập Nhật theta zero và theta một trong cùng một lúc. Vì vậy, hãy xem làm thế nào gradient descent hoạt động. Một trong những vấn đề chúng tôi giải quyết Gradient descent là nó có thể được dễ bị địa phương optima. Vì vậy, khi tôi lần đầu tiên giải thích chuyển màu gốc, tôi cho thấy bạn ảnh này của nó, bạn đã biết, đi xuống dốc trên bề mặt và chúng tôi thấy như thế nào, tùy thuộc vào nơi bạn đang khởi tạo, bạn có thể kết thúc với optima địa phương khác nhau. Bạn đã biết, bạn có thể sẽ chỉ ở đây hoặc ở đây. Tuy nhiên, nó chỉ ra rằng chức năng chi phí cho chuyển màu chức năng chi phí cho hồi quy tuyến tính luôn luôn có một hàm mũi có hình dạng như thế này. Thuật ngữ này là rằng điều này được gọi là một chức năng lồi. Và tôi sẽ không để cung cấp cho các định nghĩa chính thức cho những gì một chức năng lồi, c-o-n-v-e-x, nhưng không chính thức là một chức năng lồi có nghĩa là mũi có hình dạng hàm, bạn đã biết, loại giống như một cánh cung hình. Và như vậy, chức năng này không có bất kỳ optima địa phương, ngoại trừ cho một tối ưu toàn cầu. Và làm gradient descent ngày loại chi phí hoạt động mà bạn nhận được bất cứ khi nào bạn đang sử dụng tuyến tính hồi qui, nó sẽ luôn luôn chuyển đổi để tối ưu toàn cầu, vì không có không có khác optima địa phương khác hơn tối ưu toàn cầu. Vì vậy bây giờ, hãy xem thuật toán này trong hành động. Là bình thường, zero cộng với của chức năng giả thuyết và của chức năng của tôi chi phí J. Và như vậy, hãy xem như thế nào khởi tạo các tham số của tôi lúc giá trị này. Bạn đã biết, chúng ta hãy nói, thường bạn khởi tạo các tham số của bạn tại số không cho zero, theta zero và zero. Để minh hoạ này trình bày cụ thể, tôi có initialised theta zero lúc khoảng 900, và theta một lúc về trừ 0.1, rồi? Và vì vậy, điều này tương ứng với h trên X, bằng, bạn đã biết, trừ 900 trừ 0,1 x Đây có phải là dòng, rất ra ở đây vào chức năng chi phí. Bây giờ nếu chúng tôi lấy một bước của gradient descent, chúng tôi sẽ lên đi từ thời điểm này ra ở đây, một chút chút xuống bên trái đến thời điểm thứ hai trên đó. Và, bạn nhận thấy rằng dây chuyền của tôi thay đổi một chút. Và, như tôi có một bước tại gradient descent, dòng của tôi ở bên trái sẽ thay đổi. Bên phải. Và tôi cũng đã chuyển đến một điểm mới chức năng chi phí của tôi. Và như tôi nghĩ thêm bước là gradient descent, tôi sẽ xuống trong chi phí, bên phải, như vậy tham số của tôi là sau quỹ đạo này, và nếu bạn nhìn vào bên trái, điều này tương ứng để giả thuyết có vẻ để nhận tốt hơn và tốt hơn phù hợp cho các dữ liệu cho đến khi cuối cùng, Tôi có bây giờ lên vết thương ở mức tối thiểu toàn cầu. Và tối thiểu toàn cầu này tương ứng với giả thuyết này, mà mang lại cho tôi thích hợp để các dữ liệu. Và vì thế đó là chuyển sắc gốc, và chúng tôi đã chỉ cần chạy nó và nhận được một tốt để thiết lập dữ liệu của tôi của nhà ở giá cả phù hợp. Và bạn có thể bây giờ sử dụng nó để dự đoán. Bạn đã biết, nếu bạn bè của bạn có một nhà với một Kích cỡ nhà 1250 feet vuông, bạn bây giờ có thể đọc ra giá trị và nói với họ rằng, tôi không biết, có lẽ họ có thể nhận được $350,000 cho căn nhà của họ. Cuối cùng, chỉ để cung cấp cho Sửa tên khác, nó chỉ ra mà các thuật toán mà chúng tôi chỉ cần đi qua là đôi khi gọi là lô gradient descent. Và nó chỉ ra trong máy học tập, tôi cảm thấy như chúng tôi máy học tập con người, chúng tôi không phải luôn luôn tạo đã cho tôi một số thuật toán. Nhưng thuật ngữ lô gradient descent có nghĩa là đề cập đến các thực tế rằng, trong mỗi bước của gradient descent, chúng tôi đang tìm ở tất cả các ví dụ huấn luyện. Vì vậy, trong gradient descent, bạn biết, khi tính toán cụ phái sinh, chúng tôi đang tính toán những khoản tiền, số tiền này của. Vì vậy, trong mỗi riêng biệt Gradient descent, chúng tôi sẽ chỉ tính toán một cái gì đó như thế này, mà khoản tiền trong ví dụ m đào tạo của chúng tôi. Và vì vậy các thuật ngữ hàng loạt chuyển màu gốc đề cập đến thực tế khi nhìn vào toàn bộ lô Ví dụ, đào tạo và một lần nữa, Điều này là thực sự, thực sự không một cái tên lớn, nhưng đây là những gì mọi người học nhiệm vụ gọi nó. Và nó chỉ ra có đôi khi các phiên bản Gradient descent không Quay lại các phiên bản, nhưng thay vào đó làm không xem xét toàn bộ thương mại nhưng xem xét tập con nhỏ bộ đào tạo lúc đó, và chúng tôi sẽ nói về những phiên bản sau này trong khóa học này là tốt. Nhưng bây giờ, bằng cách sử dụng các thuật toán bạn chỉ biết được, bây giờ chúng tôi sử dụng lô gradient descent, bạn bây giờ biết làm thế nào để thực hiện Gradient descent, hay hồi quy tuyến tính. Vì vậy, đó là hồi quy tuyến tính với gradient descent. Nếu bạn đã nhìn thấy tiên tiến đại số tuyến tính trước khi vì vậy một số bạn có thể đã lấy một lớp học với nâng cao đại số tuyến tính, bạn có thể biết rằng có tồn tại một giải pháp để tính giải quyết cho các tối thiểu của hàm chi phí J, mà không cần phải sử dụng và thuật toán lặp như gradient descent. Sau đó trong khóa học này, chúng tôi sẽ nói về phương pháp đó là Vâng đó chỉ giải quyết cho các chức năng của chi phí tối thiểu, J, mà không có cần sửa nhiều bước của gradient descent. Phương pháp khác được gọi là phương pháp phương trình bình thường. Và, nhưng trong trường hợp bạn có nghe nói về phương pháp đó, nó chỉ trong gradient descent sẽ quy mô tốt hơn cho dữ liệu lớn hơn bộ hơn đó bằng bình thường phương pháp và bây giờ mà chúng tôi biết về gradient descent, chúng tôi sẽ có thể sử dụng nó trong rất nhiều bối cảnh khác nhau, và chúng tôi sẽ sử dụng nó trong rất nhiều vấn đề học tập nhiệm vụ khác nhau là tốt. Vì vậy, congrats về việc học về thuật toán học nhiệm vụ đầu tiên của bạn. Chúng tôi sau đó sẽ có các bài tập trong mà chúng tôi sẽ yêu cầu bạn thực hiện các gradient descent và hy vọng rằng xem những thuật toán làm việc cho mình. Nhưng trước khi đó tôi đầu tiên muốn nói với bạn trong tập tiếp theo của video, các trước tiên, muốn cho bạn biết về một tổng quát của gradient descent thuật toán mà sẽ làm cho nó mạnh hơn nhiều và tôi đoán tôi sẽ cho bạn biết về điều đó trong video tiếp theo.