1
00:00:00,520 --> 00:00:04,480
이전 강의에서 우리는 기울기 하강 알고리즘에 대해 이야기했습니다.

2
00:00:04,480 --> 00:00:09,540
그리고 선형회귀, 비용함수의 차의 제곱도 이야기 했었습니다.

3
00:00:09,540 --> 00:00:14,280
이번 수업에서는 비용함수와 기울기 하강을 함께 사용해서

4
00:00:14,280 --> 00:00:17,400
선형회귀를 위한 알고리즘을 구하거나

5
00:00:17,400 --> 00:00:18,730
우리의 자료의 일차함수를 구해보도록 하겠습니다.

6
00:00:20,800 --> 00:00:24,950
그래서 우리가 이전 수업에서 배웠던 것이,

7
00:00:24,950 --> 00:00:28,920
익숙하실, 이 기울기 하강 알고리즘입니다.

8
00:00:28,920 --> 00:00:34,210
이쪽에는 선형 가설을 위한 선형 회귀 모형이 있습니다.

9
00:00:34,210 --> 00:00:36,540
그리고 비용함수의 차의 제곱도 있네요.

10
00:00:36,540 --> 00:00:42,312
우리가 이제부터 하려는 것은,
기울기 하강을 사용해서

11
00:00:42,312 --> 00:00:47,820
비용함수 차의 제곱을 최소화 하려는 것입니다.

12
00:00:47,820 --> 00:00:51,275
기울기 하강을 적용하기 위해서, 아시다시피

13
00:00:51,275 --> 00:00:59,810
미분계수를 의미하는 이 기호를 사용해야합니다.

14
00:00:59,810 --> 00:01:04,060
그래서 알아야 할것은 이 부분 미분계수와

15
00:01:04,060 --> 00:01:07,710
함수 j에 끼치는 영향을 알아야합니다.

16
00:01:07,710 --> 00:01:11,670
이런 공식이 됩니다.

17
00:01:13,020 --> 00:01:15,550
y값이 1 부터 m까지인

18
00:01:15,550 --> 00:01:21,400
비용함수의 차의 제곱을 더해줍니다.

19
00:01:21,400 --> 00:01:23,520
그래서 제가 지금까지 한 것은,

20
00:01:23,520 --> 00:01:26,190
비용함수의 정의를 적었습니다.

21
00:01:27,290 --> 00:01:34,820
더 간략하게 적어봅시다.
이런식으로 적을 수 있습니다.

22
00:01:34,820 --> 00:01:43,280
i 값이 1일때부터 m일때 까지의 시그마값에 
Θ0 + Θ1x - y i 의 제곱값

23
00:01:43,280 --> 00:01:47,830
이 가설값이 의미하는 바는

24
00:01:47,830 --> 00:01:50,782
여기에 쓴 이 값입니다.

25
00:01:50,782 --> 00:01:53,190
그리고 부분 미분계수의 값을 구해보면,

26
00:01:53,190 --> 00:01:56,570
j 가 0일때와 1일때, 2가지 경우가 있습니다.

27
00:01:56,570 --> 00:02:00,310
부분 미분계수를 구하기 위해서

28
00:02:00,310 --> 00:02:04,170
Θ0과 Θ1 두가지 경우를 사용해 봅시다.

29
00:02:04,170 --> 00:02:06,940
여기에 답을 적어보겠습니다.

30
00:02:06,940 --> 00:02:12,064
공식을 간소화해보겠습니다.
1/m을 곱하고,

31
00:02:12,064 --> 00:02:18,354
X(i)- Y(i)의 합계를 더하고

32
00:02:18,354 --> 00:02:24,294
부분 미분계수를 구합니다.
Θ1인 경우를 구해봅시다.

33
00:02:24,294 --> 00:02:27,114
이런식으로 공식을 쓸 수 있습니다.

34
00:02:27,114 --> 00:02:34,008
Y(i)를 빼고, X(i)를 곱해줍니다.

35
00:02:34,008 --> 00:02:37,440
좋습니다, 그리고

36
00:02:37,440 --> 00:02:41,720
부분 미분계수를 계산해봅시다.
우리는 여기 이 공식과

37
00:02:41,720 --> 00:02:46,000
밑에 있는 이 공식을 같게 해줄겁니다.

38
00:02:46,000 --> 00:02:51,020
이 부분 미분계수를 계산하기 위해서는, 
다변수의 미적분학이 필요합니다.

39
00:02:51,020 --> 00:02:54,930
미적분학을 아신다면,
혼자서 이 공식을 풀어보시고,

40
00:02:54,930 --> 00:02:59,510
미분계수를 구해보시기 바랍니다.
실제로 제가 구한 답을 구하실 수 있을겁니다.

41
00:02:59,510 --> 00:03:04,050
하지만 미적분학을 잘 아시지 못한다해도,
걱정하지마세요.

42
00:03:04,050 --> 00:03:08,100
이 공식을 외우시고, 그냥 풀어보시면 됩니다.

43
00:03:08,100 --> 00:03:11,350
미적분학에 대해 아실 필요도 없습니다.
숙제로 남겨드리겠습니다.

44
00:03:11,350 --> 00:03:13,390
기울기 하강으로 돌아가봅시다.

45
00:03:14,750 --> 00:03:18,490
이 정의를 가지고,

46
00:03:18,490 --> 00:03:22,310
비용함수 j의 기울기값인 미분계수를 구해봅시다.

47
00:03:23,310 --> 00:03:27,160
그 값을 하강 기울기에 넣어봅시다.

48
00:03:27,160 --> 00:03:28,640
이게 선형회귀에서 쓰이는 
하강 기울기 알고리즘입니다.

49
00:03:28,640 --> 00:03:32,728
여기서도 우리는 Θ0의 경우와

50
00:03:32,728 --> 00:03:38,380
Θ1의 경우로 나눠서 생각해봅시다.
아시다시피 미분계수는 -α에 곱해줘야합니다.

51
00:03:39,390 --> 00:03:41,070
여기에 공식이 있습니다.

52
00:03:43,080 --> 00:03:46,050
우리의 선형회귀 알고리즘입니다.

53
00:03:47,160 --> 00:03:48,628
여기가 첫번째 공식이고요,

54
00:03:52,529 --> 00:03:56,804
부분 미분계수의 공식은 d/dΘ0를 곱한 값입니다.

55
00:03:56,804 --> 00:03:59,790
이전 슬라이드에서 말씀드린 내용입니다.

56
00:03:59,790 --> 00:04:05,730
여기에 있는 두번째 공식은

57
00:04:05,730 --> 00:04:11,420
아까 썼던 공식에서 Θ1의 값을 사용합니다.

58
00:04:11,420 --> 00:04:15,230
빠르게 상기시켜드리자면, 
기울기 하강을 사용하셔야합니다.

59
00:04:15,230 --> 00:04:19,265
이런식으로 여러분이 지켜주셔야 할 것은

60
00:04:19,265 --> 00:04:22,250
Θ0 값과 Θ1값을 둘다 구해주셔야 한다는 것입니다.

61
00:04:24,290 --> 00:04:25,570
그래서,

62
00:04:25,570 --> 00:04:28,120
기울기 하강이 어떻게 변하는지 보겠습니다.

63
00:04:28,120 --> 00:04:31,862
중요한 사실은 기울기 하강이

64
00:04:31,862 --> 00:04:32,700
지역 최적값에 민감하다는 것입니다.

65
00:04:32,700 --> 00:04:36,780
그래서 제가 처음에 기울기 하강에 대해 설명할 때,

66
00:04:36,780 --> 00:04:40,900
이 언덕의 표면에서 내려가는 그림을 보셨을겁니다. 
어디서 시작하는지에 따라

67
00:04:40,900 --> 00:04:43,014
지역 최적값이 다르다는 것을 설명드렸습니다.

68
00:04:43,014 --> 00:04:45,480
이 점에서 끝나게 되거나,
이 점에서 끝나게 될 것입니다.

69
00:04:45,480 --> 00:04:50,390
그러나 선형회귀의 비용함수는 항상

70
00:04:50,390 --> 00:04:55,220
이런 활 모양의 함수가 된다고 말씀드렸습니다.

71
00:04:55,220 --> 00:05:00,190
기술적인 용어로는 볼록 함수라고 부릅니다.

72
00:05:03,230 --> 00:05:07,800
그러나 저는 볼록함수의 
일반적인 정의에 대해 말씀드리자 않을겁니다.

73
00:05:07,800 --> 00:05:09,490
C, O, N, V, E, X. - 볼록

74
00:05:09,490 --> 00:05:16,620
그러나 의미적으로는 볼록함수의 의미는
볼록한 모양의 함수라는 뜻이고,

75
00:05:16,620 --> 00:05:22,295
전역적 최적값만 가지고 있을 뿐,
지역적 최적값은 없습니다.

76
00:05:22,295 --> 00:05:26,465
그리고 선형회귀에서 사용되는 비용함수의

77
00:05:26,465 --> 00:05:30,445
기울기 하강은 항상 전역적 최적값을 향하게됩니다.

78
00:05:30,445 --> 00:05:33,155
왜냐면 다른 전역적 최적값이 없기 때문입니다.

79
00:05:33,155 --> 00:05:36,615
이제 이 알고리즘을 활용해봅시다.

80
00:05:38,250 --> 00:05:45,910
여기에 가설 그래프와 비용함수 j의 그래프가 있습니다.

81
00:05:45,910 --> 00:05:50,020
이 값들로 파라메터를 초기화해봅시다.

82
00:05:50,020 --> 00:05:54,220
주로 파라메터를 0과 0으로 초기화합니다.

83
00:05:54,220 --> 00:05:56,370
Θ0과 Θ1을 0과 같다고 둡니다.

84
00:05:56,370 --> 00:06:01,354
하지만 여기서 증명하기 위해서,

85
00:06:01,354 --> 00:06:07,619
여기서는 Θ0를 900으로 초기화하고,
Θ1을 -0,1로 초기화하겠습니다.

86
00:06:07,619 --> 00:06:12,644
공식은  h(x)=-900-0.1x 가 됩니다.

87
00:06:12,644 --> 00:06:16,547
이 선에서 비용함수를 구하는 공식입니다.

88
00:06:16,547 --> 00:06:21,060
기울기 하강을 한번 해본다면,

89
00:06:21,060 --> 00:06:26,845
이 점에서 아래로 내려가고,

90
00:06:26,845 --> 00:06:31,510
왼쪽에 위치한 이 점으로 가게됩니다.

91
00:06:31,510 --> 00:06:35,450
제 일차함수가 약간 바뀐 것을 아실 수 있으실겁니다.

92
00:06:35,450 --> 00:06:39,780
기울기가 하강했기때문에, 
일차함수도 변하게됩니다.

93
00:06:41,230 --> 00:06:42,380
그렇지 않습니까?

94
00:06:42,380 --> 00:06:46,370
비용함수 점을 또 옮겨봅시다.

95
00:06:47,670 --> 00:06:52,760
비용이 내려가게 되면서,
기울기 하강이 변하게됩니다.

96
00:06:52,760 --> 00:06:56,190
이런 궤도로 바뀌게됩니다.

97
00:06:57,340 --> 00:07:02,430
왼쪽을 보시면 제 가설과 일치하게됩니다.

98
00:07:02,430 --> 00:07:06,520
자료와 더 잘 맞아 보입니다.

99
00:07:08,200 --> 00:07:14,660
결국 이 전역적 최소값이

100
00:07:14,660 --> 00:07:20,090
가설과 일치할 때, 자료와 잘 일치하게됩니다.

101
00:07:21,400 --> 00:07:25,800
이것이 기울기 하강이고,

102
00:07:25,800 --> 00:07:31,230
우리가 계속 했던 주택가격의 자료와 잘 일치하게 됩니다.

103
00:07:31,230 --> 00:07:34,490
이제 이 것을 예측하는데 사용할 수 있습니다.

104
00:07:34,490 --> 00:07:38,900
만약 당신의 친구의 집크기가 1250이라면

105
00:07:38,900 --> 00:07:43,350
당신은 이제 친구에게

106
00:07:43,350 --> 00:07:48,720
집 가격이 $250,000이라고 말해줄 수 있습니다.

107
00:07:48,720 --> 00:07:52,620
마지막으로, 이 알고리즘의 다른 이름을 봅시다.

108
00:07:52,620 --> 00:07:57,510
집단 기울기 하강이라고 부릅니다.

109
00:07:57,510 --> 00:08:00,730
기계학습에서 사용되는 용어긴 하지만,

110
00:08:00,730 --> 00:08:04,310
저는 기계학습에 있는 사람들이 알고리즘의
이름을 짓는 것을 잘 못한다고 생각합니다.

111
00:08:04,310 --> 00:08:08,880
그렇지만 집단 기울기 하강이라는 용어는

112
00:08:08,880 --> 00:08:13,850
훈련예제에서 모든 기울기 하강의 
단계를 포함한것을 의미합니다.

113
00:08:13,850 --> 00:08:17,760
그래서 기울기 하강에서 미분계수를 계산할때,

114
00:08:17,760 --> 00:08:21,400
우리는 합계를 계산합니다. [무음]

115
00:08:21,400 --> 00:08:25,660
기울기 하강의 단계를 지나다 보면,

116
00:08:25,660 --> 00:08:30,620
m개의 훈련 예제들을 값을 더하게 됩니다.
그런 이유로 집단 기울기 하강이라는 말은

117
00:08:30,620 --> 00:08:34,175
모든 훈련예제들의 전체 집단을 
나타내는 의미입니다.

118
00:08:34,175 --> 00:08:36,365
다시 말씀드리자면, 저는 좋은 이름은 아니라고 생각합니다.

119
00:08:36,365 --> 00:08:39,585
그렇지만 기계학습 분야의 사람들이
칭하는 이름입니다.

120
00:08:39,585 --> 00:08:43,715
가끔 몇몇의 하강 기울기는

121
00:08:43,715 --> 00:08:46,247
집단의 값과 다를 때도 있습니다.

122
00:08:46,247 --> 00:08:48,837
전체적인 훈련집합을 보지마시고,

123
00:08:48,837 --> 00:08:51,247
훈련집합의 작은 부분집합을 보세요.

124
00:08:51,247 --> 00:08:55,207
이 코스의 다른 강의에서도 설명할 예정입니다.

125
00:08:55,207 --> 00:08:58,357
하지만 우리가 지금 알고리즘을 배우는 이유는

126
00:08:58,357 --> 00:09:03,497
선형회귀를 위한 하강 기울기 알고리즘에 대해
알기 위해서 입니다.

127
00:09:05,980 --> 00:09:09,550
지금까지가 선형회귀와 관련된
하강 기울기에 관련된 내용이였습니다.

128
00:09:09,550 --> 00:09:12,260
만약 이전에 고급 선형대수에 대해 배웠거나

129
00:09:12,260 --> 00:09:15,510
고급 선형대수에 관련된 
수업을 들으신 분이 있을 겁니다.

130
00:09:15,510 --> 00:09:19,410
비용 함수 j의 해답을 얻기 위해서는

131
00:09:19,410 --> 00:09:22,270
하강 기울기 알고리즘같은

132
00:09:22,270 --> 00:09:25,870
반복적 알고리즘을 
사용해야 한다는 것을 아셨을겁니다.

133
00:09:25,870 --> 00:09:29,730
이 코스의 나중 쯤, 
우리는 하강 기울기의 반복 없이

134
00:09:29,730 --> 00:09:33,020
비용함수 j의 최소값을 구하는

135
00:09:33,020 --> 00:09:34,520
방법에 대해 배울 것입니다.

136
00:09:34,520 --> 00:09:37,020
반복 최소 이승법이라고 불리는 방법입니다.

137
00:09:37,020 --> 00:09:41,000
자료의 범위나 크기가 너무 거대해서,
하강 기울기 알고리즘을 사용할 수 없을때

138
00:09:41,000 --> 00:09:46,420
반복최소이승법을 사용합니다.

139
00:09:46,420 --> 00:09:50,140
우리는 이제 다양한 예제에서

140
00:09:50,140 --> 00:09:51,400
하강 기울기 알고리즘을 사용 할 수 있게 됐습니다.

141
00:09:51,400 --> 00:09:53,910
다양한 기계 학습 문제에서도 사용하게 될 것입니다.

142
00:09:55,340 --> 00:10:00,430
첫번째 기계학습 알고리즘을 
배운 것을 축하합시다.

143
00:10:00,430 --> 00:10:04,990
하강 기울기에 대한 다양한 예제들을

144
00:10:04,990 --> 00:10:07,480
직접 풀어보시길 바랍니다.

145
00:10:07,480 --> 00:10:11,460
다음 비디오를 시작하기 전에 해보시길 바랍니다.

146
00:10:11,460 --> 00:10:14,510
하강 기울기 알고리즘이 더 강력한 
알고리즘이라는 것을 말씀드리고 싶었습니다.

147
00:10:14,510 --> 00:10:17,900
더 강력한 하강 기울기 알고리즘에 대해
설명드릴겁니다.

148
00:10:17,900 --> 00:10:20,420
다음 비디오에서  설명드리겠습니다.