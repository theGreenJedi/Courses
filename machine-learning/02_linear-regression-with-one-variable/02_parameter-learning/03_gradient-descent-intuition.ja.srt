1
00:00:00,000 --> 00:00:04,353
前のビデオでは、最急降下法の数学的定義をしました。

2
00:00:04,353 --> 00:00:09,464
もっと掘り下げて行きましょう。このビデオでは、アルゴリズムの挙動、

3
00:00:09,464 --> 00:00:14,701
アルゴリズムの手順の意味について直感的理解を深めて行きます。

4
00:00:14,701 --> 00:00:20,639
ここに前回見た最急降下法アルゴリズムがあります。思い返して頂きたいのは、

5
00:00:20,639 --> 00:00:26,427
このパラメータ、この項、alpha は学習率といいます。

6
00:00:26,427 --> 00:00:32,444
そしてそれは パラメータ theta j を更新する際のステップの大きさを制御します。そして

7
00:00:32,444 --> 00:00:41,360
この二つ目の項は、導関数項です。このビデオでは

8
00:00:41,360 --> 00:00:47,360
二点について直感的理解を深めます。この二つの項が何をしていりるのか。なぜこの二つを

9
00:00:47,360 --> 00:00:53,077
組み合わせるとこの更新が全体として意味をなすのか。こうした直感的理解を

10
00:00:53,077 --> 00:00:58,460
お伝えするために、少し簡素化した例を使いたいと思います。最小化する

11
00:00:58,460 --> 00:01:03,022
関数のパラメータを一つだけにします。ですから目的関数 J の

12
00:01:03,022 --> 00:01:07,294
パラメータは一つ、theta 1 だけになります。ちょうど少し前のビデオでやったように。そして

13
00:01:07,294 --> 00:01:11,913
theta 1 は実数です。これで一次元のプロットになりますので、

14
00:01:11,913 --> 00:01:16,416
見て理解するのが少し容易になります。そして最急降下法がこの関数に対して何を行うか

15
00:01:16,416 --> 00:01:23,940
理解します。さて、これが関数

16
00:01:24,660 --> 00:01:31,696
J(theta 1) だとします。そして theta 1 は実数です。

17
00:01:31,696 --> 00:01:39,202
では、theta 1 をこの位置にして最急降下法を初期化したとします。

18
00:01:39,202 --> 00:01:46,989
ですから、関数上のこの点から開始すると想像してください。最急降下法が行うのは、

19
00:01:46,989 --> 00:01:56,935
更新です。 theta 1 は theta 1 - alpha 掛ける dd

20
00:01:56,935 --> 00:02:04,694
theta 1 J(theta 1) として更新されます。余談ですが、この

21
00:02:04,694 --> 00:02:11,636
導関数項について、もしなぜ私が表記方法を変えて

22
00:02:11,636 --> 00:02:16,132
こうした偏微分の記号を使わないのかと疑問をお持ちの方。もしこの記号の違いの意味が分からない場合、

23
00:02:16,132 --> 00:02:20,523
偏微分の記号と dd theta の違いは気にしないでください。厳密には数学上、

24
00:02:20,523 --> 00:02:24,491
これは偏微分、そしてこれは常微分といいます。違いは

25
00:02:24,491 --> 00:02:28,299
関数 J のパラメータの数によります。しかしこれは数学上の

26
00:02:28,299 --> 00:02:32,428
独特の表現ですので、この授業の目的としては、こうした

27
00:02:32,428 --> 00:02:36,768
偏微分の記号と dd theta の表記は全く同じものと考えて頂いて構いませんので、心配しないでください。

28
00:02:36,768 --> 00:02:41,056
何が違うのかなどと。授業の中では、数学的に

29
00:02:41,056 --> 00:02:45,190
正確な表記を使っていきますが、実際の目的上は、こうした表記は実際には同じことです。

30
00:02:45,360 --> 00:02:49,627
では、この式が何をするか見てみましょう。まず、計算するのは、

31
00:02:49,627 --> 00:02:54,293
この微分の導関数です。皆さんが微分積分の微分を見たことがあるか定かではありませんが、

32
00:02:54,293 --> 00:02:58,666
微分というのは、ここでは、基本的にこういうことです。この

33
00:02:58,666 --> 00:03:02,877
点に対する接線を取ります。この直線、赤い線、ちょうど

34
00:03:02,877 --> 00:03:06,976
この関数に接していますが、その赤い線の勾配を見てみようということ、

35
00:03:06,976 --> 00:03:11,352
それが微分です。つまり、関数に対する接線の勾配は何かということです。

36
00:03:11,352 --> 00:03:15,563
そして線の勾配はもちろん単に

37
00:03:15,563 --> 00:03:20,789
高さを幅で割ったものです。さて、この線は

38
00:03:20,789 --> 00:03:28,378
正の勾配となっていますので、正の微分となります。ですから、theta の更新は

39
00:03:28,378 --> 00:03:36,258
theta 1 := theta 1 - alpha 掛ける何が正の

40
00:03:36,258 --> 00:03:43,103
値。ちなみに、alpha、学習率、は常に正の値です。ですから

41
00:03:43,103 --> 00:03:47,932
theta 1 に対し、それを (theta 1 - 何か) で更新するわけです。ということは

42
00:03:47,932 --> 00:03:52,644
結果的に theta 1 を左に移動することになります。theta 1 を減少すると、ご覧のように

43
00:03:52,644 --> 00:03:57,473
これが正しい動作ですよね、この方向に移動して

44
00:03:57,473 --> 00:04:02,582
ここの最小値に近づいたわけですから。最急降下法はここまでは

45
00:04:02,582 --> 00:04:08,115
正しく動作しているようです。では、別の例を見てみましう。同じ関数 J を使います。

46
00:04:08,115 --> 00:04:13,787
同じ関数 J(theta 1) を書きます。では、仮に

47
00:04:13,787 --> 00:04:19,181
今回はパラメータをこの左ところで初期化したとします。theta 1 はここになります。

48
00:04:19,181 --> 00:04:24,161
表面のここに点を追加します。では、導関数項、dd

49
00:04:24,161 --> 00:04:29,567
theta 1 J(theta 1) をこの点の値で評価すると、

50
00:04:29,567 --> 00:04:35,035
この線の勾配になります。この導関数項はこの線の勾配となります。しかしこの

51
00:04:35,035 --> 00:04:42,745
線は下に傾斜していますので、この線は負の勾配です。言い換えると、

52
00:04:42,745 --> 00:04:48,718
この関数は負の導関数を持つということです。つまりその点で負の勾配となっていることを意味します。

53
00:04:48,718 --> 00:04:54,770
ですから、これは 0 以下となります。そして theta を更新すると、theta が

54
00:04:54,770 --> 00:05:02,840
(theta - alpha 掛ける負の値) で更新されます。つまり

55
00:05:02,840 --> 00:05:07,881
(theta 1 - 負の値) となりますから、これは theta の値を増大するということを意味します。

56
00:05:07,881 --> 00:05:13,106
そうですよね。負の値の引き算ですから、実際には theta に加算することになり、

57
00:05:13,106 --> 00:05:17,900
それが意味するのは、結果的に theta の値が増加するということです。この

58
00:05:17,900 --> 00:05:23,002
開始点から theta を増加させるというのは、これも意図通りの動作のようです。

59
00:05:23,002 --> 00:05:28,335
最小値に近づくことになりますので。以上、これにより

60
00:05:28,335 --> 00:05:33,874
導関数項が何をしているのかご理解いただけたと思います。では次に学習率

61
00:05:33,874 --> 00:05:39,956
alpha を見て、それが何をしているのか理解しましょう。さて、ここに最急降下法の

62
00:05:39,956 --> 00:05:46,641
更新ルールがあります。この式です。さて、どうなるでしょか。

63
00:05:46,641 --> 00:05:52,845
もし alpha が小さすぎり、あるいは alpha が大きすぎた場合。この最初の例は、

64
00:05:52,845 --> 00:05:59,583
alpha が小さすぎた場合にどうなるかです。これが関数 J、J(theta) 。

65
00:05:59,583 --> 00:06:04,230
ではここから始めましょう。alpha が小さすぎる場合、どうなるかというと、

66
00:06:04,230 --> 00:06:09,322
更新がある小さな値で乗算されることになります。すると結果的にこのような小さなステップ

67
00:06:09,322 --> 00:06:13,841
となります。これが一つのステップです。次にこの新しい点から

68
00:06:13,841 --> 00:06:18,870
もう一歩進みますが、alpha が小さすぎますのでまた

69
00:06:18,870 --> 00:06:25,342
小さなステップとなります。ですから、学習率が小さすぎる場合、結果的に

70
00:06:25,342 --> 00:06:30,589
こうした小さな小さなステップで最小値に進むことになりますので、

71
00:06:30,589 --> 00:06:35,837
最小値に辿り着くには、非常に多くのステップが必要になります。alpha が小さすぎると、

72
00:06:35,837 --> 00:06:41,019
小さな小さなステップで進みますので最急降下法の実行に時間がかかります。

73
00:06:41,019 --> 00:06:45,829
さらに、大域的最小点に近づくには非常に多くのステップが必要になります。

74
00:06:45,829 --> 00:06:52,236
では、alpha が大きすぎる場合はどうでしょうか。これが関数 J(theta) です。実は

75
00:06:52,236 --> 00:06:57,590
alpha が大きすぎる場合、最急降下法が最小値を通り越して

76
00:06:57,590 --> 00:07:03,362
収束しなかったり、さらには発散してしまうことがあります。つまり、こういうことです。ここから始めるとします。これは既に最小値に近いです。

77
00:07:03,362 --> 00:07:08,647
この場合、微分は右向きになります。しかし alpha が大きすぎると、大きなステップで進むので、

78
00:07:08,686 --> 00:07:14,140
このように大きなステップで進むので、結果的に目的関数の値が悪化します。なぜなら、

79
00:07:14,140 --> 00:07:20,051
この値で始めたのに、結果的に値が悪くなるからです。今度は微分が

80
00:07:20,051 --> 00:07:25,190
左を向きますので、theta の値は減少します。しかし学習率が大きすぎると、

81
00:07:25,190 --> 00:07:29,792
大きなステップでここからそこまで飛び移りここに落ち着きます。

82
00:07:29,792 --> 00:07:35,372
そして学習率が大きすぎると、また大きなステップで

83
00:07:35,372 --> 00:07:41,034
次回も更新されて、行き過ぎ、行き過ぎ、を繰り返すことになり、ご覧のように

84
00:07:41,034 --> 00:07:46,765
実際には最小値からどんどん遠ざかっていきます。ですから、alpha が大きすぎると、

85
00:07:46,765 --> 00:07:51,905
収束しなかったり、さらには発散してしまうことがあります。ではもう一つ皆さんに質問があります。

86
00:07:51,905 --> 00:07:56,057
これはちょっと難問です。私が最初にこれを学んだ時、

87
00:07:56,057 --> 00:08:00,005
実はこれを理解するのにかなり時間がかかりました。もし最初に設定した theta 1 が既に局所的最小値だったら

88
00:08:00,005 --> 00:08:04,106
どうなるでしょうか。最急降下法の一ステップで何が起きるでしょうか。

89
00:08:04,106 --> 00:08:10,857
では、theta 1 を局所的最小値で初期化したとします。ですから

90
00:08:10,857 --> 00:08:16,713
theta 1 の初期値がここで、それは既に局所的

91
00:08:16,713 --> 00:08:22,718
最適値、局所的最小値です。実は局所的最適値では微分は

92
00:08:22,718 --> 00:08:28,796
ゼロとなります。これが勾配、ここが接点ですので、この線の勾配は

93
00:08:28,796 --> 00:08:35,528
ゼロと等しくなり、この導関数項もゼロとなります。

94
00:08:35,528 --> 00:08:40,941
ですから、最急降下法の更新では、theta 1 := theta 1

95
00:08:40,941 --> 00:08:46,284
- alpha 掛けるゼロ。ですから、この意味は、もし

96
00:08:46,284 --> 00:08:51,222
既に局所的最小値にある場合は、theta 1 は変わらないということです。なぜなら、更新は

97
00:08:51,222 --> 00:08:56,132
theta 1 := theta 1 となりますので。パラメータが既に局所的

98
00:08:56,132 --> 00:09:00,694
最小値の場合、最急降下法の一ステップは全く何も変えません。つまり

99
00:09:00,694 --> 00:09:05,257
パラメータは変わりません。そしてこれは望ましいことです。なぜなら、解が

100
00:09:05,257 --> 00:09:09,706
局所的最適値に留まるからです。これは、なぜ最急降下法が収束して

101
00:09:09,706 --> 00:09:14,326
学習率が固定されてても局所的最小値に落ち着くのかの説明にもなります。そのわけはこうです。では

102
00:09:14,326 --> 00:09:21,550
例を見てみましょう。これが目的関数 J(theta) です。これを

103
00:09:21,550 --> 00:09:26,811
最小化したいとします。そしてアルゴリズムを、最急降下法のアルゴリズムをここで初期化したとします。

104
00:09:26,811 --> 00:09:32,080
このマジェンタ色の点です。最急降下法で更新を一ステップ実行すると、例えば、

105
00:09:32,080 --> 00:09:36,941
この点に移動するとします。なぜならそこの微分はかなり急だからです。さて、

106
00:09:36,941 --> 00:09:42,051
今はこの緑の点にあり、最急降下法でまた一ステップ更新したとします。

107
00:09:42,051 --> 00:09:47,036
お気づきの通り、微分、つまり勾配は、緑の点ではそれほど急ではありません。

108
00:09:47,036 --> 00:09:51,959
あそこのマジェンタ色の点と比較して。なぜなら、

109
00:09:51,959 --> 00:09:56,883
最小値に近づくにつれ、微分がゼロにどんどん近づくからです。

110
00:09:56,883 --> 00:10:01,794
さて、最急降下法で一ステップ更新した後、新しい微分はやや小さくなりました。

111
00:10:01,794 --> 00:10:06,635
最急降下法でまた一ステップ更新したいと思います。当然、いくぶん

112
00:10:06,635 --> 00:10:11,598
マジェンタ色の点からの場合よりさらに小さなステップでこの緑の点から進みます。さて、新しい

113
00:10:11,598 --> 00:10:16,038
点、赤い点、は大域的最小値にさらに近くなっています。ですから

114
00:10:16,038 --> 00:10:21,229
ここの微分は緑の点の時よりさらに小さくなります。そして、最急降下法で

115
00:10:21,229 --> 00:10:26,420
また一ステップ更新すると、導関数項はさらに小さくなるので

116
00:10:26,420 --> 00:10:31,360
theta 1 の更新の大きさもさらに小さくなりますので、ステップのまたこのように小さくなり、

117
00:10:31,360 --> 00:10:39,145
このように最急降下法が進むにつれ、自動的にステップが

118
00:10:39,145 --> 00:10:46,343
どんどん小さくなり、やがて非常に小さなステップで進むことになります。そして

119
00:10:46,343 --> 00:10:52,737
最後には、局所的最小値に収束します。さて、おさらいをします。最急降下法では

120
00:10:52,737 --> 00:10:57,716
局所的最小値に近づくにつれ、最急降下法は自動的に

121
00:10:57,716 --> 00:11:02,634
小さなステップを取るようになり、それは局所的最小値に近づくにつれ、定義により

122
00:11:02,634 --> 00:11:07,122
局所的最小値での微分はゼロに等しくなるからです。よって、

123
00:11:07,122 --> 00:11:12,408
局所的最小値に近づくにつれ、この導関数項も自動的に小さくなり、

124
00:11:12,408 --> 00:11:16,957
このため最急降下法のステップも自動的に小さくなります。これが

125
00:11:16,957 --> 00:11:21,506
最急降下法の仕組みですので、経過的にalpha を減少させる実際の必要はないのです。

126
00:11:21,506 --> 00:11:26,258
さて、これが最急降下法アルゴリズムです。そしてこれを使って

127
00:11:26,258 --> 00:11:30,713
どのような目的関数 J でも最小化を試みることが出来ます。単に線形回帰で定義された目的関数に

128
00:11:30,713 --> 00:11:34,738
限定されません。次のビデオでは、関数 J を

129
00:11:34,738 --> 00:11:38,549
元通りの線形回帰の目的関数に戻します。

130
00:11:38,549 --> 00:11:43,057
前に定義した二乗誤差の目的関数です。そして最急降下法と

131
00:11:43,057 --> 00:11:47,351
二乗誤差の目的関数を組み合わせます。これによって最初の

132
00:11:47,351 --> 00:11:50,948
学習アルゴリズム、線形回帰アルゴリズムが構築できます。