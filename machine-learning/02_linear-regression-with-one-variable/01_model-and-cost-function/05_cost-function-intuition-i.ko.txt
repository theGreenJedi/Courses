전 비디오에서, 우리는 비용함수의 수학적 정의에 
대해 알아봤습니다. 이번 비디오에서는, 몇몇 예제를 살펴보고, 비용함수가 무슨 일을 하고, 
왜 우리가 사용해야 하는지에 대한 직감으로 돌아가보겠습니다. 요약하자면, 이 것이 우리가 저번 시간에 했던 것들입니다. 
우리는 자료에 일직선을 맞추고 싶고, 그래서 우리는 이런 형태의 가설과 
이런 파라메터 Θ0과 Θ1, 그리고 다른 파라메터 값은 
다른 직선으로 표현된다는 것을 배웠습니다. 그래서 자료는 이런 식으로 되고, 
이것은 비용함수 그리고 이것이 최적화된 목표입니다. 
[sound] 이 비디오에서 비디오에서 비용 함수 j를 더 시각화하기 위해서, 저는 오른쪽에 보이는 간소화 한, Θ1에 x를 곱한 가설 함수를 사용하겠습니다. 
만약 원한다면, Θ0값은 0과 같다고 둘 수 있습니다. 
그래서 저는 파라메터 Θ1만 가진 상태이고, 제 비용함수는 hΘxi값이 
Θ1xi라는 것만 빼고 비슷합니다. 그리고 하나의 파라메터 Θ1만 
가진 상태이기 때문에, 저의 최적화된 목표는 jΘ1의 최소값입니다. 그림에서 의미하는 바를 보자면, 
Θ0가 0일 때 선택된 하나의 가설함수는 원점을 지나고, 
(0, 0) 점을 지나게 됩니다. 이 간소화 한 가설 비용 함수를 이용하면, 
비용 함수 개념을 더 잘 이해할 수 있습니다 두개의 중요한 함수를 이해해야 합니다. 첫번째는 가설 함수이고, 
두번째는 비용 함수입니다. 가설함수를 보시면, 오른쪽에 hΘx가 있습니다. 액면가로는 Θ1, 이것은 x의 함수입니다. 
그래서 가설은 함수로서, 집 x의 크기를 말합니다. 반대로, 비용 함수 J는, 
직선의 경사를 좌지우지하는 파라메터 Θ1의 함수입니다. 
이 함수들에 표시를 해보고 이 둘을 더 잘 이해해봅시다. 
가설부터 시작합시다. 왼쪽에, 제 훈련집합이 (1, 1), (2, 2), (3, 3), 
세가지 점으로 이루어져 있다고 가정해봅시다. Θ1의 값을 구해보면, Θ1은 1과 같고, 
만약 제 선택이 Θ1이라면, 제 가설은 여기에 있는 
직선처럼 보여질 것입니다. 만약 제 가설 함수가 그래프에 
그려진다고 해보겠습니다 x라고 이름 붙여진 수평의 축, 
즉 x축은 여기 써 있는 대로 주택의 크기입니다. 이제, 일시적으로, Θ1이 1과 같다고 하면, Θ1이 1과 같을 때, 
jΘ1 값이 무엇인지를 알아내야 합니다. 자, 비용함수가 어떤 값인지 계산해봅시다. 
당신은 1을 절하할 것입니다. 음 보통 비용함수는 다음과 같이 표현합니다, 맞죠? 보통의 오차 제곱의 훈련 집합이고, 이 식은 이 식, Θ1xi – yi와 같습니다. 만약 이걸 간소화한다면 
0의 제곱 더하기 0의 제곱 더하기 0의 제곱이고, 당연히 이 값은 0의 제곱과 같습니다. 
이제 비용함수 안을 봅시다. 이 식들은 0과 같습니다. 왜냐하면 제가 가지고있는 특별한 훈련집합, 
3개의 훈련 예시가 (1, 1), (2, 2), (3, 3)이기 때문입니다. 만약 Θ1이 1과 같다면, 
hΘxi 값은 yi와 같습니다. 더 잘 설명해보겠습니다. 
그래서 hΘxi – yi 등 이 식들은 0과 같고, 그 이유는 제가 j함수가 0이라는 것을 
알았기 때문입니다. 그래서 이제 우리는 j1은 0임을 알게 되었습니다. 
이것을 도표로 그려봅시다. 오른쪽에 비용 함수 그래프 j를 그려보겠습니다. 
말씀드린 것처럼, 비용함수는 파라메터 Θ1의 함수와 같기 때문에, 비용함수를 그렸을 때, 
수평축은 이제 Θ1이 됩니다. j1은 0이고, 
이제 그것을 그래프에 그려보겠습니다. 여기 엑스 표시에서 끝나게 됩니다. 
다른 예시들도 같이 봅시다. 그래서 Θ-1은 다른 범위의 값들을 가지게 됩니다, 맞죠? 
Θ1은 음수 값, 0, 그리고 양수 값을 가지게 됩니다. 그래서 만약 Θ-1이 0,5와 같다면. 어떻게 될까요? 한번 그래프에 그려봅시다. 
저는 Θ-1이 0.5와 같다고 두고, 이 상황에서 가설은 이런 식으로 보이게 됩니다. 
경사는 0.5가 되고, j0.5를 계산해봅시다. 1/2m을 곱하고요, 
계속 썼던 비용 함수입니다. 비용 함수는, 
이 선들을 제곱한 것의 합계가 될 것입니다. 이 선의 길이의 제곱을 더하고, 
이 선의 길이를 제곱한 것을 더하고, 이 선의 길이를 제곱한 것을 더하겠습니다, 맞죠?
왜냐하면 이 수직의 길이는, 이 둘의 차이는, 아시다시피, yi값과, 예측 값인 hΘxi의 차입니다. 그래서 첫번째 예시는 
0.5 빼기 1의 제곱이 됩니다. 가설이 0.5로 예측되기 때문에, 
실제 값은 1이 됩니다. 두번째 예시로는, 
1에서 2를 제곱한 것을 빼 주는데, 가설이 1로 예측되지만, 그러나 실제 집의 가격은 2입니다. 
그리고 마지막으로 1.5 – 3의 제곱 값을 더해줍니다. 그래서 이 값은 1/2 곱하기 1/3과 같아집니다. 
왜냐면 m읜 집합의 개수이고, 우리는 3가지의 훈련 집합을 
가지고 있기 때문입니다. 그래서, 괄호안의 값은 간단하게 3.5로 정리할 수 있습니다. 
그래서 3.5 곱하기 3.5 곱하기 1/6은 0.68이 됩니다. 
그래서 이제 우리는 j0.5값은 0.68이라는 것을 알게 되었습니다. 이것을 그래프에 그려보겠습니다. 
아, 계산 실수를 했네요, 사실은 0.58입니다. 그래서 그래프에 그려보겠습니다. 
한가지 더 해봅시다. 만약 Θ1이 1과 같다면, j0도 같을까요? 
Θ1이 0과 같다고 두고, hx도 이 납작한 선처럼, 이런 식으로 수평으로 간다고 해봅시다. 그 다음에 오차를 계산해봅시다. j0값은 값은 1/2m을 곱하고 
1의 제곱에 2의 제곱과 3의 제곱을 더한 값입니다. 이 값은, 1/6에 2.3을 곱한 값이 되며. 
2.3정도가 됩니다. 이제 이것도 그래프에 그려봅시다. 
그래서 2.3 값에서 끝나게 되고, 우리는 Θ1의 다른 값도 구할 수 있습니다. Θ1의 값이 음수라면, hx의 값은 -0.5에 x를 곱한 값이 되며, Θ1은 -0.5가 해서, 
-0.5의 경사와 가설이 일치하게 됩니다. 그리고 당신은 이 차를 계산할 수 있습니다. 
당신도 아시다시피 0.5값이 되는데, 아주 큰 차가 됩니다. 
이 값은 5.25가 되고요. 계속해서 당신의 범위 안의 값을 계산하면, 
이런 점들을 갖게 됩니다. 그리고 많은 범위의 값들을 계산 하다 보면, 실제로는 천천히 이런 식으로 
그래프가 만들어집니다. J에 Θ를 넣으면 어떻게 될까요? 
이게 바로 j에 Θ를 넣은 값입니다. 요약하자면, 
Θ1값, 각각의 Θ1값은 1을 지나게 되고, 다른 가설 또는 왼쪽에 있는 
다른 직선과 일치하게 됩니다. 그리고 Θ1의 각각의 값은,
다른 jΘ1의 값을 이끌어 내게 됩니다. 예를 들자면, Θ1이 1일 때 
자료는 이런 직선과 일치하게 되고, Θ1이 0.5일때는. 
그럴 때는 지금 분홍색으로 표시하고 있는 이 점과 이 선이 일치하게 되고, 
Θ1이 0일때는 이 파란색 점이 이 수평선과 일치하게 됩니다. 
그래서 각각의 Θ1의 값을 jΘ1의 다른 값들과 처리했고, 
우리는 오른쪽에 있는 이 그래프를 그릴 수 있었습니다. 
이제 기억하시는 것처럼, 우리가 배우는 지도 알고리즘의 최선의 목표는 Θ1입니다. 
jΘ1값을 최소화 하는 것입니다, 맞죠? 이것은 선형 회귀를 배울 때 목적 함수였습니다. 이 곡선을 보시면, jΘ1의 최소값은, 
Θ1이 1일 때 입니다. 보시게 되면, 당연히 가장 최선의 직선은 자료와 잘 맞고, 
Θ1이 1과 같다고 두면서,
우리는 실제로 완벽하게 일치하게 됩니다. 그리고 이것이 왜 jΘ1을 최소화한 값이 자료에 잘 맞는 직선을 찾는 것과 
일치하는 이유입니다. 정리해보자면요. 
이 비디오에서는 우리는 몇 개의 그래프를 봤습니다. 비용함수에 대해 이해하기 위해서요.
그렇게 하기 위해서, 우리는 알고리즘을 간소화했죠. 그래서 1개의 파라메터 Θ1만이 가지고 있었습니다. 
그리고 파라메터 Θ0는 0밖에 될 수 없었죠. 다음 비디오에서는, 
우리는 우리는 다시 공식 문제로 돌아가서 Θ0과 Θ1이 포함된 시각적 자료를 보도록 하겠습니다. Θ0를 0으로 설정하지 않고요. 
그렇게 한다면, 선형 회귀 공식을 배우면서 
비용함수 j에 대해 더 나은 이해를 할 수 있을 것입니다