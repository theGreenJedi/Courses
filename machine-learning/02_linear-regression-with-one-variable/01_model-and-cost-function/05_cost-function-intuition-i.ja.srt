1
00:00:00,000 --> 00:00:04,100
前のビデオで数学的に目的関数を定義しました。

2
00:00:04,100 --> 00:00:08,616
このビデオでは、いくつか具体例を見て、目的関数が何をしていて

3
00:00:08,616 --> 00:00:14,466
なぜそれを使いたいのか、直感的な理解を得て行きます。

4
00:00:14,466 --> 00:00:19,396
おさらいをしますと、これが最後に見たものです。データに直線を当てはめようとしていますので、

5
00:00:19,396 --> 00:00:23,958
この数式を仮説として用意し、そのパラメータは theta 0 と theta 1 で

6
00:00:23,958 --> 00:00:28,888
そしてパラメータの選択次第で、異なる直線となることを見ました。ここにデータがあり、h(x) がこうなります。

7
00:00:31,323 --> 00:00:33,758
そして目的関数があり、そしてそれが

8
00:00:33,758 --> 00:00:38,554
最適化の目的でした。このビデオでは、

9
00:00:38,554 --> 00:00:43,293
目的関数 J をより可視化するため、簡素化された仮説関数を使います。

10
00:00:43,293 --> 00:00:48,220
それを右に表示します。これからこの簡素化された仮説を使っていきます。

11
00:00:48,220 --> 00:00:53,275
これは、単純に theta 1 * x です。これは見方を変えれば、

12
00:00:53,275 --> 00:00:58,721
パラメータ theta 0 = 0 と設定したものと見なすことが出来ます。ですからパラメータは theta 1 の一つだけとなり、

13
00:00:58,721 --> 00:01:04,372
目的関数は前と似ていますが、違いは h(x) 、これが今度は

14
00:01:04,372 --> 00:01:10,309
= theta 1掛けるx となっている点です。そしてパラメータは theta 1 一つだけなので、

15
00:01:10,309 --> 00:01:16,246
最適化の目的は J(theta 1) の最小化となります。この意味を図で示すと、

16
00:01:16,246 --> 00:01:21,611
theta 0 = 0 だとすると、それが相当するのは、

17
00:01:21,611 --> 00:01:27,176
仮説関数として原点を通るもの、点 (0, 0) を通るものだけを選ぶことです。

18
00:01:27,176 --> 00:01:33,415
この簡素化された仮説の定義と目的関数を使って、目的関数の理解を

19
00:01:33,415 --> 00:01:40,178
深めて行きましょう。実は、理解したいのは二つの重要な関数です。

20
00:01:40,178 --> 00:01:46,432
最初に仮説関数、そして二番目に目的関数です。さて、お気づきでしょうか、

21
00:01:46,432 --> 00:01:52,068
仮説、つまり h(x) 、これは theta 1 の値が固定されていれば、これは x に対する

22
00:01:52,068 --> 00:01:58,168
関数です。ですから、仮説は家のサイズ x に対する関数です。

23
00:01:58,168 --> 00:02:03,959
対照的に、目的関数 J は、パラメータ theta 1 に対する関数で、

24
00:02:03,959 --> 00:02:09,993
それは直線の傾きを決定します。これらの関数をプロットして

25
00:02:09,993 --> 00:02:15,481
この両方をもっと理解してみましょう。では仮説から始めましょう。左は、

26
00:02:15,481 --> 00:02:20,283
例えば訓練セットがこのように三つの点 (1, 1)、(2, 2)、そして (3, 3) から成り立っているとします。

27
00:02:20,283 --> 00:02:25,338
では theta 1 の値を選びましょう。仮に theta 1 = 1 の場合、それがtheta 1 に選んだ値であれば、

28
00:02:25,338 --> 00:02:30,392
すると仮説はこの直線のようになります。

29
00:02:30,392 --> 00:02:35,234
すこし指摘しますと、仮説関数をプロットする時には x 軸

30
00:02:35,234 --> 00:02:40,525
横軸は x というラベルがつけられていて、家のサイズを表しています。

31
00:02:40,525 --> 00:02:46,551
今は、一時的に theta 1 = 1 としました。次に行うのは

32
00:02:46,551 --> 00:02:52,430
theta 1 = 1 の時に、J(theta 1) がどうなるか見てみることです。では早速

33
00:02:52,430 --> 00:02:58,781
theta 1 の値を 1 として目的関数を計算してみましょう。通常通り

34
00:02:58,781 --> 00:03:05,761
目的関数は以下のように定義されます。訓練セットの総計、

35
00:03:05,761 --> 00:03:13,840
通常通りの二乗誤差項。従って、これは、これに等しくなります。

36
00:03:14,740 --> 00:03:25,066
theta 1 * x(i) - y(i) そしてこれを簡略化するとこうなります

37
00:03:25,066 --> 00:03:31,995
ゼロの二乗足すゼロの二乗足すゼロの二乗、そしてこれはもちろん、ゼロに等しくなります。さて

38
00:03:31,995 --> 00:03:39,098
目的関数の内部では、それぞれの項がゼロと等しくなりました。なぜならここにある特定の訓練セット、

39
00:03:39,098 --> 00:03:46,288
三つの訓練サンプルが (1, 1)、(2, 2)、(3, 3) だからです。もし theta 1 = 1 なら、

40
00:03:46,288 --> 00:03:54,667
h(x(i)) が y(i) に正に等しく なります。きれいに

41
00:03:54,667 --> 00:04:04,164
書き直します。ですよね。ですから、h(x) - y のそれぞれの項はゼロとなり

42
00:04:04,164 --> 00:04:14,821
それが理由で、J(1) = 0 だと導いたわけです。さて、これでJ(1) が

43
00:04:14,821 --> 00:04:20,504
ゼロに等しいと判明しましたので、それをプロットします。右側で行うのは、

44
00:04:20,504 --> 00:04:26,187
目的関数 J のプロットです。ご覧の通り、目的関数はパラメータ

45
00:04:26,187 --> 00:04:32,017
theta 1 の関数なので、目的関数をプロットする時は、横軸のラベルは theta 1 になります。

46
00:04:32,017 --> 00:04:38,069
ですから、J(1) は 0 なので、それをプロットすると

47
00:04:38,069 --> 00:04:46,464
この x の位置になります。では、他の例もいくつか見てみましょう。theta 1 は

48
00:04:46,464 --> 00:04:52,470
ある範囲の別の値を取ることが出来ます。theta 1 は負の値、ゼロ、あるいは正の値を取ることが出来ます。

49
00:04:52,470 --> 00:04:58,876
では、theta 1 = 0.5 の場合はどうでしょう。何が起きるでしょうか。早速プロットしてみましょう。

50
00:04:58,876 --> 00:05:05,442
ここで theta 1 = 0.5 に設定し、すると仮説はこうなります。

51
00:05:05,442 --> 00:05:11,688
線の傾きは 0.5 に等しくなります。では J を計算しましょう。

52
00:05:11,688 --> 00:05:17,855
J(0.5)。 これは 1/2m、 いつもの目的関数です。

53
00:05:17,855 --> 00:05:23,769
実は、目的関数は、次の値の二乗の合計となります。

54
00:05:23,769 --> 00:05:29,609
この線の高さ、足す、この線の高さ、足す

55
00:05:29,609 --> 00:05:34,783
この線の高さ。ですよね。この垂直の距離、

56
00:05:34,783 --> 00:05:42,854
これは y(i) と 予測値 h(x(i)) との差分

57
00:05:42,854 --> 00:05:48,772
です。いいですか。ですから、最初のサンプルは (0.5 - 1) の二乗です。

58
00:05:49,033 --> 00:05:55,647
なぜなら、仮説の予測は 0.5 でしたが、一方、実際の値は 1 だったからです。

59
00:05:55,647 --> 00:06:02,436
二番目のサンプルでは (1-2) の二乗になります。仮説が予測したのは

60
00:06:02,436 --> 00:06:09,663
1 ですが、実際の家の価格は 2 だったからです。そして最後に、+ (1.5 -3) の

61
00:06:09,663 --> 00:06:17,263
二乗。ですから、これは = 1/(2 × 3) に等しくなります。なぜなら

62
00:06:17,263 --> 00:06:24,274
m は訓練セットのサイズで、ここには三つの訓練サンプルがあるからです。そしてそれ掛ける

63
00:06:24,274 --> 00:06:33,011
括弧の中を簡略化すると、3.5です。ですから、3.5/6 となり、それは大体 0.68になります。

64
00:06:33,011 --> 00:06:41,085
これで分かったのは、J(0.5) が約 0.68になるということです。それをプロットしてみましょう。

65
00:06:41,085 --> 00:06:50,308
おっと失礼しました。計算ミスです。これは実は 0.58 でした。それをプロットすると、このへんになります。

66
00:06:50,308 --> 00:07:00,293
よろしいですか。さらに、もう一つやりましょう。もし theta 1 = 0 の場合はどうか。

67
00:07:00,293 --> 00:07:08,975
J(0) の値はどうなるか。実は、theta 1 = 0 の場合、

68
00:07:08,975 --> 00:07:16,916
結果的に h(x) は、この横線に等しくなり、このように水平になります。

69
00:07:16,916 --> 00:07:26,882
ですから、誤差を計算すると、J(0) = 1/(2 * m) 掛ける

70
00:07:26,882 --> 00:07:34,659
(1^2 + 2^2 + 3^3)、すなわち

71
00:07:34,659 --> 00:07:41,555
1/6 * 14、これは大体 2.3 です。ではこれも早速プロットしてみましょう。これは

72
00:07:41,555 --> 00:07:47,622
2.3 の値になります。そして、もちろんこれを続けて

73
00:07:47,622 --> 00:07:53,335
他の theta 1 の値で試すことも出来ます。実は負の値をtheta 1 に使うことも出来ます。

74
00:07:53,335 --> 00:07:59,327
ですから、theta 1 が負なら、h(x) は、

75
00:07:59,327 --> 00:08:05,179
例えば -0.5 * x とします。ということは theta 1 = -0.5 なので、それが対応するのは

76
00:08:05,179 --> 00:08:10,188
-0.5 の傾きの仮説になります。そして引き続き

77
00:08:10,188 --> 00:08:15,694
実際に誤差の計算を続けると、-0.5 の場合

78
00:08:15,694 --> 00:08:21,520
実は大きな誤差が発生します。結果的に、たしか 5.25ぐらいになります。

79
00:08:21,520 --> 00:08:28,087
このように、異なる値の theta 1 で、こうした値を計算できますよね。

80
00:08:28,087 --> 00:08:34,413
そしてある範囲の値を計算するとこのような結果になります。

81
00:08:34,413 --> 00:08:40,499
そしてある範囲の値を計算することにより、だんだんとその値を辿っていくことができ、

82
00:08:40,499 --> 00:08:50,999
関数 J(theta) が何をしているかが見えてきます。これが J(theta) です。まとめると、

83
00:08:50,999 --> 00:08:57,851
theta 1 のそれぞれの値は、異なる

84
00:08:57,851 --> 00:09:04,448
仮説に対応します。左側の異なる直線に当てはまります。そしてtheta 1 のそれぞれの値に対し

85
00:09:04,448 --> 00:09:11,723
J(theta 1) の異なる値を得ることが出来ます。

86
00:09:11,723 --> 00:09:19,354
例えば、theta 1 = 1 だと、この直線に対応し

87
00:09:19,354 --> 00:09:27,846
データと一致しました。一方、theta 1 = 0.5 では、マジェンタの色で示された点は

88
00:09:27,846 --> 00:09:35,340
この線に対応し、theta 1 = 0 なら、青色で示します、

89
00:09:35,340 --> 00:09:41,527
それが対応するのは、この横線です。そうですね。ですから、theta 1 のそれぞれの値は

90
00:09:41,527 --> 00:09:48,516
結果的に J(theta) の異なる値に対応しますので、それを使って、右にあるようなプロットを辿ることが出来ます。

91
00:09:48,516 --> 00:09:54,461
さて、思い返して頂きたいのは、学習アルゴリズムの最適化の目的は

92
00:09:54,461 --> 00:10:01,963
theta 1 の値を選択することです。それはJ(theta1) を最小化するものでなければなりません。

93
00:10:01,963 --> 00:10:08,076
そうですね。これは線形回帰の目的関数でした。さて、

94
00:10:08,076 --> 00:10:13,710
この曲線を見ると、J(theta 1) を最小化する値は、見ての通り、theta 1 = 1 です。

95
00:10:13,710 --> 00:10:19,132
そして、ご覧下さい、それは正にデータに当てはめることができる

96
00:10:19,132 --> 00:10:24,624
最適な直線です、theta 1 = 1 と設定することによって。この場合の訓練セットでは

97
00:10:24,624 --> 00:10:30,328
実際に完璧に適合しました。そしてこれが、 J(theta 1) を最小化することが

98
00:10:30,328 --> 00:10:36,447
データによく当てはまる直線を見つけることに相当する理由です。

99
00:10:36,447 --> 00:10:40,884
さて、まとめとして、このビデオでは、プロットをいくつか見ました。

100
00:10:40,884 --> 00:10:45,259
目的関数を理解するためです。そのために、アルゴリズムを簡素化し、

101
00:10:45,259 --> 00:10:50,258
パラメータを theta 1 一つだけにし、パラメータ theta 0 は 0 に固定しました。

102
00:10:50,258 --> 00:10:54,445
次のビデオでは、元々の問題の定式に戻り、

103
00:10:54,445 --> 00:10:59,570
theta 0 および theta 1 の両方を含む可視化を行います。つまり、

104
00:10:59,570 --> 00:11:04,757
theta 0 を 0 に固定しないでということです。これにより、元々の線形回帰の定式で

105
00:11:04,757 --> 00:11:09,257
目的関数 J が何をしているかさらによい直感的な理解を得られると思います。