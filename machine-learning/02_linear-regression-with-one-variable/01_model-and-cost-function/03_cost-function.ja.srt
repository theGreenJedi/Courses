1
00:00:00,000 --> 00:00:05,399
このビデオでは、目的関数というものを定義します。これは、

2
00:00:05,399 --> 00:00:10,688
データに対し、どのように最適な直線を当てはめるか算出する為に役に立ちます。線形

3
00:00:10,688 --> 00:00:16,758
回帰ではここに示すような訓練セットがあります。思い返して下さい。 m で表記されているのは、

4
00:00:16,758 --> 00:00:21,972
訓練サンプルの数です。ですから m=47です。そして、

5
00:00:21,972 --> 00:00:27,731
予測をするのに使う仮説の数式はこの線形関数です。

6
00:00:27,731 --> 00:00:33,723
もう少し専門用語を使うと、この theta 0 および theta 1 、これですね、

7
00:00:33,723 --> 00:00:39,759
こうした theta i は、モデルのパラメータといいます。

8
00:00:39,759 --> 00:00:44,578
このビデオで行うのは、これら二つの

9
00:00:44,578 --> 00:00:49,654
パラメータの値をどのように選択するかということをお話します。パラメータ

10
00:00:49,654 --> 00:00:54,408
theta 0 と theta 1 の選び方次第では、異なった仮説、異なった

11
00:00:54,408 --> 00:00:59,355
仮説関数となります。皆さんの中には既にご存知の方もいると承知していますが、

12
00:00:59,355 --> 00:01:04,367
このスライドでお見せするのは、いくつか例を見て行きたいと思います。もし theta 0 が 1.5 で

13
00:01:04,367 --> 00:01:09,378
theta 1 が 0 だと、仮説関数はこのようになります。

14
00:01:09,378 --> 00:01:15,701
そうですね。なぜなら、仮説関数は h(x) = 1.5 +

15
00:01:15,701 --> 00:01:22,645
0 * x、つまり、この定数値関数で、1.5 の値で水平になります。

16
00:01:22,645 --> 00:01:29,332
もし theta 0 が 0 で theta 1 が 0.5 だと、仮説はこうなります。

17
00:01:29,332 --> 00:01:35,536
それはこの点 (2, 1) を通過するはずです、ここでは h(x)

18
00:01:35,536 --> 00:01:40,666
あるいは、実際には h_theta(x) ですが、時々 theta を省略することがあります。

19
00:01:40,666 --> 00:01:46,518
ですから h(x) は単に = 0.5 * x となり、それはこのような形になります。そして最後に

20
00:01:46,518 --> 00:01:52,443
もし theta 0 が 1 で theta 1 が 0.5 だと、結果は

21
00:01:52,443 --> 00:01:58,598
このような仮説になります。これは点 (2, 2) を通過するはずです。

22
00:01:58,598 --> 00:02:04,468
こんな風に。そしてこれは新しい h(x) あるいは新しい h_theta(x) です。

23
00:02:04,468 --> 00:02:09,980
覚えていると思いますが、これは h_theta(x) ですが、単に簡略化のために

24
00:02:09,980 --> 00:02:16,584
時々私は h(x) と書くことがあります。線形回帰では訓練セットがあり、

25
00:02:16,584 --> 00:02:22,439
ここにプロットしたようなものになるかもしれません。ここで行いたいのは、

26
00:02:22,439 --> 00:02:28,295
パラメータ theta 0 と theta 1 の値を算出することです。そしてその結果として得られる直線が、

27
00:02:28,295 --> 00:02:33,799
データによく適合した直線に対応するようにすることです。ちょうどこの線のように。

28
00:02:33,799 --> 00:02:39,756
では、どのようにして theta 0 と theta 1 の値をデータに対して

29
00:02:39,756 --> 00:02:45,350
よく適合するように算出するか。考え方としては、パラメータ theta 0 と theta 1 を 選ぶ

30
00:02:45,350 --> 00:02:51,162
際に、h(x)、つまり入力 x に対する予測値が

31
00:02:51,162 --> 00:02:56,330
最低でも y の値に近似するように、

32
00:02:56,330 --> 00:03:01,133
訓練セットのサンプル、訓練サンプルのそれぞれに対して選びます。訓練セットでは、

33
00:03:01,133 --> 00:03:06,505
数件のサンプルが与えられており、x が家を指定し、その実際の

34
00:03:06,505 --> 00:03:11,796
販売価格を知っています。ですから、パラメータの値を選ぶのに、

35
00:03:11,796 --> 00:03:17,302
少なくとも訓練セットでは、訓練セットの x の値を与えて、適度な正確さで

36
00:03:17,302 --> 00:03:23,507
y の値に近い予測を出力しようということです。これを形式化しましょう。線形回帰では、

37
00:03:23,507 --> 00:03:29,401
行おうとしているのは、最小化問題を解くことです。ですから、theta 0、theta 1 の上に

38
00:03:29,401 --> 00:03:38,787
minimize と書きます。そして、これを

39
00:03:38,787 --> 00:03:44,379
小さくしたいわけです。つまり、h(x) と y の差を小さくしたいわけです。そして

40
00:03:44,379 --> 00:03:50,493
さらに、最小化しようとするのは、仮説の出力と

41
00:03:50,493 --> 00:03:56,159
実際の家の販売価格の差の二乗です。いいですか。では少し詳細を加えて行きます。覚えていますか、

42
00:03:56,159 --> 00:04:01,379
私が (x(i), y(i)) という表記を使って

43
00:04:01,379 --> 00:04:07,418
i 番目の訓練サンプルを表したのを。ですから、実際に欲しいのは、訓練セットの総和です。

44
00:04:07,418 --> 00:04:13,202
i = 1 から m までの差の二乗の総和を行い、

45
00:04:13,202 --> 00:04:18,896
これは i 番目の家の仮説の予測値、その家の

46
00:04:18,896 --> 00:04:24,380
サイズを入力した場合ですね。引くことの、実際の価格、

47
00:04:24,380 --> 00:04:29,588
i 番目の家が実際に売れる価格です。最小化したいのは、訓練セットの総和、

48
00:04:29,588 --> 00:04:35,281
i = 1 から m の間のこの二乗誤差の差分、家の予想

49
00:04:35,281 --> 00:04:41,091
価格と実際に売れる価格との差の二乗の合計です。そして

50
00:04:41,091 --> 00:04:47,723
思い返して頂きたいのは、ここで m と表記されているのは訓練セットのサイズでしたね。

51
00:04:47,723 --> 00:04:53,347
ですから、この m は訓練サンプルの数です。「#」の記号は、

52
00:04:53,347 --> 00:04:59,045
訓練サンプルの「数」の省略です。いいですね。そして

53
00:04:59,045 --> 00:05:04,888
計算をいくらか簡略化するために、実際には 1/m を

54
00:05:04,888 --> 00:05:09,578
それに掛けます。つまり、平均誤差を最小化するわけです。これを1/2m で最小化します。

55
00:05:09,578 --> 00:05:13,926
2 を加えたのは、定数として 1/2 をその前に掛けるという

56
00:05:13,926 --> 00:05:18,386
ことで、これにより計算が少し楽になります。何かを半分にして最小化しても、

57
00:05:18,386 --> 00:05:23,073
パラメータ theta 0 と theta 1 の値はやはり同じになるはずです。その関数を最小化した場合と同じに。

58
00:05:23,073 --> 00:05:27,647
そして単に確認のためですが、この数式の意味は

59
00:05:27,647 --> 00:05:35,569
明確ですよね? この式ですが、h_theta(x)、これは

60
00:05:35,569 --> 00:05:44,880
既にお馴染みですよね? これ = theta 0 + theta 1 * x(i) です。そしてこの表記、

61
00:05:44,880 --> 00:05:49,814
theta 0 と theta 1 の上に minimizeとしてある意味は、
theta 0 と

62
00:05:49,814 --> 00:05:54,369
theta 1 の値を、この式が最小になるように求めなさい、ということです。そして

63
00:05:54,369 --> 00:05:59,557
この式は theta 0 と theta 1 に依存します。分かりましたか。まとめますと、

64
00:05:59,557 --> 00:06:04,382
この問題は、ある条件で theta 0 と theta 1 の値を見つけることとして定義しています。

65
00:06:04,575 --> 00:06:09,292
その条件とは、平均ではなく実際には 1/2m 掛ける二乗誤差の総和が最小化されるようにということです。

66
00:06:09,292 --> 00:06:14,590
差は、訓練セットに対する予測値から訓練セットの実際の家の価格を

67
00:06:14,590 --> 00:06:19,694
引いたものです。これが最終的な線形回帰の

68
00:06:19,694 --> 00:06:25,127
目的関数です。そしてこれを少し書き直してもっときれいにするために、

69
00:06:25,127 --> 00:06:30,580
私が行うのは、慣習的に通常、目的関数の定義は

70
00:06:30,860 --> 00:06:38,965
正にこの通りです。この数式、ここに書き出したものです。私がしたいのは、

71
00:06:38,965 --> 00:06:48,388
最小化するパラメータは theta 0、theta 1、関数は J(theta 0,

72
00:06:48,388 --> 00:06:57,428
theta 1) 、今書いたもの、これが目的関数です。

73
00:06:57,428 --> 00:07:06,943
さて、この目的関数は、二乗誤差関数と呼ばれたり、時には、

74
00:07:06,943 --> 00:07:14,461
二乗誤差目的関数と呼ばれることもあります。ところでなぜ

75
00:07:14,461 --> 00:07:19,006
誤差を二乗にするのでしょう? 実は、二乗誤差目的関数は

76
00:07:19,006 --> 00:07:23,214
ほとんどの問題に、回帰問題において、妥当な選択であり、

77
00:07:23,214 --> 00:07:27,815
よく機能するからです。他にもよく機能する目的関数はありますが、二乗誤差関数は

78
00:07:27,815 --> 00:07:32,473
おそらく最も一般的に回帰問題で使われているものでしょう。

79
00:07:32,473 --> 00:07:36,793
このクラスでは後に、代わりの目的関数についてもお話しますが、この選択、今お見せしたものは

80
00:07:36,793 --> 00:07:41,282
ほとんどの線形回帰問題で試してみるのに非常に妥当なものです。

81
00:07:41,282 --> 00:07:45,706
さて、これが目的関数です。ここまでは、

82
00:07:45,706 --> 00:07:50,899
単にこの目的関数の数学的な定義を見ただけで、

83
00:07:50,899 --> 00:07:55,973
万が一この関数 J(theta 0, theta 1)、万が一この関数が少し抽象的に見える場合は、

84
00:07:55,973 --> 00:08:00,808
それが何をしているのか感覚が掴めていなければ、次のビデオで

85
00:08:00,808 --> 00:08:05,882
次の数件のビデオで、実際に少しもっと深く目的関数 J が

86
00:08:05,882 --> 00:08:10,776
何をしているのかを学び、それが何を計算していて、なぜそれを使いたいのか、

87
00:08:10,776 --> 00:08:12,329
皆さんがもっと直感的に理解できるようにしたいと思います。