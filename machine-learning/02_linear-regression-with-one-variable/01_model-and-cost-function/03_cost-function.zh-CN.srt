1
00:00:00,000 --> 00:00:05,399
在这段视频中我们将定义代价函数的概念 这有助于我们

2
00:00:05,399 --> 00:00:10,688
弄清楚如何把最有可能的直线与我们的数据相拟合

3
00:00:10,688 --> 00:00:16,758
在线性回归中我们有一个像这样的训练集 记住

4
00:00:16,758 --> 00:00:21,972
M代表了训练样本的数量  所以 比如 M = 47

5
00:00:21,972 --> 00:00:27,731
而我们的假设函数 也就是用来进行预测的函数 是这样的线性函数形式

6
00:00:27,731 --> 00:00:33,723
接下来我们会引入一些术语 这些θ0和θ1

7
00:00:33,723 --> 00:00:39,759
这些θi我把它们称为模型参数 在这个视频中

8
00:00:39,759 --> 00:00:44,578
我们要做的就是谈谈如何选择这两个参数值θ0和θ1 

9
00:00:44,578 --> 00:00:49,654
选择不同的参数θ0和θ1

10
00:00:49,654 --> 00:00:54,408
我们会得到不同的假设 不同的假设函数

11
00:00:54,408 --> 00:00:59,355
我知道你们中的有些人可能已经知道我在这张幻灯片上要讲的

12
00:00:59,355 --> 00:01:04,367
但我们还是用这几个例子来复习回顾一下

13
00:01:04,367 --> 00:01:09,378
如果θ0是1.5 θ1是0 那么假设函数会看起来是这样

14
00:01:09,378 --> 00:01:15,701
是吧 因为你的假设函数是h(x)=1.5+0*x

15
00:01:15,701 --> 00:01:22,645
是这样一个常数函数 恒等于1.5

16
00:01:22,645 --> 00:01:29,332
如果θ0=0并且θ1=0.5 那么假设会看起来像这样

17
00:01:29,332 --> 00:01:35,536
它会通过点(2,1) 这样你又得到了h(x)

18
00:01:35,536 --> 00:01:40,666
或者hθ(x) 但是有时我们为了简洁会省略θ

19
00:01:40,666 --> 00:01:46,518
因此 h(x)将等于0.5倍的x 就像这样

20
00:01:46,518 --> 00:01:52,443
最后 如果θ0=1并且θ1=0.5 我们最后得到的假设会看起来像这样

21
00:01:52,443 --> 00:01:58,598
让我们来看看 它应该通过点(2,2)

22
00:01:58,598 --> 00:02:04,468
这是我的新的h(x)或者写作hθ(x) 对吧？

23
00:02:04,468 --> 00:02:09,980
你还记得之前我们提到过hθ(x)的 但作为简写 我们通常只把它写作h(x) 

24
00:02:09,980 --> 00:02:16,584
在线性回归中 我们有一个训练集

25
00:02:16,584 --> 00:02:22,439
可能就像我在这里绘制的 我们要做的就是

26
00:02:22,439 --> 00:02:28,295
得出θ0 θ1这两个参数的值 来让假设函数表示的直线

27
00:02:28,295 --> 00:02:33,799
尽量地与这些数据点很好的拟合

28
00:02:33,799 --> 00:02:39,756
也许就像这里的这条线一样 那么我们如何得出θ0 θ1的值

29
00:02:39,756 --> 00:02:45,350
来使它很好地拟合数据的呢？我们的想法是 我们要选择

30
00:02:45,350 --> 00:02:51,162
能使h(x) 也就是 输入x时我们预测的值

31
00:02:51,162 --> 00:02:56,330
最接近该样本对应的y值的参数θ0 θ1

32
00:02:56,330 --> 00:03:01,133
所以 在我们的训练集中我们会得到一定数量的样本

33
00:03:01,133 --> 00:03:06,505
我们知道x表示卖出哪所房子 并且知道这所房子的实际价格

34
00:03:06,505 --> 00:03:11,796
所以 我们要尽量选择参数值 使得

35
00:03:11,796 --> 00:03:17,302
在训练集中 给出训练集中的x值

36
00:03:17,302 --> 00:03:23,507
我们能合理准确地预测y的值

37
00:03:23,507 --> 00:03:29,401
让我们给出标准的定义 在线性回归中 我们要解决的是一个最小化问题

38
00:03:29,401 --> 00:03:38,787
所以我要写出关于θ0 θ1的最小化 而且

39
00:03:38,787 --> 00:03:44,379
我希望这个式子极其小 是吧 我想要h(x)和y之间的差异要小

40
00:03:44,379 --> 00:03:50,493
我要做的事情是尽量减少假设的输出与房子真实价格

41
00:03:50,493 --> 00:03:56,159
之间的差的平方 明白吗？接下来我会详细的阐述

42
00:03:56,159 --> 00:04:01,379
别忘了 我用符号( x(i),y(i) )代表第i个样本

43
00:04:01,379 --> 00:04:07,418
所以我想要做的是对所有训练样本进行一个求和

44
00:04:07,418 --> 00:04:13,202
对i=1到i=M的样本 将对假设进行预测得到的结果

45
00:04:13,202 --> 00:04:18,896
此时的输入是第i号房子的面积 对吧

46
00:04:18,896 --> 00:04:24,380
将第i号对应的预测结果 减去第i号房子的实际价格 所得的差的平方相加得到总和

47
00:04:24,380 --> 00:04:29,588
而我希望尽量减小这个值

48
00:04:29,588 --> 00:04:35,281
也就是预测值和实际值的差的平方误差和 或者说预测价格和

49
00:04:35,281 --> 00:04:41,091
实际卖出价格的差的平方

50
00:04:41,091 --> 00:04:47,723
我说了这里的m指的是训练集的样本容量

51
00:04:47,723 --> 00:04:53,347
对吧

52
00:04:53,347 --> 00:04:59,045
这个井号是训练样本“个数”的缩写 对吧 而为了让表达式的数学意义

53
00:04:59,045 --> 00:05:04,888
变得容易理解一点 我们实际上考虑的是

54
00:05:04,888 --> 00:05:09,578
这个数的1/m 因此我们要尝试尽量减少我们的平均误差

55
00:05:09,578 --> 00:05:13,926
也就是尽量减少其1/2m 通常是这个数的一半

56
00:05:13,926 --> 00:05:18,386
前面的这些只是为了使数学更直白一点 因此对这个求和值的二分之一求最小值

57
00:05:18,386 --> 00:05:23,073
应该得出相同的θ0值和相同的θ1值来

58
00:05:23,073 --> 00:05:27,647
请大家一定弄清楚这个道理

59
00:05:27,647 --> 00:05:35,569
没问题吧？在这里hθ(x)的这种表达 这是我们的假设

60
00:05:35,569 --> 00:05:44,880
它等于θ0加上θ1与x(i)的乘积 而这个表达

61
00:05:44,880 --> 00:05:49,814
表示关于θ0和θ1的最小化过程 这意味着我们要找到θ0和θ1

62
00:05:49,814 --> 00:05:54,369
的值来使这个表达式的值最小

63
00:05:54,369 --> 00:05:59,557
这个表达式因θ0和θ1的变化而变化对吧？

64
00:05:59,557 --> 00:06:04,382
因此 简单地说 我们正在把这个问题变成 找到能使

65
00:06:04,575 --> 00:06:09,292
我的训练集中预测值和真实值的差的平方的和

66
00:06:09,292 --> 00:06:14,590
的1/2M最小的θ0和θ1的值

67
00:06:14,590 --> 00:06:19,694
因此 这将是我的线性回归的整体目标函数

68
00:06:19,694 --> 00:06:25,127
为了使它更明确一点 我们要改写这个函数

69
00:06:25,127 --> 00:06:30,580
按照惯例 我要定义一个代价函数

70
00:06:30,860 --> 00:06:38,965
正如屏幕中所示 这里的这个公式

71
00:06:38,965 --> 00:06:48,388
我们想要做的就是关于θ0和θ1 对函数J(θ0,θ1)求最小值

72
00:06:48,388 --> 00:06:57,428
这就是我的代价函数

73
00:06:57,428 --> 00:07:06,943
代价函数也被称作平方误差函数 有时也被称为

74
00:07:06,943 --> 00:07:14,461
平方误差代价函数 事实上 我们之所以要求出

75
00:07:14,461 --> 00:07:19,006
误差的平方和 是因为误差平方代价函数

76
00:07:19,006 --> 00:07:23,214
对于大多数问题 特别是回归问题 都是一个合理的选择

77
00:07:23,214 --> 00:07:27,815
还有其他的代价函数也能很好地发挥作用

78
00:07:27,815 --> 00:07:32,473
但是平方误差代价函数可能是解决回归问题最常用的手段了

79
00:07:32,473 --> 00:07:36,793
在后续课程中 我们还会谈论其他的代价函数

80
00:07:36,793 --> 00:07:41,282
但我们刚刚讲的选择是对于大多数线性回归问题非常合理的

81
00:07:41,282 --> 00:07:45,706
好吧 所以这是代价函数 到目前为止 我们已经

82
00:07:45,706 --> 00:07:50,899
介绍了代价函数的数学定义

83
00:07:50,899 --> 00:07:55,973
也许这个函数J(θ0,θ1)有点抽象

84
00:07:55,973 --> 00:08:00,808
可能你仍然不知道它的内涵

85
00:08:00,808 --> 00:08:05,882
在接下来的几个视频里 我们要更进一步解释

86
00:08:05,882 --> 00:08:10,776
代价函数J的工作原理 并尝试更直观地解释它在计算什么

87
00:08:10,776 --> 00:08:12,329
以及我们使用它的目的 【教育无边界字幕组】翻译: antis  校对: cheerzzh 审核: 所罗门捷列夫