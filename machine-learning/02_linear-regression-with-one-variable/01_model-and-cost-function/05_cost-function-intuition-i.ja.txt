前のビデオで数学的に目的関数を定義しました。 このビデオでは、いくつか具体例を見て、目的関数が何をしていて なぜそれを使いたいのか、直感的な理解を得て行きます。 おさらいをしますと、これが最後に見たものです。データに直線を当てはめようとしていますので、 この数式を仮説として用意し、そのパラメータは theta 0 と theta 1 で そしてパラメータの選択次第で、異なる直線となることを見ました。ここにデータがあり、h(x) がこうなります。 そして目的関数があり、そしてそれが 最適化の目的でした。このビデオでは、 目的関数 J をより可視化するため、簡素化された仮説関数を使います。 それを右に表示します。これからこの簡素化された仮説を使っていきます。 これは、単純に theta 1 * x です。これは見方を変えれば、 パラメータ theta 0 = 0 と設定したものと見なすことが出来ます。ですからパラメータは theta 1 の一つだけとなり、 目的関数は前と似ていますが、違いは h(x) 、これが今度は = theta 1掛けるx となっている点です。そしてパラメータは theta 1 一つだけなので、 最適化の目的は J(theta 1) の最小化となります。この意味を図で示すと、 theta 0 = 0 だとすると、それが相当するのは、 仮説関数として原点を通るもの、点 (0, 0) を通るものだけを選ぶことです。 この簡素化された仮説の定義と目的関数を使って、目的関数の理解を 深めて行きましょう。実は、理解したいのは二つの重要な関数です。 最初に仮説関数、そして二番目に目的関数です。さて、お気づきでしょうか、 仮説、つまり h(x) 、これは theta 1 の値が固定されていれば、これは x に対する 関数です。ですから、仮説は家のサイズ x に対する関数です。 対照的に、目的関数 J は、パラメータ theta 1 に対する関数で、 それは直線の傾きを決定します。これらの関数をプロットして この両方をもっと理解してみましょう。では仮説から始めましょう。左は、 例えば訓練セットがこのように三つの点 (1, 1)、(2, 2)、そして (3, 3) から成り立っているとします。 では theta 1 の値を選びましょう。仮に theta 1 = 1 の場合、それがtheta 1 に選んだ値であれば、 すると仮説はこの直線のようになります。 すこし指摘しますと、仮説関数をプロットする時には x 軸 横軸は x というラベルがつけられていて、家のサイズを表しています。 今は、一時的に theta 1 = 1 としました。次に行うのは theta 1 = 1 の時に、J(theta 1) がどうなるか見てみることです。では早速 theta 1 の値を 1 として目的関数を計算してみましょう。通常通り 目的関数は以下のように定義されます。訓練セットの総計、 通常通りの二乗誤差項。従って、これは、これに等しくなります。 theta 1 * x(i) - y(i) そしてこれを簡略化するとこうなります ゼロの二乗足すゼロの二乗足すゼロの二乗、そしてこれはもちろん、ゼロに等しくなります。さて 目的関数の内部では、それぞれの項がゼロと等しくなりました。なぜならここにある特定の訓練セット、 三つの訓練サンプルが (1, 1)、(2, 2)、(3, 3) だからです。もし theta 1 = 1 なら、 h(x(i)) が y(i) に正に等しく なります。きれいに 書き直します。ですよね。ですから、h(x) - y のそれぞれの項はゼロとなり それが理由で、J(1) = 0 だと導いたわけです。さて、これでJ(1) が ゼロに等しいと判明しましたので、それをプロットします。右側で行うのは、 目的関数 J のプロットです。ご覧の通り、目的関数はパラメータ theta 1 の関数なので、目的関数をプロットする時は、横軸のラベルは theta 1 になります。 ですから、J(1) は 0 なので、それをプロットすると この x の位置になります。では、他の例もいくつか見てみましょう。theta 1 は ある範囲の別の値を取ることが出来ます。theta 1 は負の値、ゼロ、あるいは正の値を取ることが出来ます。 では、theta 1 = 0.5 の場合はどうでしょう。何が起きるでしょうか。早速プロットしてみましょう。 ここで theta 1 = 0.5 に設定し、すると仮説はこうなります。 線の傾きは 0.5 に等しくなります。では J を計算しましょう。 J(0.5)。 これは 1/2m、 いつもの目的関数です。 実は、目的関数は、次の値の二乗の合計となります。 この線の高さ、足す、この線の高さ、足す この線の高さ。ですよね。この垂直の距離、 これは y(i) と 予測値 h(x(i)) との差分 です。いいですか。ですから、最初のサンプルは (0.5 - 1) の二乗です。 なぜなら、仮説の予測は 0.5 でしたが、一方、実際の値は 1 だったからです。 二番目のサンプルでは (1-2) の二乗になります。仮説が予測したのは 1 ですが、実際の家の価格は 2 だったからです。そして最後に、+ (1.5 -3) の 二乗。ですから、これは = 1/(2 × 3) に等しくなります。なぜなら m は訓練セットのサイズで、ここには三つの訓練サンプルがあるからです。そしてそれ掛ける 括弧の中を簡略化すると、3.5です。ですから、3.5/6 となり、それは大体 0.68になります。 これで分かったのは、J(0.5) が約 0.68になるということです。それをプロットしてみましょう。 おっと失礼しました。計算ミスです。これは実は 0.58 でした。それをプロットすると、このへんになります。 よろしいですか。さらに、もう一つやりましょう。もし theta 1 = 0 の場合はどうか。 J(0) の値はどうなるか。実は、theta 1 = 0 の場合、 結果的に h(x) は、この横線に等しくなり、このように水平になります。 ですから、誤差を計算すると、J(0) = 1/(2 * m) 掛ける (1^2 + 2^2 + 3^3)、すなわち 1/6 * 14、これは大体 2.3 です。ではこれも早速プロットしてみましょう。これは 2.3 の値になります。そして、もちろんこれを続けて 他の theta 1 の値で試すことも出来ます。実は負の値をtheta 1 に使うことも出来ます。 ですから、theta 1 が負なら、h(x) は、 例えば -0.5 * x とします。ということは theta 1 = -0.5 なので、それが対応するのは -0.5 の傾きの仮説になります。そして引き続き 実際に誤差の計算を続けると、-0.5 の場合 実は大きな誤差が発生します。結果的に、たしか 5.25ぐらいになります。 このように、異なる値の theta 1 で、こうした値を計算できますよね。 そしてある範囲の値を計算するとこのような結果になります。 そしてある範囲の値を計算することにより、だんだんとその値を辿っていくことができ、 関数 J(theta) が何をしているかが見えてきます。これが J(theta) です。まとめると、 theta 1 のそれぞれの値は、異なる 仮説に対応します。左側の異なる直線に当てはまります。そしてtheta 1 のそれぞれの値に対し J(theta 1) の異なる値を得ることが出来ます。 例えば、theta 1 = 1 だと、この直線に対応し データと一致しました。一方、theta 1 = 0.5 では、マジェンタの色で示された点は この線に対応し、theta 1 = 0 なら、青色で示します、 それが対応するのは、この横線です。そうですね。ですから、theta 1 のそれぞれの値は 結果的に J(theta) の異なる値に対応しますので、それを使って、右にあるようなプロットを辿ることが出来ます。 さて、思い返して頂きたいのは、学習アルゴリズムの最適化の目的は theta 1 の値を選択することです。それはJ(theta1) を最小化するものでなければなりません。 そうですね。これは線形回帰の目的関数でした。さて、 この曲線を見ると、J(theta 1) を最小化する値は、見ての通り、theta 1 = 1 です。 そして、ご覧下さい、それは正にデータに当てはめることができる 最適な直線です、theta 1 = 1 と設定することによって。この場合の訓練セットでは 実際に完璧に適合しました。そしてこれが、 J(theta 1) を最小化することが データによく当てはまる直線を見つけることに相当する理由です。 さて、まとめとして、このビデオでは、プロットをいくつか見ました。 目的関数を理解するためです。そのために、アルゴリズムを簡素化し、 パラメータを theta 1 一つだけにし、パラメータ theta 0 は 0 に固定しました。 次のビデオでは、元々の問題の定式に戻り、 theta 0 および theta 1 の両方を含む可視化を行います。つまり、 theta 0 を 0 に固定しないでということです。これにより、元々の線形回帰の定式で 目的関数 J が何をしているかさらによい直感的な理解を得られると思います。