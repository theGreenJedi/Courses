1
00:00:00,000 --> 00:00:04,100
No vídeo anterior, apresentamos a definição matemática de função custo.

2
00:00:04,100 --> 00:00:08,616
Neste vídeo, vamos ver alguns exemplos, voltar a falar da percepção

3
00:00:08,616 --> 00:00:14,466
sobre o que consiste a função de custo e porque temos que usá-la. Para recapitular, aqui está

4
00:00:14,466 --> 00:00:19,396
o que vimos anteriormente. Queremos traçar uma linha reta para nossos dados, por isso nós tivemos

5
00:00:19,396 --> 00:00:23,958
esta estrutura como uma hipótese com estes parâmetros θ₀ e θ₁,

6
00:00:23,958 --> 00:00:28,888
e com diferentes escolhas de parâmetros acabamos com diferentes linhas retas traçadas.

7
00:00:31,323 --> 00:00:33,758
Assim, os dados são correspondentes como estão e há uma função de custo,

8
00:00:33,758 --> 00:00:38,554
que é nosso objetivo da otimização. [som] Para melhor visualizar

9
00:00:38,554 --> 00:00:43,293
a função de custo J, vou trabalhar com uma função hipótese simplificada,

10
00:00:43,293 --> 00:00:48,220
como esta exibida à direita. Vou usar minha hipótese simplificada,

11
00:00:48,220 --> 00:00:53,275
que é exatamente θ₁ vezes X. Se você quiser, podemos pensar nisto como estabelecer

12
00:00:53,275 --> 00:00:58,721
o parâmetro θ₀ igual a 0, Assim, tenho apenas um parâmetro θ₁

13
00:00:58,721 --> 00:01:04,372
e minha função de custo é parecida com a anterior, exceto que agora

14
00:01:04,372 --> 00:01:10,309
h(x) é igual a θ₁ vezes X. Além disso, tenho somente

15
00:01:10,309 --> 00:01:16,246
um parâmetro θ₁ e meu objetivo de otimização é minimizar j de θ₁. Nas figuras, isso significa que fazer

16
00:01:16,246 --> 00:01:21,611
θ₀ igual a zero corresponde a escolher

17
00:01:21,611 --> 00:01:27,176
apenas funções hipóteses que passam pela origem, que passam pelo ponto (0, 0).

18
00:01:27,176 --> 00:01:33,415
Usando esta definição simplificada da função de custo hipotética vamos tentar entender melhor

19
00:01:33,415 --> 00:01:40,178
o conceito de função de custo. Isso revela que queremos entender duas funções chaves.

20
00:01:40,178 --> 00:01:46,432
A primeira é a função hipótese e a segunda é a função de custo.

21
00:01:46,432 --> 00:01:52,068
Observe a hipótese, h(x). Para um valor nominal de θ₁,

22
00:01:52,068 --> 00:01:58,168
esta é uma função de X. Desta forma, a hipótese é uma função de qual é o tamanho da casa X.

23
00:01:58,168 --> 00:02:03,959
Em contraste, a função de custo J, que é uma função de um parâmetro θ₁,

24
00:02:03,959 --> 00:02:09,993
controla a inclinação da linha reta. Vamos plotar estas funções

25
00:02:09,993 --> 00:02:15,481
e tentar entendê-las melhor. Vamos começar com as hipóteses.

26
00:02:15,481 --> 00:02:20,283
À esquerda, digamos, está meu conjunto de treinamento com três pontos em (1, 1), (2, 2), e (3, 3).

27
00:02:20,283 --> 00:02:25,338
Vamos escolher um valor θ₁. Para θ₁ igual a um,

28
00:02:25,338 --> 00:02:30,392
minha hipótese parecerá como esta linha reta aqui.

29
00:02:30,392 --> 00:02:35,234
Vou chamar atenção para quando eu estou plotando minha função hipótese.

30
00:02:35,234 --> 00:02:40,525
Eixo X; meu eixo horizontal é denominado X, é chamado de tamanho da casa.

31
00:02:40,525 --> 00:02:46,551
Agora, de forma provisória, coloque θ₁ igual a um. O que quero fazer

32
00:02:46,551 --> 00:02:52,430
é calcular o que é J( θ₁ ), quando θ₁ for igual a um. Vamos continuar

33
00:02:52,430 --> 00:02:58,781
e calcular a função de custo para o valor 1. Como sempre,

34
00:02:58,781 --> 00:03:05,761
minha função de custo é definida dessa forma.
Algumas são conjuntos de treinamento

35
00:03:05,761 --> 00:03:13,840
desse termo de erro quadrático. E, portanto, este é igual. E este também.

36
00:03:14,740 --> 00:03:25,066
De θ₁ x I menos y I, e se você simplificar issos e torna

37
00:03:25,066 --> 00:03:31,995
Zero elevado a zero, elevado a zero, elevado a zero, é exatamente igual a zero.

38
00:03:31,995 --> 00:03:39,098
Agora, dentro da função de custo. Acontece que cada um destes termos aqui é igual a zero.

39
00:03:39,098 --> 00:03:46,288
Para um conjunto de treinamento específico eu tenho ou meus 3 exemplos de treinamento são (1, 1), (2, 2), (3,3).

40
00:03:46,288 --> 00:03:54,667
Se θ₁ é igual a um, então h(x_i) = y_i. Deixe-me escrever

41
00:03:54,667 --> 00:04:04,164
isto melhor. Assim, h de x menos y, cada um destes termos é igual a zero,

42
00:04:04,164 --> 00:04:14,821
de onde J(1) = zero. Se nós sabemos que J(1)=0,

43
00:04:14,821 --> 00:04:20,504
vamos plotar isso. O que eu vou fazer à direita é plotar

44
00:04:20,504 --> 00:04:26,187
minha função de custo J. Observe que, por minha função de custo ser uma função do meu parâmetro θ₁,

45
00:04:26,187 --> 00:04:32,017
quando ploto minha função de custo, o eixo horizontal é agora chamado θ₁.

46
00:04:32,017 --> 00:04:38,069
Eu tenho j de um zero um, então vamos plotar isso.

47
00:04:38,069 --> 00:04:46,464
Acabamos com um X aqui. Vamos observar alguns outros exemplos. θ₁ pode adquirir

48
00:04:46,464 --> 00:04:52,470
uma gama de diferentes valores. Certo? θ-1 pode adquirir valores negativos,

49
00:04:52,470 --> 00:04:58,876
zero e valores positivos. E se θ₁ é igual a 0.5. O que acontece?

50
00:04:58,876 --> 00:05:05,442
Vamos em frente e plotar isso. Agora vou estabelecer que θ₁ é igual 0.5 e neste caso

51
00:05:05,442 --> 00:05:11,688
minha hipótese fica assim. Como uma linha com inclinação igual a 0.5,

52
00:05:11,688 --> 00:05:17,855
vamos calcular J(0.5).  Isso será um sobre 2M de minha função de custo habitual.

53
00:05:17,855 --> 00:05:23,769
Percebe-se que a função de custo será a soma de valores ao quadrado da altura desta linha.

54
00:05:23,769 --> 00:05:29,609
Mais a soma do quadrado da altura desta linha,

55
00:05:29,609 --> 00:05:34,783
mais a soma do quadrado da altura daquela linha, certo? Pois, exatamente a distância vertical

56
00:05:34,783 --> 00:05:42,854
é a diferença entre y_i. e o valor previsto h(x_i), certo?

57
00:05:42,854 --> 00:05:48,772
O primeiro exemplo será 0.5 menos 1 ao quadrado.

58
00:05:49,033 --> 00:05:55,647
Pois minha hipótese prevê 0.5, enquanto que o valor na realidade é um.

59
00:05:55,647 --> 00:06:02,436
Para meu segundo exemplo, vou pegar um menos dois ao quadrado, pois minha hipótese prevê 1,

60
00:06:02,436 --> 00:06:09,663
mas o valor real era dois. E finalmente, mais. 1.5 menos três ao quadrado.

61
00:06:09,663 --> 00:06:17,263
Isto é igual a um sobre duas vezes 3. Porque, M quando corresponde

62
00:06:17,263 --> 00:06:24,274
ao tamanho do conjunto, tem três exemplos de treinamento. A

63
00:06:24,274 --> 00:06:33,011
Multiplicando isso e simplificando os parênteses, tenho 3.5. Assim, isso é 3.5 sobre seis,

64
00:06:33,011 --> 00:06:41,085
que dá 0.68. Agora sabemos que J(0.5)=0.68. Vamos plotar esse valor.

65
00:06:41,085 --> 00:06:50,308
Desculpe-me, erro matemático, na verdade é 0.58. Vamos plotar o valor, talvez seja aqui.

66
00:06:50,308 --> 00:07:00,293
Agora, vamos fazer mais um. E se θ₁ for igual a zero,

67
00:07:00,293 --> 00:07:08,975
J (0) é igual a que? Parece que se θ₁ é igual a zero, então h(x)

68
00:07:08,975 --> 00:07:16,916
é exatamente igual a esta linha reta, que vai horizontalmente como esta.

69
00:07:16,916 --> 00:07:26,882
E então, medindo os erros. Temos que,

70
00:07:26,882 --> 00:07:34,659
J de zero é igual a um sobre dois M, vezes um ao quadrado mais dois ao quadrado mais três ao quadrado, que é,

71
00:07:34,659 --> 00:07:41,555
um sexto de quatorze que é 2.3. Assim, vamos plotar esse valor.

72
00:07:41,555 --> 00:07:47,622
Isso dá um valor em torno de 2.3 e com certeza podemos continuar fazendo isso

73
00:07:47,622 --> 00:07:53,335
para outros valores de θ₁. Desse modo, você pode ter valores negativos de θ₁,

74
00:07:53,335 --> 00:07:59,327
também se θ₁ é negativo, então h(x) seria igual, digamos,

75
00:07:59,327 --> 00:08:05,179
menos 0.5 vezes x, então θ₁ é menos 0.5 e assim corresponde

76
00:08:05,179 --> 00:08:10,188
a uma hipótese com uma inclinação de 0.5 negativo.

77
00:08:10,188 --> 00:08:15,694
W você pode continuar calculando esses erros. Isto acaba sendo de 0.5,

78
00:08:15,694 --> 00:08:21,520
o que é realmente um erro alto. Calcula-se algo como 5.25.

79
00:08:21,520 --> 00:08:28,087
Você pode calcular para diferentes valores de θ₁

80
00:08:28,087 --> 00:08:34,413
E desse modo, em sua gama de valores computados, você a algo assim.

81
00:08:34,413 --> 00:08:40,499
E ao calcular esses valores aos poucos

82
00:08:40,499 --> 00:08:50,999
O que a função de custo J(θ) representa e isso é, para recapitular,

83
00:08:50,999 --> 00:08:57,851
para cada valor de θ₁ corresponde a uma hipótese diferente

84
00:08:57,851 --> 00:09:04,448
ou a uma linha reta diferente que se ajusta à esquerda. E para cada valor de θ₁,

85
00:09:04,448 --> 00:09:11,723
poderíamos assim derivar um valor diferente de J(θ₁).

86
00:09:11,723 --> 00:09:19,354
Por exemplo, θ₁=1, corresponde a esta linha reta

87
00:09:19,354 --> 00:09:27,846
diretamente através dos dados. Enquanto que θ₁=0.5. E este ponto exibido em magenta

88
00:09:27,846 --> 00:09:35,340
correspondeu, talvez, àquela linha, e θ₁=zero, que é exibido em azul, corresponde a

89
00:09:35,340 --> 00:09:41,527
esta linha horizontal. Assim, para cada valor de θ₁ acabaríamos

90
00:09:41,527 --> 00:09:48,516
com um valor diferente de J(θ₁) e poderíamos então traçar este plano à direita.

91
00:09:48,516 --> 00:09:54,461
Lembre-se que o objetivo da otimização para nosso algoritmo de aprendizagem

92
00:09:54,461 --> 00:10:01,963
é escolher o valor de θ₁, que minimiza J de θ₁.

93
00:10:01,963 --> 00:10:08,076
Este foi nossa função objetivo para a regressão linear. Olhando para esta curva,

94
00:10:08,076 --> 00:10:13,710
o valor que minimiza J(θ₁) é θ₁=1.

95
00:10:13,710 --> 00:10:19,132
E imaginem só, na verdade esta é a melhor linha reta possível

96
00:10:19,132 --> 00:10:24,624
que se ajusta em nossos dados ao atribuir θ₁ igual a um. E somente para este conjunto de treinamento em particular,

97
00:10:24,624 --> 00:10:30,328
acabamos a ajustando perfeitamente. Por isso que

98
00:10:30,328 --> 00:10:36,447
minimizar J(θ₁) corresponde a encontrar uma linha reta que se ajuste bem aos dados.

99
00:10:36,447 --> 00:10:40,884
Para encerrar, neste vídeo observamos alguns gráficos para entender a função de custo.

100
00:10:40,884 --> 00:10:45,259
Para isso, simplificamos o algoritimo, para que tivesse apenas um parâmetro θ₁,

101
00:10:45,259 --> 00:10:50,258
e configuramos o parâmetro θ₀ para ser somente zero. No próximo vídeo,

102
00:10:50,258 --> 00:10:54,445
voltaremos a formulação do problema original e observaremos

103
00:10:54,445 --> 00:10:59,570
algumas visualizações envolvendo θ₀ e θ₁, ou seja, sem fazer θ₀=0.

104
00:10:59,570 --> 00:11:04,757
E esperamos que isso dê a você uma noção melhor

105
00:11:04,757 --> 00:11:09,257
do que a função de de custo J está fazendo na formulação da regressão linear original.
Tradução: Debora Santo | Revisão: Eduardo Bonet