No vídeo anterior, apresentamos a definição matemática de função custo. Neste vídeo, vamos ver alguns exemplos, voltar a falar da percepção sobre o que consiste a função de custo e porque temos que usá-la. Para recapitular, aqui está o que vimos anteriormente. Queremos traçar uma linha reta para nossos dados, por isso nós tivemos esta estrutura como uma hipótese com estes parâmetros θ₀ e θ₁, e com diferentes escolhas de parâmetros acabamos com diferentes linhas retas traçadas. Assim, os dados são correspondentes como estão e há uma função de custo, que é nosso objetivo da otimização. [som] Para melhor visualizar a função de custo J, vou trabalhar com uma função hipótese simplificada, como esta exibida à direita. Vou usar minha hipótese simplificada, que é exatamente θ₁ vezes X. Se você quiser, podemos pensar nisto como estabelecer o parâmetro θ₀ igual a 0, Assim, tenho apenas um parâmetro θ₁ e minha função de custo é parecida com a anterior, exceto que agora h(x) é igual a θ₁ vezes X. Além disso, tenho somente um parâmetro θ₁ e meu objetivo de otimização é minimizar j de θ₁. Nas figuras, isso significa que fazer θ₀ igual a zero corresponde a escolher apenas funções hipóteses que passam pela origem, que passam pelo ponto (0, 0). Usando esta definição simplificada da função de custo hipotética vamos tentar entender melhor o conceito de função de custo. Isso revela que queremos entender duas funções chaves. A primeira é a função hipótese e a segunda é a função de custo. Observe a hipótese, h(x). Para um valor nominal de θ₁, esta é uma função de X. Desta forma, a hipótese é uma função de qual é o tamanho da casa X. Em contraste, a função de custo J, que é uma função de um parâmetro θ₁, controla a inclinação da linha reta. Vamos plotar estas funções e tentar entendê-las melhor. Vamos começar com as hipóteses. À esquerda, digamos, está meu conjunto de treinamento com três pontos em (1, 1), (2, 2), e (3, 3). Vamos escolher um valor θ₁. Para θ₁ igual a um, minha hipótese parecerá como esta linha reta aqui. Vou chamar atenção para quando eu estou plotando minha função hipótese. Eixo X; meu eixo horizontal é denominado X, é chamado de tamanho da casa. Agora, de forma provisória, coloque θ₁ igual a um. O que quero fazer é calcular o que é J( θ₁ ), quando θ₁ for igual a um. Vamos continuar e calcular a função de custo para o valor 1. Como sempre, minha função de custo é definida dessa forma.
Algumas são conjuntos de treinamento desse termo de erro quadrático. E, portanto, este é igual. E este também. De θ₁ x I menos y I, e se você simplificar issos e torna Zero elevado a zero, elevado a zero, elevado a zero, é exatamente igual a zero. Agora, dentro da função de custo. Acontece que cada um destes termos aqui é igual a zero. Para um conjunto de treinamento específico eu tenho ou meus 3 exemplos de treinamento são (1, 1), (2, 2), (3,3). Se θ₁ é igual a um, então h(x_i) = y_i. Deixe-me escrever isto melhor. Assim, h de x menos y, cada um destes termos é igual a zero, de onde J(1) = zero. Se nós sabemos que J(1)=0, vamos plotar isso. O que eu vou fazer à direita é plotar minha função de custo J. Observe que, por minha função de custo ser uma função do meu parâmetro θ₁, quando ploto minha função de custo, o eixo horizontal é agora chamado θ₁. Eu tenho j de um zero um, então vamos plotar isso. Acabamos com um X aqui. Vamos observar alguns outros exemplos. θ₁ pode adquirir uma gama de diferentes valores. Certo? θ-1 pode adquirir valores negativos, zero e valores positivos. E se θ₁ é igual a 0.5. O que acontece? Vamos em frente e plotar isso. Agora vou estabelecer que θ₁ é igual 0.5 e neste caso minha hipótese fica assim. Como uma linha com inclinação igual a 0.5, vamos calcular J(0.5).  Isso será um sobre 2M de minha função de custo habitual. Percebe-se que a função de custo será a soma de valores ao quadrado da altura desta linha. Mais a soma do quadrado da altura desta linha, mais a soma do quadrado da altura daquela linha, certo? Pois, exatamente a distância vertical é a diferença entre y_i. e o valor previsto h(x_i), certo? O primeiro exemplo será 0.5 menos 1 ao quadrado. Pois minha hipótese prevê 0.5, enquanto que o valor na realidade é um. Para meu segundo exemplo, vou pegar um menos dois ao quadrado, pois minha hipótese prevê 1, mas o valor real era dois. E finalmente, mais. 1.5 menos três ao quadrado. Isto é igual a um sobre duas vezes 3. Porque, M quando corresponde ao tamanho do conjunto, tem três exemplos de treinamento. A Multiplicando isso e simplificando os parênteses, tenho 3.5. Assim, isso é 3.5 sobre seis, que dá 0.68. Agora sabemos que J(0.5)=0.68. Vamos plotar esse valor. Desculpe-me, erro matemático, na verdade é 0.58. Vamos plotar o valor, talvez seja aqui. Agora, vamos fazer mais um. E se θ₁ for igual a zero, J (0) é igual a que? Parece que se θ₁ é igual a zero, então h(x) é exatamente igual a esta linha reta, que vai horizontalmente como esta. E então, medindo os erros. Temos que, J de zero é igual a um sobre dois M, vezes um ao quadrado mais dois ao quadrado mais três ao quadrado, que é, um sexto de quatorze que é 2.3. Assim, vamos plotar esse valor. Isso dá um valor em torno de 2.3 e com certeza podemos continuar fazendo isso para outros valores de θ₁. Desse modo, você pode ter valores negativos de θ₁, também se θ₁ é negativo, então h(x) seria igual, digamos, menos 0.5 vezes x, então θ₁ é menos 0.5 e assim corresponde a uma hipótese com uma inclinação de 0.5 negativo. W você pode continuar calculando esses erros. Isto acaba sendo de 0.5, o que é realmente um erro alto. Calcula-se algo como 5.25. Você pode calcular para diferentes valores de θ₁ E desse modo, em sua gama de valores computados, você a algo assim. E ao calcular esses valores aos poucos O que a função de custo J(θ) representa e isso é, para recapitular, para cada valor de θ₁ corresponde a uma hipótese diferente ou a uma linha reta diferente que se ajusta à esquerda. E para cada valor de θ₁, poderíamos assim derivar um valor diferente de J(θ₁). Por exemplo, θ₁=1, corresponde a esta linha reta diretamente através dos dados. Enquanto que θ₁=0.5. E este ponto exibido em magenta correspondeu, talvez, àquela linha, e θ₁=zero, que é exibido em azul, corresponde a esta linha horizontal. Assim, para cada valor de θ₁ acabaríamos com um valor diferente de J(θ₁) e poderíamos então traçar este plano à direita. Lembre-se que o objetivo da otimização para nosso algoritmo de aprendizagem é escolher o valor de θ₁, que minimiza J de θ₁. Este foi nossa função objetivo para a regressão linear. Olhando para esta curva, o valor que minimiza J(θ₁) é θ₁=1. E imaginem só, na verdade esta é a melhor linha reta possível que se ajusta em nossos dados ao atribuir θ₁ igual a um. E somente para este conjunto de treinamento em particular, acabamos a ajustando perfeitamente. Por isso que minimizar J(θ₁) corresponde a encontrar uma linha reta que se ajuste bem aos dados. Para encerrar, neste vídeo observamos alguns gráficos para entender a função de custo. Para isso, simplificamos o algoritimo, para que tivesse apenas um parâmetro θ₁, e configuramos o parâmetro θ₀ para ser somente zero. No próximo vídeo, voltaremos a formulação do problema original e observaremos algumas visualizações envolvendo θ₀ e θ₁, ou seja, sem fazer θ₀=0. E esperamos que isso dê a você uma noção melhor do que a função de de custo J está fazendo na formulação da regressão linear original.
Tradução: Debora Santo | Revisão: Eduardo Bonet