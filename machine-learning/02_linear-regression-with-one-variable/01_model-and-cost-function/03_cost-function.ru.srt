1
00:00:00,000 --> 00:00:05,399
В этом видео мы определим
понятие функции затрат.Она поможет нам

2
00:00:05,399 --> 00:00:10,688
наилучшим образом подобрать прямую,
описывающую наши данные.В задачах

3
00:00:10,688 --> 00:00:16,758
линейной регрессии наш обучающий набор выглядит
примерно так.Буквой m, если помните, мы

4
00:00:16,758 --> 00:00:21,972
обозначили количество обучающих примеров.Допустим,
m равняется 47.А наша гипотеза, с

5
00:00:21,972 --> 00:00:27,731
помощью которой мы будем предсказывать стоимость,
это такая линейная функция.Еще

6
00:00:27,731 --> 00:00:33,723
немного терминологии: эти переменные,
тета нулевое и

7
00:00:33,723 --> 00:00:39,759
тета первое, я буду
называть параметрами модели.В этом видео

8
00:00:39,759 --> 00:00:44,578
мы поговорим о том, как научиться
выбирать эти параметры,

9
00:00:44,578 --> 00:00:49,654
тета нулевое и тета
первое.Выбирая различные значения

10
00:00:49,654 --> 00:00:54,408
параметров тета нулевое
и тета первое, мы будем получать

11
00:00:54,408 --> 00:00:59,355
различные функции-гипотезы.Некоторые из вас наверняка
знакомы с тем, что я собираюсь продемонстрировать на этом слайде, но все

12
00:00:59,355 --> 00:01:04,367
же я приведу несколько
примеров в качестве напоминания.Если тета

13
00:01:04,367 --> 00:01:09,378
нулевое равно 1.5, а тета первое равно 0, график функции
будет выглядеть

14
00:01:09,378 --> 00:01:15,701
так.Верно? Функция
будет равна 1.5 плюс ноль, умноженный на x, то есть мы получим постоянную

15
00:01:15,701 --> 00:01:22,645
функцию, а на графике — горизонтальную
линию на уровне 1.5.

16
00:01:22,645 --> 00:01:29,332
Если тета нулевое равно
0, а тета первое равно 0.5, график

17
00:01:29,332 --> 00:01:35,536
гипотезы получится таким.Прямая должна
пройти через эту точку, (2.1), и

18
00:01:35,536 --> 00:01:40,666
наша h(x) — точнее, h-тета от x, но я иногда буду
опускать «тета»

19
00:01:40,666 --> 00:01:46,518
для краткости —так вот,
наша h(x) будет равняться просто 0.5 умножить на х, и это ее график.И, наконец,

20
00:01:46,518 --> 00:01:52,443
если тета нулевое
равняется 1, а тета первое — 0.5, мы получим такой

21
00:01:52,443 --> 00:01:58,598
график гипотезы.Он должен
пройти через точку (2.2), вот

22
00:01:58,598 --> 00:02:04,468
так.Это моя функция h(x),
то есть h-тета от x.Все понятно?Вы помните,

23
00:02:04,468 --> 00:02:09,980
что это функция h-тета
от x, но для краткости я

24
00:02:09,980 --> 00:02:16,584
иногда пишу просто h(x).В задаче
линейной регрессии у нас есть обучающий набор,

25
00:02:16,584 --> 00:02:22,439
например, такой.Наша
цель — найти такие значения параметров тета нулевое и

26
00:02:22,439 --> 00:02:28,295
тета первое,
чтобы прямая линия,

27
00:02:28,295 --> 00:02:33,799
которую они описывают, хорошо
соответствовала нашим данным.Например,

28
00:02:33,799 --> 00:02:39,756
как вот эта линия.Итак, как нам
получить значения параметров, которые

29
00:02:39,756 --> 00:02:45,350
будут хорошо соответствовать
данным?Идея в том, чтобы

30
00:02:45,350 --> 00:02:51,162
выбрать параметры тета нулевое и тета первое
так, чтобы значение h(x), то есть наше

31
00:02:51,162 --> 00:02:56,330
предсказание для входной
величины x, было бы близко к известным значениям y по меньшей мере для

32
00:02:56,330 --> 00:03:01,133
обучающего набора.
Наш обучающий набор состоит из

33
00:03:01,133 --> 00:03:06,505
множества примеров проданных домов, для которых нам
известен x — площадь дома — и цена,

34
00:03:06,505 --> 00:03:11,796
по которой дом был на деле продан.Так давайте
постараемся выбрать такие значения параметров,

35
00:03:11,796 --> 00:03:17,302
чтобы хотя бы для x из
обучающего набора

36
00:03:17,302 --> 00:03:23,507
мы получали достаточно точное
предсказание y.Теперь более формально.В задаче

37
00:03:23,507 --> 00:03:29,401
линейной регрессии мы на самом деле
решаем

38
00:03:29,401 --> 00:03:38,787
задачу оптимизации.То есть, варьируя параметры тета
нулевое и тета первое я буду минимизировать.Вот это значение, так?

39
00:03:38,787 --> 00:03:44,379
Мне нужно, чтобы вот эта разница между
h(x) и y была минимальной.Для этого я

40
00:03:44,379 --> 00:03:50,493
собираюсь минимизировать квадрат разницы
между значениями функции-гипотезы и

41
00:03:50,493 --> 00:03:56,159
фактической
ценой дома.Хорошо?Уточним еще

42
00:03:56,159 --> 00:04:01,379
кое-что.Напомню, что я обозначил
через (x(i), y(i)) - i-й пример из обучающей

43
00:04:01,379 --> 00:04:07,418
выборки.Так что на самом
деле мне нужна сумма значений

44
00:04:07,418 --> 00:04:13,202
ошибки по всему обучающему набору.Сумма
по i от 1 до m к

45
00:04:13,202 --> 00:04:18,896
вадратов разницы между
этим значением — предсказанием

46
00:04:18,896 --> 00:04:24,380
функции-гипотезы для площади
дома номер i — и

47
00:04:24,380 --> 00:04:29,588
фактической
ценой дома. Эту

48
00:04:29,588 --> 00:04:35,281
сумму квадратов разницы
между предсказанной и фактической

49
00:04:35,281 --> 00:04:41,091
ценой по всему обучающему
набору я и хочу минимизировать.Напомню,

50
00:04:41,091 --> 00:04:47,723
что мы обозначаем через
m размер обучающего

51
00:04:47,723 --> 00:04:53,347
набора, то есть количество известных
примеров.Все понятно?Символ «решетка»

52
00:04:53,347 --> 00:04:59,045
(#) в данном случае обозначает
«количество».Понятно?И, чтобы немного

53
00:04:59,045 --> 00:05:04,888
упростить математические
выкладки, я буду работать с этой величиной,

54
00:05:04,888 --> 00:05:09,578
деленной на m.Даже, на
самом деле, на 2m — то

55
00:05:09,578 --> 00:05:13,926
есть мы будем минимизировать
среднюю ошибку, деленную пополам.

56
00:05:13,926 --> 00:05:18,386
Добавление 1/2 в качестве коэффициента
призвано немного упростить расчеты.Если минимизировать

57
00:05:18,386 --> 00:05:23,073
половину функции, мы получим те же самые значения параметров тета
нулевое и тета первое, что и для

58
00:05:23,073 --> 00:05:27,647
самой функции.Продолжайте,
если это выражение вам понятно,

59
00:05:27,647 --> 00:05:35,569
хорошо?Здесь — функция h
с индексом тета от x, как обычно,

60
00:05:35,569 --> 00:05:44,880
так?Она равняется тета
нулевому плюс тета первому умножить на x(i).А эта запись —

61
00:05:44,880 --> 00:05:49,814
минимизировать по тета нулевое и тета первое — означает, что нам нужно найти
такие тета нулевое и тета первое, чтобы

62
00:05:49,814 --> 00:05:54,369
это выражение приняло минимальное
значение.И это выражение

63
00:05:54,369 --> 00:05:59,557
зависит от значений тета нулевого и тета
первого.Согласны?Резюмирую:

64
00:05:59,557 --> 00:06:04,382
мы ставим задачу найти такие значения параметров
тета нулевое и тета первое,

65
00:06:04,575 --> 00:06:09,292
чтобы усредненное значение
ошибки, то есть сумма квадратов

66
00:06:09,292 --> 00:06:14,590
отклонений предсказанного
значения от фактического по всему обучающему набору, деленная на 2m,

67
00:06:14,590 --> 00:06:19,694
было минимальным.И это будет
моей общей целевой функцией для линейной

68
00:06:19,694 --> 00:06:25,127
регрессии.Я могу переписать
задачу немного более четко, по

69
00:06:25,127 --> 00:06:30,580
традиции определив функцию ошибки, функцию
затрат.

70
00:06:30,860 --> 00:06:38,965
Она как раз будет равняться этому выражению.Вот
этой формуле, написанной наверху.Тогда

71
00:06:38,965 --> 00:06:48,388
задача состоит в том, чтобы минимизировать по тета
нулевому и тета первому функцию J от

72
00:06:48,388 --> 00:06:57,428
тета нулевого и тета первого.То есть, вот эту
функцию затрат.Такую

73
00:06:57,428 --> 00:07:06,943
функцию затрат еще называют
функцией среднеквадратических

74
00:07:06,943 --> 00:07:14,461
отклонений. Почему, собственно,
мы берем

75
00:07:14,461 --> 00:07:19,006
квадраты ошибок?Оказывается,
что такая функция затрат — разумный выбор,

76
00:07:19,006 --> 00:07:23,214
она хорошо подходит для
большинства регрессионных

77
00:07:23,214 --> 00:07:27,815
задач.Есть и другие функции
затрат, которые неплохо работают,

78
00:07:27,815 --> 00:07:32,473
но сумма квадратов отклонений, вероятно, используется
для задач регрессии чаще всего.

79
00:07:32,473 --> 00:07:36,793
Далее мы поговорим и о других функциях, но
рассмотренный вариант — разумный выбор

80
00:07:36,793 --> 00:07:41,282
для большей части задач
линейной

81
00:07:41,282 --> 00:07:45,706
регрессии.Итак.Это
функция стоимости.Пока что

82
00:07:45,706 --> 00:07:50,899
мы разобрали лишь математическое
определение функции стоимости.

83
00:07:50,899 --> 00:07:55,973
В нашем случае это функция J от тета нулевого и тета
первого. Наверное,

84
00:07:55,973 --> 00:08:00,808
она кажется немного абстрактной,
и вам еще неясно, как она работает.

85
00:08:00,808 --> 00:08:05,882
В следующих видео мы более подробно рассмотрим,
как функция J себя ведет,

86
00:08:05,882 --> 00:08:10,776
и постараемся четче
понять, что она вычисляет и почему подходит для наших

87
00:08:10,776 --> 00:08:12,329
задач.