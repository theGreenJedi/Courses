1
00:00:02,338 --> 00:00:04,677
हमारा पहला लर्निंग अल्गोरिद्म होगा लिनीअर रेग्रेशन. इस वीडियो में आप देखेंगे

2
00:00:06,956 --> 00:00:09,234
मॉडल कैसे दिखता हैं और अधिक महत्वपूर्ण है आप देखेंगे कि कैसे पूरी

3
00:00:09,234 --> 00:00:14,801
प्रक्रिया सूपर्वायज़्ड लर्निंग की होती है. चलो इस्तेमाल करते हैं कुछ प्रेरक उदाहरण प्रिडिक्ट करने के लिए

4
00:00:14,801 --> 00:00:20,036
आवास की कीमते. हम इस्तेमाल करेंगे एक डेटा सेट घरों की क़ीमतों का शहर

5
00:00:20,036 --> 00:00:25,205
पोर्टलैंड, ऑरेगॉन से. और यहाँ मैं प्लॉट करूँगा मेरे डेटा सेट को बहुत से घरों के

6
00:00:25,205 --> 00:00:30,833
जो थे भिन्न भिन्न साइज़ के जो बेचे गए थे भिन्न भिन्न क़ीमतों पर. मान लो

7
00:00:30,833 --> 00:00:35,872
कि दिया होने पर यह डेटा सेट, आपका एक मित्र है जो प्रयास कर रहा है बेचने का एक घर और

8
00:00:35,872 --> 00:00:41,238
चलो देखते हैं कि मित्र के घर का साइज़ है 1250 स्क्वेर फ़ुट और आप उन्हें बताना चाहते हो

9
00:00:41,238 --> 00:00:46,459
कितने में वे शायद बेच पाएँगे घर. वैसे तो एक काम जो आप कर सकते हैं वह है

10
00:00:46,648 --> 00:00:53,039
फ़िट करें एक मॉडल. शायद फ़िट करें एक सीधी रेखा डेटा में. जो दिखती है कुछ ऐसे और निर्भर करते हुए

11
00:00:53,039 --> 00:00:59,168
उस पर, शायद आप बता सकते हैं अपने मित्र को मान लो शायद वह बेच सकता है

12
00:00:59,168 --> 00:01:03,575
घर लगभग $ 220,000 में. तो यह है एक उदाहरण एक

13
00:01:03,575 --> 00:01:08,834
सूपर्वायज़्ड लर्निंग अल्गोरिद्म का. और यह है सूपर्वायज़्ड लर्निंग क्योंकि हमें दिया है

14
00:01:08,834 --> 00:01:14,287
"सही उत्तर" हमारे प्रत्येक इग्ज़ाम्पल के लिए. अर्थात् हमें बताया गया है कि क्या थी

15
00:01:14,287 --> 00:01:19,351
असली घर, क्या थी असली क़ीमत प्रत्येक घर की हमारे डेटा

16
00:01:19,351 --> 00:01:24,441
सेट में जिस पर वह बेचा गया था और इसके अलावा, यह है उदाहरण एक रेग्रेशन प्रॉब्लम का जहाँ

17
00:01:24,441 --> 00:01:29,545
टर्म रेग्रेशन बताती है तथ्य कि हम प्रिडिक्ट कर रहे हैं एक रियल वैल्यू आउट्पुट

18
00:01:29,545 --> 00:01:34,586
अर्थात् कीमत. और सिर्फ आप को याद दिलाने के लिए कि दूसरा आम प्रकार सूपर्वायज़्ड

19
00:01:34,586 --> 00:01:39,006
लर्निंग प्रॉब्लम का कहलाता है क्लैसिफ़िकेशन प्रॉब्लम जहाँ हम प्रिडिक्ट करते हैं

20
00:01:39,006 --> 00:01:45,202
डिस्क्रीट वैल्यू आउट्पुट की जैसे कि हम देख रहे हैं कैन्सर ट्यूमर पर और प्रयास कर रहे हैं

21
00:01:45,202 --> 00:01:52,032
तय करने का कि एक ट्यूमर घातक है या सौम्य. तो वह है एक ज़ीरो-एक वैल्यू की डिस्क्रीट आउट्पुट. अधिक

22
00:01:52,032 --> 00:01:57,087
औपचारिक रूप से, सूपर्वायज़्ड लर्निंग में, हमारे पास है एक डेटा सेट और इस डेटा सेट को कहते हैं एक

23
00:01:57,087 --> 00:02:02,018
ट्रेनिंग सेट. तो, हमारे घरों की क़ीमत के उदाहरण के लिए, हमारे पास है एक ट्रेनिंग सेट

24
00:02:02,018 --> 00:02:07,386
विभिन्न घरों की क़ीमतों का और हमारा काम है सीखना इस डेटा से कि कैसे प्रिडिक्ट करना है क़ीमत

25
00:02:07,386 --> 00:02:11,907
घरों की. चलो परिभाषित करते हैं कुछ नोटेशन जो हम इस्तेमाल कर रहे हैं इस कोर्स के दौरान.

26
00:02:11,907 --> 00:02:16,100
हम परिभाषित करेंगे बहुत से चिन्ह. यह ठीक है यदि आप नहीं याद रखते

27
00:02:16,100 --> 00:02:20,075
सारे चिन्ह अभी से लेकिन जैसे कोर्स आगे बढ़ेगा यह सहायक होगा

28
00:02:20,075 --> 00:02:24,267
[नहीं सुनाई दिया] सुविधाजनक नोटेशन के रूप में. तो मैं करूँगा लोअर केस एम का इस्तेमाल इस कोर्स के दौरान

29
00:02:24,267 --> 00:02:28,897
डिनोट करने के लिए संख्या ट्रेनिंग इग्ज़ाम्पल्ज़ की. तो इस डेटा सेट में, यदि मेरे पास हैं, आप जानते हैं,

30
00:02:28,897 --> 00:02:34,366
मान लो 47 रोज़ इस टेबल में. तब, मेरे पास 47 ट्रेनिंग इग्ज़ाम्पल्ज़ हैं और एम बराबर है 47.

31
00:02:34,366 --> 00:02:39,497
मैं इस्तेमाल करूँगा लोअर केस एक्स डिनोट करने के लिए इनपुट वेरिएबल्स जिन्हे अक्सर कहते हैं

32
00:02:39,497 --> 00:02:44,290
फ़ीचर्ज़ भी. वह होगा एक्स यहाँ, यह होगा इनपुट फ़ीचर्ज़. और मैं करूँगा

33
00:02:44,290 --> 00:02:49,556
इस्तेमाल वाय का डिनोट करने के लिए मेरे आउट्पुट वेरिएबल्स या टार्गट वेरिएबल जो मैं करूँगा

34
00:02:49,556 --> 00:02:54,552
प्रिडिक्ट और इसलिए वह है दूसरा कॉलम यहाँ. [नहीं सुनाई दिया] नोटेशन मैं

35
00:02:54,552 --> 00:03:05,749
इस्तेमाल (एक्स, वाय) डिनोट करने के लिए एक अकेला ट्रेनिंग इग्ज़ाम्पल. तो एक रो इस

36
00:03:05,749 --> 00:03:12,068
टेबल में कॉरेस्पॉंड करती हैं एक सिंगल ट्रेनिंग इग्ज़ाम्पल को और रेफ़र करने के लिए एक विशेष

37
00:03:12,068 --> 00:03:19,708
ट्रेनिंग इग्ज़ाम्पल को, मैं करूँगा इस्तेमाल यह नोटेशन एक्स(आइ) कॉमा वाय(आइ). और हम

38
00:03:25,322 --> 00:03:30,935
करेंगे उपयोग इसका रेफ़र करने के लिए आइ वाँ ट्रेनिंग इग्ज़ाम्पल. तो यह सूपर स्क्रिप्ट आइ

39
00:03:30,935 --> 00:03:37,864
यहाँ, यह एक्स्पोनेंट नही है सही? यह (एक्स(आइ),वाय(आइ)), सूपर स्क्रिप्ट आइ

40
00:03:37,864 --> 00:03:44,873
कोष्ठकों में सिर्फ एक सूचकांक / इंडेक्स है और मेरी आइ रो का और रेफ़र करता है आइ रो को

41
00:03:44,873 --> 00:03:51,629
इस टेबल में, ठीक है? तो यह नहीं है एक्स की पावर आइ, वाय की पावर आइ. इसके स्थान पर

42
00:03:51,629 --> 00:03:58,216
(एक्स (i), वाई (i)) सिर्फ इस टेबल की ith रो को दर्शाता है. तो उदाहरण के लिए, एक्स (1) बताता है

43
00:03:58,216 --> 00:04:04,972
इनपुट वैल्यू पहले ट्रेनिंग इग्ज़ाम्पल की जो है 2104. वह है यह एक्स इस पहली

44
00:04:04,972 --> 00:04:11,685
रो में. एक्स(2) होगा बराबर 1416 सही? वह दूसरा एक्स

45
00:04:11,685 --> 00:04:17,385
और वाय(1) होगा बराबर 460. पहली वाय वैल्यू मेरे पहले

46
00:04:17,385 --> 00:04:24,526
ट्रेनिंग इग्ज़ाम्पल के लिए, वह है जो (1) बताता है. तो जैसे बताया है कभी कभी मैं आपको पूछूँगा 

47
00:04:24,526 --> 00:04:28,345
प्रश्न ताकि आप जाँच सकें आपकी समझ और कुछ सेकंड में इस

48
00:04:28,345 --> 00:04:34,044
वीडियो में एक बहु-विकल्प प्रश्न आएगा वीडियो में. जब यह आए

49
00:04:34,044 --> 00:04:40,362
तो कृपया अपने माउस का इस्तेमाल करके चयन करें जो आप सोचते हैं कि सही जवाब है. क्या परिभाषित किया है

50
00:04:40,362 --> 00:04:45,124
ट्रेनिंग सेट से. तो यहाँ है कि कैसे सूपर्वायज़्ड लर्निंग अल्गोरिद्म काम करता है.

51
00:04:45,124 --> 00:04:50,513
हमने देखा कि ट्रेनिंग सेट के साथ जैसे हमारा ट्रेनिंग सेट घरों की क़ीमत का और हम फ़ीड करते हैं

52
00:04:50,513 --> 00:04:55,715
उसे हमारे लर्निंग अल्गोरिद्म को. यह काम है एक लर्निंग अल्गोरिद्म का तब आउट्पुट करना एक

53
00:04:55,715 --> 00:05:00,101
फ़ंक्शन जो कन्वेन्शन से है अक्सर डिनोट किया हुआ लोअर केस एच और एच

54
00:05:00,101 --> 00:05:06,574
चिन्हित करता है हायपॉथिसस. और क्या काम है हायपॉथिसस का, कि, यह है एक फ़ंक्शन

55
00:05:06,574 --> 00:05:12,471
जो लेता है इनपुट साइज़ एक घर का शायद साइज़ नए घर का जो आपका मित्र

56
00:05:12,471 --> 00:05:18,368
प्रयास कर रहा है बेचने का तो वह लेता है वैल्यू एक्स की और कोशिश करता है आउट्पुट करने की अनुमानित

57
00:05:18,368 --> 00:05:31,630
वैल्यू वाय की इसी घर के लिए. तो एच है एक फ़ंक्शन जो मैप करता हैं एक्स से

58
00:05:31,630 --> 00:05:37,729
वाय को. लोग अक्सर मुझसे पूछते हैं, आप जानते हैं, क्यों इस फ़ंक्शन को कहते हैं

59
00:05:37,729 --> 00:05:42,121
हायपॉथिसस. आप में से कुछ जानते होंगे मतलब टर्म हायपॉथिसस का,

60
00:05:42,121 --> 00:05:46,744
शब्दकोश या विज्ञान या किसी भी तरह. ऐसा होता है कि मशीन लर्निंग में, यह

61
00:05:46,744 --> 00:05:51,310
है एक नाम जो इस्तेमाल किया गया था शुरुआत के दिनों में मशीन लर्निंग के और यह एक प्रकार से बना रहा. क्योंकि

62
00:05:51,310 --> 00:05:55,239
शायद नहीं है एक बढ़िया नाम इस तरह के फ़ंक्शन के लिए, मैप करने के लिए साइज़

63
00:05:55,239 --> 00:05:59,978
घरों के प्रिडिक्शन में, जो आप जानते हैं... मैं सोचता हूँ कि टर्म हायपॉथिसस, शायद नहीं है

64
00:05:59,978 --> 00:06:04,543
सबसे अच्छा संभव नाम है, लेकिन यह है स्टैंडर्ड टर्म जो लोग प्रयोग करते हैं

65
00:06:04,543 --> 00:06:09,362
मशीन लर्निंग में. तो ज़्यादा चिंता न करें कि क्यों लोग उसे वह कहते हैं. जब

66
00:06:09,362 --> 00:06:14,332
डिज़ाइन कर रहे हैं एक लर्निंग अल्गोरिद्म, अगली चीज़ जो हमें तय करनी है कि कैसे हम

67
00:06:14,332 --> 00:06:20,540
दर्शाएँ इस हायपॉथिसस एच को. इसके लिए और अगले कुछ वीडियोस के लिए मैं चुनाव करूँगा

68
00:06:20,540 --> 00:06:26,978
हमारे प्रारंभिक विकल्प का, दर्शाने के लिए हायपॉथिसस, वह होगा इस प्रकार. हम करेंगे

69
00:06:26,978 --> 00:06:33,009
रेप्रेज़ेंट एच इस प्रकार. और हम लिखेंगे इस एच 1 थीटा(एक्स) बराबर है थीटा 2 0 2 1

70
00:06:33,009 --> 00:06:39,254
प्लस थीटा 1 1 ऑफ़ एक्स की तरह. और संक्षिप्त रूप में, कभी कभी बजाय लिखने के 1, आप

71
00:06:39,254 --> 00:06:45,441
जानते हैं एच सब स्क्रिप्ट थीटा ऑफ़ एक्स, कभी कभी संक्षेप में, मैं लिखूँगा इसे सिर्फ़ एच ऑफ़ एक्स.

72
00:06:45,441 --> 00:06:51,627
लेकिन अधिकांश मैं लिखूँगा इसे एक सब्स्क्रिप्ट थीटा वहाँ पर. और प्लॉट करते हुए

73
00:06:51,627 --> 00:06:58,210
इसे पिक्चर्स में, इस सब का मतलब है कि हम करेंगे प्रिडिक्ट कि वाय है है एक लिनीअर

74
00:06:58,210 --> 00:07:04,634
फ़ंक्शन एक्स का. ठीक है, तो वह है डेटा सेट और यह फ़ंक्शन क्या कर रहा है,

75
00:07:04,634 --> 00:07:11,698
कि प्रिडिक्ट कर रहा है कि वाय है कोई सीधी रेखा फ़ंक्शन एक्स का. वह है एच ऑफ़ एक्स बराबर है थीटा 0

76
00:07:11,698 --> 00:07:18,450
प्लस थीटा 1 एक्स, सही? और क्यों एक लिनीअर फ़ंक्शन? सही है कि कभी कभी हम चाहेंगे

77
00:07:18,450 --> 00:07:23,405
फ़िट करना अधिक जटिल, शायद नॉन-लिनीअर फ़ंक्शन भी. लेकिन क्योंकि यह लिनीअर

78
00:07:23,405 --> 00:07:28,298
केस है सरल केस, हम शुरू करेंगे इस उदाहरण से पहले फ़िट करते हुए

79
00:07:28,298 --> 00:07:32,943
लिनीअर फ़ंक्शन को और हम बनाएँगे इस पर अंत में और जटिल

80
00:07:32,943 --> 00:07:37,403
मॉडल, और और अधिक जटिल लर्निंग अल्गोरिद्म्स. मैं देता हूँ इस

81
00:07:37,403 --> 00:07:42,628
विशेष मॉडल को भी एक नाम. इस मॉडल को कहते हैं लिनीअर रेग्रेशन या यह,

82
00:07:42,628 --> 00:07:48,271
उदाहरण के लिए, है वास्तव में लिनीअर रेग्रेशन एक वेरिएबल में, और वेरिएबल है

83
00:07:48,271 --> 00:07:53,914
एक्स. प्रिडिक्ट करना सारी क़ीमतें एक फ़ंक्शन एक वेरिएबल एक्स के से. और दूसरा नाम

84
00:07:53,914 --> 00:07:58,852
इस मॉडल का है यूनी वेरियेट लिनीअर रेग्रेशन. और यूनी वेरियेट है सिर्फ़ एक

85
00:07:58,852 --> 00:08:04,400
सुंदर ढंग कहने का एक वेरिएबल. तो, वह है लिनीअर रेग्रेशन. अगले

86
00:08:04,400 --> 00:08:09,760
वीडियो में हम शुरू करेंगे बात करना कि कैसे हम इम्प्लमेंट करते हैं यह मॉडल.