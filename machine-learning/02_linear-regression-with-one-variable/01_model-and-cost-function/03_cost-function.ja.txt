このビデオでは、目的関数というものを定義します。これは、 データに対し、どのように最適な直線を当てはめるか算出する為に役に立ちます。線形 回帰ではここに示すような訓練セットがあります。思い返して下さい。 m で表記されているのは、 訓練サンプルの数です。ですから m=47です。そして、 予測をするのに使う仮説の数式はこの線形関数です。 もう少し専門用語を使うと、この theta 0 および theta 1 、これですね、 こうした theta i は、モデルのパラメータといいます。 このビデオで行うのは、これら二つの パラメータの値をどのように選択するかということをお話します。パラメータ theta 0 と theta 1 の選び方次第では、異なった仮説、異なった 仮説関数となります。皆さんの中には既にご存知の方もいると承知していますが、 このスライドでお見せするのは、いくつか例を見て行きたいと思います。もし theta 0 が 1.5 で theta 1 が 0 だと、仮説関数はこのようになります。 そうですね。なぜなら、仮説関数は h(x) = 1.5 + 0 * x、つまり、この定数値関数で、1.5 の値で水平になります。 もし theta 0 が 0 で theta 1 が 0.5 だと、仮説はこうなります。 それはこの点 (2, 1) を通過するはずです、ここでは h(x) あるいは、実際には h_theta(x) ですが、時々 theta を省略することがあります。 ですから h(x) は単に = 0.5 * x となり、それはこのような形になります。そして最後に もし theta 0 が 1 で theta 1 が 0.5 だと、結果は このような仮説になります。これは点 (2, 2) を通過するはずです。 こんな風に。そしてこれは新しい h(x) あるいは新しい h_theta(x) です。 覚えていると思いますが、これは h_theta(x) ですが、単に簡略化のために 時々私は h(x) と書くことがあります。線形回帰では訓練セットがあり、 ここにプロットしたようなものになるかもしれません。ここで行いたいのは、 パラメータ theta 0 と theta 1 の値を算出することです。そしてその結果として得られる直線が、 データによく適合した直線に対応するようにすることです。ちょうどこの線のように。 では、どのようにして theta 0 と theta 1 の値をデータに対して よく適合するように算出するか。考え方としては、パラメータ theta 0 と theta 1 を 選ぶ 際に、h(x)、つまり入力 x に対する予測値が 最低でも y の値に近似するように、 訓練セットのサンプル、訓練サンプルのそれぞれに対して選びます。訓練セットでは、 数件のサンプルが与えられており、x が家を指定し、その実際の 販売価格を知っています。ですから、パラメータの値を選ぶのに、 少なくとも訓練セットでは、訓練セットの x の値を与えて、適度な正確さで y の値に近い予測を出力しようということです。これを形式化しましょう。線形回帰では、 行おうとしているのは、最小化問題を解くことです。ですから、theta 0、theta 1 の上に minimize と書きます。そして、これを 小さくしたいわけです。つまり、h(x) と y の差を小さくしたいわけです。そして さらに、最小化しようとするのは、仮説の出力と 実際の家の販売価格の差の二乗です。いいですか。では少し詳細を加えて行きます。覚えていますか、 私が (x(i), y(i)) という表記を使って i 番目の訓練サンプルを表したのを。ですから、実際に欲しいのは、訓練セットの総和です。 i = 1 から m までの差の二乗の総和を行い、 これは i 番目の家の仮説の予測値、その家の サイズを入力した場合ですね。引くことの、実際の価格、 i 番目の家が実際に売れる価格です。最小化したいのは、訓練セットの総和、 i = 1 から m の間のこの二乗誤差の差分、家の予想 価格と実際に売れる価格との差の二乗の合計です。そして 思い返して頂きたいのは、ここで m と表記されているのは訓練セットのサイズでしたね。 ですから、この m は訓練サンプルの数です。「#」の記号は、 訓練サンプルの「数」の省略です。いいですね。そして 計算をいくらか簡略化するために、実際には 1/m を それに掛けます。つまり、平均誤差を最小化するわけです。これを1/2m で最小化します。 2 を加えたのは、定数として 1/2 をその前に掛けるという ことで、これにより計算が少し楽になります。何かを半分にして最小化しても、 パラメータ theta 0 と theta 1 の値はやはり同じになるはずです。その関数を最小化した場合と同じに。 そして単に確認のためですが、この数式の意味は 明確ですよね? この式ですが、h_theta(x)、これは 既にお馴染みですよね? これ = theta 0 + theta 1 * x(i) です。そしてこの表記、 theta 0 と theta 1 の上に minimizeとしてある意味は、
theta 0 と theta 1 の値を、この式が最小になるように求めなさい、ということです。そして この式は theta 0 と theta 1 に依存します。分かりましたか。まとめますと、 この問題は、ある条件で theta 0 と theta 1 の値を見つけることとして定義しています。 その条件とは、平均ではなく実際には 1/2m 掛ける二乗誤差の総和が最小化されるようにということです。 差は、訓練セットに対する予測値から訓練セットの実際の家の価格を 引いたものです。これが最終的な線形回帰の 目的関数です。そしてこれを少し書き直してもっときれいにするために、 私が行うのは、慣習的に通常、目的関数の定義は 正にこの通りです。この数式、ここに書き出したものです。私がしたいのは、 最小化するパラメータは theta 0、theta 1、関数は J(theta 0, theta 1) 、今書いたもの、これが目的関数です。 さて、この目的関数は、二乗誤差関数と呼ばれたり、時には、 二乗誤差目的関数と呼ばれることもあります。ところでなぜ 誤差を二乗にするのでしょう? 実は、二乗誤差目的関数は ほとんどの問題に、回帰問題において、妥当な選択であり、 よく機能するからです。他にもよく機能する目的関数はありますが、二乗誤差関数は おそらく最も一般的に回帰問題で使われているものでしょう。 このクラスでは後に、代わりの目的関数についてもお話しますが、この選択、今お見せしたものは ほとんどの線形回帰問題で試してみるのに非常に妥当なものです。 さて、これが目的関数です。ここまでは、 単にこの目的関数の数学的な定義を見ただけで、 万が一この関数 J(theta 0, theta 1)、万が一この関数が少し抽象的に見える場合は、 それが何をしているのか感覚が掴めていなければ、次のビデオで 次の数件のビデオで、実際に少しもっと深く目的関数 J が 何をしているのかを学び、それが何を計算していて、なぜそれを使いたいのか、 皆さんがもっと直感的に理解できるようにしたいと思います。