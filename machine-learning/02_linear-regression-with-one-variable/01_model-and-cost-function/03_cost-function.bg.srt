1
00:00:00,000 --> 00:00:05,399
В това видео ще дефинираме нещо, 
ценова функция. Това ще ни позволи

2
00:00:05,399 --> 00:00:10,688
да решим как да посторим възможно най-добрата
права линя през данните ни. В линейната

3
00:00:10,688 --> 00:00:16,758
регресия имаме тренировъчно множество като 
това, показано тук. Спомнете си означението М

4
00:00:16,758 --> 00:00:21,972
беше броя тренировъчни примери. 
Така че може би М=47. И формата на

5
00:00:21,972 --> 00:00:27,731
хипотезата, която използваме за да правим 
предположения, е линейна функция. За да

6
00:00:27,731 --> 00:00:33,723
представим още малко терминолгия,
тези тета 0 и тета 1,

7
00:00:33,723 --> 00:00:39,759
тези тета i наричам параметрите на
модела. Това, което ще направим

8
00:00:39,759 --> 00:00:44,578
в това видео, е да си поговорим за
това как да изберем тези две

9
00:00:44,578 --> 00:00:49,654
стойности на параметрите, тета 0 и тета 1.
С различен избор на параметрите

10
00:00:49,654 --> 00:00:54,408
тета 0 и тета 1 получаваме различни
хипотези, различни хипотетични

11
00:00:54,408 --> 00:00:59,355
функции. Знам, че някои от вас може би
вече са запознати с това, което ще

12
00:00:59,355 --> 00:01:04,367
направя на този слайд, но само за да
преговорим, ето няколко примера. Ако тета

13
00:01:04,367 --> 00:01:09,378
нула е 1.5 и тета едно е 0, тогава
хипотетичната функция ще изглежда

14
00:01:09,378 --> 00:01:15,701
ето така. Нали, защото хипотетичната
ви функция ще бъде h( x) = 1.5 +

15
00:01:15,701 --> 00:01:22,645
0 по x, което е функция на константна
стойност, тоест права през 1.5. Ако

16
00:01:22,645 --> 00:01:29,332
тета нула е равна на 0 и тета едно е равна
на 0.5, тогава хипотезата ще изглежда

17
00:01:29,332 --> 00:01:35,536
ето така. И трябва да минава през тази
точка (2, 1), защото сега имаме h(x) или

18
00:01:35,536 --> 00:01:40,666
всъщност някаква h<u>theta(x), но
понякога просто ще пропускам тета</u>

19
00:01:40,666 --> 00:01:46,518
за съкращение. Така, h(x) ще е равно просто на
0.5 по x, което изглежда ето така. И

20
00:01:46,518 --> 00:01:52,443
накрая, ако тета нула е равно на 1 и тета
едно е равно на 0.5, значи ще имаме

21
00:01:52,443 --> 00:01:58,598
хипотеза, която изглежда ето така. Да видим,
трябва да минава през точка (2, 2)

22
00:01:58,598 --> 00:02:04,468
ето така. И това е новата ми h(x) или новата 
ми h<u>theta(x). Нали? Добре,</u>

23
00:02:04,468 --> 00:02:09,980
помните, че това е h<u>theta(x),
но за по-кратко</u>

24
00:02:09,980 --> 00:02:16,584
понякога просто го пиша като h(x). В линейната
регресия имаме тренировъчно множество,

25
00:02:16,584 --> 00:02:22,439
като може би това, което съм отбелязал с кръстчета 
тук. Искаме да намерим стойности за

26
00:02:22,439 --> 00:02:28,295
параметрите тета нула и тета едно. Така че
правата линия, която получаваме от това,

27
00:02:28,295 --> 00:02:33,799
да отговаря на правата линия, която
някак си минава добре през данните.

28
00:02:33,799 --> 00:02:39,756
Като може би тази права ето тук. Как да 
намерим стойности за тета нула и тета едно,

29
00:02:39,756 --> 00:02:45,350
които отговарят на добро построение
през данните? Идеята е да изберем

30
00:02:45,350 --> 00:02:51,162
параметрите тета нула, тета едно, така че
h(x), тоест стойността, която предвиждаме

31
00:02:51,162 --> 00:02:56,330
при вход x, да е поне близо до 
стойностите y за примерите в

32
00:02:56,330 --> 00:03:01,133
тренировъчното ни множество, или тренировъчните
ни стойности. Така, в тренировъчното множество са ни дадени

33
00:03:01,133 --> 00:03:06,505
няколко прмера, в които знам че x 
определя къщата, и знаем истинската цена,

34
00:03:06,505 --> 00:03:11,796
на която е била продадена. Хайде да се опитаме
да изберем стойности за параметрите, такива че

35
00:03:11,796 --> 00:03:17,302
поне в тренировъчното множество, при дадени
х-ове в тренировъчното множество, да можем да

36
00:03:17,302 --> 00:03:23,507
предвидим сравнително точно стойностите на
y. Да го направим формално. Линейната

37
00:03:23,507 --> 00:03:29,401
регресия, за да я приложа, ще искам 
да реша задача за минимизация.

38
00:03:29,401 --> 00:03:38,787
Ще искам да получа стойности при тета 
нула и тета едно, като искам това да бъде

39
00:03:38,787 --> 00:03:44,379
малко, нали, искам разликата 
между h(x) и y да бъде малка.

40
00:03:44,379 --> 00:03:50,493
Едно нещо, което мога да направя, е да се опитам да 
минимизирам квадрата от разстоянията между полученото от

41
00:03:50,493 --> 00:03:56,159
хипотезата и истинската цена на къщата. 
Добре? Хайде да запълним някои

42
00:03:56,159 --> 00:04:01,379
подробности. Спомнете си, че използвах
означението (x(i), y(i)), за да представя

43
00:04:01,379 --> 00:04:07,418
i-тия поред тренировъчен пример. Това, което
наистина искам, е сума в тренировъчното ми

44
00:04:07,418 --> 00:04:13,202
множество. Сумата от i=1 до М
от квадрата от разликата между

45
00:04:13,202 --> 00:04:18,896
това, е предположението на моята хипотеза, 
когато ѝ е зададен вход размера на къща номер

46
00:04:18,896 --> 00:04:24,380
i, нали, минус истинската цена, за която
тази къща ще бъде продадена, и искам

47
00:04:24,380 --> 00:04:29,588
да минимизирам сумата в тренировъчното 
ми множество, сумата от i = 1 до М, от

48
00:04:29,588 --> 00:04:35,281
разликата, от тази грешка на квадрат,
разликата на квадрат между предвидената

49
00:04:35,281 --> 00:04:41,091
цена на къщата и цената, за която 
наистина ще бъде продадена. И само

50
00:04:41,091 --> 00:04:47,723
да напомня, че означавам с М тук големината
на тренировъчното ми множество, нали,

51
00:04:47,723 --> 00:04:53,347
така че М тук е броя тренировъчни 
примери. Нали? Символа # е

52
00:04:53,347 --> 00:04:59,045
съкращение за "броя" тренировъчни
примери. Добре? И за да направим

53
00:04:59,045 --> 00:05:04,888
смятането малко по-лесно, ще 
разглеждам всъщност,

54
00:05:04,888 --> 00:05:09,578
1 върху М, умножено по това. Така че ще се опитаме да 
минимизираме средно аритметично от грешката, което

55
00:05:09,578 --> 00:05:13,926
ще направи 1 върху 2М минимално.
Като сложим двойката, константата 1/2

56
00:05:13,926 --> 00:05:18,386
отпред, прост смятането става малко
по-лесно. Така че минимума на половината

57
00:05:18,386 --> 00:05:23,073
от нещо ще даде същите стойности 
за параметрите тета нула и тета

58
00:05:23,073 --> 00:05:27,647
едно, като минимума на тази функция.
И само за да се уверим, че това уравнение

59
00:05:27,647 --> 00:05:35,569
е ясно... Този израз тук, 
h<u>theta(x), това е</u>

60
00:05:35,569 --> 00:05:44,880
обичайното, нали? Равно е на това
плюс тета едно x(i). И този запис

61
00:05:44,880 --> 00:05:49,814
минимизираме по тета нула и тета едно,
означава, намери ми стойностите на тета

62
00:05:49,814 --> 00:05:54,369
нула и тета едно, такива че този
израз е с минимална стойност. И този

63
00:05:54,369 --> 00:05:59,557
израз зависи от тета нула и тета едно. Нали?
Сега за да преговорим, поставяме тази задача:

64
00:05:59,557 --> 00:06:04,382
намери ми стойностите на 
тета нула и тета едно, така че

65
00:06:04,575 --> 00:06:09,292
средното аритметично, 1/М умножено 
по сумата от квадратните грешки между

66
00:06:09,292 --> 00:06:14,590
предположенията за тренировъчното множество,
минус истинските стойности на къщите за

67
00:06:14,590 --> 00:06:19,694
тренировъчното множество, е с минимална стойност.
Така че това ще бъде функцията, към която се стремим

68
00:06:19,694 --> 00:06:25,127
при линейната регресия. И просто,
за да го препишем малко по

69
00:06:25,127 --> 00:06:30,580
начисто, по конвенция обикновено
дефинираме ценова функция.

70
00:06:30,860 --> 00:06:38,965
Която ще е точно това. Тази
формула, която имам тук горе.

71
00:06:38,965 --> 00:06:48,388
Искам да минимизирам по тета нула и тета 
едно функцията ми J от тета нула

72
00:06:48,388 --> 00:06:57,428
запетайка тета едно. Написано 
така, това е ценова функция.

73
00:06:57,428 --> 00:07:06,943
Ценовата функция също се нарича функция 
на квадратните грешки, или понякога

74
00:07:06,943 --> 00:07:14,461
ценова функция с квадратни грешки,
и излиза, че... Защо взимаме

75
00:07:14,461 --> 00:07:19,006
грешките на квадрат? Излиза че ценовата 
функция с квадратни грешки е

76
00:07:19,006 --> 00:07:23,214
разумен избор, и ще работи за повечето
задачи, за повечето задачи за регресия.

77
00:07:23,214 --> 00:07:27,815
Има други ценови функции, които
работят доста добре, но ценовата

78
00:07:27,815 --> 00:07:32,473
функция с квадратни грешки е може би
най-често използвана за задачи с регресия.

79
00:07:32,473 --> 00:07:36,793
По-късно в този курс ще говорим също и
за други ценови функции, но този

80
00:07:36,793 --> 00:07:41,282
избор който направихме, би трябвало да
е доста разумен за опитване върху

81
00:07:41,282 --> 00:07:45,706
повечето задачи с линейна регресия. Добре.
Това е ценовата функция. До момента

82
00:07:45,706 --> 00:07:50,899
видяхме математическа дефиниция на
ценовата функция, и в случай че тази

83
00:07:50,899 --> 00:07:55,973
функция J от тета нула 
изглежда малко абстрактна

84
00:07:55,973 --> 00:08:00,808
и още нямате добра представа какво
прави, в следващото видео,

85
00:08:00,808 --> 00:08:05,882
в следващите няколко видеа ще отидем
малко по надълбоко, във това какво прави

86
00:08:05,882 --> 00:08:10,776
ценовата функция J, и ще се опитам да ви
дам по-добра интуция за това какво пресмята

87
00:08:10,776 --> 00:08:12,329
и защо искаме да я ползваме.