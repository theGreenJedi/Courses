W poprzednim filmie podaliśmy definicję matematyczną funkcji kosztu. W tym filmie zobaczymy parę przykładów, aby wyrobić sobie intuicję, co robi funkcja kosztu i dlaczego chcemy jej użyć. Krótkie przypomnienie: oto, co mieliśmy ostatnio. Chcemy dopasować prostą do naszych danych, więc sformułowaliśmy hipotezę z tymi parametrami theta0 i theta1, a w zależności od wyboru tych parametrów uzyskujemy różne dopasowanie prostych. A więc dane są dopasowane w ten sposób, mamy tu funkcję kosztu, a to był nasz cel optymalizacji. W tym nagraniu, w celu lepszej wizualizacji funkcji kosztu J, będę korzystał z uproszczonej funkcji hipotezy - tej po prawej stronie. Zamierzam więc użyć uproszczonej hipotezy, równej po prostu theta1 razy x. Możemy o tym myśleć w ten sposób, że parametr theta0 jest równy 0. Mam więc tylko jeden parametr theta1, a moja funkcja kosztu jest podobna do tego, co było wcześniej, z tym, że h(x) równa się teraz theta1 razy x. Mam tylko jeden parametr theta1, a więc celem mojej optymalizacji jest minimalizacja J(theta1). Graficznie oznacza to, że jeśli theta0 równa się 0, to odpowiada to wyborowi jedynie tych funkcji hipotezy, które przechodzą przez punkt (0, 0). Korzystając z takiej uproszczonej definicji funkcji kosztu hipotezy, spróbujmy lepiej zrozumieć ideę funkcji kosztu. Okazuje się, że musimy zrozumieć dwie kluczowe funkcje. Pierwszą jest hipoteza, a drugą funkcja kosztu. Zauważ, że dla stałej wartości theta1, funkcja hipotezy h(x) jest funkcją x. Tak więc hipoteza jest funkcją wielkości domu - x. Z kolei funkcja kosztu J jest funkcją parametru theta1, który określa nachylenie prostej względem osi. Narysujmy te funkcje i spróbujmy zrozumieć je obie lepiej. Zacznijmy od hipotezy. Powiedzmy, że po lewej mamy zbiór uczący trzech punktów: (1, 1), (2, 2) i (3,3). Wybierzmy wartość theta1 tak, że gdy theta1 = 1, jeśli taką wybiorę wartość theta1, moja hipoteza będzie prostą taką jak ta. Zaznaczam też, że przy rysowaniu funkcji hipotezy, oś x - moja oś pozioma jest oznaczona jako x, czyli wielkość domu. Teraz, zakładając na chwilę, że theta1 = 1, chcę sprawdzić, jakie jest J(theta1), gdy theta1 = 1. Policzmy więc wartość funkcji kosztu dla wartości równej 1. Jak zwykle moja funkcja kosztu jest zdefiniowana w ten sposób. Suma po całym zbiorze uczącym błędu do kwadratu. A więc jest to równe h(theta1) razy x(i) - y(i) do kwadratu, a po uproszczeniu okazuje się, że jest to 0 kwadrat plus zero kwadrat plus 0 kwadrat, co oczywiście równa się 0. Okazuje się, że wewnątrz funkcji każdy z tych wyrazów jest równy 0, ponieważ w przypadku tego zbioru uczącego mamy punkty (1, 1), (2, 2), (3, 3). Jeśli theta1 jest równe 1, wtedy h(x(i)) równa się dokładnie y(i). Zapiszę to ładniej. Ok? A więc h(x) - y w każdym przypadku jest równe 0, i dlatego J(1) = 0. Tak więc wiemy, że J(1) = 0. Narysujmy to. Po prawej narysuję funkcję kosztu J. Zauważ, że ponieważ moja funkcja kosztu jest funkcją parametru theta1, gdy rysuję funkcję kosztu, oznaczam oś poziomą jako theta1. Mam więc J(1) = 0 - narysujmy to. Otrzymujemy "X" w tym miejscu. Spójrzmy teraz na resztę przykładów. Theta1 może przyjąć różne wartości. Theta1 może przyjąć wartości ujemne, zero, wartości dodatnie... Co więc się stanie, jeśli theta1 będzie równe 0,5? Co wtedy? Narysujmy to. Przypiszę wartość 0,5 do theta1 - wtedy moja hipoteza wygląda tak. Mamy prostą o nachyleniu równym 0,5 - policzmy J(0,5). Mamy więc 1/2m razy funkcja kosztu... Okazuje się, że funkcja kosztu będzie sumą kwadratów wysokości punktu względem tej linii plus wysokość tego punktu plus wysokość tego punktu. Jest tak, bo ta pionowa odległość jest po prostu różnicą między y(i) a h(x(i) - naszą przewidywaną wartością dla x(i), prawda? Tak więc pierwszy przykład to 0,5 - 1 do kwadratu, bo hipoteza przewiduje wartość 0,5, a rzeczywista wartość wynosi 1. W drugim przypadku mamy (1-2)^2, bo moja hipoteza przewidziała wartość 1, podczas gdy prawdziwa cena wyniosła 2. Na końcu mamy (1,5 - 3)^2. To się równa 1/(2*3), bo mój zbiór uczący ma trzy przykłady treningowe. Teraz mnożymy przez uproszczoną hipotezę: 3,5/6, co daje ok. 0,68. Teraz więc wiemy, że J(0,5) wynosi ok. 0,68. Narysujmy to. O, przepraszam, błąd - tak naprawdę to 0,58. To będzie gdzieś tutaj. Ok? Zróbmy jeszcze jeden przykład. Co, jeśli theta1 = 0? Czemu równa się J(0)? Okazuje się, że jeśli theta1 = 0, to h(x) jest po prostu linią poziomą, taką jak ta. Tak więc, mierząc błędy, otrzymujemy J(0) = (1/2m) * (1^2 + 2^2 + 3^2), a to jest równe 1/6 * 14, a to równa się ok. 2,3. Narysujmy to. Otrzymujemy więc wartość ok. 2,3. Oczywiście możemy to powtarzać dla innych wartości theta1. Okazuje się, że możesz mieć też ujemne wartości theta1. Wtedy h(x) będzie się równać -0,5*x. Theta1 = -0,5, co odpowiada hipotezie o nachyleniu -0,5. Możesz liczyć te błędy dalej. Okazuje się, że dla wartości -0,5 błąd będzie bardzo duży, ok. 5.25. Możesz też policzyć błąd dla innych wartości theta1. Okazuje się, że jeśli przeliczysz szereg wartości, otrzymasz coś takiego. A licząc błąd dla różnych wartości, możesz w końcu powoli nakreślić jak wygląda funkcja J(theta). Wygląda ona właśnie tak. Podsumowując: każda wartość theta1 odpowiada innej hipotezie, innemu dopasowaniu prostej po lewej. I dla każdej wartości theta1 otrzymalibyśmy inną wartość J(theta1). Np. dla theta1 = 1, mamy linię przechodzącą idealnie przez te dane. Jeśli theta1 = 0,5 (punkt oznaczony magentą), otrzymujemy taką linię, a jeśli theta1 = 0, co zaznaczyłem na niebiesko, odpowiada to tej linii poziomej. Tak więc w przypadku każdej wartości theta1 mamy inną wartość J(theta1) i możemy na tej podstawie sporządzić ten wykres po prawej. Teraz przypomnij sobie, że celem naszej optymalizacji jest wybór wartości theta1, która minimalizuje J(theta1). To był nasz cel optymalizacji w przypadku regresji liniowej. Patrząc na tę krzywą, wartością minimalizującą J(theta1) jest theta1 = 1. I rzeczywiście: jest to najlepiej dopasowana linia przechodząca przez dane. W przypadku tego zbioru uczącego otrzymaliśmy idealne dopasowanie. Dlatego minimalizacja J(theta1) odpowiada znalezieniu prostej dobrze dopasowanej do danych. Podsumowując: narysowaliśmy parę wykresów, aby zrozumieć funkcję kosztu. W tym celu uprościliśmy algorytm tak, że miał on tylko jeden parametr: theta1. Przyjęliśmy, że theta0 jest stałe i równe 0. W następnym materiale powrócimy do pierwotnej postaci algorytmu i zobaczymy parę wizualizacji uwzględniających zarówno theta0, jak i theta1, tzn. przyjmiemy, że theta0 nie jest równe 0. To powinno wyrobić Ci lepszą intuicję odnośnie tego, co robi funkcja kosztu J w przypadku pierwotnej wersji regresji liniowej.