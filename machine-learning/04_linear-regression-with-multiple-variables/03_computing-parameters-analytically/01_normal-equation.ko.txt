이 번 비디오에서는 normal equation에 대해 배워보겠습니다. 특정 선형 회귀 문제에서, 파라미터 θ의 최적의 값을 구하는데 효과적인 방법입니다. 구체적으로, 지금까지 지금까지 선형회귀에 사용했던 알고리즘은 gradient descent였고, J(θ)를 최소화 화기위한 것이였습니다. 수 차례에 걸쳐 알고리즘을 사용했고, gradient descent를 여러 번 반복하고 나면, 최소값으로 수렴하였습니다. 이와는 반대로, normal equation은 θ를 분석적으로 구하는 방법입니다. 그래서, 여러번 알고리즘을 돌릴 필요 없이, θ의 최적의 값을 한 번에 구합니다. 즉 한 번의 실행으로 최적의 값을 구합니다. normal equation도 장점과 단점이 있지만, 장단점과 사용시기를 말하기 전에, 어떠한 함수인지 개념을 알아 봅시다. 설명을 위한 예제로, 단순하게 이런 cost function J(θ)가 있다고 가정해봅시다. 실수 범위의 θ에 대한 함수입니다. 이번에는, θ가 스칼라 값 즉, 값만 가지고 있다고 가정하겠습니다. 그냥 숫자입니다. 벡터가 아닙니다. Cost function J는 실수 파라미터 θ에 대한 2차 함수라고 해봅시다. 그러면, J(θ)는 이런 모양이 되죠. 이제, 2차 함수를 최소화 하는 방법은 무엇일까요? 미적분을 배웠다면, 함수를 최소화하기 위해 미분을 하고, 미분값이 0이 되는 지점이 답이라는 것을 알 수 있습니다. 그래서 J를 θ에 대해 미분해보면, 이런 식이 나올 것입니다.
직접 미분은 하지 않을게요. 미분값이 0일 때, J(θ)가 최소화되는 θ를 구할 수 있습니다. data가 실수일 때 가능한 경우였습니다. 우리가 흥미롭게 다루어볼 문제는, θ가 더 이상 실수가 아니라 (n+1)차원 파라미터 벡터일 때입니다. cost function J도 벡터 값에 대한 즉, θ_0부터 θ_m까지에 대한 함수입니다. cost function은 이렇고, 오른쪽은 제곱함수입니다. cost function J를 어떻게 최소화 할 수 있을 까요? 미적분을 이용한다면, 한 가지 방법으로 J를 편미분하는 방법이 있습니다.
각각 모든 파라미터 θ_j에 따라서 차례차례로 미분하고, 모두 0이 되게 합니다. 그렇게 하면, 모든 θ에 대한 답을 구할 수 있습니다. θ_0, θ_1, θ_n까지요. 그리고나서, 그 때의 θ를 대입하면, cost function J를 최소화 할 수 있습니다. 미적분을 사용하여, 파라미터 θ_0부터 θ_n까지 식을 풀면, 결국에는 답이 나옵니다. 이 번 비디오에서는 미분을 사용하여, 답을 구하지 않을 것입니다. 미분은 시간이 오래 걸리는 방법입니다. 하지만, 이 과정을 구현하기 위해서는 알아야 할 것이 있습니다. 그러면, 편미분 방정식이 0이 되는 θ 값을 구할 수 있습니다. 같은 말이지만, 다르게 말하면, cost function J(θ)가 최소화 되는 θ를 구하는 것입니다. 이번에 다룰 내용은 미적분학에 익숙치 않은 사람에게 생소할지도 모릅니다. 하지만, 이해가 잘 안되거나, 미적분에 낯설지라도, 걱정할 필요없습니다. 알고리즘을 구현하고 적용하는데 필요한 것들은 알려드릴 겁니다. 이 예제는, 이 번에 다루어볼 예제입니다. m=4인 training example이 있습니다. normal equation을 구현하기 위해 해야할 것은 아래에 있습니다. data set을 먼저 보면, 4개의 training example가 있습니다. 이번 예제에서는 오직 4개가 가지고 있는 data 전부입니다. 해야할 것은 data set에 열을 추가하는 것입니다. 추가된 열은 feature x_0입니다. 모두 1의 값을 가집니다. 그 다음으로는 X라는 행렬을 만드는 것입니다. 행렬 X는 training data의 모든 feature를 가지고 있습니다. 즉 이것은 모두 feature이고, 이 숫자들은 전부 행렬 X에 넣습니다. 이해했죠? 단지, 한 번에 한 열씩 복사한거에요. 
그리고나서, 똑같이 y에도 합니다. 예측하려는 값을 가져와서 벡터를 만듭니다. 이렇게요. 이 벡터는 벡터 y라고 부르겠습니다. 결국 X는 (m x n+1) 차원 행렬이 되었고 y는 m차원 벡터가 되었습니다. m은 training example의 수이고, n은, n은 feature의 수입니다. 
n+1인 이유는 여기 추가로 feature x_0이 있기 때문입니다. 마지막으로, 행렬 X와 벡터 y를 가지고, 이 식을 계산하면, θ는 (X transpose X)의 역행렬에 X'y를 곱한 값이 됩니다. 이 때의 θ값이 cost function을 최소화 하는 값입니다. 이 슬라이드에서 많은 계산을 했지만, 오직 하나의 dataset을 예로 풀어보았습니다. 더 일반화된 식으로 써보고, 비디오 후반부에서, 방정식에 대한 설명을 다루어보겠습니다. 아직 어떻게 하는지는 와닿지 않을 것입니다. 일반적인 경우를 보면, m개의 training example에서 ( x^(1) , y^(1) )부터 ( x^(n), y^(n) )까지 n개의 feature가 있습니다. 그리고, 각각의 training example x^(i)는 이와 같은 벡터입니다. (n+1)차원 feature 벡터죠 이제 행렬 X를 만들어 볼 것입니다. 행렬 X는 design matrix라고도 하며, 이와 같이 만듭니다. 각각의 training example은 이와 같은 feature 벡터로 나타낼 수 있습니다. (n+1)차원 벡터죠 design matrix X를 만드는 방법은 이렇습니다. 우선 첫 번째 training example, 즉 벡터죠. 
벡터를 transpose하여, 이렇게 넣습니다. 이렇게 가로로 퍼진 형태가 되고, x^(1)을 transpose한 것이  design matrix의 첫 번째 열이 됩니다. 이제 두 번째 training example x^(2)를 보죠. transpose하고, X의 두 번째 열에 넣습니다. 나머지도 이런식으로 해서, 마지막 training example까지 내려갑니다. 마지막을 transpose하여 행렬 X의 마지막 열로 만듭니다. 이렇게 행렬 X를 만듭니다. X는 (m x n+1)차원 행렬이죠. 구체적인 예로 하나의 feature만 있는 경우를 보죠. 진짜로 값이 1인 x_0를 제외하고 하나의 feature만 가지고 있습니다. 그래서 x_i가 1이라면, 그 feature는 x_0이고, 어떤 실수를 가진다면, 집의 크기가 될 것입니다. 이제, design matrix X는 이럴 것입니다. 첫째 열은, 이 것을 가져와서 transpose하죠 그래서 여기는 1이 되고, 다음은 ( x^(1) )_1입니다. 두 번째 열은, 1이 먼저 오고나서, ( x^(1) )_2이 됩니다. 이런식으로 1과 ( x^(1) )_m까지 내려옵니다. 결국, (m x 2)차원 행렬이 되겟죠. 이렇게 행렬 X를 만들 수 있습니다. 그리고, 벡터 y, 가끔 위쪽에 화살 표시는 벡터라는 뜻을 나타내지만, 보통은 그냥 y로만 쓰겠습니다. 어찌됬든, 벡터 y는 모든 label, 모든 training set의 정확한 집 값을 가져와서 만듭니다. 그것들을 계속 쌓아서, m차원 벡터를 만듭니다. 이게 y입니다. 마지막으로, 행렬 X와 벡터 y를 만든 상태에서, (X'X)^-1에 X'Y를 곱하여 θ를 구합니다. 여기서 잠시, 여기서 잠시 이 식의 의미를 이해하고, 어떻게 구현하는지 알려주고자 합니다. 그렇다면, 명확하게, (X'X)^-1은 무엇일까요? (X'X)^-1는 X'X의 역행렬입니다. 구체적으로, A를 X'X로 정의하면, X'는 행렬이고, X'X도 행렬입니다. 그래서 A는 행렬 A라 하겠습니다. 그러면, (X'X)^-1는 행렬 A를 역행렬로 바꾼 것입니다. 알겠죠? 이 것은 A^-1이라 하겠습니다. 이게 바로 계산하는 방법입니다. X'X를 계산하고나서 그것을 역행렬로 바꿉니다. 아직 Octave에 대해서는 다루지 않았지만, 뒤의 비디오에서 다룰 것입니다. Octave 혹은 비슷한 프로그래밍 언어인 matlab에서는 아래의 명령어로 (X transpose X)의 역행렬에 X transpose와 y를 곱합니다. Octave에서 X'는 X의 transpose를 나타내는 기호입니다. 그리고, 붉은색 박스안에 있는 식은 X'에 X를 곱한 값을 계산합니다. pinv는 행렬의 역행렬을 계산하는 함수입니다. 그래서 이 식은 X'X의 역행렬을 계산하고, 그리고나서, X‘와 먼저 곱한 다음, y와 곱합니다. 이상으로 이 식의 계산이 끝났습니다. 증명하지는 않았지만, 여기서는 못하더라도 수학적으로 보여줄 수는 있습니다. 이 식으로 θ의 최적의 값을 구할 수 있습니다. 최적의 값은 이 식을 θ로 정의하면, 그 값은 선형회귀에서 cost function J(θ)를 최소화 하는 θ값이라는 뜻입니다. 이전 비디오에서 feature scaling을 자세히 배웠는데, feature의 범위를 비슷한 범위로 조절하여 각각 비슷한 값의 범위를 가지도록 하는 것입니다. 만약 normal equation을 사용한다면, feature scaling은 필요하지 않습니다. 예를들면, feature x_1이 0부터 1사이이고, feature x_2가 0에서 1000 사이에 있고, feature x_3가 0부터 0.00005 사이에 있더라도 괜찮습니다. normal equation을 사용한다면, feature scaling이 없어도 괜찮습니다. 물론 gradient descent를 사용한다면, feature scaling은 필요합니다. 마지막으로, 언제 어느 때 gradient descent와 normal equation을 써야하는지 알아봅시다. 둘의 장점과 단점입니다. m개의 training example가 있고, n개의 feature가 있다고 가정해봅시다. gradient descent의 첫 번째 단점은 learning rate 알파를 정해야 한다는 것입니다. 즉, 서로 다른 learning rate 알파를 가지고 여러번 gradient descent를 해봐야 한다는 것이고, 그 중에서 언제 가장 잘 돌아가는지도 찾아야 합니다. 귀찮은 일을 추가로 더하게 되는 것입니다. gradient descent의 또 다른 단점은 반복이 많다는 것입니다. 얼마나 세세하게 하냐에 따라서, 느려질수도 있습니다. gradient descent에 관해서 더 이야기할게 남았지만 잠시 후에 하도록 하겠습니다. normal equation은 learning rate 알파를 정할 필요가 없습니다. 그래서 매우 편리하고, 구현하기도 간단합니다. 실행만하면 됩니다. 그리고, 반복도 없습니다. 그래서 J(θ) 그래프를 그린다던지 수렴을 확인한다던지하는 추가적인 작업이 없습니다. 아직까지는, normal equation이 더 나은 것처럼 보일지도 모릅니다. 이제 normal equation의 단점과 gradient descent의 장점을 알아봅시다. gradient descent는 feature가 많을 때, 효과적입니다. 만약, 수백만개의 feature를 가지고 gradient descent를 한다면, 효율적이고, 적당하게 돌아갈 것입니다. 반대로 normal equation은 파라미터 θ를 구하기 위해, 이 식을 풀어야 합나다. X'X의 역행렬식을 계산해야 합니다. 이 행렬 X'X은 n개의 feature를 가지고 있다면, (n x n)행렬입니다. 왜냐하면, X'의 차원수와 X의 차원수를 가지고 알 수 있는데,
이 둘을 곱하면 그 결과의 차원이 얼마인지 알 수 있고, 결국, 행렬 X'X는 (n x n) 행렬이 됩니다. n은 feature의 개수입니다. 그리고, 행렬의 역행렬을 계산하는데 걸리는 시간은, 대략 행렬의 차원수의 세제곱만큼 증가합니다. 즉, 역행렬 계산의 걸리는 시간은 보통 세제곱만큼 걸립니다. 가끔 세제곱보다 빠를때도 있지만, 차이는 거의 없습니다. 결국, feature의 수 n이 엄청 커지면, 식의 계산속도는 느려질수 있고, normal equation은 결국 훨씬 더 느려집니다. 그래서 n이 엄청 클 때에는, 저는 주로 gradient descent를 씁니다. 왜냐하면 gradient descent는 세제곱만큼 시간이 들지 않기 때문입니다. 만약 n이 작다면, normal equation이 파라미터를 구하는데 더 나을것입니다. 그럼 언제 작고 언제 크다고 볼 수 있을까요? n이 100단위라면, (100 x 100)행렬의 역행렬을 구하는 것은 요즘의 컴퓨터수준으로 아무 문제가 없습니다. n이 1000이라도,
저는 여전히 normal equation을 사용할 것입니다. 1000 x 1000 행렬의 역행렬도 요즘 수준의 컴퓨터로 충분히 빠르게 구할 수 있습니다. 하지만 만약 n이 10000이라면, 한번 생각해봐야합니다. 10000 x 10000행렬부터 역행렬을 구하는 게 느려지기 시작합니다. gradient descent로 갈아탈지 말지 고민하게 되지만, 아직은 아닙니다. n이 1만이라면, 10000 x 10000 행렬은 계산 할 수는 있습니다. 하지만, 이보다 더 큰 경우에는,
gradient descent를 사용하는게 낫습니다. 결국은, n이 10^6, 즉 feature가 백만개라면, (100만 x 100만) 행렬의 역행렬은 많은 시간이 필요합니다. feature가 많다면, gradient descent를 강력하게 추천합니다. 정확히 feature가 몇 개 일 때, gradient descent를 해야하는지, 
정확한 숫자는 제시하기 어렵습니다. 하지만, 저같은 경우, 1만개 정도 일 때, gradient descent나, 아니면, 뒤에서 배울 다른 알고리즘을 고려해봅니다. 정리하자면, feature의 수가 많지 않을 때는, normal equation이 파라미터 θ를 구하는데, 좋은 방법입니다. 정확히는 feature의 수가 1000보다 작을 때 normal equation을 gradient descent가 아닌 normal equation을 사용할 것입니다. 나중에 배울 내용에 대해서 살짝 이야기하자면, 더 복잡한 알고리즘을 배울 때 예를들면, 분류(classification) 알고리즘 중에서도
logistic regression 알고리즘 같은, 나중에 실제로 배울 테지만 사실 그런 정교한 학습 알고리즘에 normal equation은 적합하지 않지만, gradient descent는 적합합니다. 그래서, gradient descent는 알고있으면 
매우 유용한 알고리즘입니다. 선형 회귀는 매우 많은 feature를 가지는 경우가 많고, 이 수업에서 배우는 gradient descent 외 알고리즘들은 예로 normal equation은 적용하기도 어렵습니다. 하지만, 선형 회귀의 특정 모델에 대해서는 normal equation이 gradient descent보다 더 빠를 수도 있습니다 결국, 알고리즘이 어떠냐에 따라, 문제가 어떻고 feature가 얼마나 많냐에 따라 달라지기 때문에, 두 알고리즘은 알아두면 좋습니다.