Neste vídeo, vamos falar sobre a equação normal, que para alguns problemas de regressão linear nos dará um caminho muito melhor para encontrar o valor ótimo dos parâmetros θ. Até agora o algoritmo que estamos usando para a regressão linear é gradiente descendente, onde, de forma a descendente para J(θ), usaríamos este algoritmo interativo que leva muitos passos, múltiplas interações para convergir ao mínimo global. A equação normal, ao contrário, nos daria um método para resolver θ analiticamente, onde em vez de precisar executar o algoritmo iterativo, poderíamos apenas encontrar o melhor valor para θ de uma só vez, de forma que em um passo você obtenha o esse valor ótimo. A equação normal tem algumas vantagens e algumas desvantagens, mas antes vamos falar sobre quando você deve usá-la, vamos obter alguma percepção sobre o que faz este método. Para o exemplo explicativo desta semana, vamos imaginar uma função de custo bem simplificada J(θ), que é apenas a função de um número real θ. Por enquanto, imagine que θ é somente um valor escalar ou que θ é apenas um valor em linha. É só um número, em vez de um vetor. Imagine que temos uma função J que é uma função quadrática deste parâmetro de valor real θ, assim J(θ) se parece com isto. Como você minimiza uma função quadrática? Para quem já tem algum conhecimento de cálculo, deve saber que a forma de minimizar uma função é utilizar as derivativas e igualá-las a zero. Assim, você parte a derivativa de J com respeito ao parâmetro θ, que é uma fórmula que eu não vou mostrar, e iguala essa derivativa a zero, permitindo encontrar θ que minimize J(θ). Este foi um caso mais simples de quando o dado é apenas um número real. No problema que estamos interessados, θ não é só um número real mas sim um vetor de parâmetros com dimensão n+1, e uma função custo J é uma função deste vetor, de θ 0 a θ m. A função de custo teria o mesmo formato de uma quadrática, como a da direita. Como minimizamos esta função de custo J? Teoria de Cálculo nos diz que uma forma de fazer isso é pegar a derivada parcial de J com respeito a todo parâmetro de θ J, e igualar todos os termos a 0. Ao fizer isso, ao encontrar os valores de θ 0, θ 1, até θ N, teríamos os valores de θ que minimizam a função de custo J. Na verdade, se você fizer todos os cálculos para encontrar a solução com os parâmetros θ 0 até θ N, a derivação faz parte do processo. Mas o que vou fazer neste vídeo, é não desenvolver os cálculos, que são meio longos que acabam fazendo parte, mas queria apenas dizer o que você precisa saber para implementar este processo, para que assim possa encontrar os valores de tetas correspondentes ao ponto onde a derivativa parcial é igual a zero. Ou de forma alternativa, encontrar os valores de θ que minimizam a função de custo J(θ). Eu percebo que alguns dos comentários que eu fiz, fizeram sentido somente para aqueles mais familiarizados com cálculo. Mas se você não sabe, se você está menos familiarizado com cálculo, não se preocupe. Vou mostrar agora o necessário para implementar este algoritmo e fazê-lo funcionar. Para o exemplo que eu quero usar como um exemplo funcionando, digamos que eu tenho m=4 exemplares de treinamento. Para implementar esta equação normal vou fazer o seguinte: Vou pegar meu conjunto de dados, esses meus quatro exemplos de treinamento. Neste caso, vamos considerar que estes quatro exemplos são todos os dados que tenho. O que vou fazer é pegar meu conjunto de dados e adicionar uma coluna extra que corresponda ao meu atributo extra, x0, que será sempre  1. O que vou fazer é construir uma matriz chamada X, que contém todos as variáveis dos meus dados de treinamento. Aqui estão todas minhas variáveis, vamos vamos pegar todos aqueles números e colocá-los nesta matriz "X". Copie os dados uma coluna de cada vez, fazendo o mesmo para os "y". vou pegar a os valores que estou tentando predizer e construir um vetor como este, e chamar o vetor de y. Assim, X será uma matriz de dimensão m por (n+1), e Y será um vetor de dimensão m, onde m é o número de exemplos de treinamento e n o número de variáveis, n+1, por causa deste recurso extra x0 que tenho. Por fim, se você pegar sua matriz X e seu vetor Y, calcular isto, fazendo com que θ seja igual a (X' * X) ^-1 * X' *  y X transposto de Y, você teria o valor de θ que minimiza sua função de custo. Muitas coisas apareceram nesse slide, e eu vou explicá-las usando um conjunto de dados específico. Deixe-me escrever isso de uma forma um pouco mais geral, e mais tarde nesse vídeo vou explicar melhor essa equação. Ainda não está totalmente claro como fazer isto. No caso geral, vamos dizer que temos "m" exemplos de treinamento, ( x1 ,y1 ) até ( xn, yn ) e "n" variáveis. Cada um dos exemplos de treinamento x(i) pode será um vetor, como este, que é um vetor de variáveis com dimensão n+1. A forma com que vou construir a matriz "X", que também é chamada de matriz de projeto, será dessa maneira. Cada exemplo de treinamento me dá um vetor de varáveis como este. Um vetor de dimensão n+1. A maneira com que vou construir minha matriz design X é a seguinte. formada pelo o primeiro exemplo de treinamento, que é um vetor, pegar seu transposto, longo, plano, fazer x1 transposto ser a primeira linha da minha matriz design. Então vou pegar meu segundo exemplo, x2, pegar a transposta dele e colocar como a segunda linha de X,  e assim por diante, até meu último exemplo de treinamento. Pegue a transposta desse vetor, que será a minha última linha da minha matriz X. Dessa forma montamos a matriz X, uma matriz de dimensão M por N+1. Como um exemplo prático, vamos dizer que eu tenho apenas uma variável, somente um atributo além de x0, que sempre é igual a 1. Assim, se meus vetores de recursos xi são iguais a este 1, que é X-0, então algum atributo real, como, talvez, o tamanho da casa, então a minha matriz X seria essa. Para a primeira linha, vou pegar isso e transpor. Desse modo, vou acabar com 1, e então x-1-1. Para a segunda linha, vamos vamos acabar com 1 e então x-1-2 e assim por diante até 1, até x-1-M. E dessa maneira, isso será uma matriz de dimensão m por 2. É assim que contruimos a matriz X. E, algumas vezes, para o vetor Y posso escrever uma seta em cima para indicar que isto é um vetor, mas geralmente vou escrever apenas "y". Obtemos o vetor Y pegando todos as classes, todos os preços corretos das casas em meu conjunto de treinamento, e empilhando eles em um vetor de dimensão M, gerando "y". Por fim, construídos a matriz X e o vetor y, precisamos apenas calcular θ como  (X' * X)^-1 * X' * y. Quero apenas ter certeza que esta equação faz sentido para você e que você saiba como implementá-la. Assim, na prática, o que significa (X' * X) ^-1? (X' * X) ^-1 é o inverso da matriz X' * X. Na prática, se você fosse executar assinala-se A a X' * X, onde X' é uma matriz, e X' * X seria outra, e chamaríamos isso de matriz A. Dessa forma, para obter (X' * X) ^-1 pegaríamos a inversa dessa matriz A. resultando em A ^-1. E é assim que você calcula isto. Você calcula X' * X e sua inversa. Ainda não falamos sobre Octave. Falaremos sobre isso em um próximo conjunto de vídeos, mas na linguagem de programação Octave ou, similarmente, em Matlab, o comando para computar esse valor, (X' * X) ^-1 * X' *  y seria esse. Em Octave, X ' (apóstrofe) é a notação usada para indicar X transposto. Assim, esta expressão que está na caixa em vermelho retorna X' * X. pinv é uma função para calcular o inverso da matriz, calcula (X' * X) ^ -1, depois multiplica por X' e depois por y. Dessa forma, você calcula aquela fórmula que eu não provei, mas que é possível ser demonstrada matematicamente, embora eu não vá fazer isso aqui, que essa fórmula lhe dá o valor ótimo de θ no sentido que se você fizer θ igual a isso, isso seria o valor de θ que minimiza a função de custo J(θ) para a regressão. Um último detalhe no vídeo anterior. Eu falei sobre a assimetria de variáveis e a ideia de como ajeitar variáveis para que possuam intervalos similares de de valores. Se você está usando esse método da equação normal então dimensionamento de características não é realmente necessário e está tudo bem se alguma característica x1 está entre zero e um, e outra característica x2 está entre 0 e 1000, e uma outra x3 vai de 0 a 10^-5, se você está usando o método da equação normal isso não daria erro, e não há necessidade de fazer redimensionamento de variáveis, embora é claro que se você está usando gradiente descendente, o redimensionamento de características ainda é importante. Finalmente, quando você deve usar o gradiente descendente e quando você deve usar o método da equação normal. Aqui estão algumas das vantagens e desvantagens deles. Vamos dizer que você tem m exemplos de treinamento e n características. Uma desvantagem do gradiente descendente é que você precisa escolher a taxa de aprendizagem alfa. E, frequentemente, isso significa rodá-lo algumas vezes com diferentes taxas de aprendizagem alfa para ver qual funciona melhor. Isso significa mais trabalho e incômodo. Outra desvantagem do gradiente descendente é que ele precisa de muito mais iterações. Então, dependendo dos detalhes, isso pode torná-lo mais lento, embora existam mais detalhes que veremos já já. Já para a equação normal, você não precisa
escolher nenhuma taxa de aprendizagem alfa. Isso o torna muito conveniente e simples de implementar. Você simplesmente o executa e ele funciona. E você não precisa iterar, você não precisa plotar J(θ) ou verificar a convergência ou passar por todos aqueles passos extras. Até agora, parece que a equação normal está ganhando. Aqui estão algumas desvantagens da equação normal, e algumas vantagens do gradiente descendente. O gradiente descendente funciona muito bem, mesmo quando você tem um número muito grande de variáveis. Mesmo se você tivesse milhões de variáveis o gradiente descendente seria razoavelmente eficiente. Ele fará algo razoável. Em contraste com a equação normal, para encontrar os parâmetros, nós calcular esse termo. Nós precisamos computar esse termo, (X' * X)^ -1. Essa matriz X' * X é uma matriz n por n,
se você tem n variáveis. Porque, se você olhar as dimensões de X' e a dimensão de X, você multiplica e descobre a dimensão do produto, a matriz X' * X é uma matriz n por n onde "n" é o número de varáveis, e para a maioria das implementações o custo de inverter a matriz aumenta aproximadamente com o cubo da dimensão da matriz. Então, calcular essa inversão custa aproximadamente ordem de n^3 em tempo. Pode ser um pouco mais rápido que n^3 mas é próximo o bastante
para nossos propósitos. Então se "n", o número de características,
é muito grande, então calcular esse valor pode ser lento e o método da equação normal pode acabar sendo muito mais lento. Então se "n" é grande eu iria nomalmente usar o Gradiente Descendente por que nós não queremos pagar esse tempo cúbico. Mas, se N é relativamente pequeno, então a equação normal pode te dar um jeito melhor de resolver os parâmetros. O que significa pequeno e grande? Bom, se "n" está na ordem das centenas, então inverter uma matriz de 100x100 não é problema para os padrões computacionais modernos. Se "N" é mil, eu ainda usaria
o método da equação normal. Inverter uma matriz 1000x1000 é na verdade muito rápido num computador moderno. Se "n" é 10.000, então eu poderia começar a pensar. Inverter uma matriz 10.000 por 10.000 começa a ficar meio lento, e eu poderia então começar a me inclinar para o lado do gradiente descendente, mas talvez nem tanto. Com "n" igual a dez mil, você até consegue inverter a matriz. Mas se ela ficar muito maior do que isso, então
provavelmente usaria o gradiente descendente. Então, se "n" é igual a 10^6, ou seja, um milhão de variáveis, inverter uma matriz 1.000.000 x 1.000.000 seria muito caro, e eu definitivamente usaria o gradiente descendente. Então exatamente quão grande o conjunto de variáveis deve ser para mudar para o gradiente descendente
é difícil de definir. Mas, para mim, é geralmente por volta de dez mil que eu começaria a pensar em mudar para o gradiente descendente ou talvez, para algum dos outros algoritmos sobre os quais falaremos depois nessa aula. Para resumir, desde que o número de características não seja grande demais, a equação normal é ótimo método alternativo
para encontrar θ. Na prática, desde que o número de características seja menor que 1000, eu usaria o método da equação normal em vez do gradiente descendente. Para antecipar algumas ideias que que falaremos depois nesse curso ao chegarmos nos algoritmos de aprendizado mais complexos, por exemplo, quando falarmos sobre algoritmos de classificação, como o algoritmo de regressão logística, nós veremos que, na verdade, O método da equação normal não funciona para esses algoritmos de aprendizado mais sofisticados, e nós teremos que recorrer ao gradiente descendente. Então, gradiente descendente é um algoritmo muito útil para se conhecer. A regressão linear terá um grande número de variáveis e para alguns dos outros algoritmos que nós veremos nesse curso pois, para eles, o método da equação normal simplesmente não se aplica e não funciona. Para esse modelo específico de regressão linear, a equação normal pode lhe dar uma alternativa que pode ser muito mais rápida que o gradiente descendente. Então, dependendo dos detalhes do seu algoritmo e do seu problema e de quantas variáveis você tem, ambos algoritmos são bem válidos de se conhecer.
Tradução: Yuri David Santos, Debora Santos | Revisão: Eduardo Bonet