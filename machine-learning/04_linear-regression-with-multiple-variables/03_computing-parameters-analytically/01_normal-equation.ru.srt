1
00:00:00,302 --> 00:00:01,883
В этом видео мы поговорим о

2
00:00:01,883 --> 00:00:03,948
нормальном уравнении, которое

3
00:00:03,948 --> 00:00:05,660
для некоторых

4
00:00:05,660 --> 00:00:06,981
случаев линейной регрессии дает

5
00:00:06,981 --> 00:00:09,115
гораздо лучший способ найти оптимальные значения

6
00:00:09,115 --> 00:00:10,879
параметров тета.

7
00:00:10,879 --> 00:00:13,096
Пока что для задач линейной регрессии

8
00:00:13,096 --> 00:00:14,399
мы пользовались методом градиентного

9
00:00:14,399 --> 00:00:16,042
спуска, в котором для

10
00:00:16,042 --> 00:00:17,823
поиска минимума функции

11
00:00:17,823 --> 00:00:19,410
затрат J от тэта

12
00:00:19,410 --> 00:00:21,354
запускался итеративный алгоритм.

13
00:00:21,354 --> 00:00:23,792
В результате множества шагов, множества

14
00:00:23,792 --> 00:00:26,410
итераций градиентного спуска,

15
00:00:26,410 --> 00:00:28,259
алгоритм сходился к глобальному

16
00:00:28,259 --> 00:00:30,396
минимуму.

17
00:00:30,396 --> 00:00:32,563
Нормальное уравнение же дает

18
00:00:32,563 --> 00:00:34,413
нам возможность найти вектор

19
00:00:34,413 --> 00:00:36,986
тета аналитически, без

20
00:00:36,986 --> 00:00:38,761
необходимости прогонять этот

21
00:00:38,761 --> 00:00:40,594
итеративный алгоритм, а просто

22
00:00:40,594 --> 00:00:41,365
вычислить оптимальное значение

23
00:00:41,365 --> 00:00:42,791
тета за раз, то

24
00:00:42,791 --> 00:00:44,403
есть фактически за один шаг

25
00:00:44,403 --> 00:00:46,096
вы получаете оптимальное

26
00:00:46,096 --> 00:00:48,136
значение.

27
00:00:49,136 --> 00:00:51,947
Оказывается, что у нормального

28
00:00:52,209 --> 00:00:54,442
уравнения есть свои достоинства и

29
00:00:54,442 --> 00:00:56,024
свои недостатки, но

30
00:00:56,024 --> 00:00:57,817
прежде чем обсуждать,

31
00:00:57,903 --> 00:00:59,426
когда стоит его

32
00:00:59,426 --> 00:01:02,539
использовать, давайте попробуем понять общую идею того, как работает этот метод.

33
00:01:02,539 --> 00:01:04,633
Давайте возьмем пример из лекций

34
00:01:04,633 --> 00:01:06,120
этой недели и

35
00:01:06,120 --> 00:01:07,505
представим себе крайне упрощенную

36
00:01:07,505 --> 00:01:09,291
функцию стоимости J(тета) -

37
00:01:09,291 --> 00:01:11,958
просто функцию от действительного числа "тета".

38
00:01:11,958 --> 00:01:13,642
То есть, сейчас представьте себе,

39
00:01:13,842 --> 00:01:16,615
что тета - это просто скаляр, принимающий вещественные значения.

40
00:01:16,769 --> 00:01:18,918
Обычное число, а не вектор.

41
00:01:19,171 --> 00:01:24,595
Представьте, что у нас есть функция
стоимость J, которая является квадратичной от

42
00:01:25,028 --> 00:01:27,420
этого действительного числа, параметра тета - то есть, J(тета) выглядит
вот так.

43
00:01:27,851 --> 00:01:30,336
Итак, как найти минимум квадратичной
функции?

44
00:01:30,720 --> 00:01:32,745
Те из вас, кто знает немного
матанализа, наверное,

45
00:01:32,858 --> 00:01:34,965
помнят, что для нахождения

46
00:01:34,965 --> 00:01:36,628
минимума функции нужно взять ее

47
00:01:36,628 --> 00:01:38,991
производные и приравнять

48
00:01:38,991 --> 00:01:41,707
производные к нулю.

49
00:01:41,707 --> 00:01:44,721
Так что вы берете производную от J по
параметру тета.

50
00:01:44,797 --> 00:01:46,847
Получается некоторая
формула, которую я не буду

51
00:01:46,847 --> 00:01:49,161
сейчас выводить. Вы приравниваете эту

52
00:01:49,161 --> 00:01:50,782
производную к нулю, и это

53
00:01:50,782 --> 00:01:53,503
позволяет вам получить

54
00:01:53,503 --> 00:01:57,866
значение параметра
тета, которое минимизирует J(тета).

55
00:01:57,866 --> 00:01:59,096
Это был простой пример, когда мы рассматривали просто

56
00:01:59,096 --> 00:02:01,716
действительное число.

57
00:02:01,716 --> 00:02:04,272
Но в задаче,
которая нас

58
00:02:04,929 --> 00:02:06,559
интересует, тета уже не

59
00:02:06,559 --> 00:02:07,847
просто вещественное число, а

60
00:02:07,847 --> 00:02:11,986
вектор размерности n+1, и функция

61
00:02:11,986 --> 00:02:13,809
затрат J - это функция

62
00:02:13,809 --> 00:02:15,742
от этого вектора, от

63
00:02:15,742 --> 00:02:17,501
чисел

64
00:02:17,501 --> 00:02:18,924
тета_0 до тета_m.И функция затрат

65
00:02:18,924 --> 00:02:21,957
выглядит как-то так, некоторая квадратичная
функция - как в правой части.

66
00:02:22,373 --> 00:02:25,712
Как нам найти минимум функции стоимости J?

67
00:02:25,712 --> 00:02:27,163
Матанализ говорит нам, что один

68
00:02:27,163 --> 00:02:29,377
способ сделать это - взять

69
00:02:29,377 --> 00:02:30,709
частную производную от J

70
00:02:30,709 --> 00:02:38,604
относительно каждого из параметров тета

71
00:02:38,604 --> 00:02:40,271
по очереди, и затем приравнять их все к нулю.

72
00:02:40,271 --> 00:02:41,394
Есть сделать

73
00:02:41,394 --> 00:02:42,718
это и

74
00:02:42,718 --> 00:02:44,000
найти значения тета_0, тета_1 и так

75
00:02:44,000 --> 00:02:45,973
далее до тета_n, получится

76
00:02:45,973 --> 00:02:47,217
вектор параметров, минимизирующий

77
00:02:47,217 --> 00:02:48,765
функцию затрат J. Если серьезно браться

78
00:02:48,765 --> 00:02:50,878
за анализ и

79
00:02:50,878 --> 00:02:52,176
вычислять все

80
00:02:52,176 --> 00:02:53,597
значения параметров тета_0 ... тета_n,

81
00:02:53,597 --> 00:02:55,194
оказывается, что нужно будет

82
00:02:55,194 --> 00:02:57,316
считать довольно много

83
00:02:57,316 --> 00:03:00,520
производных.

84
00:03:00,520 --> 00:03:01,625
Но в этом видео я на самом деле

85
00:03:01,625 --> 00:03:03,113
не планирую

86
00:03:03,113 --> 00:03:04,852
заниматься вычислением производных - это

87
00:03:04,852 --> 00:03:06,297
долго и довольно утомительно, я

88
00:03:06,297 --> 00:03:07,657
просто хочу рассказать

89
00:03:07,657 --> 00:03:08,962
вам, что нужно сделать, чтобы

90
00:03:08,962 --> 00:03:10,545
реализовать эти вычисления,

91
00:03:10,545 --> 00:03:12,619
чтобы вы могли найти значения

92
00:03:12,619 --> 00:03:14,138
вектора тета, соответствующие

93
00:03:14,138 --> 00:03:15,511
нулевым

94
00:03:15,511 --> 00:03:16,892
значениям частных

95
00:03:16,892 --> 00:03:19,273
производных.

96
00:03:19,273 --> 00:03:21,733
Или, что одно и то же, значения

97
00:03:21,733 --> 00:03:23,357
параметров тета,

98
00:03:23,357 --> 00:03:25,901
которые минимизируют функцию J(тета).

99
00:03:25,901 --> 00:03:27,283
Я понимаю, что некоторые

100
00:03:27,283 --> 00:03:28,846
мои комментарии понятны

101
00:03:28,846 --> 00:03:29,914
только тем из вас, кто знаком с

102
00:03:29,914 --> 00:03:31,896
матанализом.

103
00:03:31,896 --> 00:03:33,065
Но не беспокойтесь, если вы

104
00:03:33,065 --> 00:03:34,487
незнакомы или лишь немного

105
00:03:34,487 --> 00:03:36,354
знакомы с матанализом.

106
00:03:36,354 --> 00:03:37,404
Я просто расскажу вам все, что вам

107
00:03:37,404 --> 00:03:38,374
нужно знать, чтобы реализовать эти алгоритмы и заставить их

108
00:03:38,374 --> 00:03:41,358
работать.

109
00:03:41,358 --> 00:03:42,585
Для своего примера я возьму

110
00:03:42,585 --> 00:03:43,737
ситуацию, когда у меня,

111
00:03:43,737 --> 00:03:46,339
скажем, обучающая выборка

112
00:03:46,339 --> 00:03:49,056
состоит из m = 4 примеров.

113
00:03:50,409 --> 00:03:52,881
Чтобы составить нормальное

114
00:03:52,881 --> 00:03:56,515
уравнение, я сделаю
следующее.

115
00:03:56,515 --> 00:03:57,640
Я возьму свой набор данных, вот они,

116
00:03:57,640 --> 00:04:00,375
мои четыре обучающих примера.

117
00:04:00,375 --> 00:04:01,844
В данном случае давайте представим себе,

118
00:04:01,844 --> 00:04:06,073
что эти четыре примера - это все мои данные.

119
00:04:06,073 --> 00:04:07,890
Дальше я беру этот набор

120
00:04:07,890 --> 00:04:09,007
данных и добавляю еще один

121
00:04:09,007 --> 00:04:11,289
столбец, который

122
00:04:11,289 --> 00:04:14,579
соответствует моему дополнительному

123
00:04:14,579 --> 00:04:15,967
признаку, x0,

124
00:04:15,967 --> 00:04:17,527
который всегда принимает значение 1.

125
00:04:17,527 --> 00:04:18,681
А дальше

126
00:04:18,681 --> 00:04:19,943
я строю матрицу X,

127
00:04:19,943 --> 00:04:22,638
которая фактически

128
00:04:22,638 --> 00:04:24,632
содержит все характеристики

129
00:04:24,632 --> 00:04:26,100
моей обучающей выборки.

130
00:04:26,100 --> 00:04:28,140
Конкретно, вот они

131
00:04:28,140 --> 00:04:31,528
все характеристики выборки, я

132
00:04:31,528 --> 00:04:33,743
возьму все эти числа и подставлю их в

133
00:04:33,743 --> 00:04:34,797
матрицу X,

134
00:04:34,797 --> 00:04:37,777
хорошо?

135
00:04:37,777 --> 00:04:39,179
Так сказать, копирую данные

136
00:04:39,179 --> 00:04:41,233
столбец за столбцом, а

137
00:04:41,233 --> 00:04:45,962
затем сделаю нечто похожее для
характеристики "y".

138
00:04:45,962 --> 00:04:47,087
Я беру эти значения, которые

139
00:04:47,087 --> 00:04:47,952
мне нужно предсказать, и

140
00:04:47,952 --> 00:04:49,360
строю теперь вектор,

141
00:04:49,360 --> 00:04:52,894
вот так, и называю

142
00:04:52,894 --> 00:04:55,440
его вектор "y".

143
00:04:55,440 --> 00:04:58,038
То есть, X будет матрицей

144
00:04:59,653 --> 00:05:05,688
размерности m*(n+1), а "y"

145
00:05:05,688 --> 00:05:07,490
будет вектором размерности m,

146
00:05:07,490 --> 00:05:14,421
где m - число обучающих

147
00:05:14,421 --> 00:05:16,624
примеров, а n - число характеристик
выборки. n+1 получается из-за той

148
00:05:16,984 --> 00:05:18,688
дополнительной характеристики x_0,

149
00:05:18,688 --> 00:05:20,713
которую я

150
00:05:20,713 --> 00:05:24,825
добавил.

151
00:05:24,825 --> 00:05:26,350
Наконец, если взять матрицу X и

152
00:05:26,350 --> 00:05:27,489
вектор "у", и вычислить

153
00:05:27,489 --> 00:05:28,595
вот такое выражение,

154
00:05:28,595 --> 00:05:31,065
сделать тета равным (X транспонированное

155
00:05:31,065 --> 00:05:32,419
умножить на X) в минус первой

156
00:05:32,419 --> 00:05:34,440
эту - обратное от X транспонированного на X, умноженное на X транспонированное,

157
00:05:34,440 --> 00:05:36,516
умножить на "y", то получится значение т

158
00:05:36,516 --> 00:05:38,583
ета, минимизирующее

159
00:05:38,583 --> 00:05:42,559
значение функции затрат.

160
00:05:42,559 --> 00:05:43,436
На этом слайде написано много всего, и

161
00:05:43,436 --> 00:05:44,416
всё на единственном

162
00:05:44,416 --> 00:05:47,514
примере набора
данных.

163
00:05:47,514 --> 00:05:49,241
Давайте я теперь напишу это всё в

164
00:05:49,333 --> 00:05:50,770
более общей

165
00:05:50,955 --> 00:05:53,418
форме, а потом
в этом же видео я

166
00:05:53,621 --> 00:05:56,531
поясню еще
немного про это уравнение.

167
00:05:57,581 --> 00:06:00,687
Если пока еще не до конца понятно, как это делать.

168
00:06:00,687 --> 00:06:02,129
В общем случае,

169
00:06:02,129 --> 00:06:04,124
пусть у нас будет m

170
00:06:04,124 --> 00:06:05,697
обучающих примеров: от (х^1, y^1) до (x^m, y^m), и n

171
00:06:05,697 --> 00:06:09,319
характеристик.

172
00:06:09,319 --> 00:06:10,811
То есть каждый из обучающих примеров

173
00:06:10,811 --> 00:06:12,926
может представлять собой вот такой вектор, то есть вектор

174
00:06:12,926 --> 00:06:16,297
характеристик выборки с размерностью n+1.

175
00:06:16,943 --> 00:06:18,350
Построим матрицу X, также называемую

176
00:06:18,350 --> 00:06:20,674
матрицей плана,

177
00:06:20,674 --> 00:06:24,827
следующим

178
00:06:24,827 --> 00:06:26,712
образом.

179
00:06:26,712 --> 00:06:28,640
Каждый обучающий пример дает мне

180
00:06:28,640 --> 00:06:30,549
вектор характеристик, такой что,

181
00:06:30,549 --> 00:06:34,491
вектор размерности n+1.

182
00:06:34,491 --> 00:06:36,190
Единственный способ, как можно составить

183
00:06:36,359 --> 00:06:39,734
матрицу факторов "X", это составить ее вот так.

184
00:06:39,734 --> 00:06:40,834
Возьмем первый обучающий пример,

185
00:06:40,834 --> 00:06:42,109
вектор x^1, транспонируем

186
00:06:42,109 --> 00:06:43,711
его, получится вот такая

187
00:06:43,711 --> 00:06:46,350
длинная плоская штуковина, и

188
00:06:46,350 --> 00:06:48,692
скажем, что x^1 – это первый

189
00:06:48,692 --> 00:06:50,250
ряд матрицы

190
00:06:50,250 --> 00:06:55,153
факторов.

191
00:06:55,153 --> 00:06:56,225
Возьмем второй, x^2, транспонируем,

192
00:06:56,225 --> 00:06:58,682
поставим во второй ряд

193
00:06:58,682 --> 00:07:00,437
матрицы X. Продолжим этот

194
00:07:00,437 --> 00:07:01,838
процесс до последнего

195
00:07:01,838 --> 00:07:04,068
обучающего

196
00:07:04,068 --> 00:07:07,206
примера.

197
00:07:07,206 --> 00:07:09,279
Транспонируем, и это

198
00:07:09,279 --> 00:07:10,850
последняя

199
00:07:10,850 --> 00:07:12,665
строка в матрице "X". Таким

200
00:07:12,665 --> 00:07:14,418
образом получается матрица

201
00:07:14,418 --> 00:07:17,129
размерности m

202
00:07:17,129 --> 00:07:19,836
на n+1.

203
00:07:19,836 --> 00:07:21,953
В качестве конкретного примера рассмотрим

204
00:07:21,953 --> 00:07:23,505
простейший случай, когда у нас

205
00:07:23,505 --> 00:07:24,670
есть только одна характеристика

206
00:07:24,670 --> 00:07:26,631
выборки, отличная от

207
00:07:26,631 --> 00:07:28,165
вектора x_0, всегда состоящего из единиц.

208
00:07:28,165 --> 00:07:30,376
Тогда если мой вектор

209
00:07:30,376 --> 00:07:32,186
характеристик x(i) состоит из x0 = 1 и

210
00:07:32,186 --> 00:07:33,878
некой действительной

211
00:07:33,878 --> 00:07:35,912
характеристики, как например

212
00:07:35,912 --> 00:07:37,662
размер дома, тогда матрица

213
00:07:37,662 --> 00:07:40,947
плана, X, будет равна вот этому:

214
00:07:40,947 --> 00:07:42,589
В первый ряд я фактически подставляю вот

215
00:07:42,589 --> 00:07:46,071
этот транспонированный x(i).

216
00:07:46,071 --> 00:07:51,644
В строке будет стоять 1 и затем x^(1)_1.

217
00:07:51,644 --> 00:07:53,309
Во второй строке 1 и

218
00:07:53,309 --> 00:07:56,077
затем x^1_2 и так

219
00:07:56,077 --> 00:07:58,046
далее до

220
00:07:58,046 --> 00:07:59,046
строки 1 и

221
00:07:59,046 --> 00:08:01,420
затем x^1_m. Таким образом

222
00:08:01,420 --> 00:08:03,084
получится матрица

223
00:08:03,084 --> 00:08:07,776
размерности m на 2.

224
00:08:07,776 --> 00:08:08,821
Итак, мы разобрались, как

225
00:08:08,821 --> 00:08:11,251
построить матрицу

226
00:08:11,251 --> 00:08:13,886
плана X. Что касается вектора y,

227
00:08:13,886 --> 00:08:15,487
иногда я могу писать сверху

228
00:08:15,487 --> 00:08:16,541
стрелку, чтобы

229
00:08:16,541 --> 00:08:19,871
обозначить, что это вектор, хотя часто я буду писать просто y.

230
00:08:19,871 --> 00:08:21,182
Вектор "y" получается из всех - всех данных по

231
00:08:21,182 --> 00:08:23,275
ценам домов в

232
00:08:23,275 --> 00:08:25,098
моей обучающей выборке,

233
00:08:25,098 --> 00:08:27,076
если выстроить их одну над

234
00:08:27,076 --> 00:08:28,963
другой в виде

235
00:08:28,963 --> 00:08:32,011
вектора размерности m. В

236
00:08:32,011 --> 00:08:34,511
конечном итоге,

237
00:08:34,511 --> 00:08:36,724
имея матрицу "X" и

238
00:08:36,724 --> 00:08:38,184
вектор "y", можно

239
00:08:38,184 --> 00:08:40,887
вычислить тета

240
00:08:40,887 --> 00:08:47,243
как 1/(X'X) * X' * y.

241
00:08:47,243 --> 00:08:49,356
Хочу убедиться, что вы

242
00:08:49,356 --> 00:08:51,348
понимаете смысл этого
уравнения и как

243
00:08:51,348 --> 00:08:52,242
им пользоваться.

244
00:08:52,242 --> 00:08:55,221
А именно, что означает 1/(X'X)?

245
00:08:55,221 --> 00:08:57,903
Так вот, 1/(X'X) – это матрица, обратная произведению

246
00:08:57,903 --> 00:09:02,101
транспонированной X и просто X.

247
00:09:02,101 --> 00:09:04,498
Точнее, предположим

248
00:09:04,498 --> 00:09:08,055
вы назначили

249
00:09:08,055 --> 00:09:11,120
некоторую A = X' * X. Транспонированная

250
00:09:11,120 --> 00:09:12,542
X - это

251
00:09:12,542 --> 00:09:14,063
матрица,  X' * X дает тоже

252
00:09:14,063 --> 00:09:15,305
матрицу, которую мы назвали A. Теперь, (X' * X)^-1 означает, что

253
00:09:15,305 --> 00:09:17,560
мы взяли

254
00:09:17,560 --> 00:09:19,968
вот эту A и обратили ее,

255
00:09:19,968 --> 00:09:22,352
так?

256
00:09:23,245 --> 00:09:24,417
Получается, скажем, матрица, обратная A.

257
00:09:26,025 --> 00:09:28,919
Вы вычисляете данное произведение,

258
00:09:28,919 --> 00:09:31,451
X транспонированную на X, а затем вычисляете для получившегося обратную матрицу.

259
00:09:31,451 --> 00:09:34,296
Еще нужно будет обсудить программирование в Octave.

260
00:09:34,296 --> 00:09:35,941
Мы сделаем это в последующих видео. В

261
00:09:35,941 --> 00:09:37,211
языке Octave или в других достаточно

262
00:09:37,211 --> 00:09:39,073
похожих языках,

263
00:09:39,073 --> 00:09:40,652
например в

264
00:09:40,652 --> 00:09:42,957
MATLAB,

265
00:09:42,957 --> 00:09:46,937
команда вычислить эту величину, вернее, вот

266
00:09:47,384 --> 00:09:50,326
эту - обратное от X транспонированного на X, умноженное на X транспонированное,

267
00:09:50,326 --> 00:09:52,537
на y - выглядит следующим образом:

268
00:09:52,537 --> 00:09:54,903
В Octave, X' используется, чтобы

269
00:09:54,903 --> 00:09:58,354
обозначить X транспонированное.

270
00:09:58,354 --> 00:10:00,737
Таким образом,

271
00:10:00,737 --> 00:10:03,588
выражение в красной

272
00:10:03,588 --> 00:10:06,633
рамке вычисляет

273
00:10:06,633 --> 00:10:08,551
произведение

274
00:10:08,551 --> 00:10:09,701
транспонированной

275
00:10:09,701 --> 00:10:11,818
матрицы X и

276
00:10:11,818 --> 00:10:14,656
матрицы X. pinv - это

277
00:10:14,656 --> 00:10:16,453
функция для нахождения обратной

278
00:10:16,453 --> 00:10:18,267
матрицы, поэтому вот это вычисляет транспонированную X на

279
00:10:18,267 --> 00:10:19,712
обратную X, умноженную на

280
00:10:19,712 --> 00:10:22,325
транспонированную X и

281
00:10:22,325 --> 00:10:24,369
умноженную на Y. Так мы

282
00:10:24,369 --> 00:10:25,994
вычислили

283
00:10:25,994 --> 00:10:27,382
эту формулу, доказательство

284
00:10:27,382 --> 00:10:28,537
которой я опустил. Однако,

285
00:10:28,537 --> 00:10:31,071
математически возможно

286
00:10:31,071 --> 00:10:32,316
показать (хотя я не буду сейчас этого делать),

287
00:10:32,316 --> 00:10:34,865
что эта формула дает

288
00:10:34,865 --> 00:10:36,512
вам оптимальное значения θ. То есть то

289
00:10:36,512 --> 00:10:38,000
самое значение, которое минимизирует

290
00:10:38,000 --> 00:10:40,169
затратную функцию J (θ) для линейной

291
00:10:40,169 --> 00:10:41,993
регрессии.

292
00:10:41,993 --> 00:10:44,530
И еще одна важная деталь. В предыдущем видео

293
00:10:44,530 --> 00:10:46,131
я говорил о масштабировании факторов,

294
00:10:46,131 --> 00:10:47,061
которое позволяет добиться того, чтобы

295
00:10:47,061 --> 00:10:48,878
их значения располагались на одной

296
00:10:48,878 --> 00:10:50,726
шкале и имели бы одинаковые

297
00:10:50,726 --> 00:10:54,900
пределы.

298
00:10:54,900 --> 00:10:56,872
Если вы используете

299
00:10:56,872 --> 00:10:59,843
нормированное

300
00:10:59,843 --> 00:11:02,315
уравнение, то необходимость

301
00:11:02,315 --> 00:11:04,361
в масштабировании факторов отпадает.

302
00:11:04,361 --> 00:11:06,094
Фактически приемлемым

303
00:11:06,094 --> 00:11:07,552
является,

304
00:11:07,552 --> 00:11:08,846
например, случай, при котором некоторый

305
00:11:08,846 --> 00:11:10,550
фактор x1

306
00:11:10,550 --> 00:11:12,019
принимает значения от 0 до 1,

307
00:11:12,019 --> 00:11:14,159
некоторый x2 - от 0 до 1000, а

308
00:11:14,159 --> 00:11:15,822
некоторый x3 вообще от 0 до 10^5. И,

309
00:11:15,822 --> 00:11:17,263
если вы используете нормированное уравнение,

310
00:11:17,263 --> 00:11:18,321
все в порядке - нет надобности

311
00:11:18,321 --> 00:11:20,296
проводить масштабирование. Если

312
00:11:20,296 --> 00:11:21,550
же вы применяете градиентный спуск,

313
00:11:21,550 --> 00:11:22,740
помнить о

314
00:11:22,740 --> 00:11:25,667
масштабировании очень

315
00:11:25,667 --> 00:11:27,814
важно.

316
00:11:28,030 --> 00:11:31,020
Теперь - когда следует использовать градиентный спуска, а

317
00:11:31,020 --> 00:11:33,273
когда - метод нормированного уравнения?

318
00:11:33,273 --> 00:11:35,800
Вот некоторые из недостатков и преимуществ обоих методов.

319
00:11:35,800 --> 00:11:38,305
Предположим, у нас имеется m обучающих

320
00:11:38,305 --> 00:11:40,918
примеров и n факторов.

321
00:11:40,918 --> 00:11:42,854
Один из недостатков градиентного спуска состоит в

322
00:11:42,854 --> 00:11:46,015
том, что нам нужно как-то выбрать параметр скорости обучения α.

323
00:11:46,015 --> 00:11:47,374
Очень часто это приводит к тому, что приходится

324
00:11:47,374 --> 00:11:49,128
прогонять спуск несколько раз с разными значениями α и

325
00:11:49,128 --> 00:11:51,154
только потом выбирать, какое значение работает лучше.

326
00:11:51,154 --> 00:11:54,274
А это - в некотором смысле дополнительная забота.

327
00:11:54,274 --> 00:11:55,976
Другой недостаток градиентного спуска - большое число итераций

328
00:11:55,976 --> 00:11:57,841
при его прогоне.

329
00:11:57,841 --> 00:11:59,346
И, при прочих равных, это может серьезно

330
00:11:59,346 --> 00:12:00,839
замедлить его работу. Хотя, как мы увидим через пару

331
00:12:00,839 --> 00:12:04,391
секунд, здесь есть некоторые нюансы.

332
00:12:04,391 --> 00:12:07,544
При работе с нормированным уравнением не
возникает надобности выбирать параметр скорости обучения α.

333
00:12:07,821 --> 00:12:11,208
Это, как нетрудно догадаться, делает его чрезвычайно
легким в работе и реализации.

334
00:12:11,208 --> 00:12:13,888
Запустили и готово - оно просто работает.

335
00:12:13,888 --> 00:12:15,061
И не нужно никаких итераций.

336
00:12:15,061 --> 00:12:16,129
Не нужно строить график J(θ),

337
00:12:16,129 --> 00:12:17,456
проверять сходимость и вообще

338
00:12:17,456 --> 00:12:20,497
предпринимать какие-то дополнительные шаги.

339
00:12:20,497 --> 00:12:21,931
Казалось бы, все говорит в пользу

340
00:12:21,931 --> 00:12:23,846
нормированного уравнения.

341
00:12:24,826 --> 00:12:27,085
Однако, рассмотрим некоторые недостатки

342
00:12:27,612 --> 00:12:29,435
нормированного уравнения и некоторые преимущества градиентного спуска.

343
00:12:29,681 --> 00:12:31,447
Градиентный спуск неплохо справляется даже в случае

344
00:12:31,928 --> 00:12:34,698
огромного числа факторов.

345
00:12:34,698 --> 00:12:36,168
Так что, даже если у вас миллионы факторов,

346
00:12:36,168 --> 00:12:37,812
можно использовать градиентный спуск, который в этом случае

347
00:12:37,812 --> 00:12:40,865
будет достаточно эффективен.

348
00:12:40,865 --> 00:12:43,381
Результат будет более менее вменяемый.

349
00:12:43,381 --> 00:12:46,566
В случае же нормированного уравнения, для

350
00:12:46,566 --> 00:12:48,014
того, чтобы получить параметры θ, необходимо

351
00:12:48,014 --> 00:12:50,394
вычислить вот это выражение.

352
00:12:50,394 --> 00:12:53,058
Нам нужно вычислить обратную матрицу произведения транспонированной X на X.

353
00:12:53,058 --> 00:12:56,328
Вот эта матрица (транспонированная X на X)

354
00:12:56,328 --> 00:13:00,206
имеет размер n на n, если n - число
факторов.

355
00:13:00,770 --> 00:13:02,947
Почему так? Если вы

356
00:13:02,947 --> 00:13:03,917
посмотрите

357
00:13:03,917 --> 00:13:05,529
на размер

358
00:13:05,529 --> 00:13:07,024
транспонированной X, а затем на

359
00:13:07,024 --> 00:13:08,749
размер X, то увидите, что размер их

360
00:13:08,749 --> 00:13:10,983
произведения - n на n, где n - число факторов.А

361
00:13:10,983 --> 00:13:13,727
для большинства компьютерных

362
00:13:13,727 --> 00:13:15,853
реализаций алгоритма обращения

363
00:13:15,853 --> 00:13:18,641
матриц, затраты на обращение

364
00:13:18,641 --> 00:13:20,990
резко растут как куб размера

365
00:13:20,990 --> 00:13:23,087
обращаемой

366
00:13:23,087 --> 00:13:25,707
матрицы.

367
00:13:25,707 --> 00:13:28,180
Иными словами, порядок роста времени

368
00:13:28,180 --> 00:13:29,964
выполнения алгоритма обращения матрицы составляет n^3.

369
00:13:29,964 --> 00:13:31,213
Иногда он выполняется чуть

370
00:13:31,213 --> 00:13:35,050
быстрее, но мы можем
считать, что время все равно очень близко к n^3.

371
00:13:35,489 --> 00:13:36,605
Так что, если число факторов очень
велико, то вычисление

372
00:13:37,643 --> 00:13:39,025
вот этого значения может быть

373
00:13:39,025 --> 00:13:40,570
очень медленным, что в свою

374
00:13:40,570 --> 00:13:44,289
очередь сильно замедлит  метод нормального уравнения.

375
00:13:44,289 --> 00:13:45,491
Поэтому, если n слишком

376
00:13:45,491 --> 00:13:47,622
велико, я предпочитаю

377
00:13:47,622 --> 00:13:49,490
использовать градиентный спуск, поскольку не хочу

378
00:13:49,490 --> 00:13:51,872
расплачиваться n-кубическим ростом времени выполнения.

379
00:13:51,872 --> 00:13:53,525
Однако, если n относительно мало,

380
00:13:53,525 --> 00:13:57,395
нормальное уравнение дает более удобный способ найти параметры гипотезы.

381
00:13:57,395 --> 00:13:59,080
А что значит "n мало" и "n велико"?

382
00:13:59,080 --> 00:14:00,741
Ну, скажем, если n составляет

383
00:14:00,741 --> 00:14:02,130
порядка 100, то обращения

384
00:14:02,130 --> 00:14:03,822
матрицы размером 100 на 100 не

385
00:14:03,822 --> 00:14:06,539
составляет труда по меркам сегодняшних вычислительных ресурсов.

386
00:14:06,539 --> 00:14:10,966
Если n составляет 1000, я бы все еще воспользовался
методом нормированного уравнения.

387
00:14:10,966 --> 00:14:12,583
Обращение матрицы размером 1000 на 1000, в общем,

388
00:14:12,583 --> 00:14:15,408
происходит достаточно быстро на современных компьютерах.

389
00:14:15,408 --> 00:14:18,406
Если n составляет 10 000, тут я уже задумаюсь.

390
00:14:18,406 --> 00:14:20,618
Обращение матрицы размером 10 000 на 10 000 будет уже подтормаживать. Я начну

391
00:14:20,618 --> 00:14:22,208
посматривать в

392
00:14:22,208 --> 00:14:23,471
сторону градиентного

393
00:14:23,471 --> 00:14:25,007
спуска, но не так чтобы очень

394
00:14:25,007 --> 00:14:27,007
активно.

395
00:14:27,114 --> 00:14:28,672
Обратить матрицу такого размера в

396
00:14:28,672 --> 00:14:31,148
принципе возможно.

397
00:14:31,148 --> 00:14:34,345
Но, если это займет гораздо больше времени, я скорее всего
воспользуюсь градиентным спуском.

398
00:14:34,345 --> 00:14:35,834
Так что, если n составляет 10 в 6-й степени,

399
00:14:35,834 --> 00:14:36,920
то есть миллион факторов, то обращение

400
00:14:36,920 --> 00:14:38,963
матрицы размером миллион на миллион будет

401
00:14:38,963 --> 00:14:41,565
очень затратным. Если у вас

402
00:14:41,565 --> 00:14:42,631
такое количество факторов, я бы

403
00:14:42,631 --> 00:14:46,163
определенно порекомендовал градиентный спуск.

404
00:14:46,163 --> 00:14:47,859
Так насколько велико должно быть число

405
00:14:47,859 --> 00:14:49,282
факторов для того, чтобы принять решение о переходе на

406
00:14:49,282 --> 00:14:52,655
градиентный спуск? Конкретное
число определить сложно.

407
00:14:52,655 --> 00:14:53,855
Лично я начинаю задумываться о применении

408
00:14:53,855 --> 00:14:55,501
градиентного спуска или других

409
00:14:55,501 --> 00:14:58,258
алгоритмов, о которых мы

410
00:14:58,335 --> 00:15:00,663
поговорим позже в курсе, когда

411
00:15:00,663 --> 00:15:04,324
число факторов начинает приближаться к 10 000.

412
00:15:04,324 --> 00:15:05,765
Итак, если число факторов не слишком велико,

413
00:15:05,765 --> 00:15:06,999
нормированное уравнение дает

414
00:15:06,999 --> 00:15:08,475
отличный альтернативный метод для

415
00:15:08,475 --> 00:15:12,229
нахождения параметров
тета.

416
00:15:12,583 --> 00:15:13,983
Точнее, если число факторов меньше

417
00:15:13,983 --> 00:15:15,749
1000, я обычно

418
00:15:15,749 --> 00:15:17,472
использую нормированное уравнение, а

419
00:15:17,472 --> 00:15:18,881
не градиентный

420
00:15:18,881 --> 00:15:21,955
спуск.

421
00:15:21,955 --> 00:15:23,549
Немного забегая вперёд, скажу, что

422
00:15:23,549 --> 00:15:24,493
для более сложных

423
00:15:24,493 --> 00:15:26,235
алгоритмов обучения... алгоритмов

424
00:15:26,235 --> 00:15:27,912
классификации, о которых мы

425
00:15:27,912 --> 00:15:29,617
будем говорить далее в нашем

426
00:15:29,617 --> 00:15:32,188
курсе - таких, например, как

427
00:15:32,834 --> 00:15:34,319
логистическая регрессия, - так вот

428
00:15:34,319 --> 00:15:35,467
для этих алгоритмов

429
00:15:35,467 --> 00:15:37,592
метод нормального
уравнения не

430
00:15:37,592 --> 00:15:39,388
работает. Так что мы

431
00:15:39,388 --> 00:15:41,190
будем вынуждены вновь

432
00:15:41,190 --> 00:15:43,916
вернуться к градиентному
спуску.

433
00:15:43,916 --> 00:15:46,682
Поэтому, знать, как работает градиентный спуск, очень полезно.

434
00:15:46,682 --> 00:15:48,859
Именно потому, что, как в случае с

435
00:15:48,982 --> 00:15:50,017
линейной регрессией с большим числом факторов, так и в

436
00:15:50,017 --> 00:15:52,373
случае некоторых других алгоритмов,

437
00:15:52,373 --> 00:15:53,893
которые мы рассмотрим позже,

438
00:15:53,893 --> 00:15:55,438
метод нормированного уравнения

439
00:15:55,438 --> 00:15:58,747
просто не применим, он просто не работает.

440
00:15:58,747 --> 00:16:00,537
Однако, при определенной модели линейной

441
00:16:00,537 --> 00:16:02,904
регресии нормированное уравнение может

442
00:16:02,904 --> 00:16:05,827
представлять более быструю

443
00:16:07,219 --> 00:16:08,612
альтернативу градиентному спуску.

444
00:16:09,604 --> 00:16:11,920
Так что, при должном понимании особенностей

445
00:16:12,007 --> 00:16:14,164
работы алгоритмов, особенностей задачи, которую вы решаете,

446
00:16:14,164 --> 00:16:15,550
понимании влияния числа факторов, оба алгоритма

447
00:16:15,550 --> 00:16:19,550
заслуживают знания и
понимания.