このビデオでは、 正規方程式について議論する。 それは幾つかの線形回帰の問題では パラメータのシータを求める より良い解法を 提供する。 具体的には、ここまでの所 線形回帰に用いてきた アルゴリズムは、最急降下法だった。 それはコスト関数 Jのシータを最小化する為に この繰り返しのアルゴリズムを 用いるが、これはたくさんのステップ、 複数回の繰り返しの 最急降下を、グローバル最小に 収束するまでに、要する物だった。 対照的に正規方程式は シータを解析的に解く方法を 提供する為、 この繰り返しアルゴリズムを 走らせる代わりに シータの最適の値を 一度に解く事が 出来る、つまり、 基本的には1ステップで ここの最適値が得られる。 結局のところ、正規方程式は いくらかの利点と いくらかの欠点がある事が分かる。 だがそれについて議論したり それをいつ使うべきかを議論する前に この手法が何をやるのかの直感を得る事にしよう。 この週の探索的な例として、 とても単純化した コスト関数Jのシータを 考えてみよう。 これは単なる実数シータの関数だ。 つまり当面は、シータは 単なるスカラーの値、生の値だと考えてみよう。 それはベクトルでは無くて単なる数だとする。 実数のパラメータシータに関する二次関数の、コスト関数Jを考えてみよう。 つまりJのシータはこんな感じ。 さて、どうやって二次関数を最小化すればよい？ 解析学をいくらか知っていれば、 関数を最小化する方法は、 微分を取って、 その微分をイコール0とする事だと 知っているかもしれない。 だからJをシータに関して微分を取る。 すると何かしらの式が得られるが、ここには書かない。 微分をイコール0として、 この結果シータの値について 解くことが出来、 その解がJのシータを最小化するシータだ。 以上はデータが単なる実数の時の 簡単な場合の話だ。 我らが現在関心がある問題においては、 シータは単なる実数じゃない。 そうでは無く、このn+1次元の パラメータベクトルだ。 そしてコスト関数Jは このベクトルあるいは シータ0からシータmまでの関数だ。 そしてコスト関数は こんな感じとなる。
右側のような二次関数。 コスト関数Jをどうやって最小化しよう？ 解析学によれば、 もし知ってれば、 これを行う一つの手としては Jの偏微分をとる、各パラメータシータjに関する。 そして次に、それを全てイコール0と置く。 そうすると、 そしてシータ0、 シータ1、、、とシータnまでの 値を解くと、 これがコスト関数Jを 最小化するシータの値となる。 ここで実際に 解析を修めて パラメータの シータ0からシータnまで やっていけば、 何かしら導出する方法が得られる。 だがこのビデオで 私は実際に 導出には 踏み込まない。 それは長くて多くを要求するから。 その代わりに私がやりたいのは この過程を実装する為に必要な事を あなたに伝えたい。 つまりあなたは 偏微分項イコール0に 対応したシータの値を 解くことが出来る。 あるいは、等価な事だが コスト関数Jのシータを 最小化するシータの値を。 私の幾つかのコメントは 解析学に慣れ親しんだ人にしか より事態を分かりやすく出来てない、という 自覚はある。 だから、もしあまり知らなければ、 あまり解析学に親しみが無ければ、 あまり気にしないでよろしい。 アルゴリズムを実装して 動かすために必要な事は この後にちゃんと教えるから。 ここで見ていく例として 使う例として、 m=4のトレーニング手本が あるとしよう。 この正規方程式法を実装する為に 私は以下のようにする。 データセットを持ってきて さて、ここに4つのトレーニング手本がある。 今回は、これら4つの手本が 私の持ってるデータの全てだとしよう。 私がやる事は、 データセットを持ってきて、 追加の列を足す、これは 追加のフィーチャーx0に対応する。 それはいつもこの値、1を 取る。 そこでやる事は、 Xと呼ばれる行列を 構築する、それは ようするに トレーニングデータの全てのフィーチャーを 含んでいる。具体的には、 これが、、、これが、 フィーチャーの全てだとして、 これらの数を全て持ってきて、 この行列Xに突っ込む。オーケー？ つまり、このデータを 一度に一列ずつコピーする。 そしてyにも似たような事をする。 予測したいと思っている 値を持ってきて、 新しいベクトルを 前と同様に構築すると、 ベクトルyと呼ぶ。 つまりXは m 掛ける (n+1) 次元行列 となる。そして Yはm次元ベクトル となる。ここで mはトレーニング手本の数で、 nはフィーチャーの数、 n+1なのは、 この追加のフィーチャーx0があるから。 最後に行列Xと ベクトルyを 持ってきて、 そしてこれを計算し、 それをシータに代入すると、、、 (X転置 X)の逆行列 掛けることの X転置 y。 これはコスト関数を最小化する シータを与える。 このスライドでは たくさんの事をやった。 一つの具体的なデータセットの例を使ってそれを見てきた。 そこで以上をもっと一般的な形に 書いておこう。 その後で、このビデオの後半で この方程式についてもうちょっと解説を加える事にする。 これをどうやるのか、まだ完全にクリアでは無いだろう。 一般的なケースでは、 m個のトレーニング手本があるとして、これは x1, y1,からxm, ymまでで、 n個のフィーチャーがあるとしよう。 つまり各トレーニング手本、 x(i)は、こんなベクトルで、 n+1次元のフィーチャーベクトルだ。 行列Xを構築する方法は、 ところで、この行列はまた デザイン行列とも呼ばれているが、 それは以下のように作る。 各トレーニング手本がこんな感じの フィーチャーベクトルを与える。 ある種のn+1次元ベクトルだ。 デザイン行列Xを構築する方法は 単にこんな行列を作るだけ。 どうやるかというと 最初のトレーニング手本を 持ってきて、それはベクトルな訳だが、 その転置を取る、すると こんな感じになる。 こんな、横長のひらべったい物。 このx1の転置を、デザイン行列の最初の行にする。 次に二番目のトレーニング手本、x2を 持ってきて、 そしてそれを転置する。そしてそれを Xの二番目の行に置く。 以下同様に最後のトレーニング手本まで 降りていく。 その転置を取ると、 それが行列Xの 最後の行となる。 以上が私の行列Xの作り方だ。Xは M掛ける N+1 次元の 行列。 具体例で考えよう。 1フィーチャーだけだとしよう。 本当に、たった一つだけの フィーチャーしか無いとしよう、x0以外では。 x0はいつもイコール1だった。 つまり、今回のフィーチャーベクトル x(i)はイコール、 この1とーーこれはx0だが、 さらに実際のフィーチャーとなる、例えば 住居のサイズとか。 そしてデザイン行列Xは、イコール、 最初の行は、、、基本的には これを持ってきてその転置を取る。 だから結局、1とx(1)の1だ。 二番目の行は、結局 1と、 x(1)の2だ。 以下同様にx(1)のmまで 降りていく。 かくして、これは m掛ける2 次元の行列となる。 以上が、Xを構築する やり方だ。 そしてベクトルYは、、、 たまにベクトルであるという事を明示する為に Yの上に矢印を書く事もあるが、 だいたいは単にYって書くに留める。どちらにせよ、 ベクトルYは、 トレーニングセットの 住居の全てのラベル、 全ての正解の価格を持ってきて、 それを単にM次元のベクトルとして 積み上げれば良い。 それがYとなる。
最後に、 行列XとベクトルYを 構築したら、 シータを単に (X転置 X)^-1 掛けることの X転置 Yで計算出来る。 ここでちょっと、、、 ここでちょっと、この式の意味を確認しておきたい。 これをどうやって実装したらいいか分かるように。 具体的には、この (X転置 X)^-1、 とはなんだろう？ (X転置 X)^-1 とは 行列(X転置 X)の逆行列だ。 具体的には例えば、 Aに、イコール X転置 掛ける X を代入する。 X転置 は行列で、 X転置 掛ける X はまた別の 行列となる。そしてこれを 行列Aと呼ぶ訳だ。 次に、 (X転置 X)^-1は、 行列Aの逆行列をとれば良い。分かった？ つまり、Aの逆行列。 以上がこれを計算する方法だ。 X転置 X を計算して、その逆行列を計算する。 Octaveでどうやるのかはまだ話してないね。 これは後の一連のビデオでやるが、 だがOctaveの プログラミング言語では あるいは似たような言語である matlabのプログラミング言語もそっくりだが、 そこではこの量を計算するコマンドは、 (X転置 X)の逆行列 掛ける X転置 Y は、以下のようになる。 Octaveでは X'は Xの転置を表すのに使う記法だ。 つまりこの赤の箱で囲まれた 式は、それは X転置 掛ける Xを 計算していて、 pinvは行列の 逆行列を計算する関数だ。 つまりこれは、 (X転置 X) の逆行列を計算する。 そしてそれをXの転置に掛けて、 それをさらにYに掛ける。 以上でこの式の 計算が終わる。 この式の証明はしていないが。 だけど以下の事を、数学的に示すことが出来る、 私はここでそれを やる気は無いけれど、、、 この式は、最適なシータの値を 与える、という事を。 ここで最適とは、その値をシータにセットすると、 そのシータこそがまさに その線形回帰の コスト関数Jのシータを 最小化するシータだという意味だ。 最後に詳細な事を一つ。以前のビデオで フィーチャースケーリングについて議論した。 それはフィーチャーの範囲を だいたい似たような スケールの範囲にする、 お互いにだいたい似たような値の範囲にする、というアイデアだった。 もしあなたがこの正規方程式法を 使うならば、その時は フィーチャースケーリングをする必要は無い。 実際に以下みたいなケースでも構わないのだ： 例えばあるフィーチャーx1が 0から1の間で、 あるフィーチャーx2が 0から1000の範囲の間で、 そしてあるフィーチャーx3が 0から10の-5乗とかの 範囲を取るとしても。 もし正規方程式法を 使うなら、 これでもOKだ。 フィーチャースケーリングする必要は無い。 もちろん、 もし最急降下法を使う時は、 その場合は相変わらずフィーチャースケーリングは重要だが。 最後に、どこで最急降下法を使い、 どこで正規方程式法を使うべきか。 これがそれらの長所と短所だ。 m個のトレーニング手本があり、 n個のフィーチャーがあるとする。 最急降下法の欠点の一つには ラーニングレートのアルファを選ぶ必要がある。 そして、しばしばこれは、 最急降下法を異なるラーニングレートのアルファで 走らせ直して、どれが一番うまく行くか見てみるハメになる事を意味する。 つまりこれは追加の仕事であり、追加の面倒だ。 もう一つの最急降下法の欠点は、 それはより多くの繰り返しを必要とする、という事だ。 だから詳細によっては、 より遅くなりうる。だけどこれに関しては もうちょっと続きがある。それはすぐ後で見る事になる。 正規方程式では、ラーニングレートのアルファを選ぶ必要は無い。 だから本当に便利で、実装もシンプル。 単に走らせるだけでよく、普通はちゃんと機能する。 そして繰り返しの必要も無い。 つまりJのシータをプロットしたり 収束をチェックしたり、 それら追加の作業が何も必要無い事を意味する。 ここまでの所、天秤は正規方程式が好ましい方に 傾いているように見える。 ここで、正規方程式の欠点も述べておこう。 それは最急降下法の利点でもある。 最急降下法は、フィーチャーが 凄いたくさんあってもとても良く機能する。 だからたとえ数百万個の フィーチャーの場合であっても、 最急降下法を実行出来て、それはかなり効率的だ。 それは納得出来る振る舞いをする。 それとは対照的に正規方程式では、 パラメータの為にデータを解くには、 この項を解く必要がある。 このX転置 Xの逆行列を計算する必要がある。 この行列 X転置 X。 これは n 掛ける n 行列となる。ここでnはフィーチャーの数。 これは、X転置の次元と Xの次元を 見てみれば分かるように、 この二つを掛けあわせると、 積の次元は、 行列 X転置 X は n 掛ける n 次元の行列となる、ここで nはフィーチャーの数。 そしてほとんどの実装において、 行列の逆行列の計算のコストは だいたい行列の次元の 三乗で上昇する。 だからこの逆行列のコストは だいたい3乗のオーダーとなる。 時にはそれはnの三乗よりはわずかに早くなる事もあるが、 我らの目的では似たような物だ。 つまりn, フィーチャーの数がとても大きくなると、 この量を計算する事は おそくなる可能性があり、 正規方程式法は実際さらに遅くなる。 だからnが大きい時には 私はふつう 最急降下法を使う。何故なら この3乗の時間を支払いたくはないから。 だがnが相対的に小さければ、 正規方程式はパラメータを解く、より良い方法を与え得る。 小さいとか大きいとはどういう意味か？ うーん、nが100とかの オーダーなら、 100x100行列の逆行列を求める事は、 現代のコンピューティングの水準なら問題ありません。 もしnが1000なら？そのくらいなら私はまだ正規方程式法を使うだろうな。 1000x1000行列の逆行列を求めるのは、 現代のコンピュータなら実際はとても早い。 nが1万なら？この位から迷い始める。 1万行x1万行 の行列の逆行列を求めるのは、 ある程度遅くなり始める。 だから最急降下法に 乗り換えても良いかなぁ、と思うし、 乗り換えなくても良いようにも思う。 n=1万 なら、 1万 x 1万 行列の逆行列を計算する事は出来る。 だがそれよりずっと大きくなると、たぶん私だったら最急降下法を使うだろうね。 だからnが10の6乗、つまり 100万個のフィーチャーなら、 100万 x 100万 の行列の逆行列を 計算するのは、 とても高くつく。 だからそんなに多くのフィーチャーならば、私はきっと最急降下法を選ぶと思う。 だから正確にどれだけの数の フィーチャーだったら、最急降下法にするのか、という きっかりとした数字を提示するのは難しい。 だが私だったらだいたい、 1万くらいから最急降下法に 変えようかなぁ、と 考え始める。または このクラスの後半で扱う別のアルゴリズムの幾つかの時も最急降下法を使う。 まとめよう。 フィーチャーの数がそんなに多く無い間は 正規方程式はパラメータシータを解く為の とても素晴らしい代替案たりえる。 具体的には、フィーチャーの数が 1000以下なら、 私はふだん、 最急降下法では無く 正規方程式法を使う。 このコースの後半でやる より複雑な学習アルゴリズムの アイデアを、 先取りして見てみると、例えば、 分類器の話をする時には、 ロジスティック回帰のアルゴリズムなどは、 後で見る事になるがこれらのアルゴリズムは 実は、、、 正規方程式法は実際にはうまく行かないのだ、 それらのより洗練された 学習アルゴリズムでは。 それらのアルゴリズムに対しては、最急降下法にうったえなくてはならない。 だから最急降下法は知っておくととても良いアルゴリズムだ。 線形回帰は 大量のフィーチャーを持つ場合があり、 このコースで見る事になる これ以外のアルゴリズムの幾つかは、 単純にそれらには正規方程式法は 適用出来ず、使えない。 だが線形回帰のこの特定のモデルに関しては、 正規方程式は最急降下法より より早い代替案と なりえる。 だからアルゴリズムの詳細に応じて、 問題の詳細に応じて、 そして手持ちのフィーチャーの数に応じて、 これらのアルゴリズムはどちらも知るに値する物となる。