1
00:00:00,302 --> 00:00:01,883
이 번 비디오에서는

2
00:00:01,883 --> 00:00:03,948
normal equation에 대해 배워보겠습니다.

3
00:00:03,948 --> 00:00:05,660
특정 선형 회귀 문제에서,

4
00:00:05,660 --> 00:00:06,981
파라미터 θ의

5
00:00:06,981 --> 00:00:09,115
최적의 값을 구하는데

6
00:00:09,115 --> 00:00:10,879
효과적인 방법입니다.

7
00:00:10,879 --> 00:00:13,096
구체적으로, 지금까지

8
00:00:13,096 --> 00:00:14,399
지금까지

9
00:00:14,399 --> 00:00:16,042
선형회귀에 사용했던 알고리즘은

10
00:00:16,042 --> 00:00:17,823
gradient descent였고,

11
00:00:17,823 --> 00:00:19,410
J(θ)를 최소화 화기위한 것이였습니다.

12
00:00:19,410 --> 00:00:21,354
수 차례에 걸쳐

13
00:00:21,354 --> 00:00:23,792
알고리즘을 사용했고,

14
00:00:23,792 --> 00:00:26,410
gradient descent를

15
00:00:26,410 --> 00:00:28,259
여러 번 반복하고 나면,

16
00:00:28,259 --> 00:00:30,396
최소값으로 수렴하였습니다.

17
00:00:30,396 --> 00:00:32,563
이와는 반대로, normal equation은

18
00:00:32,563 --> 00:00:34,413
θ를 분석적으로

19
00:00:34,413 --> 00:00:36,986
구하는 방법입니다.

20
00:00:36,986 --> 00:00:38,761
그래서,

21
00:00:38,761 --> 00:00:40,594
여러번 알고리즘을 돌릴 필요 없이,

22
00:00:40,594 --> 00:00:41,365
θ의 최적의 값을

23
00:00:41,365 --> 00:00:42,791
한 번에

24
00:00:42,791 --> 00:00:44,403
구합니다.

25
00:00:44,403 --> 00:00:46,096
즉 한 번의 실행으로

26
00:00:46,096 --> 00:00:48,136
최적의 값을 구합니다.

27
00:00:49,136 --> 00:00:51,947
normal equation도

28
00:00:52,209 --> 00:00:54,442
장점과

29
00:00:54,442 --> 00:00:56,024
단점이 있지만,

30
00:00:56,024 --> 00:00:57,817
장단점과

31
00:00:57,903 --> 00:00:59,426
사용시기를 말하기 전에,

32
00:00:59,426 --> 00:01:02,539
어떠한 함수인지 개념을 알아 봅시다.

33
00:01:02,539 --> 00:01:04,633
설명을 위한 예제로,

34
00:01:04,633 --> 00:01:06,120
단순하게

35
00:01:06,120 --> 00:01:07,505
이런 cost function J(θ)가

36
00:01:07,505 --> 00:01:09,291
있다고 가정해봅시다.

37
00:01:09,291 --> 00:01:11,958
실수 범위의 θ에 대한 함수입니다.

38
00:01:11,958 --> 00:01:13,642
이번에는, θ가

39
00:01:13,842 --> 00:01:16,615
스칼라 값 즉, 값만 가지고 있다고 가정하겠습니다.

40
00:01:16,769 --> 00:01:18,918
그냥 숫자입니다. 벡터가 아닙니다.

41
00:01:19,171 --> 00:01:24,595
Cost function J는 실수 파라미터 θ에 대한 2차 함수라고 해봅시다.

42
00:01:25,028 --> 00:01:27,420
그러면, J(θ)는 이런 모양이 되죠.

43
00:01:27,851 --> 00:01:30,336
이제, 2차 함수를 최소화 하는 방법은 무엇일까요?

44
00:01:30,720 --> 00:01:32,745
미적분을 배웠다면,

45
00:01:32,858 --> 00:01:34,965
함수를

46
00:01:34,965 --> 00:01:36,628
최소화하기 위해

47
00:01:36,628 --> 00:01:38,991
미분을 하고,

48
00:01:38,991 --> 00:01:41,707
미분값이 0이 되는 지점이 답이라는 것을 알 수 있습니다.

49
00:01:41,707 --> 00:01:44,721
그래서 J를 θ에 대해 미분해보면,

50
00:01:44,797 --> 00:01:46,847
이런 식이 나올 것입니다.
직접 미분은 하지 않을게요.

51
00:01:46,847 --> 00:01:49,161
미분값이

52
00:01:49,161 --> 00:01:50,782
0일 때,

53
00:01:50,782 --> 00:01:53,503
J(θ)가 최소화되는

54
00:01:53,503 --> 00:01:57,866
θ를 구할 수 있습니다.

55
00:01:57,866 --> 00:01:59,096
data가 실수일 때

56
00:01:59,096 --> 00:02:01,716
가능한 경우였습니다.

57
00:02:01,716 --> 00:02:04,272
우리가 흥미롭게 다루어볼 문제는,

58
00:02:04,929 --> 00:02:06,559
θ가 더 이상

59
00:02:06,559 --> 00:02:07,847
실수가 아니라

60
00:02:07,847 --> 00:02:11,986
(n+1)차원 파라미터 벡터일 때입니다.

61
00:02:11,986 --> 00:02:13,809
cost function J도

62
00:02:13,809 --> 00:02:15,742
벡터 값에 대한 즉,

63
00:02:15,742 --> 00:02:17,501
θ_0부터

64
00:02:17,501 --> 00:02:18,924
θ_m까지에 대한 함수입니다.

65
00:02:18,924 --> 00:02:21,957
cost function은 이렇고, 오른쪽은 제곱함수입니다.

66
00:02:22,373 --> 00:02:25,712
cost function J를 어떻게 최소화 할 수 있을 까요?

67
00:02:25,712 --> 00:02:27,163
미적분을

68
00:02:27,163 --> 00:02:29,377
이용한다면,

69
00:02:29,377 --> 00:02:30,709
한 가지 방법으로

70
00:02:30,709 --> 00:02:38,604
J를 편미분하는 방법이 있습니다.
각각 모든 파라미터 θ_j에 따라서 차례차례로 미분하고,

71
00:02:38,604 --> 00:02:40,271
모두 0이 되게 합니다.

72
00:02:40,271 --> 00:02:41,394
그렇게 하면,

73
00:02:41,394 --> 00:02:42,718
모든 θ에 대한 답을 구할 수 있습니다.

74
00:02:42,718 --> 00:02:44,000
θ_0, θ_1,

75
00:02:44,000 --> 00:02:45,973
θ_n까지요.

76
00:02:45,973 --> 00:02:47,217
그리고나서, 그 때의 θ를 대입하면,

77
00:02:47,217 --> 00:02:48,765
cost function J를

78
00:02:48,765 --> 00:02:50,878
최소화 할 수 있습니다.

79
00:02:50,878 --> 00:02:52,176
미적분을

80
00:02:52,176 --> 00:02:53,597
사용하여,

81
00:02:53,597 --> 00:02:55,194
파라미터 θ_0부터

82
00:02:55,194 --> 00:02:57,316
θ_n까지

83
00:02:57,316 --> 00:03:00,520
식을 풀면, 결국에는 답이 나옵니다.

84
00:03:00,520 --> 00:03:01,625
이 번

85
00:03:01,625 --> 00:03:03,113
비디오에서는

86
00:03:03,113 --> 00:03:04,852
미분을 사용하여,

87
00:03:04,852 --> 00:03:06,297
답을 구하지 않을 것입니다.

88
00:03:06,297 --> 00:03:07,657
미분은 시간이 오래 걸리는 방법입니다.

89
00:03:07,657 --> 00:03:08,962
하지만,

90
00:03:08,962 --> 00:03:10,545
이 과정을 구현하기 위해서는

91
00:03:10,545 --> 00:03:12,619
알아야 할 것이 있습니다.

92
00:03:12,619 --> 00:03:14,138
그러면,

93
00:03:14,138 --> 00:03:15,511
편미분 방정식이

94
00:03:15,511 --> 00:03:16,892
0이 되는

95
00:03:16,892 --> 00:03:19,273
θ 값을 구할 수 있습니다.

96
00:03:19,273 --> 00:03:21,733
같은 말이지만, 다르게 말하면,

97
00:03:21,733 --> 00:03:23,357
cost function J(θ)가

98
00:03:23,357 --> 00:03:25,901
최소화 되는 θ를 구하는 것입니다.

99
00:03:25,901 --> 00:03:27,283
이번에 다룰 내용은

100
00:03:27,283 --> 00:03:28,846
미적분학에

101
00:03:28,846 --> 00:03:29,914
익숙치 않은 사람에게

102
00:03:29,914 --> 00:03:31,896
생소할지도 모릅니다.

103
00:03:31,896 --> 00:03:33,065
하지만, 이해가 잘 안되거나,

104
00:03:33,065 --> 00:03:34,487
미적분에 낯설지라도,

105
00:03:34,487 --> 00:03:36,354
걱정할 필요없습니다.

106
00:03:36,354 --> 00:03:37,404
알고리즘을 구현하고

107
00:03:37,404 --> 00:03:38,374
적용하는데 필요한 것들은

108
00:03:38,374 --> 00:03:41,358
알려드릴 겁니다.

109
00:03:41,358 --> 00:03:42,585
이 예제는,

110
00:03:42,585 --> 00:03:43,737
이 번에 다루어볼

111
00:03:43,737 --> 00:03:46,339
예제입니다.

112
00:03:46,339 --> 00:03:49,056
m=4인 training example이 있습니다.

113
00:03:50,409 --> 00:03:52,881
normal equation을 구현하기 위해

114
00:03:52,881 --> 00:03:56,515
해야할 것은 아래에 있습니다.

115
00:03:56,515 --> 00:03:57,640
data set을 먼저 보면,

116
00:03:57,640 --> 00:04:00,375
4개의 training example가 있습니다.

117
00:04:00,375 --> 00:04:01,844
이번 예제에서는

118
00:04:01,844 --> 00:04:06,073
오직 4개가 가지고 있는 data 전부입니다.

119
00:04:06,073 --> 00:04:07,890
해야할 것은

120
00:04:07,890 --> 00:04:09,007
data set에

121
00:04:09,007 --> 00:04:11,289
열을 추가하는 것입니다.

122
00:04:11,289 --> 00:04:14,579
추가된 열은 feature x_0입니다.

123
00:04:14,579 --> 00:04:15,967
모두

124
00:04:15,967 --> 00:04:17,527
1의 값을 가집니다.

125
00:04:17,527 --> 00:04:18,681
그 다음으로는

126
00:04:18,681 --> 00:04:19,943
X라는 행렬을

127
00:04:19,943 --> 00:04:22,638
만드는 것입니다.

128
00:04:22,638 --> 00:04:24,632
행렬 X는

129
00:04:24,632 --> 00:04:26,100
training data의 모든 feature를

130
00:04:26,100 --> 00:04:28,140
가지고 있습니다.

131
00:04:28,140 --> 00:04:31,528
즉 이것은 모두

132
00:04:31,528 --> 00:04:33,743
feature이고,

133
00:04:33,743 --> 00:04:34,797
이 숫자들은 전부

134
00:04:34,797 --> 00:04:37,777
행렬 X에 넣습니다. 이해했죠?

135
00:04:37,777 --> 00:04:39,179
단지,

136
00:04:39,179 --> 00:04:41,233
한 번에 한 열씩

137
00:04:41,233 --> 00:04:45,962
복사한거에요. 
그리고나서, 똑같이 y에도 합니다.

138
00:04:45,962 --> 00:04:47,087
예측하려는

139
00:04:47,087 --> 00:04:47,952
값을 가져와서

140
00:04:47,952 --> 00:04:49,360
벡터를 만듭니다.

141
00:04:49,360 --> 00:04:52,894
이렇게요.

142
00:04:52,894 --> 00:04:55,440
이 벡터는 벡터 y라고 부르겠습니다.

143
00:04:55,440 --> 00:04:58,038
결국 X는

144
00:04:59,653 --> 00:05:05,688
(m x n+1) 차원 행렬이 되었고

145
00:05:05,688 --> 00:05:07,490
y는

146
00:05:07,490 --> 00:05:14,421
m차원 벡터가 되었습니다.

147
00:05:14,421 --> 00:05:16,624
m은 training example의 수이고,

148
00:05:16,984 --> 00:05:18,688
n은,

149
00:05:18,688 --> 00:05:20,713
n은 feature의 수입니다. 
n+1인 이유는

150
00:05:20,713 --> 00:05:24,825
여기 추가로 feature x_0이 있기 때문입니다.

151
00:05:24,825 --> 00:05:26,350
마지막으로,

152
00:05:26,350 --> 00:05:27,489
행렬 X와

153
00:05:27,489 --> 00:05:28,595
벡터 y를 가지고,

154
00:05:28,595 --> 00:05:31,065
이 식을 계산하면,

155
00:05:31,065 --> 00:05:32,419
θ는

156
00:05:32,419 --> 00:05:34,440
(X transpose X)의 역행렬에

157
00:05:34,440 --> 00:05:36,516
X'y를 곱한 값이 됩니다.

158
00:05:36,516 --> 00:05:38,583
이 때의 θ값이

159
00:05:38,583 --> 00:05:42,559
cost function을 최소화 하는 값입니다.

160
00:05:42,559 --> 00:05:43,436
이 슬라이드에서

161
00:05:43,436 --> 00:05:44,416
많은 계산을 했지만,

162
00:05:44,416 --> 00:05:47,514
오직 하나의 dataset을 예로 풀어보았습니다.

163
00:05:47,514 --> 00:05:49,241
더 일반화된 식으로

164
00:05:49,333 --> 00:05:50,770
써보고,

165
00:05:50,955 --> 00:05:53,418
비디오 후반부에서,

166
00:05:53,621 --> 00:05:56,531
방정식에 대한 설명을 다루어보겠습니다.

167
00:05:57,581 --> 00:06:00,687
아직 어떻게 하는지는 와닿지 않을 것입니다.

168
00:06:00,687 --> 00:06:02,129
일반적인 경우를 보면,

169
00:06:02,129 --> 00:06:04,124
m개의 training example에서

170
00:06:04,124 --> 00:06:05,697
( x^(1) , y^(1) )부터

171
00:06:05,697 --> 00:06:09,319
( x^(n), y^(n) )까지 n개의 feature가 있습니다.

172
00:06:09,319 --> 00:06:10,811
그리고, 각각의 training example x^(i)는

173
00:06:10,811 --> 00:06:12,926
이와 같은 벡터입니다.

174
00:06:12,926 --> 00:06:16,297
(n+1)차원 feature 벡터죠

175
00:06:16,943 --> 00:06:18,350
이제 행렬 X를

176
00:06:18,350 --> 00:06:20,674
만들어 볼 것입니다.

177
00:06:20,674 --> 00:06:24,827
행렬 X는 design matrix라고도 하며,

178
00:06:24,827 --> 00:06:26,712
이와 같이 만듭니다.

179
00:06:26,712 --> 00:06:28,640
각각의 training example은

180
00:06:28,640 --> 00:06:30,549
이와 같은 feature 벡터로 나타낼 수 있습니다.

181
00:06:30,549 --> 00:06:34,491
(n+1)차원 벡터죠

182
00:06:34,491 --> 00:06:36,190
design matrix X를

183
00:06:36,359 --> 00:06:39,734
만드는 방법은 이렇습니다.

184
00:06:39,734 --> 00:06:40,834
우선

185
00:06:40,834 --> 00:06:42,109
첫 번째

186
00:06:42,109 --> 00:06:43,711
training example,

187
00:06:43,711 --> 00:06:46,350
즉 벡터죠. 
벡터를 transpose하여,

188
00:06:46,350 --> 00:06:48,692
이렇게 넣습니다.

189
00:06:48,692 --> 00:06:50,250
이렇게 가로로 퍼진 형태가 되고,

190
00:06:50,250 --> 00:06:55,153
x^(1)을 transpose한 것이  design matrix의 첫 번째 열이 됩니다.

191
00:06:55,153 --> 00:06:56,225
이제 두 번째 training example

192
00:06:56,225 --> 00:06:58,682
x^(2)를 보죠.

193
00:06:58,682 --> 00:07:00,437
transpose하고,

194
00:07:00,437 --> 00:07:01,838
X의 두 번째 열에 넣습니다.

195
00:07:01,838 --> 00:07:04,068
나머지도 이런식으로 해서,

196
00:07:04,068 --> 00:07:07,206
마지막 training example까지 내려갑니다.

197
00:07:07,206 --> 00:07:09,279
마지막을 transpose하여

198
00:07:09,279 --> 00:07:10,850
행렬 X의 마지막 열로 만듭니다.

199
00:07:10,850 --> 00:07:12,665
이렇게

200
00:07:12,665 --> 00:07:14,418
행렬 X를 만듭니다.

201
00:07:14,418 --> 00:07:17,129
X는 (m x n+1)차원

202
00:07:17,129 --> 00:07:19,836
행렬이죠.

203
00:07:19,836 --> 00:07:21,953
구체적인 예로

204
00:07:21,953 --> 00:07:23,505
하나의 feature만

205
00:07:23,505 --> 00:07:24,670
있는 경우를 보죠.

206
00:07:24,670 --> 00:07:26,631
진짜로 값이 1인 x_0를 제외하고

207
00:07:26,631 --> 00:07:28,165
하나의 feature만 가지고 있습니다.

208
00:07:28,165 --> 00:07:30,376
그래서

209
00:07:30,376 --> 00:07:32,186
x_i가 1이라면,

210
00:07:32,186 --> 00:07:33,878
그 feature는 x_0이고,

211
00:07:33,878 --> 00:07:35,912
어떤 실수를 가진다면,

212
00:07:35,912 --> 00:07:37,662
집의 크기가 될 것입니다.

213
00:07:37,662 --> 00:07:40,947
이제, design matrix X는 이럴 것입니다.

214
00:07:40,947 --> 00:07:42,589
첫째 열은,

215
00:07:42,589 --> 00:07:46,071
이 것을 가져와서 transpose하죠

216
00:07:46,071 --> 00:07:51,644
그래서 여기는 1이 되고, 다음은 ( x^(1) )_1입니다.

217
00:07:51,644 --> 00:07:53,309
두 번째 열은,

218
00:07:53,309 --> 00:07:56,077
1이 먼저 오고나서,

219
00:07:56,077 --> 00:07:58,046
( x^(1) )_2이 됩니다.

220
00:07:58,046 --> 00:07:59,046
이런식으로 1과

221
00:07:59,046 --> 00:08:01,420
( x^(1) )_m까지 내려옵니다.

222
00:08:01,420 --> 00:08:03,084
결국,

223
00:08:03,084 --> 00:08:07,776
(m x 2)차원 행렬이 되겟죠.

224
00:08:07,776 --> 00:08:08,821
이렇게 행렬 X를

225
00:08:08,821 --> 00:08:11,251
만들 수 있습니다.

226
00:08:11,251 --> 00:08:13,886
그리고, 벡터 y,

227
00:08:13,886 --> 00:08:15,487
가끔 위쪽에 화살 표시는

228
00:08:15,487 --> 00:08:16,541
벡터라는 뜻을 나타내지만,

229
00:08:16,541 --> 00:08:19,871
보통은 그냥 y로만 쓰겠습니다.

230
00:08:19,871 --> 00:08:21,182
어찌됬든, 벡터 y는

231
00:08:21,182 --> 00:08:23,275
모든 label,

232
00:08:23,275 --> 00:08:25,098
모든 training set의

233
00:08:25,098 --> 00:08:27,076
정확한 집 값을 가져와서 만듭니다.

234
00:08:27,076 --> 00:08:28,963
그것들을 계속 쌓아서,

235
00:08:28,963 --> 00:08:32,011
m차원 벡터를 만듭니다.

236
00:08:32,011 --> 00:08:34,511
이게 y입니다.

237
00:08:34,511 --> 00:08:36,724
마지막으로, 행렬 X와

238
00:08:36,724 --> 00:08:38,184
벡터 y를 만든 상태에서,

239
00:08:38,184 --> 00:08:40,887
(X'X)^-1에

240
00:08:40,887 --> 00:08:47,243
X'Y를 곱하여 θ를 구합니다.

241
00:08:47,243 --> 00:08:49,356
여기서 잠시,

242
00:08:49,356 --> 00:08:51,348
여기서 잠시 이 식의 의미를 이해하고,

243
00:08:51,348 --> 00:08:52,242
어떻게 구현하는지 알려주고자 합니다.

244
00:08:52,242 --> 00:08:55,221
그렇다면, 명확하게, (X'X)^-1은 무엇일까요?

245
00:08:55,221 --> 00:08:57,903
(X'X)^-1는

246
00:08:57,903 --> 00:09:02,101
X'X의 역행렬입니다.

247
00:09:02,101 --> 00:09:04,498
구체적으로,

248
00:09:04,498 --> 00:09:08,055
A를

249
00:09:08,055 --> 00:09:11,120
X'X로 정의하면,

250
00:09:11,120 --> 00:09:12,542
X'는 행렬이고,

251
00:09:12,542 --> 00:09:14,063
X'X도 행렬입니다.

252
00:09:14,063 --> 00:09:15,305
그래서 A는

253
00:09:15,305 --> 00:09:17,560
행렬 A라 하겠습니다.

254
00:09:17,560 --> 00:09:19,968
그러면, (X'X)^-1는

255
00:09:19,968 --> 00:09:22,352
행렬 A를 역행렬로 바꾼 것입니다. 알겠죠?

256
00:09:23,245 --> 00:09:24,417
이 것은 A^-1이라 하겠습니다.

257
00:09:26,025 --> 00:09:28,919
이게 바로 계산하는 방법입니다.

258
00:09:28,919 --> 00:09:31,451
X'X를 계산하고나서 그것을 역행렬로 바꿉니다.

259
00:09:31,451 --> 00:09:34,296
아직 Octave에 대해서는 다루지 않았지만,

260
00:09:34,296 --> 00:09:35,941
뒤의 비디오에서

261
00:09:35,941 --> 00:09:37,211
다룰 것입니다.

262
00:09:37,211 --> 00:09:39,073
Octave 혹은

263
00:09:39,073 --> 00:09:40,652
비슷한 프로그래밍 언어인

264
00:09:40,652 --> 00:09:42,957
matlab에서는

265
00:09:42,957 --> 00:09:46,937
아래의 명령어로

266
00:09:47,384 --> 00:09:50,326
(X transpose X)의 역행렬에

267
00:09:50,326 --> 00:09:52,537
X transpose와 y를 곱합니다.

268
00:09:52,537 --> 00:09:54,903
Octave에서 X'는

269
00:09:54,903 --> 00:09:58,354
X의 transpose를 나타내는 기호입니다.

270
00:09:58,354 --> 00:10:00,737
그리고, 붉은색 박스안에 있는

271
00:10:00,737 --> 00:10:03,588
식은

272
00:10:03,588 --> 00:10:06,633
X'에 X를 곱한 값을 계산합니다.

273
00:10:06,633 --> 00:10:08,551
pinv는 행렬의 역행렬을

274
00:10:08,551 --> 00:10:09,701
계산하는 함수입니다.

275
00:10:09,701 --> 00:10:11,818
그래서 이 식은

276
00:10:11,818 --> 00:10:14,656
X'X의 역행렬을 계산하고,

277
00:10:14,656 --> 00:10:16,453
그리고나서,

278
00:10:16,453 --> 00:10:18,267
X‘와 먼저 곱한 다음,

279
00:10:18,267 --> 00:10:19,712
y와 곱합니다.

280
00:10:19,712 --> 00:10:22,325
이상으로 이 식의 계산이 끝났습니다.

281
00:10:22,325 --> 00:10:24,369
증명하지는 않았지만,

282
00:10:24,369 --> 00:10:25,994
여기서는 못하더라도

283
00:10:25,994 --> 00:10:27,382
수학적으로 보여줄 수는

284
00:10:27,382 --> 00:10:28,537
있습니다.

285
00:10:28,537 --> 00:10:31,071
이 식으로

286
00:10:31,071 --> 00:10:32,316
θ의 최적의 값을

287
00:10:32,316 --> 00:10:34,865
구할 수 있습니다.

288
00:10:34,865 --> 00:10:36,512
최적의 값은 이 식을 θ로 정의하면,

289
00:10:36,512 --> 00:10:38,000
그 값은 선형회귀에서

290
00:10:38,000 --> 00:10:40,169
cost function J(θ)를 최소화 하는

291
00:10:40,169 --> 00:10:41,993
θ값이라는 뜻입니다.

292
00:10:41,993 --> 00:10:44,530
이전 비디오에서

293
00:10:44,530 --> 00:10:46,131
feature scaling을 자세히 배웠는데,

294
00:10:46,131 --> 00:10:47,061
feature의 범위를

295
00:10:47,061 --> 00:10:48,878
비슷한 범위로 조절하여

296
00:10:48,878 --> 00:10:50,726
각각 비슷한 값의 범위를

297
00:10:50,726 --> 00:10:54,900
가지도록 하는 것입니다.

298
00:10:54,900 --> 00:10:56,872
만약 normal equation을

299
00:10:56,872 --> 00:10:59,843
사용한다면,

300
00:10:59,843 --> 00:11:02,315
feature scaling은 필요하지 않습니다.

301
00:11:02,315 --> 00:11:04,361
예를들면,

302
00:11:04,361 --> 00:11:06,094
feature x_1이

303
00:11:06,094 --> 00:11:07,552
0부터 1사이이고,

304
00:11:07,552 --> 00:11:08,846
feature x_2가

305
00:11:08,846 --> 00:11:10,550
0에서

306
00:11:10,550 --> 00:11:12,019
1000 사이에 있고,

307
00:11:12,019 --> 00:11:14,159
feature x_3가

308
00:11:14,159 --> 00:11:15,822
0부터

309
00:11:15,822 --> 00:11:17,263
0.00005 사이에 있더라도 괜찮습니다.

310
00:11:17,263 --> 00:11:18,321
normal equation을 사용한다면,

311
00:11:18,321 --> 00:11:20,296
feature scaling이 없어도

312
00:11:20,296 --> 00:11:21,550
괜찮습니다.

313
00:11:21,550 --> 00:11:22,740
물론

314
00:11:22,740 --> 00:11:25,667
gradient descent를 사용한다면,

315
00:11:25,667 --> 00:11:27,814
feature scaling은 필요합니다.

316
00:11:28,030 --> 00:11:31,020
마지막으로, 언제 어느 때 gradient descent와

317
00:11:31,020 --> 00:11:33,273
normal equation을 써야하는지 알아봅시다.

318
00:11:33,273 --> 00:11:35,800
둘의 장점과 단점입니다.

319
00:11:35,800 --> 00:11:38,305
m개의 training example가 있고,

320
00:11:38,305 --> 00:11:40,918
n개의 feature가 있다고 가정해봅시다.

321
00:11:40,918 --> 00:11:42,854
gradient descent의 첫 번째 단점은

322
00:11:42,854 --> 00:11:46,015
learning rate 알파를 정해야 한다는 것입니다.

323
00:11:46,015 --> 00:11:47,374
즉, 서로 다른 learning rate 알파를 가지고

324
00:11:47,374 --> 00:11:49,128
여러번 gradient descent를 해봐야 한다는 것이고,

325
00:11:49,128 --> 00:11:51,154
그 중에서 언제 가장 잘 돌아가는지도 찾아야 합니다.

326
00:11:51,154 --> 00:11:54,274
귀찮은 일을 추가로 더하게 되는 것입니다.

327
00:11:54,274 --> 00:11:55,976
gradient descent의 또 다른 단점은

328
00:11:55,976 --> 00:11:57,841
반복이 많다는 것입니다.

329
00:11:57,841 --> 00:11:59,346
얼마나 세세하게 하냐에 따라서,

330
00:11:59,346 --> 00:12:00,839
느려질수도 있습니다.

331
00:12:00,839 --> 00:12:04,391
gradient descent에 관해서 더 이야기할게 남았지만 잠시 후에 하도록 하겠습니다.

332
00:12:04,391 --> 00:12:07,544
normal equation은 learning rate 알파를 정할 필요가 없습니다.

333
00:12:07,821 --> 00:12:11,208
그래서 매우 편리하고, 구현하기도 간단합니다.

334
00:12:11,208 --> 00:12:13,888
실행만하면 됩니다.

335
00:12:13,888 --> 00:12:15,061
그리고,

336
00:12:15,061 --> 00:12:16,129
반복도 없습니다.

337
00:12:16,129 --> 00:12:17,456
그래서 J(θ) 그래프를 그린다던지

338
00:12:17,456 --> 00:12:20,497
수렴을 확인한다던지하는 추가적인 작업이 없습니다.

339
00:12:20,497 --> 00:12:21,931
아직까지는,

340
00:12:21,931 --> 00:12:23,846
normal equation이 더 나은 것처럼 보일지도 모릅니다.

341
00:12:24,826 --> 00:12:27,085
이제 normal equation의 단점과

342
00:12:27,612 --> 00:12:29,435
gradient descent의 장점을 알아봅시다.

343
00:12:29,681 --> 00:12:31,447
gradient descent는

344
00:12:31,928 --> 00:12:34,698
feature가 많을 때, 효과적입니다.

345
00:12:34,698 --> 00:12:36,168
만약,

346
00:12:36,168 --> 00:12:37,812
수백만개의 feature를 가지고

347
00:12:37,812 --> 00:12:40,865
gradient descent를 한다면, 효율적이고,

348
00:12:40,865 --> 00:12:43,381
적당하게 돌아갈 것입니다.

349
00:12:43,381 --> 00:12:46,566
반대로 normal equation은

350
00:12:46,566 --> 00:12:48,014
파라미터 θ를 구하기 위해,

351
00:12:48,014 --> 00:12:50,394
이 식을 풀어야 합나다.

352
00:12:50,394 --> 00:12:53,058
X'X의 역행렬식을 계산해야 합니다.

353
00:12:53,058 --> 00:12:56,328
이 행렬 X'X은

354
00:12:56,328 --> 00:13:00,206
n개의 feature를 가지고 있다면, (n x n)행렬입니다.

355
00:13:00,770 --> 00:13:02,947
왜냐하면,

356
00:13:02,947 --> 00:13:03,917
X'의 차원수와

357
00:13:03,917 --> 00:13:05,529
X의 차원수를

358
00:13:05,529 --> 00:13:07,024
가지고 알 수 있는데,
이 둘을 곱하면

359
00:13:07,024 --> 00:13:08,749
그 결과의 차원이 얼마인지 알 수 있고,

360
00:13:08,749 --> 00:13:10,983
결국, 행렬 X'X는

361
00:13:10,983 --> 00:13:13,727
(n x n) 행렬이 됩니다.

362
00:13:13,727 --> 00:13:15,853
n은 feature의 개수입니다.

363
00:13:15,853 --> 00:13:18,641
그리고, 행렬의 역행렬을 계산하는데

364
00:13:18,641 --> 00:13:20,990
걸리는 시간은,

365
00:13:20,990 --> 00:13:23,087
대략

366
00:13:23,087 --> 00:13:25,707
행렬의 차원수의 세제곱만큼 증가합니다.

367
00:13:25,707 --> 00:13:28,180
즉, 역행렬 계산의 걸리는 시간은

368
00:13:28,180 --> 00:13:29,964
보통 세제곱만큼 걸립니다.

369
00:13:29,964 --> 00:13:31,213
가끔 세제곱보다 빠를때도 있지만,

370
00:13:31,213 --> 00:13:35,050
차이는 거의 없습니다.

371
00:13:35,489 --> 00:13:36,605
결국, feature의 수 n이 엄청 커지면,

372
00:13:37,643 --> 00:13:39,025
식의 계산속도는

373
00:13:39,025 --> 00:13:40,570
느려질수 있고,

374
00:13:40,570 --> 00:13:44,289
normal equation은 결국 훨씬 더 느려집니다.

375
00:13:44,289 --> 00:13:45,491
그래서 n이

376
00:13:45,491 --> 00:13:47,622
엄청 클 때에는,

377
00:13:47,622 --> 00:13:49,490
저는 주로 gradient descent를 씁니다.

378
00:13:49,490 --> 00:13:51,872
왜냐하면 gradient descent는 세제곱만큼 시간이 들지 않기 때문입니다.

379
00:13:51,872 --> 00:13:53,525
만약 n이 작다면,

380
00:13:53,525 --> 00:13:57,395
normal equation이 파라미터를 구하는데 더 나을것입니다.

381
00:13:57,395 --> 00:13:59,080
그럼 언제 작고 언제 크다고 볼 수 있을까요?

382
00:13:59,080 --> 00:14:00,741
n이

383
00:14:00,741 --> 00:14:02,130
100단위라면,

384
00:14:02,130 --> 00:14:03,822
(100 x 100)행렬의 역행렬을 구하는 것은

385
00:14:03,822 --> 00:14:06,539
요즘의 컴퓨터수준으로 아무 문제가 없습니다.

386
00:14:06,539 --> 00:14:10,966
n이 1000이라도,
저는 여전히 normal equation을 사용할 것입니다.

387
00:14:10,966 --> 00:14:12,583
1000 x 1000 행렬의 역행렬도

388
00:14:12,583 --> 00:14:15,408
요즘 수준의 컴퓨터로 충분히 빠르게 구할 수 있습니다.

389
00:14:15,408 --> 00:14:18,406
하지만 만약 n이 10000이라면, 한번 생각해봐야합니다.

390
00:14:18,406 --> 00:14:20,618
10000 x 10000행렬부터 역행렬을 구하는 게

391
00:14:20,618 --> 00:14:22,208
느려지기 시작합니다.

392
00:14:22,208 --> 00:14:23,471
gradient descent로

393
00:14:23,471 --> 00:14:25,007
갈아탈지 말지

394
00:14:25,007 --> 00:14:27,007
고민하게 되지만, 아직은 아닙니다.

395
00:14:27,114 --> 00:14:28,672
n이 1만이라면,

396
00:14:28,672 --> 00:14:31,148
10000 x 10000 행렬은 계산 할 수는 있습니다.

397
00:14:31,148 --> 00:14:34,345
하지만, 이보다 더 큰 경우에는,
gradient descent를 사용하는게 낫습니다.

398
00:14:34,345 --> 00:14:35,834
결국은, n이

399
00:14:35,834 --> 00:14:36,920
10^6, 즉

400
00:14:36,920 --> 00:14:38,963
feature가 백만개라면,

401
00:14:38,963 --> 00:14:41,565
(100만 x 100만) 행렬의 역행렬은

402
00:14:41,565 --> 00:14:42,631
많은 시간이 필요합니다.

403
00:14:42,631 --> 00:14:46,163
feature가 많다면, gradient descent를 강력하게 추천합니다.

404
00:14:46,163 --> 00:14:47,859
정확히 feature가

405
00:14:47,859 --> 00:14:49,282
몇 개 일 때,

406
00:14:49,282 --> 00:14:52,655
gradient descent를 해야하는지, 
정확한 숫자는 제시하기 어렵습니다.

407
00:14:52,655 --> 00:14:53,855
하지만, 저같은 경우,

408
00:14:53,855 --> 00:14:55,501
1만개 정도 일 때,

409
00:14:55,501 --> 00:14:58,258
gradient descent나,

410
00:14:58,335 --> 00:15:00,663
아니면,

411
00:15:00,663 --> 00:15:04,324
뒤에서 배울 다른 알고리즘을 고려해봅니다.

412
00:15:04,324 --> 00:15:05,765
정리하자면,

413
00:15:05,765 --> 00:15:06,999
feature의 수가 많지 않을 때는,

414
00:15:06,999 --> 00:15:08,475
normal equation이

415
00:15:08,475 --> 00:15:12,229
파라미터 θ를 구하는데, 좋은 방법입니다.

416
00:15:12,583 --> 00:15:13,983
정확히는

417
00:15:13,983 --> 00:15:15,749
feature의 수가

418
00:15:15,749 --> 00:15:17,472
1000보다 작을 때

419
00:15:17,472 --> 00:15:18,881
normal equation을

420
00:15:18,881 --> 00:15:21,955
gradient descent가 아닌 normal equation을 사용할 것입니다.

421
00:15:21,955 --> 00:15:23,549
나중에 배울 내용에 대해서

422
00:15:23,549 --> 00:15:24,493
살짝 이야기하자면,

423
00:15:24,493 --> 00:15:26,235
더 복잡한

424
00:15:26,235 --> 00:15:27,912
알고리즘을 배울 때

425
00:15:27,912 --> 00:15:29,617
예를들면,

426
00:15:29,617 --> 00:15:32,188
분류(classification) 알고리즘 중에서도
logistic regression 알고리즘 같은,

427
00:15:32,834 --> 00:15:34,319
나중에 실제로 배울 테지만

428
00:15:34,319 --> 00:15:35,467
사실

429
00:15:35,467 --> 00:15:37,592
그런 정교한 학습 알고리즘에

430
00:15:37,592 --> 00:15:39,388
normal equation은

431
00:15:39,388 --> 00:15:41,190
적합하지 않지만,

432
00:15:41,190 --> 00:15:43,916
gradient descent는 적합합니다.

433
00:15:43,916 --> 00:15:46,682
그래서, gradient descent는 알고있으면 
매우 유용한 알고리즘입니다.

434
00:15:46,682 --> 00:15:48,859
선형 회귀는

435
00:15:48,982 --> 00:15:50,017
매우 많은 feature를 가지는 경우가 많고,

436
00:15:50,017 --> 00:15:52,373
이 수업에서 배우는

437
00:15:52,373 --> 00:15:53,893
gradient descent 외 알고리즘들은

438
00:15:53,893 --> 00:15:55,438
예로 normal equation은

439
00:15:55,438 --> 00:15:58,747
적용하기도 어렵습니다.

440
00:15:58,747 --> 00:16:00,537
하지만, 선형 회귀의

441
00:16:00,537 --> 00:16:02,904
특정 모델에 대해서는

442
00:16:02,904 --> 00:16:05,827
normal equation이

443
00:16:07,219 --> 00:16:08,612
gradient descent보다 더 빠를 수도 있습니다

444
00:16:09,604 --> 00:16:11,920
결국, 알고리즘이 어떠냐에 따라,

445
00:16:12,007 --> 00:16:14,164
문제가 어떻고

446
00:16:14,164 --> 00:16:15,550
feature가 얼마나 많냐에 따라 달라지기 때문에,

447
00:16:15,550 --> 00:16:19,550
두 알고리즘은 알아두면 좋습니다.