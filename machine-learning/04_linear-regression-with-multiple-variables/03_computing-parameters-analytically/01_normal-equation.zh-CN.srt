1
00:00:00,302 --> 00:00:01,883
在这段视频中 我们要讲

2
00:00:01,883 --> 00:00:03,948
正规方程 (Normal Equation)

3
00:00:03,948 --> 00:00:05,660
对于某些线性回归问题

4
00:00:05,660 --> 00:00:06,981
用正规方程法求解参数 θ 的最优值更好

5
00:00:06,981 --> 00:00:09,115
用正规方程法求解参数 θ 的最优值更好

6
00:00:09,115 --> 00:00:10,879
用正规方程法求解参数 θ 的最优值更好

7
00:00:10,879 --> 00:00:13,096
具体而言 到目前为止

8
00:00:13,096 --> 00:00:14,399
我们一直在使用的线性回归的算法

9
00:00:14,399 --> 00:00:16,042
是梯度下降法

10
00:00:16,042 --> 00:00:17,823
就是说 为了最小化代价函数 J(θ)

11
00:00:17,823 --> 00:00:19,410
来最小化这个

12
00:00:19,410 --> 00:00:21,354
我们使用的迭代算法

13
00:00:21,354 --> 00:00:23,792
需要经过很多步

14
00:00:23,792 --> 00:00:26,410
也就是说通过多次迭代来计算梯度下降

15
00:00:26,410 --> 00:00:28,259
也就是说通过多次迭代来计算梯度下降

16
00:00:28,259 --> 00:00:30,396
来收敛到全局最小值

17
00:00:30,396 --> 00:00:32,563
相反地

18
00:00:32,563 --> 00:00:34,413
正规方程法提供了一种求 θ 的解析解法

19
00:00:34,413 --> 00:00:36,986
正规方程法提供了一种求 θ 的解析解法

20
00:00:36,986 --> 00:00:38,761
所以与其使用迭代算法

21
00:00:38,761 --> 00:00:40,594
我们可以直接一次性求解θ的最优值

22
00:00:40,594 --> 00:00:41,365
我们可以直接一次性求解θ的最优值

23
00:00:41,365 --> 00:00:42,791
我们可以直接一次性求解θ的最优值

24
00:00:42,791 --> 00:00:44,403
所以说基本上

25
00:00:44,403 --> 00:00:46,096
一步就可以得到优化值

26
00:00:46,096 --> 00:00:48,136
一步就可以得到优化值

27
00:00:49,136 --> 00:00:51,947
正规方程法有一些优点

28
00:00:52,209 --> 00:00:54,442
也有一些缺点

29
00:00:54,442 --> 00:00:56,024
但是在我们讲解这个

30
00:00:56,024 --> 00:00:57,817
和何时使用标准方程之前

31
00:00:57,903 --> 00:00:59,426
让我们先对这个算法有一个直观的理解

32
00:00:59,426 --> 00:01:02,539
让我们先对这个算法有一个直观的理解

33
00:01:02,539 --> 00:01:04,633
我们举一个例子来解释这个问题

34
00:01:04,633 --> 00:01:06,120
我们假设 有一个非常简单的代价函数 J(θ)

35
00:01:06,120 --> 00:01:07,505
我们假设 有一个非常简单的代价函数 J(θ)

36
00:01:07,505 --> 00:01:09,291
它就是一个实数 θ 的函数

37
00:01:09,291 --> 00:01:11,958
它就是一个实数 θ 的函数

38
00:01:11,958 --> 00:01:13,642
所以现在 假设 θ 只是一个标量

39
00:01:13,842 --> 00:01:16,615
或者说 θ 只有一行

40
00:01:16,769 --> 00:01:18,918
它是一个数字 不是向量

41
00:01:19,171 --> 00:01:24,595
假设我们的代价函数 J 是这个实参数 θ 的二次函数

42
00:01:25,028 --> 00:01:27,420
所以 J(θ) 看起来是这样的

43
00:01:27,851 --> 00:01:30,336
那么如何最小化一个二次函数呢?

44
00:01:30,720 --> 00:01:32,745
对于那些了解一点微积分的同学来说

45
00:01:32,858 --> 00:01:34,965
你可能知道

46
00:01:34,965 --> 00:01:36,628
最小化的一个函数的方法是

47
00:01:36,628 --> 00:01:38,991
对它求导 并且将导数置零

48
00:01:38,991 --> 00:01:41,707
对它求导 并且将导数置零

49
00:01:41,707 --> 00:01:44,721
所以对 J 求关于 θ 的导数

50
00:01:44,797 --> 00:01:46,847
我不打算推导那些公式

51
00:01:46,847 --> 00:01:49,161
你把那个导数置零

52
00:01:49,161 --> 00:01:50,782
这样你就可以求得

53
00:01:50,782 --> 00:01:53,503
使得 J(θ) 最小的 θ 值

54
00:01:53,503 --> 00:01:57,866
使得 J(θ) 最小的 θ 值

55
00:01:57,866 --> 00:01:59,096
这是数据为实数的

56
00:01:59,096 --> 00:02:01,716
一个比较简单的例子

57
00:02:01,716 --> 00:02:04,272
在这个问题中 我们感兴趣的是

58
00:02:04,929 --> 00:02:06,559
θ不是一个实数的情况

59
00:02:06,559 --> 00:02:07,847
它是一个n+1维的参数向量

60
00:02:07,847 --> 00:02:11,986
它是一个n+1维的参数向量

61
00:02:11,986 --> 00:02:13,809
并且 代价函数 J 是这个向量的函数

62
00:02:13,809 --> 00:02:15,742
并且 代价函数 J 是这个向量的函数

63
00:02:15,742 --> 00:02:17,501
也就是 θ0 到 θm 的函数

64
00:02:17,501 --> 00:02:18,924
一个代价函数看起来是这样

65
00:02:18,924 --> 00:02:21,957
像右边的这个平方代价函数

66
00:02:22,373 --> 00:02:25,712
我们如何最小化这个代价函数J?

67
00:02:25,712 --> 00:02:27,163
实际上 微积分告诉我们一种方法

68
00:02:27,163 --> 00:02:29,377
实际上 微积分告诉我们一种方法

69
00:02:29,377 --> 00:02:30,709
对每个参数 θ 求 J 的偏导数

70
00:02:30,709 --> 00:02:38,604
对每个参数 θ 求 J 的偏导数

71
00:02:38,604 --> 00:02:40,271
然后把它们全部置零

72
00:02:40,271 --> 00:02:41,394
如果你这样做

73
00:02:41,394 --> 00:02:42,718
并且求出θ0 θ1 一直到θn的值

74
00:02:42,718 --> 00:02:44,000
并且求出θ0 θ1 一直到θn的值

75
00:02:44,000 --> 00:02:45,973
并且求出θ0 θ1 一直到θn的值

76
00:02:45,973 --> 00:02:47,217
这样就能得到能够最小化代价函数 J 的 θ 值

77
00:02:47,217 --> 00:02:48,765
这样就能得到能够最小化代价函数 J 的 θ 值

78
00:02:48,765 --> 00:02:50,878
这样就能得到能够最小化代价函数 J 的 θ 值

79
00:02:50,878 --> 00:02:52,176
如果你真的做完微积分和求解参数 θ0 到 θn

80
00:02:52,176 --> 00:02:53,597
如果你真的做完微积分和求解参数 θ0 到 θn

81
00:02:53,597 --> 00:02:55,194
如果你真的做完微积分和求解参数 θ0 到 θn

82
00:02:55,194 --> 00:02:57,316
如果你真的做完微积分和求解参数 θ0 到 θn

83
00:02:57,316 --> 00:03:00,520
你会发现这个偏微分最终可能很复杂

84
00:03:00,520 --> 00:03:01,625
接下来我在视频中要做的

85
00:03:01,625 --> 00:03:03,113
接下来我在视频中要做的

86
00:03:03,113 --> 00:03:04,852
实际上不是遍历所有的偏微分

87
00:03:04,852 --> 00:03:06,297
实际上不是遍历所有的偏微分

88
00:03:06,297 --> 00:03:07,657
因为这样太久太费事

89
00:03:07,657 --> 00:03:08,962
我只是想告诉你们

90
00:03:08,962 --> 00:03:10,545
你们想要实现这个过程所需要知道内容

91
00:03:10,545 --> 00:03:12,619
你们想要实现这个过程所需要知道内容

92
00:03:12,619 --> 00:03:14,138
这样你就可以解出

93
00:03:14,138 --> 00:03:15,511
偏导数为0时 θ的值

94
00:03:15,511 --> 00:03:16,892
偏导数为0时 θ的值

95
00:03:16,892 --> 00:03:19,273
偏导数为0时 θ的值

96
00:03:19,273 --> 00:03:21,733
换个方式说 或者等价地

97
00:03:21,733 --> 00:03:23,357
这个 θ 能够使得代价函数 J(θ) 最小化

98
00:03:23,357 --> 00:03:25,901
这个 θ 能够使得代价函数 J(θ) 最小化

99
00:03:25,901 --> 00:03:27,283
我发现可能只有熟悉微积分的同学

100
00:03:27,283 --> 00:03:28,846
我发现可能只有熟悉微积分的同学

101
00:03:28,846 --> 00:03:29,914
比较容易理解我的话

102
00:03:29,914 --> 00:03:31,896
比较容易理解我的话

103
00:03:31,896 --> 00:03:33,065
所以 如果你不了解

104
00:03:33,065 --> 00:03:34,487
或者不那么了解微积分

105
00:03:34,487 --> 00:03:36,354
也不必担心

106
00:03:36,354 --> 00:03:37,404
我会告诉你

107
00:03:37,404 --> 00:03:38,374
要实现这个算法并且使其正常运行

108
00:03:38,374 --> 00:03:41,358
你所需的必要知识

109
00:03:41,358 --> 00:03:42,585
举个例子

110
00:03:42,585 --> 00:03:43,737
我想运行这样一个例子

111
00:03:43,737 --> 00:03:46,339
假如说我有 m=4 个训练样本

112
00:03:46,339 --> 00:03:49,056
假如说我有 m=4 个训练样本

113
00:03:50,409 --> 00:03:52,881
为了实现正规方程法

114
00:03:52,881 --> 00:03:56,515
我要这样做

115
00:03:56,515 --> 00:03:57,640
看我的训练集

116
00:03:57,640 --> 00:04:00,375
在这里就是这四个训练样本

117
00:04:00,375 --> 00:04:01,844
在这种情况下 我们假设

118
00:04:01,844 --> 00:04:06,073
这四个训练样本就是我的所有数据

119
00:04:06,073 --> 00:04:07,890
我所要做的是

120
00:04:07,890 --> 00:04:09,007
在我的训练集中加上一列对应额外特征变量的x0

121
00:04:09,007 --> 00:04:11,289
在我的训练集中加上一列对应额外特征变量的x0

122
00:04:11,289 --> 00:04:14,579
在我的训练集中加上一列对应额外特征变量的x0

123
00:04:14,579 --> 00:04:15,967
就是那个取值永远是1的

124
00:04:15,967 --> 00:04:17,527
就是那个取值永远是1的

125
00:04:17,527 --> 00:04:18,681
接下来我要做的是

126
00:04:18,681 --> 00:04:19,943
构建一个矩阵 X

127
00:04:19,943 --> 00:04:22,638
这个矩阵基本包含了训练样本的所有特征变量

128
00:04:22,638 --> 00:04:24,632
这个矩阵基本包含了训练样本的所有特征变量

129
00:04:24,632 --> 00:04:26,100
这个矩阵基本包含了训练样本的所有特征变量

130
00:04:26,100 --> 00:04:28,140
所以具体地说

131
00:04:28,140 --> 00:04:31,528
这里有我所有的特征变量

132
00:04:31,528 --> 00:04:33,743
这里有我所有的特征变量

133
00:04:33,743 --> 00:04:34,797
我们要把这些数字

134
00:04:34,797 --> 00:04:37,777
全部放到矩阵中 X 中 好吧？

135
00:04:37,777 --> 00:04:39,179
所以只是

136
00:04:39,179 --> 00:04:41,233
每次复制一列的数据

137
00:04:41,233 --> 00:04:45,962
我要对 y 做类似的事情

138
00:04:45,962 --> 00:04:47,087
我要对我们将要预测的值

139
00:04:47,087 --> 00:04:47,952
我要对我们将要预测的值

140
00:04:47,952 --> 00:04:49,360
构建一个向量

141
00:04:49,360 --> 00:04:52,894
像这样的

142
00:04:52,894 --> 00:04:55,440
并且称之为向量 y

143
00:04:55,440 --> 00:04:58,038
所以 X 会是一个 m*(n+1) 维矩阵

144
00:04:59,653 --> 00:05:05,688
所以 X 会是一个 m*(n+1) 维矩阵

145
00:05:05,688 --> 00:05:07,490
y 会是一个 m 维向量

146
00:05:07,490 --> 00:05:14,421
y 会是一个 m 维向量

147
00:05:14,421 --> 00:05:16,624
其中 m 是训练样本数量

148
00:05:16,984 --> 00:05:18,688
n 是特征变量数

149
00:05:18,688 --> 00:05:20,713
n+1 是因为我加的这个额外的特征变量 x0

150
00:05:20,713 --> 00:05:24,825
n+1 是因为我加的这个额外的特征变量 x0

151
00:05:24,825 --> 00:05:26,350
最后 如​​果你用矩阵 X 和向量 y 来计算这个

152
00:05:26,350 --> 00:05:27,489
最后 如​​果你用矩阵 X 和向量 y 来计算这个

153
00:05:27,489 --> 00:05:28,595
最后 如​​果你用矩阵 X 和向量 y 来计算这个

154
00:05:28,595 --> 00:05:31,065
θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y

155
00:05:31,065 --> 00:05:32,419
θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y

156
00:05:32,419 --> 00:05:34,440
X转置 乘以X的逆 乘以X转置 乘以y

157
00:05:34,440 --> 00:05:36,516
θ 等于 X 转置乘以 X 的逆 乘以 X 转置 乘以 y

158
00:05:36,516 --> 00:05:38,583
这样就得到能够使得代价函数最小化的 θ

159
00:05:38,583 --> 00:05:42,559
这样就得到能够使得代价函数最小化的 θ

160
00:05:42,559 --> 00:05:43,436
幻灯片上的内容比较多

161
00:05:43,436 --> 00:05:44,416
幻灯片上的内容比较多

162
00:05:44,416 --> 00:05:47,514
我讲解了这样一个数据组的一个例子

163
00:05:47,514 --> 00:05:49,241
让我把这个写成更加通用的形式

164
00:05:49,333 --> 00:05:50,770
让我把这个写成更加通用的形式

165
00:05:50,955 --> 00:05:53,418
在之后的视频中

166
00:05:53,621 --> 00:05:56,531
我会仔细介绍这个方程

167
00:05:57,581 --> 00:06:00,687
以防你不完全清楚要如何做

168
00:06:00,687 --> 00:06:02,129
在一般情况下

169
00:06:02,129 --> 00:06:04,124
假如我们有 m 个训练样本

170
00:06:04,124 --> 00:06:05,697
x(1) y(1) 直到 x(m) y(m)

171
00:06:05,697 --> 00:06:09,319
n 个特征变量

172
00:06:09,319 --> 00:06:10,811
所以每一个训练样本

173
00:06:10,811 --> 00:06:12,926
xi 可能看起来像一个向量

174
00:06:12,926 --> 00:06:16,297
像这样一个 n+1 维特征向量

175
00:06:16,943 --> 00:06:18,350
我要构建矩阵 X 的方法

176
00:06:18,350 --> 00:06:20,674
我要构建矩阵 X 的方法

177
00:06:20,674 --> 00:06:24,827
也被称为设计矩阵

178
00:06:24,827 --> 00:06:26,712
如下所示

179
00:06:26,712 --> 00:06:28,640
每个训练样本给出一个这样的特征向量

180
00:06:28,640 --> 00:06:30,549
每个训练样本给出一个这样的特征向量

181
00:06:30,549 --> 00:06:34,491
也就是说 这样的 n+1 维向量

182
00:06:34,491 --> 00:06:36,190
我构建我的设计矩阵 X 的方法

183
00:06:36,359 --> 00:06:39,734
就是构建这样的矩阵

184
00:06:39,734 --> 00:06:40,834
接下来我要做的是将

185
00:06:40,834 --> 00:06:42,109
取第一个训练样本

186
00:06:42,109 --> 00:06:43,711
取第一个训练样本

187
00:06:43,711 --> 00:06:46,350
也就是一个向量 取它的转置

188
00:06:46,350 --> 00:06:48,692
它最后是这样

189
00:06:48,692 --> 00:06:50,250
扁长的样子

190
00:06:50,250 --> 00:06:55,153
让 x1 转置作为我设计矩阵的第一行

191
00:06:55,153 --> 00:06:56,225
然后我要把我的

192
00:06:56,225 --> 00:06:58,682
第二个训练样本 x2

193
00:06:58,682 --> 00:07:00,437
进行转置 让它作为 X 的第二行

194
00:07:00,437 --> 00:07:01,838
进行转置 让它作为 X 的第二行

195
00:07:01,838 --> 00:07:04,068
以此类推

196
00:07:04,068 --> 00:07:07,206
直到最后一个训练样本

197
00:07:07,206 --> 00:07:09,279
取它的转置作为矩阵 X 的最后一行

198
00:07:09,279 --> 00:07:10,850
取它的转置作为矩阵 X 的最后一行

199
00:07:10,850 --> 00:07:12,665
取它的转置作为矩阵 X 的最后一行

200
00:07:12,665 --> 00:07:14,418
这样矩阵 X 就是一个 m*(n+1) 维矩阵

201
00:07:14,418 --> 00:07:17,129
这样矩阵 X 就是一个 m*(n+1) 维矩阵

202
00:07:17,129 --> 00:07:19,836
这样矩阵 X 就是一个 m*(n+1) 维矩阵

203
00:07:19,836 --> 00:07:21,953
举个具体的例子

204
00:07:21,953 --> 00:07:23,505
假如我只有一个特征变量

205
00:07:23,505 --> 00:07:24,670
就是说除了 x0 之外只有一个特征变量

206
00:07:24,670 --> 00:07:26,631
就是说除了 x0 之外只有一个特征变量

207
00:07:26,631 --> 00:07:28,165
而 x0 始终为1

208
00:07:28,165 --> 00:07:30,376
所以如果我的特征向量

209
00:07:30,376 --> 00:07:32,186
xi等于1 也就是x0 和某个实际的特征变量

210
00:07:32,186 --> 00:07:33,878
xi等于1 也就是x0 和某个实际的特征变量

211
00:07:33,878 --> 00:07:35,912
xi等于1 也就是x0 和某个实际的特征变量

212
00:07:35,912 --> 00:07:37,662
比如说房屋大小

213
00:07:37,662 --> 00:07:40,947
那么我的设计矩阵 X 会是这样

214
00:07:40,947 --> 00:07:42,589
第一行 就是这个的转置

215
00:07:42,589 --> 00:07:46,071
第一行 就是这个的转置

216
00:07:46,071 --> 00:07:51,644
所以最后得到1 然后 x(1)1

217
00:07:51,644 --> 00:07:53,309
对于第二行 我们得到1 然后 x(1)2

218
00:07:53,309 --> 00:07:56,077
对于第二行 我们得到1 然后 x(1)2

219
00:07:56,077 --> 00:07:58,046
对于第二行 我们得到1 然后 x(1)2

220
00:07:58,046 --> 00:07:59,046
这样直到1 然后 x(1)m

221
00:07:59,046 --> 00:08:01,420
这样直到1 然后 x(1)m

222
00:08:01,420 --> 00:08:03,084
这样 这就会是一个 m*2 维矩阵

223
00:08:03,084 --> 00:08:07,776
这样 这就会是一个 m*2 维矩阵

224
00:08:07,776 --> 00:08:08,821
所以 这就是如何构建矩阵X 和向量y

225
00:08:08,821 --> 00:08:11,251
所以 这就是如何构建矩阵X 和向量y

226
00:08:11,251 --> 00:08:13,886
有时我可能会在上面画一个箭头

227
00:08:13,886 --> 00:08:15,487
有时我可能会在上面画一个箭头

228
00:08:15,487 --> 00:08:16,541
来表示这是一个向量

229
00:08:16,541 --> 00:08:19,871
但很多时候 我就只写y 是一样的

230
00:08:19,871 --> 00:08:21,182
向量y 是这样求得的

231
00:08:21,182 --> 00:08:23,275
把所有标签

232
00:08:23,275 --> 00:08:25,098
所有训练集中正确的房子价格

233
00:08:25,098 --> 00:08:27,076
所有训练集中正确的房子价格

234
00:08:27,076 --> 00:08:28,963
放在一起 得到一个 m 维向量 y

235
00:08:28,963 --> 00:08:32,011
放在一起 得到一个 m 维向量 y

236
00:08:32,011 --> 00:08:34,511
最后 构建完矩阵 X 和向量 y

237
00:08:34,511 --> 00:08:36,724
最后 构建完矩阵 X 和向量 y

238
00:08:36,724 --> 00:08:38,184
我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ

239
00:08:38,184 --> 00:08:40,887
我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ

240
00:08:40,887 --> 00:08:47,243
我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到 θ

241
00:08:47,243 --> 00:08:49,356
我现在就想确保你明白这个等式

242
00:08:49,356 --> 00:08:51,348
我现在就想确保你明白这个等式

243
00:08:51,348 --> 00:08:52,242
并且知道如何实现它

244
00:08:52,242 --> 00:08:55,221
所以具体来说 什么是 X 的转置乘以 X 的逆？

245
00:08:55,221 --> 00:08:57,903
X的转置 乘以X的逆 是 X转置 乘以X的逆矩阵

246
00:08:57,903 --> 00:09:02,101
X的转置 乘以X的逆 是 X转置 乘以X的逆矩阵

247
00:09:02,101 --> 00:09:04,498
具体来说

248
00:09:04,498 --> 00:09:08,055
如果你令A等于 X转置乘以X

249
00:09:08,055 --> 00:09:11,120
如果你令A等于 X转置乘以X

250
00:09:11,120 --> 00:09:12,542
X的转置是一个矩阵

251
00:09:12,542 --> 00:09:14,063
X的转置乘以X 是另一个矩阵

252
00:09:14,063 --> 00:09:15,305
X的转置乘以X 是另一个矩阵

253
00:09:15,305 --> 00:09:17,560
我们把这个矩阵称为 A

254
00:09:17,560 --> 00:09:19,968
那么 X转置乘以X的逆 就是矩阵 A 的逆

255
00:09:19,968 --> 00:09:22,352
那么 X转置乘以X的逆 就是矩阵 A 的逆

256
00:09:23,245 --> 00:09:24,417
也就是 1/A

257
00:09:26,025 --> 00:09:28,919
这就是计算过程

258
00:09:28,919 --> 00:09:31,451
先计算 X转置乘以X 然后计算它的逆

259
00:09:31,451 --> 00:09:34,296
我们还没有谈到Octave

260
00:09:34,296 --> 00:09:35,941
我们将在之后的视频中谈到这个

261
00:09:35,941 --> 00:09:37,211
但是在 Octave 编程语言

262
00:09:37,211 --> 00:09:39,073
但是在 Octave 编程语言

263
00:09:39,073 --> 00:09:40,652
或者类似的 MATLAB 编程语言里

264
00:09:40,652 --> 00:09:42,957
或者类似的 MATLAB编程语言里

265
00:09:42,957 --> 00:09:46,937
计算这个量的命令是基本相同的

266
00:09:47,384 --> 00:09:50,326
X转置 乘以X的逆 乘以X转置 乘以y

267
00:09:50,326 --> 00:09:52,537
的代码命令如下所示

268
00:09:52,537 --> 00:09:54,903
在 Octave 中 X’ 表示 X 转置

269
00:09:54,903 --> 00:09:58,354
在 Octave 中 X’ 表示 X 转置

270
00:09:58,354 --> 00:10:00,737
这个用红色框起来的表达式

271
00:10:00,737 --> 00:10:03,588
计算的是 X 转置乘以 X

272
00:10:03,588 --> 00:10:06,633
计算的是 X 转置乘以 X

273
00:10:06,633 --> 00:10:08,551
pinv 是用来计算逆矩阵的函数

274
00:10:08,551 --> 00:10:09,701
pinv 是用来计算逆矩阵的函数

275
00:10:09,701 --> 00:10:11,818
所以这个计算 X转置 乘以X的逆

276
00:10:11,818 --> 00:10:14,656
所以这个计算 X转置 乘以X的逆

277
00:10:14,656 --> 00:10:16,453
然后乘以X转置 再乘以y

278
00:10:16,453 --> 00:10:18,267
然后乘以X转置 再乘以y

279
00:10:18,267 --> 00:10:19,712
然后乘以X转置 再乘以y

280
00:10:19,712 --> 00:10:22,325
这样就算完了这个式子

281
00:10:22,325 --> 00:10:24,369
我没有证明这个式子

282
00:10:24,369 --> 00:10:25,994
尽管我并不打算这么做

283
00:10:25,994 --> 00:10:27,382
但是数学上是可以证明的

284
00:10:27,382 --> 00:10:28,537
这个式子会给出最优的 θ 值

285
00:10:28,537 --> 00:10:31,071
这个式子会给出最优的 θ 值

286
00:10:31,071 --> 00:10:32,316
这个式子会给出最优的 θ 值

287
00:10:32,316 --> 00:10:34,865
就是说如果你令 θ 等于这个

288
00:10:34,865 --> 00:10:36,512
就是说如果你令 θ 等于这个

289
00:10:36,512 --> 00:10:38,000
这个 θ 值会最小化这个线性回归的代价函数 J(θ)

290
00:10:38,000 --> 00:10:40,169
这个 θ 值会最小化这个线性回归的代价函数 J(θ)

291
00:10:40,169 --> 00:10:41,993
这个 θ 值会最小化这个线性回归的代价函数 J(θ)

292
00:10:41,993 --> 00:10:44,530
最后一点

293
00:10:44,530 --> 00:10:46,131
在之前视频中我提到特征变量归一化

294
00:10:46,131 --> 00:10:47,061
在之前视频中我提到特征变量归一化

295
00:10:47,061 --> 00:10:48,878
和让特征变量在相似的范围内的想法

296
00:10:48,878 --> 00:10:50,726
和让特征变量在相似的范围内的想法

297
00:10:50,726 --> 00:10:54,900
将所有的值归一化在类似范围内

298
00:10:54,900 --> 00:10:56,872
如果你使用正规方程法

299
00:10:56,872 --> 00:10:59,843
那么就不需要归一化特征变量

300
00:10:59,843 --> 00:11:02,315
那么就不需要归一化特征变量

301
00:11:02,315 --> 00:11:04,361
实际上这是没问题的

302
00:11:04,361 --> 00:11:06,094
如果某个特征变量 x1 在 0到1的区间

303
00:11:06,094 --> 00:11:07,552
如果某个特征变量 x1 在 0到1的区间

304
00:11:07,552 --> 00:11:08,846
某个特征变量 x2 在0到1000的区间

305
00:11:08,846 --> 00:11:10,550
某个特征变量 x2 在0到1000的区间

306
00:11:10,550 --> 00:11:12,019
某个特征变量 x2 在0到1000的区间

307
00:11:12,019 --> 00:11:14,159
某个特征变量x3 在0到10^-5的区间

308
00:11:14,159 --> 00:11:15,822
某个特征变量x3 在0到10^-5的区间

309
00:11:15,822 --> 00:11:17,263
某个特征变量x3 在0到10^-5的区间

310
00:11:17,263 --> 00:11:18,321
然后如果使用正规方程法

311
00:11:18,321 --> 00:11:20,296
这样就没有问题

312
00:11:20,296 --> 00:11:21,550
不需要做特征变量归一化

313
00:11:21,550 --> 00:11:22,740
但如果你使用梯度下降法

314
00:11:22,740 --> 00:11:25,667
但如果你使用梯度下降法

315
00:11:25,667 --> 00:11:27,814
特征变量归一化就很重要

316
00:11:28,030 --> 00:11:31,020
最后 你何时应该使用梯度下降法

317
00:11:31,020 --> 00:11:33,273
而何时应该使用正规方程法呢？

318
00:11:33,273 --> 00:11:35,800
这里列举了一些它们的优点和缺点

319
00:11:35,800 --> 00:11:38,305
假如你有 m 个训练样本和 n 个特征变量

320
00:11:38,305 --> 00:11:40,918
假如你有 m 个训练样本和 n 个特征变量

321
00:11:40,918 --> 00:11:42,854
梯度下降法的缺点之一就是

322
00:11:42,854 --> 00:11:46,015
你需要选择学习速率 α

323
00:11:46,015 --> 00:11:47,374
这通常表示需要运行多次 尝试不同的学习速率 α

324
00:11:47,374 --> 00:11:49,128
这通常表示需要运行多次 尝试不同的学习速率 α

325
00:11:49,128 --> 00:11:51,154
然后找到运行效果最好的那个

326
00:11:51,154 --> 00:11:54,274
所以这是一种额外的工作和麻烦

327
00:11:54,274 --> 00:11:55,976
梯度下降法的另一个缺点是

328
00:11:55,976 --> 00:11:57,841
它需要更多次的迭代

329
00:11:57,841 --> 00:11:59,346
因为一些细节 计算可能会更慢

330
00:11:59,346 --> 00:12:00,839
因为一些细节 计算可能会更慢

331
00:12:00,839 --> 00:12:04,391
我们一会儿会看到更多的东西

332
00:12:04,391 --> 00:12:07,544
至于正规方程 你不需要选择学习速率 α

333
00:12:07,821 --> 00:12:11,208
所以就非常方便 也容易实现

334
00:12:11,208 --> 00:12:13,888
你只要运行一下 通常这就够了

335
00:12:13,888 --> 00:12:15,061
并且你也不需要迭代

336
00:12:15,061 --> 00:12:16,129
所以不需要画出 J(θ) 的曲线

337
00:12:16,129 --> 00:12:17,456
所以不需要画出 J(θ 的曲线

338
00:12:17,456 --> 00:12:20,497
来检查收敛性或者采取所有的额外步骤

339
00:12:20,497 --> 00:12:21,931
到目前为止

340
00:12:21,931 --> 00:12:23,846
天平似乎倾向于正规方程法

341
00:12:24,826 --> 00:12:27,085
这里列举一些正规方程法的缺点

342
00:12:27,612 --> 00:12:29,435
和梯度下降法的优点

343
00:12:29,681 --> 00:12:31,447
梯度下降法在有很多特征变量的情况下也能运行地相当好

344
00:12:31,928 --> 00:12:34,698
梯度下降法在有很多特征变量的情况下也能运行地相当好

345
00:12:34,698 --> 00:12:36,168
所以即使你有上百万的特征变量

346
00:12:36,168 --> 00:12:37,812
所以即使你有上百万的特征变量

347
00:12:37,812 --> 00:12:40,865
你可以运行梯度下降法 并且通常很有效

348
00:12:40,865 --> 00:12:43,381
它会正常的运行

349
00:12:43,381 --> 00:12:46,566
相对地 正规方程法

350
00:12:46,566 --> 00:12:48,014
为了求解参数θ 需要求解这一项

351
00:12:48,014 --> 00:12:50,394
为了求解参数θ 需要求解这一项

352
00:12:50,394 --> 00:12:53,058
我们需要计算这项 X转置乘以X的逆

353
00:12:53,058 --> 00:12:56,328
这个 X转置乘以X矩阵 是一个 n*n 的矩阵

354
00:12:56,328 --> 00:13:00,206
如果你有 n 个特征变量的话

355
00:13:00,770 --> 00:13:02,947
因为如果你看一下 X转置乘以X 的维度

356
00:13:02,947 --> 00:13:03,917
因为如果你看一下 X转置乘以X 的维度

357
00:13:03,917 --> 00:13:05,529
因为如果你看一下 X转置乘以X 的维度

358
00:13:05,529 --> 00:13:07,024
你可以发现他们的积的维度

359
00:13:07,024 --> 00:13:08,749
你可以发现他们的积的维度

360
00:13:08,749 --> 00:13:10,983
X转置乘以X 是一个 n*n 的矩阵

361
00:13:10,983 --> 00:13:13,727
X转置乘以X 是一个 n*n 的矩阵

362
00:13:13,727 --> 00:13:15,853
其中 n是特征变量的数量

363
00:13:15,853 --> 00:13:18,641
实现逆矩阵计算所需要的计算量

364
00:13:18,641 --> 00:13:20,990
实现逆矩阵计算所需要的计算量

365
00:13:20,990 --> 00:13:23,087
大致是矩阵维度的三次方

366
00:13:23,087 --> 00:13:25,707
大致是矩阵维度的三次方

367
00:13:25,707 --> 00:13:28,180
因此计算这个逆矩阵需要计算大致 n 的三次方

368
00:13:28,180 --> 00:13:29,964
因此计算这个逆矩阵需要计算大致 n 的三次方

369
00:13:29,964 --> 00:13:31,213
有时稍微比计算 n 的三次方快一些

370
00:13:31,213 --> 00:13:35,050
但是对我们来说很接近

371
00:13:35,489 --> 00:13:36,605
所以如果特征变量的数量 n 很大的话

372
00:13:37,643 --> 00:13:39,025
那么计算这个量会很慢

373
00:13:39,025 --> 00:13:40,570
那么计算这个量会很慢

374
00:13:40,570 --> 00:13:44,289
实际上标准方程法会慢很多

375
00:13:44,289 --> 00:13:45,491
因此如果 n 很大

376
00:13:45,491 --> 00:13:47,622
因此如果 n 很大

377
00:13:47,622 --> 00:13:49,490
我可能还是会使用梯度下降法

378
00:13:49,490 --> 00:13:51,872
因为我们不想花费 n 的三次方的时间

379
00:13:51,872 --> 00:13:53,525
但如果 n 比较小

380
00:13:53,525 --> 00:13:57,395
那么标准方程法可能更好地求解参数 θ

381
00:13:57,395 --> 00:13:59,080
那么怎么叫大或者小呢？

382
00:13:59,080 --> 00:14:00,741
那么 如果 n 是上百的

383
00:14:00,741 --> 00:14:02,130
那么 如果 n 是上百的

384
00:14:02,130 --> 00:14:03,822
计算百位数乘百位数的矩阵

385
00:14:03,822 --> 00:14:06,539
对于现代计算机来说没有问题

386
00:14:06,539 --> 00:14:10,966
如果 n 是上千的 我还是会使用正规方程法

387
00:14:10,966 --> 00:14:12,583
千位数乘千位数的矩阵做逆变换

388
00:14:12,583 --> 00:14:15,408
对于现代计算机来说实际上是非常快的

389
00:14:15,408 --> 00:14:18,406
但如果 n 上万 那么我可能会开始犹豫

390
00:14:18,406 --> 00:14:20,618
上万乘上万维的矩阵作逆变换

391
00:14:20,618 --> 00:14:22,208
会开始有点慢

392
00:14:22,208 --> 00:14:23,471
此时我可能开始倾向于

393
00:14:23,471 --> 00:14:25,007
此时我可能开始倾向于

394
00:14:25,007 --> 00:14:27,007
梯度下降法 但也不绝对

395
00:14:27,114 --> 00:14:28,672
n 等于一万 你可以

396
00:14:28,672 --> 00:14:31,148
逆变换一个一万乘一万的矩阵

397
00:14:31,148 --> 00:14:34,345
但如果 n 远大于此 我可能就会使用梯度下降法了

398
00:14:34,345 --> 00:14:35,834
所以如果 n 等于10^6

399
00:14:35,834 --> 00:14:36,920
有一百万个特征变量

400
00:14:36,920 --> 00:14:38,963
那么做百万乘百万的矩阵的逆变换

401
00:14:38,963 --> 00:14:41,565
那么做百万乘百万的矩阵的逆变换

402
00:14:41,565 --> 00:14:42,631
就会变得非常费时间

403
00:14:42,631 --> 00:14:46,163
在这种情况下我一定会使用梯度下降法

404
00:14:46,163 --> 00:14:47,859
所以很难给出一个确定的值

405
00:14:47,859 --> 00:14:49,282
来决定何时该换成梯度下降法

406
00:14:49,282 --> 00:14:52,655
来决定何时该换成梯度下降法

407
00:14:52,655 --> 00:14:53,855
但是 对我来说通常是

408
00:14:53,855 --> 00:14:55,501
在一万左右 我会开始考虑换成梯度下降法

409
00:14:55,501 --> 00:14:58,258
在一万左右 我会开始考虑换成梯度下降法

410
00:14:58,335 --> 00:15:00,663
在一万左右 我会开始考虑换成梯度下降法

411
00:15:00,663 --> 00:15:04,324
或者我们将在以后讨论到的其他算法

412
00:15:04,324 --> 00:15:05,765
总结一下

413
00:15:05,765 --> 00:15:06,999
只要特征变量的数目并不大

414
00:15:06,999 --> 00:15:08,475
正规方程是一个很好的

415
00:15:08,475 --> 00:15:12,229
计算参数 θ 的替代方法

416
00:15:12,583 --> 00:15:13,983
具体地说 只要特征变量数量小于一万

417
00:15:13,983 --> 00:15:15,749
具体地说 只要特征变量数量小于一万

418
00:15:15,749 --> 00:15:17,472
我通常使用正规方程法

419
00:15:17,472 --> 00:15:18,881
我通常使用正规方程法

420
00:15:18,881 --> 00:15:21,955
而不使用梯度下降法

421
00:15:21,955 --> 00:15:23,549
预告一下在之后的课程中我们要讲的

422
00:15:23,549 --> 00:15:24,493
预告一下在之后的课程中我们要讲的

423
00:15:24,493 --> 00:15:26,235
随着我们要讲的学习算法越来越复杂

424
00:15:26,235 --> 00:15:27,912
随着我们要讲的学习算法越来越复杂

425
00:15:27,912 --> 00:15:29,617
例如 当我们讲到分类算法

426
00:15:29,617 --> 00:15:32,188
像逻辑回归算法

427
00:15:32,834 --> 00:15:34,319
我们会看到

428
00:15:34,319 --> 00:15:35,467
实际上对于那些算法

429
00:15:35,467 --> 00:15:37,592
并不能使用正规方程法

430
00:15:37,592 --> 00:15:39,388
对于那些更复杂的学习算法

431
00:15:39,388 --> 00:15:41,190
我们将不得不仍然使用梯度下降法

432
00:15:41,190 --> 00:15:43,916
我们将不得不仍然使用梯度下降法

433
00:15:43,916 --> 00:15:46,682
因此 梯度下降法是一个非常有用的算法

434
00:15:46,682 --> 00:15:48,859
可以用在有大量特征变量的线性回归问题

435
00:15:48,982 --> 00:15:50,017
可以用在有大量特征变量的线性回归问题

436
00:15:50,017 --> 00:15:52,373
或者我们以后在课程中

437
00:15:52,373 --> 00:15:53,893
会讲到的一些其他的算法

438
00:15:53,893 --> 00:15:55,438
因为 标准方程法不适合或者不能用在它们上

439
00:15:55,438 --> 00:15:58,747
因为 标准方程法不适合或者不能用在它们上

440
00:15:58,747 --> 00:16:00,537
但对于这个特定的线性回归模型

441
00:16:00,537 --> 00:16:02,904
但对于这个特定的线性回归模型

442
00:16:02,904 --> 00:16:05,827
正规方程法是一个

443
00:16:07,219 --> 00:16:08,612
比梯度下降法更快的替代算法

444
00:16:09,604 --> 00:16:11,920
所以 根据具体的问题

445
00:16:12,007 --> 00:16:14,164
所以 根据具体的问题

446
00:16:14,164 --> 00:16:15,550
以及你的特征变量的数量

447
00:16:15,550 --> 00:16:19,550
这两算法都是值得学习的 【果壳教育无边界字幕组】翻译：竹二个 校对：所罗门捷列夫 审核：Naplesssss