En este video, quiero hablar de la ecuación normal y la no invertibilidad. Este es un concepto algo más avanzado, pero es algo sobre lo que muchas veces me han preguntado. Y por eso quería hablar de eso aquí. Pero este es un concepto algo más avanzado, así que puedes considerar esto como material opcional. Hay un fenómeno con el que puedes tropezarte y que tal vez para algunos de ustedes sea útil comprender. Pero incluso si no lo entiendes la ecuación normal y la regresión lineal, realmente deberías conseguir que funcione bien. Aquí está el problema: Para aquellos de ustedes que tal vez están más familiarizados con el álgebra lineal, lo que algunos estudiantes me han preguntado es, al calcular esto «theta» es igual a (X transpuesta X) X inversa transpuesta  y</u></u></u> ¿y si la matriz X transpuesta X es no invertible?</u> Así que, aquellos que saben un poco más de álgebra lineal tal vez saben que solo algunas matrices son invertibles y que algunas matrices no tienen una inversa, a esas les llamamos matrices no invertibles, matrices singulares o degeneradas. El tema o el problema de que X traspone X sea no invertible</u> debería ocurrir muy raramente. Y en Octave, si se implementa esto para calcular «theta», resulta que en realidad hará lo correcto. Me estoy poniendo un poco técnico ahora y no quiero entrar en detalles, pero Octave tiene dos funciones para invertir matrices: Una se llama pinv(), y la otra se llama inv(). Las diferencias entre ambas son algo un poco técnico. Una se llama la pseudoinversa, la otra se llama la inversa. Puedes demostrarlo matemáticamente mientras se utiliza la función pinv(), entonces esto va a calcular el valor de «theta2 que deseas, incluso si (X transpuesta X) es no invertible.</u> Los detalles específicos sobre cuáles son las diferencias entre pinv() y lo que es inv() son conceptos de cálculo numérico algo avanzados, a los que realmente no quiero entrar. Pero creo que en este video opcional trato de darte un poco de intuición sobre lo que significa para X transpuesta X ser no invertible.</u> Para quienes saben un poco más de álgebra lineal y que pueden estar interesados. No voy a probar esto matemáticamente, pero si X transpuesta X es no invertible,</u> hay por lo general dos causas comunes: La primera causa es que si de alguna manera, en tu problema de aprendizaje, tienes características redundantes, específicamente, si intentas predecir los precios de la vivienda y si x1 es el tamaño de una casa en pies cuadrados,</u> y x2 es el tamaño de la casa en metros cuadrados,</u> entonces, como sabes, 1 metro es igual a 3.28 pies, redondeando a dos decimales, y así tus dos características siempre satisfacerán la restricción de que x1 es igual a (3.28)^2 multiplicado por x2.</u></u> Y puedes demostrar, para algunos de ustedes - esto es álgebra lineal un poco avanzada-, ahora, pero si eres experto en álgebra lineal, en realidad puedes demostrar que si tus dos características están relacionadas mediante una ecuación lineal como esta, la matriz X transpuesta X será no invertible.</u> Lo segundo que puede causar que X transpuesta X sea no invertible</u> es si estás tratando de ejecutar un algoritmo de aprendizaje con un lote de características. Específicamente, si "m" es menor que o igual a "n". Por ejemplo, si imaginas que tienes "m" igual a 10 ejemplos de entrenamiento y que tienes "n" igual a 100 características, entonces estás tratando de ajustar un parámetro del vector «theta», el cual es de dimensión (n+1), así que si es de dimensiones uno a uno estás intentando ajustar parámetros uno a uno para sólo 10 ejemplos de entrenamiento. Y esto resulta funcionar a veces, pero no siempre es una buena idea. Porque, como veremos más adelante, puedes no tener  datos suficientes si sólo tienes 10 ejemplos para ajustar 100 o 101 parámetros. Veremos más adelante en este curso, que estos pueden ser muy pocos datos para ajustar tantos parámetros. Pero por lo general, lo que hacemos entonces si "m" es menor que "n", es ver si podemos eliminar algunas de las características o bien usar una técnica llamada regularización, que es algo de lo que también vamos a hablar un poco más adelante en este curso, que te permitirá ajustar un lote de parámetros utilizando un lote de características incluso si tienes un conjunto de entrenamiento relativamente pequeño. Pero esta regularización será un tema más adelante en este curso. Para resumir, si alguna vez te encuentras con que X transpuesta X es singular</u> o alternativamente es no invertible, lo que recomiendo que hagas es primero: observar tus características y ver si tienes características redundantes como estas x1 y x2 siendo linealmente dependientes,</u></u> o siendo una función lineal una de la otra, así y si tienes características redundantes y si sólo eliminas una de estas características realmente no necesitas ambas características, así que si simplemente eliminas una de estas características eso resolverá tu problema de no invertibilidad así es que, primero analizo mis características y compruebo si alguna es redundante y si las hay, entonces, como sabes, sigo eliminando las características redundantes hasta que ya no son redundantes. Y si tus características no son redundantes, comprobaría si podría tener demasiadas características, y si ese es el caso, ya sea que elimine algunas características si puedo simplemente utilizar menos características, o de lo contrario consideraría usar la regularización, que es el tema del que vamos a hablar más adelante. Por lo tanto, eso es todo sobre la ecuación normal y lo que significa que la matriz X transpuesta X sea no invertible.</u> Pero esto es un problema que espero que te encuentres muy rara vez. Y si solo lo implementas en Octave utilizando la función pinv() que se llama la función pseudoinversa por lo que utilizas una librería de álgebra lineal diferente, eso se llama pseudoinversa, pero esa implementación debe hacer solo lo correcto incluso si X transpuesta X no es invertible</u> lo que de todos modos debería suceder muy rara vez por lo que esto no debería ser un problema para la mayoría de las implementaciones de regresión lineal.