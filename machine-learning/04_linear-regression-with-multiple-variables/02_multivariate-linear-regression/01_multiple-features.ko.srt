1
00:00:00,150 --> 00:00:01,160
이번 영상에서는

2
00:00:01,520 --> 00:00:02,600
하나 이상의 변수나

3
00:00:03,250 --> 00:00:04,880
features를 다루기 위한

4
00:00:05,800 --> 00:00:07,230
더 강력한

5
00:00:08,230 --> 00:00:09,070
선형 회귀에 대해 알아보겠습니다.

6
00:00:10,320 --> 00:00:10,860
여기 보시죠.

7
00:00:12,200 --> 00:00:13,670
우리가 앞서 배운

8
00:00:13,900 --> 00:00:14,920
선형 회귀에서는

9
00:00:15,780 --> 00:00:17,590
단일 feature를 가진 x가 있었죠.

10
00:00:18,030 --> 00:00:19,450
그 x로 집의 크기를 나타냈구요.

11
00:00:19,600 --> 00:00:20,650
그리고 이걸

12
00:00:20,760 --> 00:00:22,510
집의 가격을 결정하는

13
00:00:22,660 --> 00:00:24,210
요소로 썼고,

14
00:00:25,310 --> 00:00:26,590
이 공식을 추론- hypothesis 이라 배웠습니다.

15
00:00:28,540 --> 00:00:29,210
그런데 생각해보세요.

16
00:00:29,410 --> 00:00:30,580
집의 가격을 결정하는

17
00:00:31,020 --> 00:00:32,440
요소 - feature 또는

18
00:00:33,140 --> 00:00:34,450
variable로

19
00:00:34,600 --> 00:00:35,490
여태까지는

20
00:00:36,450 --> 00:00:38,270
‘집의 크기'만 갖고 있었죠.

21
00:00:38,410 --> 00:00:39,710
그런데 우리가 '침실의 개수'나

22
00:00:39,990 --> 00:00:42,490
집이 지어진 지 얼마나 오래되었는 지도

23
00:00:43,180 --> 00:00:44,050
알고 있다면

24
00:00:44,230 --> 00:00:46,630
가격을 결정하는 
더 많은 요소를 갖고 있는 것이겠지요.

25
00:00:47,810 --> 00:00:49,130
먼저 notation - 변수명 지정을

26
00:00:49,290 --> 00:00:50,760
먼저 얘기해보도록 하겠습니다.

27
00:00:50,940 --> 00:00:51,910
이건 앞서 잠시 얘기 한 적이 있었어요.

28
00:00:52,900 --> 00:00:53,800
이 변수들은

29
00:00:54,560 --> 00:00:56,300
X1,

30
00:00:56,880 --> 00:00:59,320
X2 … 처럼

31
00:00:59,480 --> 00:01:00,780
X에 숫자를 붙여서

32
00:01:00,960 --> 00:01:03,000
변수로 사용하도록 하겠습니다.

33
00:01:03,310 --> 00:01:04,500
총 네 개네요.

34
00:01:04,850 --> 00:01:06,780
그리고 결과값인 '집의 가격'은

35
00:01:07,370 --> 00:01:09,720
Y로 지정하도록 합니다.

36
00:01:11,010 --> 00:01:12,600
notation을 좀 더 보도록 하죠.

37
00:01:13,850 --> 00:01:15,210
우리는 네 개의 features가 있네요.

38
00:01:16,560 --> 00:01:18,490
Features의 개수를 나타내는 변수는

39
00:01:19,540 --> 00:01:20,670
소문자 n을 사용하겠습니다.

40
00:01:21,180 --> 00:01:22,460
그래서 이 예제에서

41
00:01:23,030 --> 00:01:24,420
우리는 총 n4가 있겠네요.

42
00:01:24,820 --> 00:01:27,610
총 4개의 feature가 있으니.

43
00:01:28,850 --> 00:01:30,880
그리고 n은 우리가 앞서 봐온

44
00:01:31,700 --> 00:01:33,280
'M' 변수와는 달라요.

45
00:01:33,570 --> 00:01:36,670
예제의 개수를 나타냅니다.

46
00:01:37,330 --> 00:01:38,640
그래서

47
00:01:39,050 --> 00:01:41,070
47행의 "M"은 테이블의 행

48
00:01:41,300 --> 00:01:43,580
또는 training 예제의 개수를 
나타내는 것으로 보시면 됩니다.

49
00:01:45,480 --> 00:01:47,290
그리고 또

50
00:01:47,500 --> 00:01:48,910
이제 X i의

51
00:01:49,540 --> 00:01:51,050
i는 training example의

52
00:01:51,260 --> 00:01:53,460
입력 feature로 사용할 것입니다.

53
00:01:55,190 --> 00:01:58,100
예제를 좀 더 구체화 하기 위해

54
00:01:58,720 --> 00:02:00,580
x2가 두 번째 트레이닝 예제 - training example feature의

55
00:02:00,710 --> 00:02:02,300
vector가 될 거에요.

56
00:02:02,550 --> 00:02:05,690
그러면

57
00:02:06,430 --> 00:02:08,020
x2는

58
00:02:08,160 --> 00:02:09,260
1416,

59
00:02:09,520 --> 00:02:10,560
3, 2, 40 이 되겠죠.

60
00:02:11,060 --> 00:02:14,110
여기에 있는

61
00:02:14,410 --> 00:02:16,100
네 개의 feature들이

62
00:02:17,500 --> 00:02:19,410
두 번째 집의 가격 결정을 짓는 요소이니까요.

63
00:02:20,990 --> 00:02:22,470
그래서 여기 변수명을 보면,

64
00:02:24,200 --> 00:02:25,250
x2로 지정되어 있지요.

65
00:02:26,720 --> 00:02:28,620
이건 제 training set의 index에요.

66
00:02:28,990 --> 00:02:31,630
이건 x의 2승이 아닙니다.

67
00:02:32,010 --> 00:02:33,150
index는 이 테이블의 두 번째 줄을 보시죠.

68
00:02:33,370 --> 00:02:36,430
이것은 두 번째

69
00:02:36,960 --> 00:02:38,260
training example을 참조하고 있는 게 보이시죠.

70
00:02:39,280 --> 00:02:41,780
X2랑 같이요.

71
00:02:42,140 --> 00:02:43,890
X2는 4차원 벡터예요.

72
00:02:44,400 --> 00:02:45,760
사실상, 더 일반적으로,

73
00:02:45,930 --> 00:02:48,630
이건 in-dimentional feature*에요.

74
00:02:51,030 --> 00:02:52,730
이 notation으로, x2는

75
00:02:53,290 --> 00:02:55,320
이제 벡터가 된 것입니다.

76
00:02:55,770 --> 00:02:58,300
xi 첨자 값으로

77
00:02:58,790 --> 00:03:00,030
j를 이용해서

78
00:03:00,550 --> 00:03:01,740
j값을 나타내겠습니다.

79
00:03:02,850 --> 00:03:04,420
여기서 j는 training 예제의

80
00:03:05,170 --> 00:03:06,360
feature 개수를 나타냅니다.

81
00:03:07,950 --> 00:03:11,490
그래서, 구체적으로 x2의 첨자 3은

82
00:03:11,920 --> 00:03:14,130
여기 x factor의 feature 의

83
00:03:14,420 --> 00:03:15,800
수 3을 의미하고

84
00:03:15,930 --> 00:03:17,670
이건 2와 같죠. 맞나요?

85
00:03:18,300 --> 00:03:20,360
여기에 3이 있었죠, 
잠시 필기 좀 고치겠습니다.

86
00:03:20,860 --> 00:03:23,810
그래서 x2 첨자의 3이 2와 같아집니다.

87
00:03:26,810 --> 00:03:28,010
자, 우리는 이제 여러 개의 features 가 있네요.

88
00:03:29,110 --> 00:03:30,390
그럼 이제 추론 - hypothesis이

89
00:03:30,470 --> 00:03:32,360
어떤 모습으로 형성되야 할지 한 번 살펴봅시다.

90
00:03:33,220 --> 00:03:34,790
앞서 봤던 추론의 형태는,

91
00:03:34,860 --> 00:03:36,650
x가 단일 feature였죠.

92
00:03:37,250 --> 00:03:39,280
하지만 이제 우리는

93
00:03:39,440 --> 00:03:40,450
여러 개의 features가 있으니까

94
00:03:41,010 --> 00:03:43,350
단순하게 표현하지 않을 겁니다.

95
00:03:44,460 --> 00:03:46,040
이제 이게

96
00:03:46,630 --> 00:03:48,140
선형 회귀 추론의 형태를

97
00:03:49,380 --> 00:03:50,630
하고 있는 걸 볼 수 있습니다.

98
00:03:50,820 --> 00:03:52,190
이거는 θ 0 +

99
00:03:52,440 --> 00:03:55,690
θ 0 + θ 1 x1 + θ 2 x2

100
00:03:55,840 --> 00:03:57,320
θ 0 + θ 1 x1 + θ 2 x2 + θ 3 x3

101
00:03:58,610 --> 00:04:00,140
θ 0 + θ 1 x1 + θ 2 x2 + θ 3 x3 + θ 4 x4 에요.

102
00:04:00,910 --> 00:04:02,610
그리고 우리가 4개features가 아닌,

103
00:04:02,860 --> 00:04:04,110
n개의 features가 있다면

104
00:04:04,340 --> 00:04:05,380
n 개의 feature를

105
00:04:05,570 --> 00:04:07,050
더해야 하겠지요.

106
00:04:08,570 --> 00:04:10,270
다시 구체적으로

107
00:04:11,480 --> 00:04:12,880
이 parameters의

108
00:04:13,010 --> 00:04:15,500
특정 setting을 보겠습니다.

109
00:04:17,370 --> 00:04:18,990
X 80 + 0.1 X1 + 0.01x2 + 3x3 - 2x4.

110
00:04:19,160 --> 00:04:23,070
우리는 h가 있었죠.

111
00:04:25,710 --> 00:04:27,060
이건 추론 - hypothesis의

112
00:04:27,700 --> 00:04:29,170
한 예로 볼 수 있어요.

113
00:04:29,760 --> 00:04:30,710
그리고 추론은 가령

114
00:04:31,100 --> 00:04:32,020
몇 천 달러가 나가는 집의 가격을

115
00:04:32,360 --> 00:04:33,910
예측하는 거죠.

116
00:04:34,250 --> 00:04:35,020
아시다시피,

117
00:04:35,360 --> 00:04:37,270
집의 기본 가격은

118
00:04:37,470 --> 00:04:39,960
80,000 + @ 일거에요.

119
00:04:40,690 --> 00:04:41,960
그러니 이건 추가적으로, 1평 당

120
00:04:42,460 --> 00:04:43,680
1평 당 몇 백 달러

121
00:04:44,430 --> 00:04:45,710
1평 당 몇 백 달러 + 층 수

122
00:04:45,860 --> 00:04:47,340
1평 당 몇 백 달러 + 층 수, 그리고

123
00:04:47,920 --> 00:04:50,120
1평 당 몇 백 달러 + 층 수, 그리고 침실 개수

124
00:04:50,690 --> 00:04:51,480
1평 당 몇 백 달러 + 층 수, 그리고 침실 개수 정도

125
00:04:51,740 --> 00:04:53,020
정도 하겠죠.

126
00:04:53,170 --> 00:04:54,300
왜냐면 x3 은

127
00:04:54,790 --> 00:04:55,870
침실의 개수이고,

128
00:04:56,190 --> 00:04:57,390
집 가격은

129
00:04:57,570 --> 00:04:58,890
집이

130
00:04:59,220 --> 00:05:01,090
오래되면

131
00:05:01,540 --> 00:05:03,930
오래될 수록

132
00:05:04,230 --> 00:05:07,150
떨어질테니까요.

133
00:05:08,930 --> 00:05:11,630
여기에 추론의 형태를 다시 정리해 놓았어요

134
00:05:11,990 --> 00:05:13,390
그리고 이제 제가

135
00:05:13,590 --> 00:05:14,560
이 식을 좀 더 간편하게

136
00:05:14,650 --> 00:05:16,300
notation해볼게요.

137
00:05:17,840 --> 00:05:19,660
notation의 편의를 위해서,

138
00:05:19,770 --> 00:05:22,800
x0이 1이라고 가정해봅시다.

139
00:05:23,870 --> 00:05:25,080
구체적으로, 이건

140
00:05:25,270 --> 00:05:27,770
각 각의 모든 예제 i,

141
00:05:27,850 --> 00:05:29,300
여기에 벡터 xi

142
00:05:29,850 --> 00:05:31,500
그리고

143
00:05:32,000 --> 00:05:34,370
xi 0 는 1이랑 같은 것이겠죠.

144
00:05:34,970 --> 00:05:35,990
이걸 추가적인

145
00:05:36,810 --> 00:05:38,590
zero feature라고 정의해 볼 수 있습니다.

146
00:05:39,290 --> 00:05:40,320
그리고 여기에는 앞서

147
00:05:40,670 --> 00:05:41,790
n features 가 있었지요. 
왜냐면 x1, x2

148
00:05:41,930 --> 00:05:43,920
x1, x2 ~ xn까지

149
00:05:44,830 --> 00:05:46,150
추가적으로

150
00:05:47,210 --> 00:05:48,910
1의 값 만을 갖는

151
00:05:49,310 --> 00:05:50,590
zero feature 벡터를 정의하고 있으니까요.

152
00:05:52,130 --> 00:05:53,860
자, 그래서 이렇게 여기의 feature 벡터 x는

153
00:05:54,200 --> 00:05:56,390
n+1 차원이 될 것입니다.

154
00:05:58,410 --> 00:06:01,020
index값이 0인 백터가 됩니다.

155
00:06:02,430 --> 00:06:04,080
이제 여기의 n+1 차원의

156
00:06:04,190 --> 00:06:05,650
feature 벡터인데요.

157
00:06:05,940 --> 00:06:07,200
index를

158
00:06:07,420 --> 00:06:09,400
0부터 줄 거에요.

159
00:06:09,700 --> 00:06:10,950
그리고

160
00:06:11,090 --> 00:06:13,240
이 parameter가 vector라고 하겠습니다.

161
00:06:13,610 --> 00:06:15,620
자, 여기의 parameter가 있어요.

162
00:06:15,790 --> 00:06:16,800
여기는 θ 0이고,

163
00:06:17,150 --> 00:06:18,130
θ1, θ2… θn까지,

164
00:06:18,380 --> 00:06:18,780
그리고 이걸 다

165
00:06:18,790 --> 00:06:19,950
하나의

166
00:06:20,340 --> 00:06:21,580
parameter vector인

167
00:06:22,380 --> 00:06:24,030
θ 0

168
00:06:24,190 --> 00:06:25,990
θ0, θ1, θ2 … θn으로

169
00:06:26,280 --> 00:06:27,390
θ0, θ1, θ2 … θn으로 모아보겠습니다.

170
00:06:28,330 --> 00:06:30,160
이건 다른 zero index vector에요.

171
00:06:30,560 --> 00:06:31,590
0부터 지정된 인덱스지요.

172
00:06:32,820 --> 00:06:35,380
이건 다른 형식의 n + 1 차원 벡터입니다.

173
00:06:37,180 --> 00:06:39,840
그래서, 이 추론 - hypothesis는

174
00:06:40,000 --> 00:06:42,720
θ x0

175
00:06:42,910 --> 00:06:45,560
θ x0+ θ1x1

176
00:06:46,400 --> 00:06:47,330
θ x0+ θ1x1 + ... + θnXn 로 쓰여질 수 없는 거에요.

177
00:06:48,820 --> 00:06:50,310
그리고 이 공식은

178
00:06:50,460 --> 00:06:51,600
위에 있는 이것과

179
00:06:51,910 --> 00:06:53,670
같다고 볼 수 있겠죠,

180
00:06:54,080 --> 00:06:55,710
왜냐하면, 8개의 0는 1이랑 같으니까요.

181
00:06:58,270 --> 00:06:59,300
아래에선

182
00:06:59,390 --> 00:07:00,700
이 추론 형태를 이용해서

183
00:07:00,740 --> 00:07:02,130
당신이 벡터의 내적에 대해

184
00:07:02,500 --> 00:07:04,990
얼마나 알고 있느냐

185
00:07:05,370 --> 00:07:06,910
여부에 따라,

186
00:07:07,320 --> 00:07:08,960
θt x로

187
00:07:09,720 --> 00:07:12,050
θt x로 바꿔 쓸 수 있어요.

188
00:07:12,180 --> 00:07:13,880
θ transfers x는

189
00:07:14,110 --> 00:07:15,260
θ0,

190
00:07:15,360 --> 00:07:17,370
θ0, θ1

191
00:07:17,840 --> 00:07:19,730
θ0, θ1~ θn입니다.

192
00:07:20,070 --> 00:07:22,880
그리고 바로

193
00:07:23,140 --> 00:07:24,910
이것이 θ transpose 이고요.

194
00:07:25,810 --> 00:07:27,820
이건 원래 하나의 매트릭스마다

195
00:07:27,960 --> 00:07:30,930
n+1 x1 인거죠.

196
00:07:31,850 --> 00:07:32,600
이걸 다른 말로 행 벡터라고도 합니다.

197
00:07:34,090 --> 00:07:35,160
이걸로

198
00:07:35,420 --> 00:07:37,420
vector x로

199
00:07:37,510 --> 00:07:38,440
x0

200
00:07:38,640 --> 00:07:40,560
x0, x1

201
00:07:40,820 --> 00:07:41,790
x0, x1, xn까지 곱할 수 있습니다.

202
00:07:43,030 --> 00:07:44,400
그리고 내적,

203
00:07:44,940 --> 00:07:47,050
θ transpose x는

204
00:07:47,910 --> 00:07:48,810
이것과 같은 거에요.

205
00:07:49,520 --> 00:07:50,610
이것은 편리한 방법으로

206
00:07:50,770 --> 00:07:51,830
추론의 형태를

207
00:07:52,110 --> 00:07:53,310
쓸 수 있도록 하죠.

208
00:07:53,510 --> 00:07:55,240
파라미터 벡터 세터와

209
00:07:55,760 --> 00:07:57,200
세터 벡터 x간의

210
00:07:57,550 --> 00:07:59,220
내적처럼.

211
00:07:59,350 --> 00:08:00,360
그리고 이런 notation을 통해

212
00:08:01,000 --> 00:08:02,270
축약된 태로

213
00:08:02,320 --> 00:08:03,690
쓸 수 있는 편의를

214
00:08:03,740 --> 00:08:05,530
느낄 수 있습니다.

215
00:08:06,360 --> 00:08:09,230
자, 여러 개의 features 를 갖고 있을 때의 
추론의 형태를 살펴보았습니다.

216
00:08:09,980 --> 00:08:10,940
이건

217
00:08:11,230 --> 00:08:12,330
다변량 선형 회귀 
multivariate linear regression

218
00:08:12,570 --> 00:08:13,860
라고 부르기도 합니다.

219
00:08:15,200 --> 00:08:16,640
그리고 용어 다변수량  - multivariable는

220
00:08:17,120 --> 00:08:18,300
Y값을 예측하는

221
00:08:18,730 --> 00:08:20,370
여러 개의 feature

222
00:08:20,830 --> 00:08:22,900
혹은 변수를 표현하는 말입니다.