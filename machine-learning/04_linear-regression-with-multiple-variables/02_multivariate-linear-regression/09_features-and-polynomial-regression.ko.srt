1
00:00:00,200 --> 00:00:03,878
이제 여러분들은 변수가 여러개인 linear regression을 알고 있습니다.

2
00:00:03,910 --> 00:00:05,185
이 번 비디오에서는

3
00:00:05,185 --> 00:00:06,369
feature를 간단하게

4
00:00:06,380 --> 00:00:07,830
선택하는 방법과

5
00:00:07,830 --> 00:00:09,742
적절한 feature의 선택으로

6
00:00:09,750 --> 00:00:11,477
강력한 학습 알고리즘을

7
00:00:11,477 --> 00:00:13,803
만드는 방법에 대해 이야기해보겠습니다.

8
00:00:13,810 --> 00:00:15,229
그리고

9
00:00:15,229 --> 00:00:17,826
polynomial regression도 다루어 볼 건데,

10
00:00:17,826 --> 00:00:19,535
선형 회귀를

11
00:00:19,535 --> 00:00:21,247
이용하여

12
00:00:21,247 --> 00:00:25,060
복잡한 비선형 함수에도 적용해 볼 것입니다.

13
00:00:25,690 --> 00:00:28,827
집값 예측 예제를 보죠.

14
00:00:29,300 --> 00:00:31,147
두 개의 feature가 있는데

15
00:00:31,147 --> 00:00:33,805
그 두 개는 집의 frontage와 집의 depth입니다.

16
00:00:33,805 --> 00:00:35,428
여기 판매하려는 집의 그림도 있어요.

17
00:00:35,428 --> 00:00:37,264
그럼, frontage는

18
00:00:37,264 --> 00:00:40,103
너비

19
00:00:40,103 --> 00:00:43,009
즉 집 폭의

20
00:00:43,009 --> 00:00:44,949
길이로

21
00:00:44,960 --> 00:00:46,652
정의하고

22
00:00:46,652 --> 00:00:47,994
집의

23
00:00:48,020 --> 00:00:49,468
depth는

24
00:00:49,500 --> 00:00:53,120
집의

25
00:00:53,130 --> 00:00:54,758
세로길이입니다.

26
00:00:54,770 --> 00:00:57,992
그래서, 이게 frontage이고, 이게 depth입니다.

27
00:00:57,992 --> 00:00:59,858
frontage와 depth라고 부릅니다.

28
00:00:59,858 --> 00:01:01,355
선형 회귀 모델은 이렇게 만들 수 있습니다.

29
00:01:01,360 --> 00:01:04,163
frontage가

30
00:01:04,180 --> 00:01:06,062
첫 번째 feature x_1이고,

31
00:01:06,062 --> 00:01:07,535
depth가 두 번째

32
00:01:07,535 --> 00:01:10,169
feature인 x_2입니다.
하지만

33
00:01:10,169 --> 00:01:11,772
선형 회귀를 할 때에는

34
00:01:11,772 --> 00:01:13,342
feature x_1과 x_2를

35
00:01:13,342 --> 00:01:16,607
사용할 필요는 없습니다.

36
00:01:16,610 --> 00:01:20,531
여러분이 직접 새로운 feature를 만드는 것입니다.

37
00:01:20,531 --> 00:01:21,709
그래서, 집값을

38
00:01:21,710 --> 00:01:22,895
예측하고 싶을 때,

39
00:01:22,895 --> 00:01:24,840
feature x_1, x_2대신에

40
00:01:24,850 --> 00:01:27,468
사용할 것은

41
00:01:27,490 --> 00:01:29,133
집의 크기 즉,

42
00:01:29,133 --> 00:01:32,164
이 집의 면적입니다.

43
00:01:32,190 --> 00:01:33,365
그럼, 새로운 feature가 만들어졌고,

44
00:01:33,380 --> 00:01:34,609
이 것은 feature x라고 부르겠습니다.

45
00:01:34,609 --> 00:01:40,409
x는 frontage * depth입니다.

46
00:01:40,440 --> 00:01:42,404
이건 곱하기 기호입니다.

47
00:01:42,404 --> 00:01:44,334
frontage * depth는

48
00:01:44,334 --> 00:01:46,040
집의 넓이이기 때문에

49
00:01:46,090 --> 00:01:48,035
이 feature

50
00:01:48,035 --> 00:01:50,651
하나만 가지고

51
00:01:50,710 --> 00:01:53,327
가설 함수를

52
00:01:53,350 --> 00:01:54,785
정의할 수 있습니다.

53
00:01:54,785 --> 00:01:57,430
그렇죠?

54
00:01:57,580 --> 00:01:58,939
사각형의 넓이는

55
00:01:58,940 --> 00:02:00,345
길이의

56
00:02:00,345 --> 00:02:01,432
곱이기 때문입니다.

57
00:02:01,460 --> 00:02:03,822
즉, 문제를

58
00:02:03,822 --> 00:02:05,253
어떻게 이해하느냐에 따라,

59
00:02:05,280 --> 00:02:07,481
곧이곧대로

60
00:02:07,490 --> 00:02:09,604
feature를 쓰는것보다는

61
00:02:09,620 --> 00:02:11,103
때로는 새로운 feature로

62
00:02:11,130 --> 00:02:13,489
더 나은 모델을

63
00:02:13,489 --> 00:02:16,771
정의해도 좋습니다.

64
00:02:16,790 --> 00:02:18,163
feature를 선택하는 것과

65
00:02:18,163 --> 00:02:19,745
밀접하게 관련된 것으로

66
00:02:19,745 --> 00:02:22,973
다항 회귀(polynomial regression)가 있습니다.

67
00:02:23,010 --> 00:02:26,868
이와 같은 집 값 data set이 있다고 해봅시다.

68
00:02:26,880 --> 00:02:29,646
이 data set을 표현할 수 있는 몇 가지 다른 모델들이 있습니다.

69
00:02:29,660 --> 00:02:32,587
그 중 하나는 이와 같은 2차식 모델입니다.

70
00:02:32,600 --> 00:02:35,598
직선은 그다지 이 data set을 잘 표현할 수 없을 것 같습니다.

71
00:02:35,598 --> 00:02:36,788
그래서 이와같은

72
00:02:36,788 --> 00:02:38,408
2차식 모델로 표현할 수 있습니다.

73
00:02:38,420 --> 00:02:40,248
이 모델은 가격을

74
00:02:40,248 --> 00:02:42,017
사이즈의 2차 함수로

75
00:02:42,020 --> 00:02:43,956
표현한 것입니다.

76
00:02:43,970 --> 00:02:45,018
이 모델을 사용하여,

77
00:02:45,020 --> 00:02:47,070
이와 같이 data를 표현할 수 있습니다.

78
00:02:47,280 --> 00:02:48,560
하지만 이 2차식 모델은

79
00:02:48,570 --> 00:02:50,013
사실 적합하지가 않습니다.

80
00:02:50,013 --> 00:02:52,582
왜냐하면 이 함수는 결국

81
00:02:52,582 --> 00:02:53,858
하강하기 때문입니다.

82
00:02:53,858 --> 00:02:55,591
집 크기는 커지는데,

83
00:02:55,600 --> 00:02:58,899
집 값은 작아질리는 없죠

84
00:02:58,970 --> 00:03:00,649
그래서, 다른 다항식 모델을

85
00:03:00,650 --> 00:03:02,700
사용해보겠습니다.

86
00:03:02,700 --> 00:03:04,274
2차 함수

87
00:03:04,290 --> 00:03:07,480
대신에,

88
00:03:07,480 --> 00:03:09,225
3차식을 이용하여

89
00:03:09,225 --> 00:03:10,764
표현하면,

90
00:03:10,800 --> 00:03:12,367
이러한 모양의

91
00:03:12,390 --> 00:03:13,907
모델이 나오고,

92
00:03:13,910 --> 00:03:15,278
이번에 초록색 선은 data를 더 잘 표현합니다.

93
00:03:15,278 --> 00:03:18,052
왜냐하면 이번에는 하강하지 않기 때문입니다.

94
00:03:18,052 --> 00:03:21,992
그럼, 어떻게 data를 잘 표현하는 이런 모델을 만들 수 있을까요?

95
00:03:22,020 --> 00:03:23,868
multivariant linear regression의 구조를 이용하여,

96
00:03:23,868 --> 00:03:27,059
알고리즘을

97
00:03:27,059 --> 00:03:30,692
간단히 수정해 봅시다.

98
00:03:30,692 --> 00:03:32,632
가설 함수가 있고,

99
00:03:32,632 --> 00:03:34,217
이렇게 표현할 수 있습니다.

100
00:03:34,217 --> 00:03:35,782
h(x)는

101
00:03:35,782 --> 00:03:37,612
h(x)는 θ_0

102
00:03:37,612 --> 00:03:41,608
h(x)는 θ_0 + θ_1 x_1 + θ_2 x_2 + θ_3 x_3입니다.

103
00:03:41,608 --> 00:03:42,775
이것을 3차식 모델로

104
00:03:42,775 --> 00:03:45,220
표현하고 싶으면,

105
00:03:45,250 --> 00:03:47,239
초록색 박스안에

106
00:03:47,239 --> 00:03:48,940
있는 것처럼,

107
00:03:48,940 --> 00:03:49,825
집 값은

108
00:03:49,825 --> 00:03:51,364
θ_0 +

109
00:03:51,364 --> 00:03:53,056
θ_0 + θ_1*(집의 크기)

110
00:03:53,056 --> 00:03:55,905
+ θ_2*(집의 크기의 제곱)

111
00:03:55,910 --> 00:03:58,974
즉 이 항은 이 항과 똑같습니다.

112
00:03:58,974 --> 00:04:00,885
그리고

113
00:04:00,890 --> 00:04:02,343
+ θ_3*(집의 크기의 세제곱은)

114
00:04:02,350 --> 00:04:05,302
이 세 번째 항입니다.

115
00:04:05,470 --> 00:04:06,967
두 식이 같아지도록

116
00:04:06,990 --> 00:04:08,668
하기 위해서는,

117
00:04:08,668 --> 00:04:10,339
첫 번째 feature x_1은

118
00:04:10,339 --> 00:04:12,128
집의 크기로

119
00:04:12,150 --> 00:04:13,568
치환하고,

120
00:04:13,568 --> 00:04:15,320
두 번째 feature x_2는

121
00:04:15,320 --> 00:04:16,721
집의 크기의 제곱으로

122
00:04:16,721 --> 00:04:17,766
치환하고,

123
00:04:17,766 --> 00:04:20,400
세 번째 x_3는

124
00:04:20,400 --> 00:04:22,780
집의 크기의 세제곱으로 치환합니다.

125
00:04:22,800 --> 00:04:24,292
이와 같이

126
00:04:24,292 --> 00:04:26,311
세 개의 feature를

127
00:04:26,311 --> 00:04:27,720
선형 회귀형태로

128
00:04:27,720 --> 00:04:30,540
적용시키면,

129
00:04:30,540 --> 00:04:31,901
이 모델을

130
00:04:31,901 --> 00:04:34,374
삼차식으로 표현할 수 있습니다.

131
00:04:34,374 --> 00:04:35,523
한 가지 더

132
00:04:35,523 --> 00:04:36,799
강조하고 싶은 것은

133
00:04:36,800 --> 00:04:38,610
이렇게 feature를 선택했을 때,

134
00:04:38,610 --> 00:04:40,925
eature scaling의 적용은

135
00:04:40,925 --> 00:04:43,688
훨씬 더 중요해진다는 것입니다.

136
00:04:44,130 --> 00:04:45,254
만약

137
00:04:45,254 --> 00:04:46,794
집의 크기가 1에서

138
00:04:46,800 --> 00:04:47,992
1000사이의 범위를 가진다면,

139
00:04:47,992 --> 00:04:49,300
즉 1에서 1000 평방 피트라면,

140
00:04:49,310 --> 00:04:50,918
집의

141
00:04:50,930 --> 00:04:52,175
제곱 크기는

142
00:04:52,175 --> 00:04:54,519
1에서

143
00:04:54,520 --> 00:04:55,953
백만까지,

144
00:04:55,953 --> 00:04:58,468
즉 1000의 제곱까지 됩니다.

145
00:04:58,490 --> 00:05:01,335
그리고 세 번째 feature x 세제곱, 
아 정정할게요.

146
00:05:01,360 --> 00:05:03,106
세 번째 featuren x_3는

147
00:05:03,120 --> 00:05:04,732
집의

148
00:05:04,732 --> 00:05:05,941
세제곱 크기라

149
00:05:05,950 --> 00:05:07,478
1에서

150
00:05:07,478 --> 00:05:09,311
10의 9승의 범위가 될것입니다.

151
00:05:09,330 --> 00:05:10,955
결국 이 세 개의 feature는

152
00:05:10,955 --> 00:05:13,459
매우 다른 값의 범위를 가지게 되므로,

153
00:05:13,490 --> 00:05:15,105
feature scaling을 적용시키는 것이 중요합니다.

154
00:05:15,110 --> 00:05:16,509
그래야

155
00:05:16,509 --> 00:05:18,554
gradient descent를 할 때

156
00:05:18,554 --> 00:05:21,139
서로 비슷한 범위를 가질수 있습니다.

157
00:05:21,140 --> 00:05:23,243
그럼, 마지막으로

158
00:05:23,250 --> 00:05:25,138
feature를 다루는 다양한 방법을 보여주는

159
00:05:25,150 --> 00:05:29,056
예제를 들어보겠습니다.

160
00:05:29,090 --> 00:05:30,446
처음에,

161
00:05:30,446 --> 00:05:31,559
이러한 2차식 모델은

162
00:05:31,559 --> 00:05:33,122
이상적이지 않다고 말했는데,

163
00:05:33,122 --> 00:05:34,408
data를 표현하는데는

164
00:05:34,408 --> 00:05:35,952
괜찮지만,

165
00:05:35,952 --> 00:05:37,514
2차 함수는 하강하는 모양이고,

166
00:05:37,514 --> 00:05:39,065
그런 모양은 우리가 원하는 모양이 아니기 때문이였습니다.

167
00:05:39,070 --> 00:05:40,352
하강하는 모양이라면, 집의 크기가 폭등할 때,

168
00:05:40,352 --> 00:05:43,567
집 값은 떨어지는 것으로 예측됩니다.

169
00:05:43,567 --> 00:05:45,388
그래서 3차 식 모델을

170
00:05:45,388 --> 00:05:46,938
소개했지만,

171
00:05:46,938 --> 00:05:48,389
그것 말고도

172
00:05:48,389 --> 00:05:50,798
다른 방법이 있습니다.

173
00:05:50,800 --> 00:05:52,313
다른 괜찮은 방법 중

174
00:05:52,313 --> 00:05:53,691
한 가지

175
00:05:53,691 --> 00:05:55,620
예를 들어보면,

176
00:05:55,620 --> 00:05:57,263
그 예는

177
00:05:57,263 --> 00:05:58,832
집 값을 θ_0

178
00:05:58,850 --> 00:05:59,992
+ θ_1 * (크기)

179
00:05:59,992 --> 00:06:01,264
+ θ_2*(크기의 제곱근)으로

180
00:06:01,320 --> 00:06:03,625
나타내는 것입니다. 그렇죠?

181
00:06:03,630 --> 00:06:05,364
제곱근 함수는

182
00:06:05,364 --> 00:06:08,110
이런 모양입니다.

183
00:06:08,110 --> 00:06:09,318
그리고,

184
00:06:09,318 --> 00:06:11,355
θ_0, θ_1, θ_2의 값이 있으면,

185
00:06:11,355 --> 00:06:14,049
이러한 모델이 만들어질 것입니다.

186
00:06:14,080 --> 00:06:15,445
이 곡선은 갈수록

187
00:06:15,445 --> 00:06:16,952
이렇게

188
00:06:16,952 --> 00:06:19,500
평평해지지만

189
00:06:19,520 --> 00:06:21,529
하강하지는

190
00:06:21,540 --> 00:06:23,877
않습니다.

191
00:06:24,154 --> 00:06:26,584
결국, 문제를 다른 관점에서 본다면,

192
00:06:26,584 --> 00:06:27,630
이 경우에서는

193
00:06:27,630 --> 00:06:30,952
data의 모양을

194
00:06:30,990 --> 00:06:32,555
제곱근 함수의 관점에서 본다면,

195
00:06:32,555 --> 00:06:36,469
feature를 가지고 더 나은 모델을 만들 수 있습니다.

196
00:06:36,469 --> 00:06:39,026
이 번 비디오에서는, 다항 회귀에 대해 배워보았습니다.

197
00:06:39,026 --> 00:06:40,672
어떻게 다항식을,

198
00:06:40,672 --> 00:06:42,298
즉 2차 함수나,

199
00:06:42,298 --> 00:06:43,868
3차 함수를 data에 맞게 표현하는지에 대해 배웠습니다.

200
00:06:43,868 --> 00:06:45,112
그리고 어떻게

201
00:06:45,112 --> 00:06:46,640
feature를 사용할지

202
00:06:46,640 --> 00:06:47,732
여러 방법이 있다는 것도 배웠습니다.

203
00:06:47,748 --> 00:06:48,804
예를들면,

204
00:06:48,804 --> 00:06:50,078
집의 frontage와

205
00:06:50,078 --> 00:06:51,092
depth를 사용하는 것보다는

206
00:06:51,092 --> 00:06:53,133
둘을 곱하여,

207
00:06:53,133 --> 00:06:55,317
집의 면적에 대한 feature를 만들었습니다.

208
00:06:55,317 --> 00:06:57,551
여러개의 서로 다른

209
00:06:57,551 --> 00:06:58,895
feature가 있을 때는,

210
00:06:58,896 --> 00:07:03,265
어떤 feature를 사용해야하는지 혼란스러운 상황이 있을 수도 있습니다.

211
00:07:03,265 --> 00:07:04,594
이 수업 이후에,

212
00:07:04,594 --> 00:07:06,622
자동으로 어떤 feature를 사용할지 골라주는

213
00:07:06,622 --> 00:07:08,083
알고리즘에 대해서 배울 것입니다.

214
00:07:08,083 --> 00:07:09,466
그 알고리즘은

215
00:07:09,466 --> 00:07:10,611
data를 보고,

216
00:07:10,611 --> 00:07:12,040
자동으로

217
00:07:12,040 --> 00:07:13,357
2차 함수나

218
00:07:13,357 --> 00:07:15,528
3차 함수, 혹은 다른 함수들 중 알맞은 것을 선택합니다.

219
00:07:15,528 --> 00:07:17,164
하지만, 이 알고리즘을 배우기전인

220
00:07:17,164 --> 00:07:18,764
지금,

221
00:07:18,764 --> 00:07:20,295
어떤 feature를 사용할지는

222
00:07:20,295 --> 00:07:21,582
여러 방법이 있다는 것을

223
00:07:21,582 --> 00:07:23,094
알아두었으면 합니다.

224
00:07:23,094 --> 00:07:25,256
그리고, 다른 feature를 고안하여,

225
00:07:25,256 --> 00:07:26,888
data를 직선으로 표현하는게 아닌

226
00:07:26,888 --> 00:07:28,156
복잡한 함수로

227
00:07:28,156 --> 00:07:30,471
표현할 수 있습니다.

228
00:07:30,471 --> 00:07:32,092
특히, 다항 함수나

229
00:07:32,092 --> 00:07:35,065
때로는,

230
00:07:35,065 --> 00:07:36,072
적절한 feature로

231
00:07:36,072 --> 00:07:37,564
더 나은 모델을

232
00:07:37,564 --> 00:07:40,020
새울 수 있습니다.