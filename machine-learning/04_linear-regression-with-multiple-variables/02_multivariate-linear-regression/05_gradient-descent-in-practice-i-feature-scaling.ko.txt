이번 시간과 다음 시간에는 gradient descent를 잘 활용할 수 있는 몇 가지 방법들을 알려주고자 합니다. 이번 시간에는 먼저 feature scaling이라고 하는 것을 알려주게습니다. 개념은 이렇습니다. 만약 여러개의 feature가 있고, feature의 단위크기가 비슷하다면, 이 말은 즉, 서로 다른 feature라도 범위가 같다면 gradient descent는 더 빠르게 수렴할 수 있습니다. 구체적으로 두 개의 feature가 있다고 해봅시다. x_1은 집의 크기이고 범위는 0에서 2000사이에 값을 가집니다. 두 번째는 침실의 수이고, 범위는 1에서 5사이에 값을 가집니다. 만약 cost function J(θ)를 그래프로 그린다면, 이렇게 그릴 수 있을 것입니다. 음, J(θ)는 파라미터 θ_0, θ_1, θ_2의 함수입니다. θ_0는 고려하지 않고, 그래서 θ0은 없애고, θ1과 θ2의 함수들만 보도록 하겠습니다. θ_1과 θ_2의 함수로 보겠습니다. 하지만, x_1의 범위가 x_2보다 크다면, cost function J(θ)의 모양은 굉장히 뾰족한 타원모양이 나올 수 있습니다. 2000:5의 비율만 아니면, 더 안정한 모양이 될 수 있습니다. 엄청나게, 길고 얇은 타원 모양의 cost function J(θ)가 그러졌습니다. 만약, 이 cost function에 gradient descent를 적용한다면, gradient는 오랜 시간 동안 앞 뒤로 진동하며, 엄청난 시간이 지나고 나서야 마침내 최소값에 도달할 것입니다. 실제로, 등고선이 극단적인 경우를 생각해보면 즉 엄청 얇은, 얇고 긴 등고선이라면, 그리고 더욱 극단적으로 과장한다면, gradient descent는 훨씬 더 많은 시간을 소요하며, 구불구불하게 가다가, 오랜 시간이 지나서야 최소값을 찾을 수 있습니다. 이 때, 유용한 방법이 feature를 조절(scale)하는 것입니다. 구체적으로 feature x_1은 집 크기를 2000으로 나눈 값이라 하고, x_2는 침실의 수를 5로 나눈 값이라 한다면 cost function (θ)의 모양은 훨씬 덜 뾰족한 원에 가까운 타원모양이 될 수 있습니다. 이러한 cost function에 gradient descent를 하면은 정확하게, 똑바른 방향으로 최소값을 찾는 것을 볼 수 있습니다 난해한 경로로 복잡한 궤도를 그리며 구불구불하게 가지 않습니다. 즉, feature를 조절하여, 적절한 범위가 되도록 하는 것입니다. 예를 들면, x_1과 x_2는 0에서 1사이의 값을 가지죠. 이제 더 빠르게 수렴하는 gradient descent를 구현할 수 있게 되었습니다. 보통은, feature scaling을 할 때, 모든 feature가 대략 -1에서 +1사이의 범위가 있기를 원합니다. 확실히, feature x_0은 항상 1입니다. 그래서, x_0은 그 범위안에 있죠. 하지만 다른 feature들은 각각 다른 수로 나누어서 이 범위로 만들 수도 있습니다. 그렇지만 -1과 +1은 중요하지 않습니다. 만약 x_1이 0에서 3 사이의 범위를 가지더라도, 문제가 되지 않습니다. 만약 다른 feature도 -2에서 +0.5의 범위를 가지더라도, 역시, -1에서 +1과 근사하기 때문에, 괜찮습니다. 괜찮아요. 하지만 만약 또 다른 feature x_3의 범위가 -100에서 +100의 범위라면 x_3는 -1과 +1과는 완전히 다른 값이 됩니다. 그래서 이것은 좋은 feature가 아닙니다. 그리고 유사한 경우로는, feature가 엄청 작은 범위일 때, 즉 x_4가 -0.0001에서 +0.0001사이의 값을 가질 때, 역시 -1에서 +1의 범위보다 작은 범위를 가집니다 feature를 조절해야합니다. 결국 값의 범위는 1보다 크거나 작더라도 괜찮지만 100처럼 엄청 크거나, 혹은 0.0001처럼 엄청 작으면 안됩니다. 사람마다 규칙은 다릅니다. 저 같은 경우, feature의 범위가 -3에서 +3의 범위를 가진다면 괜찮다고 보지만, +3에서 -3의 범위보다 크다면 한 번 생각해 봐야 합니다. -3분의 1에서 3분의 1범위도 괜찮습니다. 0에서 3분의 1이나 -3분의 1에서 0도요. 보통 0 근처의 범위면 괜찮습니다. 하지만, x_4의 범위만큼 작다면, 문제가 됩니다. 그래서 알아두었으면 하는 점은 feature가 같은 크기나 정확하게 똑같은 범위가 아니더라도 걱정할 필요가 없다는 것입니다. gradient descent하기에 충분히 근접하는 한은 괜찮습니다. feature scaling을 할 때, 최대값으로 나누고 추가적으로, mean normalization이라는 것을 할 수 있습니다. mean normalization은 feature x_i를 x_i - μ_i로 바꾸면 됩니다. 그러면 feature의 평균이 대략 0이 됩니다. 분명한 것은 feature x_0에는 사용하지 않는다는 것입니다. feature x_0은 모두 1이기 때문에, 평균이 0이 될 수 없습니다. 하지만 다른 feature의 경우, 집 크기의 범위가 0부터 2000사이이고, 집의 평균 크기가 만약 1000이면, 이 식을 사용하면 됩니다. 크기, feature x_1는 평균값을 빼고나서, 2000으로 나누고 평균과 비슷하게 합니다. 집이 집이 1개에서 5개 사이의 침실을 가지고, 평균적으로 2개의 침실을 가진다면, 이 식으로 두 번째 feature x_2에 mean normalize를 하면 됩니다. 두 가지 경우 모두, feature x_1과 x_2가 대략 -0.5에서 +0.5사이의 값을 가지게 됩니다. 정확히는 x_2가 0.5보다 약간 크게 되지만, 거의 근접합니다. 일반화된 식은 feature x_1을 s_1분의 x_1-μ_1로 바꿉니다. μ_1은 training set에서 x_1의 평균값이고 s_1은 feature의 범위, 즉, 최대값에서 최소값을 뺀 값이 될 수도 있고, 편차를 알고있다면, s_1은 표준편차로 써도 괜찮습니다. 하지만 최대값 - 최소값도 괜찮습니다. 비슷하게 두 번째  feature인 x_2도, x_2를 feature의 평균만큼 빼고, 최대값-최소값으로 나눕니다. feature에 관한 이 식은 정확하게는 아니더라도 대략 저 범위만큼 될 것입니다. 엄밀히 따진다면, 최대값에서 최소값을 뺏을 때, 실제로는 4가 됩니다. 즉, 최대값이 5고, 최소값이 1이면 range는 4가 되지만 여기서 대부분이 근사값이고, feature의 값도 range와 거의 차이가 없기 때문에 괜찮습니다. feature scaling은 너무 정확할 필요는 없습니다. gradient descent가 훨씬 더 빨라지면 되기 때문이죠 이제 feature scaling을 배웠고, 이 방법을 적용시키면, gradient descent는 더 빨라지고, 적은 수의 반복으로도 수렴합니다. 여기까지가 feature scaling입니다. 다음 시간에는, gradient descent를 효율적으로 다루는 다른 방법에 대해 이야기를 해보겠습니다.