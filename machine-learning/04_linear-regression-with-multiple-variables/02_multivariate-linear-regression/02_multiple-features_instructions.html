<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Multiple Features
 </h1>
 <p hasmath="true">
  <strong>
   Note:
  </strong>
  [7:25 - $$\theta^T$$ is a 1 by (n+1) matrix and not an (n+1) by 1 matrix]
 </p>
 <p>
  Linear regression with multiple variables is also known as "multivariate linear regression".
 </p>
 <p>
  We now introduce notation for equations where we can have any number of input variables.
 </p>
 <table columns="1" rows="1">
  <tr>
   <td>
    <p hasmath="true">
     $$\begin{align*}x_j^{(i)} &amp;= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}&amp; = \text{the input (features) of the }i^{th}\text{ training example} \newline m &amp;= \text{the number of training examples} \newline n &amp;= \text{the number of features} \end{align*}$$
    </p>
   </td>
  </tr>
 </table>
 <p>
  The multivariable form of the hypothesis function accommodating these multiple features is as follows:
 </p>
 <p hasmath="true">
  $$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n$$
 </p>
 <p hasmath="true">
  In order to develop intuition about this function, we can think about $$\theta_0$$ as the basic price of a house, $$\theta_1$$ as the price per square meter, $$\theta_2$$ as the price per floor, etc. $$x_1$$ will be the number of square meters in the house, $$x_2$$ the number of floors, etc.
 </p>
 <p>
  Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:
 </p>
 <table columns="1" rows="1">
  <tr>
   <td>
    <p hasmath="true">
     $$\begin{align*}h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} ... \hspace{2em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x\end{align*}$$
    </p>
   </td>
  </tr>
 </table>
 <p>
  This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.
 </p>
 <p hasmath="true">
  Remark: Note that for convenience reasons in this course we assume $$x_{0}^{(i)} =1 \text{ for } (i\in { 1,\dots, m } )$$. This allows us to do matrix operations with theta and x. Hence making the two vectors '$$\theta$$' and $$x^{(i)}$$ match each other element-wise (that is, have the same number of elements: n+1).]
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
 <p>
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
