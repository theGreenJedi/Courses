1
00:00:00,190 --> 00:00:01,270
在这段视频

2
00:00:01,440 --> 00:00:02,720
以及下一段视频中

3
00:00:02,850 --> 00:00:04,040
我想告诉你一些关于

4
00:00:04,180 --> 00:00:06,940
梯度下降运算中的实用技巧

5
00:00:07,680 --> 00:00:10,250
在这段视频中 我会告诉你一个称为特征缩放 (feature scaling) 的方法

6
00:00:11,770 --> 00:00:12,210
我们用一个 for 循环

7
00:00:13,030 --> 00:00:14,080
如果你有一个机器学习问题

8
00:00:14,180 --> 00:00:15,880
这个问题有多个特征

9
00:00:16,320 --> 00:00:17,410
如果你能确保这些特征

10
00:00:18,050 --> 00:00:19,440
都处在一个相近的范围

11
00:00:19,570 --> 00:00:20,480
我的意思是确保

12
00:00:20,650 --> 00:00:22,130
不同特征的取值

13
00:00:22,300 --> 00:00:23,390
在相近的范围内

14
00:00:24,420 --> 00:00:26,490
这样梯度下降法就能更快地收敛

15
00:00:27,510 --> 00:00:28,680
具体地说

16
00:00:28,820 --> 00:00:29,860
假如你有一个具有两个特征的问题

17
00:00:30,380 --> 00:00:31,680
其中 x1 是房屋面积大小

18
00:00:31,950 --> 00:00:32,860
它的取值

19
00:00:33,530 --> 00:00:34,540
在0到2000之间

20
00:00:35,490 --> 00:00:36,270
x2 是卧室的数量

21
00:00:36,520 --> 00:00:37,570
可能这个值

22
00:00:37,820 --> 00:00:39,250
取值范围在1到5之间

23
00:00:40,100 --> 00:00:41,690
如果你画出代价函数

24
00:00:41,800 --> 00:00:43,000
J(θ) 的轮廓图

25
00:00:44,810 --> 00:00:46,540
那么这个轮廓看起来

26
00:00:46,750 --> 00:00:49,010
应该是像这样的

27
00:00:49,230 --> 00:00:50,570
J(θ) 是一个关于

28
00:00:50,910 --> 00:00:53,590
参数 θ0 θ1 和 θ2 的函数

29
00:00:54,300 --> 00:00:55,400
但我要忽略 θ0

30
00:00:56,020 --> 00:00:57,230
所以暂时不考虑 θ0

31
00:00:57,480 --> 00:00:58,730
并假想一个函数的变量

32
00:00:58,840 --> 00:01:01,080
只有 θ1 和 θ2

33
00:01:01,510 --> 00:01:02,810
但如果 x1 的取值范围

34
00:01:02,940 --> 00:01:04,110
远远大于 x2 的取值范围的话

35
00:01:04,370 --> 00:01:05,790
那么最终画出来的

36
00:01:06,120 --> 00:01:07,270
代价函数 J(θ) 的轮廓图

37
00:01:07,340 --> 00:01:08,320
就会呈现出这样一种

38
00:01:09,420 --> 00:01:11,400
非常偏斜

39
00:01:11,690 --> 00:01:14,720
并且椭圆的形状

40
00:01:15,070 --> 00:01:16,620
2000 和 5的比例

41
00:01:16,770 --> 00:01:18,470
会让这个椭圆更加瘦长

42
00:01:18,800 --> 00:01:20,190
所以 这是一个又瘦又高的

43
00:01:20,560 --> 00:01:23,070
椭圆形轮廓图

44
00:01:23,320 --> 00:01:24,950
就是这些非常高大细长的椭圆形

45
00:01:25,310 --> 00:01:27,940
构成了代价函数 J(θ)

46
00:01:29,420 --> 00:01:30,860
而如果你用这个代价函数

47
00:01:30,930 --> 00:01:34,290
来运行梯度下降的话

48
00:01:34,830 --> 00:01:36,480
你要得到梯度值 最终可能

49
00:01:36,970 --> 00:01:38,660
需要花很长一段时间

50
00:01:39,080 --> 00:01:40,360
并且可能会来回波动

51
00:01:41,100 --> 00:01:43,130
然后会经过很长时间

52
00:01:43,190 --> 00:01:46,120
最终才收敛到全局最小值

53
00:01:47,470 --> 00:01:48,720
事实上 你可以想像 如果这些

54
00:01:48,890 --> 00:01:50,400
轮廓再被放大一些的话

55
00:01:50,580 --> 00:01:51,970
如果你画的再夸张一些

56
00:01:52,480 --> 00:01:54,300
把它画的更细更长

57
00:01:56,230 --> 00:01:57,030
那么可能情况会更糟糕

58
00:01:57,380 --> 00:01:59,060
梯度下降的过程

59
00:01:59,790 --> 00:02:02,310
可能更加缓慢

60
00:02:02,630 --> 00:02:04,280
需要花更长的时间

61
00:02:04,690 --> 00:02:06,030
反复来回振荡

62
00:02:06,120 --> 00:02:08,270
最终才找到一条正确通往全局最小值的路

63
00:02:12,130 --> 00:02:14,370
在这样的情况下

64
00:02:14,780 --> 00:02:16,280
一种有效的方法是进行特征缩放(feature scaling)

65
00:02:17,380 --> 00:02:18,760
具体来说

66
00:02:19,200 --> 00:02:20,370
把特征 x 定义为

67
00:02:20,570 --> 00:02:21,770
房子的面积大小

68
00:02:21,870 --> 00:02:23,070
除以2000的话

69
00:02:24,040 --> 00:02:25,140
并且把 x2 定义为

70
00:02:25,270 --> 00:02:26,520
卧室的数量除以5

71
00:02:26,940 --> 00:02:29,010
那么这样的话

72
00:02:29,170 --> 00:02:30,020
表示代价函数 J(θ)

73
00:02:30,090 --> 00:02:31,840
的轮廓图的形状

74
00:02:32,900 --> 00:02:34,430
就会变得偏移没那么严重

75
00:02:34,840 --> 00:02:36,990
可能看起来更圆一些了

76
00:02:38,210 --> 00:02:39,180
如果你用这样的代价函数

77
00:02:39,520 --> 00:02:40,540
来执行梯度下降的话

78
00:02:40,750 --> 00:02:42,120
那么 梯度下降算法

79
00:02:44,110 --> 00:02:45,630
你可以从数学上来证明

80
00:02:45,860 --> 00:02:47,430
梯度下降算法 就会找到一条

81
00:02:47,540 --> 00:02:48,830
更捷径的路径通向全局最小

82
00:02:49,390 --> 00:02:51,200
而不是像刚才那样

83
00:02:51,530 --> 00:02:52,530
沿着一条让人摸不着头脑的路径

84
00:02:52,620 --> 00:02:53,520
一条复杂得多的轨迹

85
00:02:54,310 --> 00:02:55,910
来找到全局最小值

86
00:02:57,300 --> 00:02:58,710
因此 通过特征缩放

87
00:02:58,950 --> 00:03:01,000
通过"消耗掉"这些值的范围

88
00:03:01,620 --> 00:03:02,810
在这个例子中

89
00:03:02,970 --> 00:03:04,150
我们最终得到的两个特征

90
00:03:04,300 --> 00:03:06,960
x1 和 x2 都在0和1之间

91
00:03:09,580 --> 00:03:12,290
这样你得到的梯度下降算法

92
00:03:12,690 --> 00:03:13,810
就会更快地收敛

93
00:03:18,120 --> 00:03:19,640
更一般地

94
00:03:20,160 --> 00:03:21,240
我们执行特征缩放时 也就是我们经常

95
00:03:21,530 --> 00:03:22,480
我们通常的目的是

96
00:03:22,750 --> 00:03:25,670
将特征的取值约束到

97
00:03:25,780 --> 00:03:28,170
-1 到 +1 的范围内

98
00:03:28,960 --> 00:03:31,710
你的特征 x0 是总是等于1

99
00:03:31,760 --> 00:03:32,810
因此 这已经是在这个范围内

100
00:03:34,110 --> 00:03:35,150
但对其他的特征

101
00:03:35,630 --> 00:03:36,950
你可能需要通过除以不同的数

102
00:03:37,330 --> 00:03:39,150
来让它们处于同一范围内

103
00:03:39,510 --> 00:03:41,520
-1 和 +1 这两个数字并不是太重要

104
00:03:42,270 --> 00:03:42,900
所以 如果你有一个特征

105
00:03:44,150 --> 00:03:45,340
x1 它的取值

106
00:03:45,510 --> 00:03:48,000
在0和3之间 这没问题

107
00:03:48,400 --> 00:03:49,410
如果你有另外一个特征

108
00:03:49,600 --> 00:03:51,190
取值在-2 到 +0.5之间

109
00:03:52,140 --> 00:03:54,020
这也没什么关系

110
00:03:54,300 --> 00:03:55,710
这也非常接近

111
00:03:56,070 --> 00:03:57,070
-1 到 +1的范围

112
00:03:57,320 --> 00:03:59,160
这些都可以

113
00:04:00,310 --> 00:04:01,260
但如果你有另一个特征

114
00:04:01,340 --> 00:04:02,580
比如叫  x3

115
00:04:02,820 --> 00:04:04,780
假如它的范围

116
00:04:05,840 --> 00:04:09,070
在 -100 到 +100之间

117
00:04:09,330 --> 00:04:10,850
那么 这个范围

118
00:04:11,090 --> 00:04:13,570
跟-1到+1就有很大不同了

119
00:04:13,860 --> 00:04:15,020
所以 这可能是一个

120
00:04:15,230 --> 00:04:17,480
不那么好的特征

121
00:04:17,970 --> 00:04:19,340
类似地 如果你的特征在一个

122
00:04:19,420 --> 00:04:20,680
非常非常小的范围内

123
00:04:20,950 --> 00:04:22,060
比如另外一个特征

124
00:04:22,340 --> 00:04:25,530
x4 它的范围在

125
00:04:25,740 --> 00:04:28,290
0.0001和+0.0001之间 那么

126
00:04:29,720 --> 00:04:30,780
这同样是一个

127
00:04:30,910 --> 00:04:31,960
比-1到+1小得多的范围

128
00:04:32,460 --> 00:04:33,760
比-1到+1小得多的范围

129
00:04:34,040 --> 00:04:36,630
因此 我同样会认为这个特征也不太好

130
00:04:37,850 --> 00:04:39,150
所以 可能你认可的范围

131
00:04:39,430 --> 00:04:40,350
也许可以大于

132
00:04:41,070 --> 00:04:42,010
或者小于 -1 到 +1

133
00:04:42,370 --> 00:04:43,840
但是也别太大

134
00:04:44,040 --> 00:04:45,170
只要大得不多就可以接受

135
00:04:45,610 --> 00:04:47,470
比如 +100

136
00:04:47,650 --> 00:04:49,990
或者也别太小 比如这里的0.001

137
00:04:50,770 --> 00:04:52,530
不同的人有不同的经验

138
00:04:52,870 --> 00:04:53,910
但是我一般是这么考虑的

139
00:04:54,070 --> 00:04:55,440
如果一个特征是在

140
00:04:55,670 --> 00:04:56,750
-3 到 +3 的范围内

141
00:04:56,980 --> 00:04:58,590
那么你应该认为

142
00:04:58,840 --> 00:05:00,120
这个范围是可以接受的

143
00:05:00,170 --> 00:05:01,690
但如果这个范围

144
00:05:02,000 --> 00:05:03,050
大于了 -3 到 +3 的范围

145
00:05:03,440 --> 00:05:04,360
我可能就要开始注意了

146
00:05:04,530 --> 00:05:06,400
如果它的取值

147
00:05:06,700 --> 00:05:09,660
在-1/3 到+1/3的话

148
00:05:10,920 --> 00:05:12,020
我觉得 还不错 可以接受

149
00:05:12,270 --> 00:05:14,880
或者是0到1/3 或-1/3到0

150
00:05:14,910 --> 00:05:17,890
这些典型的范围 我都认为是可以接受的

151
00:05:18,560 --> 00:05:19,310
但如果特征的范围

152
00:05:19,450 --> 00:05:20,640
取得很小的话

153
00:05:20,900 --> 00:05:23,220
比如像这里的 x4 你就要开始考虑进行特征缩放了

154
00:05:23,790 --> 00:05:25,060
因此 总的来说

155
00:05:25,500 --> 00:05:26,780
不用过于担心

156
00:05:27,000 --> 00:05:28,550
你的特征是否在完全

157
00:05:28,700 --> 00:05:30,920
相同的范围或区间内

158
00:05:31,170 --> 00:05:31,930
但是只要他们都

159
00:05:32,090 --> 00:05:35,060
只要它们足够接近的话 梯度下降法就会正常地工作

160
00:05:35,930 --> 00:05:37,530
除了在特征缩放中

161
00:05:37,930 --> 00:05:39,960
将特征除以最大值以外

162
00:05:40,220 --> 00:05:42,080
有时候我们也会进行一个

163
00:05:42,730 --> 00:05:45,070
称为均值归一化的工作(mean normalization)

164
00:05:45,330 --> 00:05:47,150
我的意思是这样的

165
00:05:47,320 --> 00:05:48,130
如果你有一个特征 xi

166
00:05:48,350 --> 00:05:49,810
你就用 xi - μi 来替换

167
00:05:50,230 --> 00:05:51,850
通过这样做 让你的特征值

168
00:05:52,870 --> 00:05:55,260
具有为0的平均值

169
00:05:56,530 --> 00:05:57,730
很明显 我们不需要

170
00:05:57,890 --> 00:05:59,260
把这一步应用到

171
00:05:59,650 --> 00:06:00,750
x0中

172
00:06:00,940 --> 00:06:02,260
因为 x0 总是等于1的

173
00:06:02,360 --> 00:06:03,600
所以它不可能有

174
00:06:03,810 --> 00:06:05,100
为0的的平均值

175
00:06:06,370 --> 00:06:07,760
但是

176
00:06:07,950 --> 00:06:09,320
对其他的特征来说

177
00:06:09,600 --> 00:06:10,320
比如房子的大小

178
00:06:10,960 --> 00:06:14,170
取值介于0到2000

179
00:06:14,310 --> 00:06:15,080
并且假如

180
00:06:15,230 --> 00:06:16,230
房子面积

181
00:06:16,470 --> 00:06:18,340
的平均值

182
00:06:18,500 --> 00:06:20,080
是等于1000的

183
00:06:21,470 --> 00:06:21,950
那么你可以用这个公式

184
00:06:23,940 --> 00:06:24,970
将 x1 的值变为

185
00:06:25,250 --> 00:06:26,270
x1 减去平均值 μ1

186
00:06:26,590 --> 00:06:28,010
再除以2000

187
00:06:28,630 --> 00:06:31,820
类似地

188
00:06:32,530 --> 00:06:34,010
如果你的房子有

189
00:06:34,520 --> 00:06:37,630
五间卧室

190
00:06:39,240 --> 00:06:40,460
并且平均一套房子有

191
00:06:40,890 --> 00:06:41,920
两间卧室 那么你可以

192
00:06:42,110 --> 00:06:44,750
使用这个公式

193
00:06:45,080 --> 00:06:47,460
来归一化你的第二个特征 x2

194
00:06:49,340 --> 00:06:50,720
在这两种情况下

195
00:06:50,840 --> 00:06:52,730
你可以算出新的特征 x1 和 x2

196
00:06:52,930 --> 00:06:54,490
这样它们的范围

197
00:06:54,880 --> 00:06:56,580
可以在-0.5和+0.5之间

198
00:06:57,130 --> 00:06:57,880
当然这肯定不对

199
00:06:58,210 --> 00:07:00,920
x2的值实际上肯定会大于0.5 但很接近

200
00:07:01,800 --> 00:07:03,140
更一般的规律是

201
00:07:03,530 --> 00:07:04,860
你可以用这样的公式

202
00:07:04,900 --> 00:07:06,390
你可以用 (x1 - μ1)/S1

203
00:07:08,060 --> 00:07:10,110
来替换原来的特征 x1

204
00:07:10,940 --> 00:07:13,410
其中定义

205
00:07:13,550 --> 00:07:15,890
μ1的意思是

206
00:07:16,200 --> 00:07:18,290
在训练集中

207
00:07:19,960 --> 00:07:21,310
特征 x1 的平均值

208
00:07:22,320 --> 00:07:24,190
而 S1 是

209
00:07:24,350 --> 00:07:27,420
该特征值的范围

210
00:07:27,820 --> 00:07:28,940
我说的范围是指

211
00:07:29,040 --> 00:07:30,110
最大值减去最小值

212
00:07:30,630 --> 00:07:31,900
最大值减去最小值

213
00:07:32,290 --> 00:07:33,350
或者学过

214
00:07:33,590 --> 00:07:35,360
标准差的同学可以记住

215
00:07:35,850 --> 00:07:37,390
也可以把 S1 设为

216
00:07:37,760 --> 00:07:40,790
变量的标准差

217
00:07:41,020 --> 00:07:43,240
但其实用最大值减最小值就可以了

218
00:07:44,330 --> 00:07:45,170
类似地 对于第二个

219
00:07:45,610 --> 00:07:47,380
特征 x2

220
00:07:47,840 --> 00:07:49,740
你也可以用同样的这个

221
00:07:51,040 --> 00:07:52,220
特征减去平均值

222
00:07:52,800 --> 00:07:54,110
再除以范围 来替换原特征

223
00:07:54,380 --> 00:07:55,980
范围的意思依然是最大值减最小值

224
00:07:56,880 --> 00:07:57,910
这类公式将

225
00:07:58,370 --> 00:07:59,630
把你的特征

226
00:07:59,850 --> 00:08:01,020
变成这样的范围

227
00:08:01,920 --> 00:08:03,320
也许不是完全这样

228
00:08:03,490 --> 00:08:04,820
但大概是这样的范围

229
00:08:04,890 --> 00:08:05,700
顺便提一下

230
00:08:05,940 --> 00:08:07,570
有些同学可能比较仔细

231
00:08:07,710 --> 00:08:09,300
如果我们用最大值减最小值

232
00:08:09,610 --> 00:08:12,410
来表示范围的话 这里的5有可能应该是4

233
00:08:13,140 --> 00:08:14,390
如果最大值为5

234
00:08:14,600 --> 00:08:15,830
那么减去最小值1

235
00:08:16,320 --> 00:08:17,160
这个范围值就是4

236
00:08:17,860 --> 00:08:18,530
但不管咋说 这些取值

237
00:08:18,690 --> 00:08:20,380
都是非常近似的

238
00:08:20,830 --> 00:08:22,010
只要将特征转换为

239
00:08:22,450 --> 00:08:24,750
相近似的范围 就都是可以的

240
00:08:25,200 --> 00:08:27,220
特征缩放其实

241
00:08:27,660 --> 00:08:28,520
并不需要太精确

242
00:08:29,050 --> 00:08:30,390
只是为了让梯度下降

243
00:08:30,790 --> 00:08:32,290
能够运行得更快一点而已

244
00:08:34,610 --> 00:08:35,840
好的 现在你知道了

245
00:08:36,020 --> 00:08:37,420
什么是特征缩放

246
00:08:37,530 --> 00:08:39,040
通过使用这个简单的方法

247
00:08:39,250 --> 00:08:40,650
你可以将梯度下降的速度变得更快

248
00:08:40,870 --> 00:08:43,680
让梯度下降收敛所需的循环次数更少

249
00:08:44,990 --> 00:08:45,540
这就是特征缩放

250
00:08:46,080 --> 00:08:47,190
在接下来的视频中

251
00:08:47,350 --> 00:08:49,410
我将介绍另一种技巧来使梯度下降

252
00:08:49,710 --> 00:08:50,970
在实践中工作地更好 【教育无边界字幕组】翻译：Eroica_CMCS 校对：所罗门捷列夫
审核：Naplessss