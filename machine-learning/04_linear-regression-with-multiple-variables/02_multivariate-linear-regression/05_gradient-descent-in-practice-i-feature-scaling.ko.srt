1
00:00:00,190 --> 00:00:01,270
이번 시간과

2
00:00:01,440 --> 00:00:02,720
다음 시간에는

3
00:00:02,850 --> 00:00:04,040
gradient descent를 잘 활용할 수 있는

4
00:00:04,180 --> 00:00:06,940
몇 가지 방법들을 알려주고자 합니다.

5
00:00:07,680 --> 00:00:10,250
이번 시간에는 먼저 feature scaling이라고 하는 것을 알려주게습니다.

6
00:00:11,770 --> 00:00:12,210
개념은 이렇습니다.

7
00:00:13,030 --> 00:00:14,080
만약

8
00:00:14,180 --> 00:00:15,880
여러개의 feature가 있고,

9
00:00:16,320 --> 00:00:17,410
feature의 단위크기가

10
00:00:18,050 --> 00:00:19,440
비슷하다면,

11
00:00:19,570 --> 00:00:20,480
이 말은 즉,

12
00:00:20,650 --> 00:00:22,130
서로 다른 feature라도

13
00:00:22,300 --> 00:00:23,390
범위가 같다면

14
00:00:24,420 --> 00:00:26,490
gradient descent는 더 빠르게 수렴할 수 있습니다.

15
00:00:27,510 --> 00:00:28,680
구체적으로

16
00:00:28,820 --> 00:00:29,860
두 개의 feature가 있다고 해봅시다.

17
00:00:30,380 --> 00:00:31,680
x_1은 집의 크기이고

18
00:00:31,950 --> 00:00:32,860
범위는

19
00:00:33,530 --> 00:00:34,540
0에서 2000사이에 값을 가집니다.

20
00:00:35,490 --> 00:00:36,270
두 번째는 침실의 수이고,

21
00:00:36,520 --> 00:00:37,570
범위는

22
00:00:37,820 --> 00:00:39,250
1에서 5사이에 값을 가집니다.

23
00:00:40,100 --> 00:00:41,690
만약 cost function J(θ)를

24
00:00:41,800 --> 00:00:43,000
그래프로 그린다면,

25
00:00:44,810 --> 00:00:46,540
이렇게 그릴 수 있을 것입니다.

26
00:00:46,750 --> 00:00:49,010
음,

27
00:00:49,230 --> 00:00:50,570
J(θ)는

28
00:00:50,910 --> 00:00:53,590
파라미터 θ_0, θ_1, θ_2의 함수입니다.

29
00:00:54,300 --> 00:00:55,400
θ_0는 고려하지 않고,

30
00:00:56,020 --> 00:00:57,230
그래서 θ0은 없애고, θ1과 θ2의 함수들만 보도록 하겠습니다.

31
00:00:57,480 --> 00:00:58,730
θ_1과

32
00:00:58,840 --> 00:01:01,080
θ_2의 함수로 보겠습니다.

33
00:01:01,510 --> 00:01:02,810
하지만, x_1의 범위가

34
00:01:02,940 --> 00:01:04,110
x_2보다

35
00:01:04,370 --> 00:01:05,790
크다면,

36
00:01:06,120 --> 00:01:07,270
cost function J(θ)의

37
00:01:07,340 --> 00:01:08,320
모양은

38
00:01:09,420 --> 00:01:11,400
굉장히

39
00:01:11,690 --> 00:01:14,720
뾰족한 타원모양이 나올 수 있습니다.

40
00:01:15,070 --> 00:01:16,620
2000:5의 비율만 아니면,

41
00:01:16,770 --> 00:01:18,470
더 안정한 모양이 될 수 있습니다.

42
00:01:18,800 --> 00:01:20,190
엄청나게, 길고

43
00:01:20,560 --> 00:01:23,070
얇은 타원 모양의

44
00:01:23,320 --> 00:01:24,950
cost function J(θ)가

45
00:01:25,310 --> 00:01:27,940
그러졌습니다.

46
00:01:29,420 --> 00:01:30,860
만약, 이 cost function에

47
00:01:30,930 --> 00:01:34,290
gradient descent를 적용한다면,

48
00:01:34,830 --> 00:01:36,480
gradient는

49
00:01:36,970 --> 00:01:38,660
오랜 시간 동안

50
00:01:39,080 --> 00:01:40,360
앞 뒤로 진동하며,

51
00:01:41,100 --> 00:01:43,130
엄청난 시간이 지나고 나서야

52
00:01:43,190 --> 00:01:46,120
마침내 최소값에 도달할 것입니다.

53
00:01:47,470 --> 00:01:48,720
실제로,

54
00:01:48,890 --> 00:01:50,400
등고선이 극단적인 경우를 생각해보면

55
00:01:50,580 --> 00:01:51,970
즉 엄청 얇은,

56
00:01:52,480 --> 00:01:54,300
얇고 긴 등고선이라면,

57
00:01:56,230 --> 00:01:57,030
그리고 더욱 극단적으로 과장한다면,

58
00:01:57,380 --> 00:01:59,060
gradient descent는

59
00:01:59,790 --> 00:02:02,310
훨씬 더 많은

60
00:02:02,630 --> 00:02:04,280
시간을 소요하며,

61
00:02:04,690 --> 00:02:06,030
구불구불하게 가다가,

62
00:02:06,120 --> 00:02:08,270
오랜 시간이 지나서야 최소값을 찾을 수 있습니다.

63
00:02:12,130 --> 00:02:14,370
이 때, 유용한 방법이

64
00:02:14,780 --> 00:02:16,280
feature를 조절(scale)하는 것입니다.

65
00:02:17,380 --> 00:02:18,760
구체적으로

66
00:02:19,200 --> 00:02:20,370
feature x_1은

67
00:02:20,570 --> 00:02:21,770
집 크기를

68
00:02:21,870 --> 00:02:23,070
2000으로 나눈 값이라 하고,

69
00:02:24,040 --> 00:02:25,140
x_2는

70
00:02:25,270 --> 00:02:26,520
침실의 수를

71
00:02:26,940 --> 00:02:29,010
5로 나눈 값이라 한다면

72
00:02:29,170 --> 00:02:30,020
cost function (θ)의

73
00:02:30,090 --> 00:02:31,840
모양은

74
00:02:32,900 --> 00:02:34,430
훨씬

75
00:02:34,840 --> 00:02:36,990
덜 뾰족한 원에 가까운 타원모양이 될 수 있습니다.

76
00:02:38,210 --> 00:02:39,180
이러한

77
00:02:39,520 --> 00:02:40,540
cost function에

78
00:02:40,750 --> 00:02:42,120
gradient descent를 하면은

79
00:02:44,110 --> 00:02:45,630
정확하게,

80
00:02:45,860 --> 00:02:47,430
똑바른 방향으로

81
00:02:47,540 --> 00:02:48,830
최소값을 찾는 것을 볼 수 있습니다

82
00:02:49,390 --> 00:02:51,200
난해한 경로로

83
00:02:51,530 --> 00:02:52,530
복잡한 궤도를 그리며

84
00:02:52,620 --> 00:02:53,520
구불구불하게

85
00:02:54,310 --> 00:02:55,910
가지 않습니다.

86
00:02:57,300 --> 00:02:58,710
즉, feature를 조절하여,

87
00:02:58,950 --> 00:03:01,000
적절한 범위가 되도록 하는 것입니다.

88
00:03:01,620 --> 00:03:02,810
예를 들면,

89
00:03:02,970 --> 00:03:04,150
x_1과

90
00:03:04,300 --> 00:03:06,960
x_2는 0에서 1사이의 값을 가지죠.

91
00:03:09,580 --> 00:03:12,290
이제 더 빠르게 수렴하는 gradient descent를

92
00:03:12,690 --> 00:03:13,810
구현할 수 있게 되었습니다.

93
00:03:18,120 --> 00:03:19,640
보통은,

94
00:03:20,160 --> 00:03:21,240
feature scaling을 할 때,

95
00:03:21,530 --> 00:03:22,480
모든 feature가

96
00:03:22,750 --> 00:03:25,670
대략 -1에서

97
00:03:25,780 --> 00:03:28,170
+1사이의 범위가 있기를 원합니다.

98
00:03:28,960 --> 00:03:31,710
확실히, feature x_0은 항상 1입니다.

99
00:03:31,760 --> 00:03:32,810
그래서, x_0은 그 범위안에 있죠.

100
00:03:34,110 --> 00:03:35,150
하지만 다른 feature들은

101
00:03:35,630 --> 00:03:36,950
각각 다른 수로 나누어서

102
00:03:37,330 --> 00:03:39,150
이 범위로 만들 수도 있습니다.

103
00:03:39,510 --> 00:03:41,520
그렇지만 -1과 +1은 중요하지 않습니다.

104
00:03:42,270 --> 00:03:42,900
만약

105
00:03:44,150 --> 00:03:45,340
x_1이

106
00:03:45,510 --> 00:03:48,000
0에서 3 사이의 범위를 가지더라도, 문제가 되지 않습니다.

107
00:03:48,400 --> 00:03:49,410
만약

108
00:03:49,600 --> 00:03:51,190
다른 feature도

109
00:03:52,140 --> 00:03:54,020
-2에서 +0.5의 범위를 가지더라도,

110
00:03:54,300 --> 00:03:55,710
역시,

111
00:03:56,070 --> 00:03:57,070
-1에서 +1과

112
00:03:57,320 --> 00:03:59,160
근사하기 때문에, 괜찮습니다. 괜찮아요.

113
00:04:00,310 --> 00:04:01,260
하지만 만약

114
00:04:01,340 --> 00:04:02,580
또 다른 feature x_3의

115
00:04:02,820 --> 00:04:04,780
범위가

116
00:04:05,840 --> 00:04:09,070
-100에서 +100의 범위라면

117
00:04:09,330 --> 00:04:10,850
x_3는

118
00:04:11,090 --> 00:04:13,570
-1과 +1과는 완전히 다른 값이 됩니다.

119
00:04:13,860 --> 00:04:15,020
그래서

120
00:04:15,230 --> 00:04:17,480
이것은 좋은 feature가 아닙니다.

121
00:04:17,970 --> 00:04:19,340
그리고 유사한 경우로는, feature가

122
00:04:19,420 --> 00:04:20,680
엄청 작은 범위일 때,

123
00:04:20,950 --> 00:04:22,060
즉 x_4가

124
00:04:22,340 --> 00:04:25,530
-0.0001에서

125
00:04:25,740 --> 00:04:28,290
+0.0001사이의 값을 가질 때,

126
00:04:29,720 --> 00:04:30,780
역시

127
00:04:30,910 --> 00:04:31,960
-1에서 +1의 범위보다

128
00:04:32,460 --> 00:04:33,760
작은 범위를 가집니다

129
00:04:34,040 --> 00:04:36,630
feature를 조절해야합니다.

130
00:04:37,850 --> 00:04:39,150
결국 값의 범위는

131
00:04:39,430 --> 00:04:40,350
1보다 크거나

132
00:04:41,070 --> 00:04:42,010
작더라도

133
00:04:42,370 --> 00:04:43,840
괜찮지만

134
00:04:44,040 --> 00:04:45,170
100처럼

135
00:04:45,610 --> 00:04:47,470
엄청 크거나,

136
00:04:47,650 --> 00:04:49,990
혹은 0.0001처럼 엄청 작으면 안됩니다.

137
00:04:50,770 --> 00:04:52,530
사람마다 규칙은 다릅니다.

138
00:04:52,870 --> 00:04:53,910
저 같은 경우,

139
00:04:54,070 --> 00:04:55,440
feature의

140
00:04:55,670 --> 00:04:56,750
범위가

141
00:04:56,980 --> 00:04:58,590
-3에서

142
00:04:58,840 --> 00:05:00,120
+3의 범위를 가진다면

143
00:05:00,170 --> 00:05:01,690
괜찮다고 보지만,

144
00:05:02,000 --> 00:05:03,050
+3에서 -3의 범위보다

145
00:05:03,440 --> 00:05:04,360
크다면

146
00:05:04,530 --> 00:05:06,400
한 번 생각해 봐야 합니다.

147
00:05:06,700 --> 00:05:09,660
-3분의 1에서 3분의 1범위도

148
00:05:10,920 --> 00:05:12,020
괜찮습니다.

149
00:05:12,270 --> 00:05:14,880
0에서 3분의 1이나 -3분의 1에서 0도요.

150
00:05:14,910 --> 00:05:17,890
보통 0 근처의 범위면 괜찮습니다.

151
00:05:18,560 --> 00:05:19,310
하지만,

152
00:05:19,450 --> 00:05:20,640
x_4의 범위만큼 작다면,

153
00:05:20,900 --> 00:05:23,220
문제가 됩니다.

154
00:05:23,790 --> 00:05:25,060
그래서 알아두었으면 하는 점은

155
00:05:25,500 --> 00:05:26,780
feature가

156
00:05:27,000 --> 00:05:28,550
같은 크기나

157
00:05:28,700 --> 00:05:30,920
정확하게 똑같은 범위가 아니더라도 걱정할 필요가 없다는 것입니다.

158
00:05:31,170 --> 00:05:31,930
gradient descent하기에

159
00:05:32,090 --> 00:05:35,060
충분히 근접하는 한은 괜찮습니다.

160
00:05:35,930 --> 00:05:37,530
feature scaling을 할 때,

161
00:05:37,930 --> 00:05:39,960
최대값으로 나누고 추가적으로,

162
00:05:40,220 --> 00:05:42,080
mean normalization이라는 것을

163
00:05:42,730 --> 00:05:45,070
할 수 있습니다.

164
00:05:45,330 --> 00:05:47,150
mean normalization은

165
00:05:47,320 --> 00:05:48,130
feature x_i를

166
00:05:48,350 --> 00:05:49,810
x_i - μ_i로

167
00:05:50,230 --> 00:05:51,850
바꾸면 됩니다.

168
00:05:52,870 --> 00:05:55,260
그러면 feature의 평균이 대략 0이 됩니다.

169
00:05:56,530 --> 00:05:57,730
분명한 것은

170
00:05:57,890 --> 00:05:59,260
feature x_0에는 사용하지 않는다는 것입니다.

171
00:05:59,650 --> 00:06:00,750
feature x_0은

172
00:06:00,940 --> 00:06:02,260
모두 1이기 때문에,

173
00:06:02,360 --> 00:06:03,600
평균이

174
00:06:03,810 --> 00:06:05,100
0이 될 수 없습니다.

175
00:06:06,370 --> 00:06:07,760
하지만

176
00:06:07,950 --> 00:06:09,320
다른 feature의 경우,

177
00:06:09,600 --> 00:06:10,320
집 크기의 범위가

178
00:06:10,960 --> 00:06:14,170
0부터

179
00:06:14,310 --> 00:06:15,080
2000사이이고,

180
00:06:15,230 --> 00:06:16,230
집의 평균 크기가

181
00:06:16,470 --> 00:06:18,340
만약

182
00:06:18,500 --> 00:06:20,080
1000이면,

183
00:06:21,470 --> 00:06:21,950
이 식을 사용하면 됩니다.

184
00:06:23,940 --> 00:06:24,970
크기, feature x_1는

185
00:06:25,250 --> 00:06:26,270
평균값을 빼고나서,

186
00:06:26,590 --> 00:06:28,010
2000으로 나누고

187
00:06:28,630 --> 00:06:31,820
평균과 비슷하게 합니다.

188
00:06:32,530 --> 00:06:34,010
집이

189
00:06:34,520 --> 00:06:37,630
집이 1개에서 5개 사이의 침실을 가지고,

190
00:06:39,240 --> 00:06:40,460
평균적으로

191
00:06:40,890 --> 00:06:41,920
2개의 침실을 가진다면,

192
00:06:42,110 --> 00:06:44,750
이 식으로

193
00:06:45,080 --> 00:06:47,460
두 번째 feature x_2에 mean normalize를 하면 됩니다.

194
00:06:49,340 --> 00:06:50,720
두 가지 경우 모두,

195
00:06:50,840 --> 00:06:52,730
feature x_1과 x_2가

196
00:06:52,930 --> 00:06:54,490
대략

197
00:06:54,880 --> 00:06:56,580
-0.5에서 +0.5사이의 값을 가지게 됩니다.

198
00:06:57,130 --> 00:06:57,880
정확히는 x_2가

199
00:06:58,210 --> 00:07:00,920
0.5보다 약간 크게 되지만, 거의 근접합니다.

200
00:07:01,800 --> 00:07:03,140
일반화된 식은

201
00:07:03,530 --> 00:07:04,860
feature x_1을

202
00:07:04,900 --> 00:07:06,390
s_1분의

203
00:07:08,060 --> 00:07:10,110
x_1-μ_1로

204
00:07:10,940 --> 00:07:13,410
바꿉니다.

205
00:07:13,550 --> 00:07:15,890
μ_1은

206
00:07:16,200 --> 00:07:18,290
training set에서

207
00:07:19,960 --> 00:07:21,310
x_1의 평균값이고

208
00:07:22,320 --> 00:07:24,190
s_1은

209
00:07:24,350 --> 00:07:27,420
feature의

210
00:07:27,820 --> 00:07:28,940
범위,

211
00:07:29,040 --> 00:07:30,110
즉, 최대값에서

212
00:07:30,630 --> 00:07:31,900
최소값을 뺀 값이

213
00:07:32,290 --> 00:07:33,350
될 수도 있고,

214
00:07:33,590 --> 00:07:35,360
편차를 알고있다면,

215
00:07:35,850 --> 00:07:37,390
s_1은

216
00:07:37,760 --> 00:07:40,790
표준편차로 써도 괜찮습니다.

217
00:07:41,020 --> 00:07:43,240
하지만 최대값 - 최소값도 괜찮습니다.

218
00:07:44,330 --> 00:07:45,170
비슷하게 두 번째  feature인

219
00:07:45,610 --> 00:07:47,380
x_2도,

220
00:07:47,840 --> 00:07:49,740
x_2를

221
00:07:51,040 --> 00:07:52,220
feature의 평균만큼 빼고,

222
00:07:52,800 --> 00:07:54,110
최대값-최소값으로

223
00:07:54,380 --> 00:07:55,980
나눕니다.

224
00:07:56,880 --> 00:07:57,910
feature에 관한 이 식은

225
00:07:58,370 --> 00:07:59,630
정확하게는 아니더라도

226
00:07:59,850 --> 00:08:01,020
대략

227
00:08:01,920 --> 00:08:03,320
저 범위만큼

228
00:08:03,490 --> 00:08:04,820
될 것입니다.

229
00:08:04,890 --> 00:08:05,700
엄밀히

230
00:08:05,940 --> 00:08:07,570
따진다면,

231
00:08:07,710 --> 00:08:09,300
최대값에서

232
00:08:09,610 --> 00:08:12,410
최소값을 뺏을 때, 실제로는 4가 됩니다.

233
00:08:13,140 --> 00:08:14,390
즉, 최대값이 5고,

234
00:08:14,600 --> 00:08:15,830
최소값이 1이면

235
00:08:16,320 --> 00:08:17,160
range는

236
00:08:17,860 --> 00:08:18,530
4가 되지만

237
00:08:18,690 --> 00:08:20,380
여기서 대부분이 근사값이고,

238
00:08:20,830 --> 00:08:22,010
feature의 값도

239
00:08:22,450 --> 00:08:24,750
range와 거의 차이가 없기 때문에 괜찮습니다.

240
00:08:25,200 --> 00:08:27,220
feature scaling은

241
00:08:27,660 --> 00:08:28,520
너무 정확할 필요는 없습니다.

242
00:08:29,050 --> 00:08:30,390
gradient descent가

243
00:08:30,790 --> 00:08:32,290
훨씬 더 빨라지면 되기 때문이죠

244
00:08:34,610 --> 00:08:35,840
이제

245
00:08:36,020 --> 00:08:37,420
feature scaling을 배웠고,

246
00:08:37,530 --> 00:08:39,040
이 방법을 적용시키면,

247
00:08:39,250 --> 00:08:40,650
gradient descent는 더 빨라지고,

248
00:08:40,870 --> 00:08:43,680
적은 수의 반복으로도 수렴합니다.

249
00:08:44,990 --> 00:08:45,540
여기까지가 feature scaling입니다.

250
00:08:46,080 --> 00:08:47,190
다음 시간에는,

251
00:08:47,350 --> 00:08:49,410
gradient descent를 효율적으로 다루는

252
00:08:49,710 --> 00:08:50,970
다른 방법에 대해 이야기를 해보겠습니다.