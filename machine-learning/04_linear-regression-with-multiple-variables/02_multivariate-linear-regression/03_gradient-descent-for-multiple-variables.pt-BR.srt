1
00:00:00,220 --> 00:00:03,688
No vídeo anterior, nós falamos sobre
o formato da hipótese para Regressão Linear

2
00:00:03,688 --> 00:00:07,246
com múltiplos atributos ou múltiplas variáveis.

3
00:00:07,246 --> 00:00:11,912
Nesse vídeo, iremos conversar sobre como
ajustar os parâmetros daquelas hipóteses.

4
00:00:11,912 --> 00:00:15,175
Iremos particularmente discutir sobre
como usar o Gradiente Descendente

5
00:00:15,175 --> 00:00:19,875
para Regressão Linear com múltiplos atributos.

6
00:00:19,875 --> 00:00:24,802
Resumindo rapidamente nossa notação,
essa é nossa hipótese formal para

7
00:00:24,802 --> 00:00:31,509
Regressão Linear Multivariável, onde
adotamos a convenção de que x0=1.

8
00:00:31,509 --> 00:00:37,505
Os parâmetros desse modelo são "θ₀" até "θn",

9
00:00:37,505 --> 00:00:42,385
mas ao invés de pensar neles como parâmetros
independentes, o que também seria válido,

10
00:00:42,385 --> 00:00:51,175
eu vou considerá-los como Θ onde
Θ é um vetor de dimensão n+1.

11
00:00:51,175 --> 00:00:55,498
Assim, os parâmetros desses modelo

12
00:00:55,498 --> 00:00:58,674
serão um vetor.

13
00:00:58,674 --> 00:01:03,507
Nossa função de custo é "J(θ₀,...,θn)",
que é dada pela

14
00:01:03,507 --> 00:01:08,983
soma dos quadrados dos erros.
Novamente, em vez de pensarmos em "J" como

15
00:01:08,983 --> 00:01:14,016
uma função desses n+1 números,
eu vou começar a escrever "J" como

16
00:01:14,016 --> 00:01:22,275
uma função de um vetor parâmetro Θ,
então esse Θ (em azul) é um vetor.

17
00:01:22,275 --> 00:01:26,897
Aqui está o esqueleto do Gradiente Descendente.
Nós vamos atualizar repetidamente

18
00:01:26,897 --> 00:01:32,142
cada parâmetro "θj", de acordo com a regra:
"θj-α·(termo derivativo)".

19
00:01:32,142 --> 00:01:37,868
E vamos escrever isso apenas como "J(Θ)".
Onde "θj" é atualizado como:

20
00:01:37,868 --> 00:01:41,840
"θj" menos a taxa de aprendizagem "α",
vezes a derivada parcial

21
00:01:41,840 --> 00:01:47,840
da função de custo com relação ao parâmetro "θj".

22
00:01:47,840 --> 00:01:51,305
Vejamos o resultado disso quando
implementados o Gradiente Descendente e,

23
00:01:51,305 --> 00:01:55,985
particularmente, vejamos qual a
aparência da derivada parcial.

24
00:01:55,985 --> 00:02:01,383
Isto é o nosso Gradiente Descendente
quando tínhamos "n=1" variáveis.

25
00:02:01,383 --> 00:02:06,782
Nós temos duas regras para atualização separadas
para os parâmetros "θ0" e "θ1".

26
00:02:06,782 --> 00:02:12,779
Espero que você esteja familiarizado com isso, e esse termo

27
00:02:12,779 --> 00:02:17,672
era a derivada parcial da função de custo
com respeito ao parâmetro θ0,

28
00:02:17,672 --> 00:02:21,891
e analogamente, nós temos uma regra de
atualização diferente para o parâmetro "θ1".

29
00:02:21,891 --> 00:02:26,259
A única diferença é que antes,
quando tínhamos apenas um atributo,

30
00:02:26,259 --> 00:02:31,992
nós chamávamos a variável de "x(i)", mas agora,

31
00:02:31,992 --> 00:02:38,462
nós chamaremos isso de
"x(i)₁" para indicar nossa variável 1.

32
00:02:38,462 --> 00:02:41,019
Então isso é o que faríamos
se tivéssemos apenas 1 variável.

33
00:02:41,019 --> 00:02:44,496
Vamos analisar o novo algoritmo. Quando
tivermos mais de um atributo,

34
00:02:44,496 --> 00:02:47,350
onde o número de atributos "n", pode ser
muito maior que 1,

35
00:02:47,350 --> 00:02:53,158
nós teremos essa regra de atualização para o
Gradiente Descendente e, talvez para aqueles

36
00:02:53,158 --> 00:02:57,781
que saibam Cálculo, se vocês pegarem
a definição da função de custo,

37
00:02:57,781 --> 00:03:03,312
e as derivadas parciais da função de
custo "J" com relação ao parâmetro "θj",

38
00:03:03,312 --> 00:03:08,119
vocês descobrirão que essa derivada parcial é exatamente

39
00:03:08,119 --> 00:03:10,665
o termo dentro do retângulo azul.

40
00:03:10,665 --> 00:03:14,837
E, se você implementar isso,
terá uma implementação funcional

41
00:03:14,837 --> 00:03:18,962
do Gradiente Descendente
para Regressão Linear Multivariável.

42
00:03:18,962 --> 00:03:21,572
A última coisa que quero fazer nesse slide, é dar uma noção

43
00:03:21,572 --> 00:03:26,882
do porquê esses dois algoritmos são a mesmo coisa,
ou porque eles são similares,

44
00:03:26,882 --> 00:03:30,904
ou por que ambos são algoritmos de Gradiente Descendente.

45
00:03:30,904 --> 00:03:34,363
Vamos considerar um caso onde temos dois ou mais atributos,

46
00:03:34,363 --> 00:03:37,488
resultando em três regras de atualização

47
00:03:37,488 --> 00:03:42,680
para os parâmetros θ₀, θ₁, θ₂,
e talvez ainda outros valores de Θ.

48
00:03:42,680 --> 00:03:49,457
Se você observar a regra de atualização para θ₀, o que você descobrirá é que

49
00:03:49,457 --> 00:03:55,300
teremos a mesma regra de atualização que tínhamos anteriormente,

50
00:03:55,300 --> 00:03:57,350
para o caso de "n=1".

51
00:03:57,350 --> 00:04:00,203
E a razão para eles serem equivalentes

52
00:04:00,203 --> 00:04:06,871
é porque na nossa notação convencional, tínhamos "x(i)₀=1",

53
00:04:06,871 --> 00:04:12,003
e por isso esses dois termos dentro da caixa magenta são equivalentes.

54
00:04:12,003 --> 00:04:16,010
Analogamente, se você observar a regra
de atualização para θ₁, você descobrirá que

55
00:04:16,010 --> 00:04:21,540
esse termo (caixa em azual claro) é equivalente ao anterior,

56
00:04:21,540 --> 00:04:25,020
à regra de atualização que tínhamos para θ₁,

57
00:04:25,020 --> 00:04:30,222
sendo que, estamos usando essa nova notação x(i)₁ apenas para indicar

58
00:04:30,222 --> 00:04:37,605
a nossa primeira variável.
E agora que temos mais de uma variável, nós podemos ter

59
00:04:37,605 --> 00:04:43,560
regras de atualização similares para os
outros parâmetros θ₂ e assim por diante.

60
00:04:43,560 --> 00:04:48,219
Tem muita coisa acontecendo nesse slide,
então eu sugiro que,

61
00:04:48,219 --> 00:04:52,020
se você precisar, pause o vídeo
e observe toda a matemática nesse slide

62
00:04:52,020 --> 00:04:55,446
com calma para ter certeza que você tenha
entendido tudo isso.

63
00:04:55,446 --> 00:05:00,440
Mas, se você implementar o algoritmo escrito aqui, então você terá

64
00:05:00,440 --> 00:05:51,300
uma implementação funcional de
Regressão Linear para múltiplas variáveis.
Tradução: Eduardo Bonet | Revisão: Pablo de Morais Andrade