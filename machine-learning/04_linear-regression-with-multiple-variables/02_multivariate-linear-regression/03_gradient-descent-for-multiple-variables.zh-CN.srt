1
00:00:00,220 --> 00:00:03,688
在之前的视频中 我们谈到了一种线性回归的假设形式

2
00:00:03,688 --> 00:00:07,246
这是一种有多特征或者是多变量的形式

3
00:00:07,246 --> 00:00:11,912
在本节视频中 我们将会谈到如何找到满足这一假设的参数

4
00:00:11,912 --> 00:00:15,175
尤其是如何使用梯度下降法

5
00:00:15,175 --> 00:00:19,875
来解决多特征的线性回归问题

6
00:00:19,875 --> 00:00:24,802
为尽快让你理解 现假设现有多元线性回归

7
00:00:24,802 --> 00:00:31,509
并约定 x0=1

8
00:00:31,509 --> 00:00:37,505
该模型的参数是从 θ0 到 θn

9
00:00:37,505 --> 00:00:42,385
不要认为这是 n+1 个单独的参数

10
00:00:42,385 --> 00:00:51,175
你可以把这 n+1 个 θ 参数想象成一个 n+1 维的向量 θ

11
00:00:51,175 --> 00:00:55,498
所以 你现在就可以把这个模型的参数

12
00:00:55,498 --> 00:00:58,674
想象成其本身就是一个 n+1 维的向量

13
00:00:58,674 --> 00:01:03,507
我们的代价函数是从 θ0 到 θn 的函数 J 并给出了误差项平方的和

14
00:01:03,507 --> 00:01:08,983
但同样地 不要把函数 J

15
00:01:08,983 --> 00:01:14,016
想成是一个关于 n+1 个自变量的函数

16
00:01:14,016 --> 00:01:22,275
而是看成带有一个 n+1 维向量的函数

17
00:01:22,275 --> 00:01:26,897
这就是梯度下降法

18
00:01:26,897 --> 00:01:32,142
我们将会不停地用 θj 减去 α 倍的导数项 来替代 θj

19
00:01:32,142 --> 00:01:37,868
同样的方法 我们写出函数J(θ)

20
00:01:37,868 --> 00:01:41,840
因此 θj 被更新成 θj 减去学习率 α 与对应导数的乘积

21
00:01:41,840 --> 00:01:47,840
就是代价函数的对参数 θj 的偏导数

22
00:01:47,840 --> 00:01:51,305
当我们实现梯度下降法后 你可以仔细观察一下

23
00:01:51,305 --> 00:01:55,985
尤其是它的偏导数项

24
00:01:55,985 --> 00:02:01,383
下面是我们当特征 n=1 时 梯度下降的情况

25
00:02:01,383 --> 00:02:06,782
我们有两条针对参数 θ0 和 θ1 不同的更新规则

26
00:02:06,782 --> 00:02:12,779
希望这些对你来说并不陌生 这一项是代价函数里部分求导的结果

27
00:02:12,779 --> 00:02:17,672
就是代价函数相对于 θ0 的偏导数

28
00:02:17,672 --> 00:02:21,891
同样 对参数 θ1 我们有另一个更新规则 

29
00:02:21,891 --> 00:02:26,259
仅有的一点区别是 当我们之前只有一个特征

30
00:02:26,259 --> 00:02:31,992
我们称该特征为x(i) 但现在我们在新符号里

31
00:02:31,992 --> 00:02:38,462
我们会标记它为 x 上标 (i) 下标1 来表示我们的特征

32
00:02:38,462 --> 00:02:41,019
以上就是当我们仅有一个特征时候的算法

33
00:02:41,019 --> 00:02:44,496
下面我们来讲讲当有一个以上特征时候的算法

34
00:02:44,496 --> 00:02:47,350
现有数目远大于1的很多特征

35
00:02:47,350 --> 00:02:53,158
我们的梯度下降更新规则变成了这样

36
00:02:53,158 --> 00:02:57,781
有些同学可能知道微积分 如果你看看代价函数

37
00:02:57,781 --> 00:03:03,312
代价函数 J 对参数 θj 求偏导数

38
00:03:03,312 --> 00:03:08,119
你会发现 求其偏导数的那一项

39
00:03:08,119 --> 00:03:10,665
我已经用蓝线圈出来了

40
00:03:10,665 --> 00:03:14,837
如果你实现了这一步

41
00:03:14,837 --> 00:03:18,962
你将会得到多元线性回归的梯度下降算法

42
00:03:18,962 --> 00:03:21,572
最后 我想让你明白

43
00:03:21,572 --> 00:03:26,882
为什么新旧两种算法实际上是一回事儿

44
00:03:26,882 --> 00:03:30,904
或者说为什么这两个是类似的算法 为什么它们都是梯度下降算法

45
00:03:30,904 --> 00:03:34,363
考虑这样一个情况

46
00:03:34,363 --> 00:03:37,488
有两个或以上个数的特征

47
00:03:37,488 --> 00:03:42,680
同时我们有对θ1、θ2、θ3的三条更新规则 当然可能还有其它参数

48
00:03:42,680 --> 00:03:49,457
如果你观察θ0的更新规则

49
00:03:49,457 --> 00:03:55,300
 你会发现这跟之前

50
00:03:55,300 --> 00:03:57,350
n=1的情况相同

51
00:03:57,350 --> 00:04:00,203
它们之所以是等价的

52
00:04:00,203 --> 00:04:06,871
这是因为在我们的标记约定里有 x(i)0=1 也就是

53
00:04:06,871 --> 00:04:12,003
我用品红色圈起来的两项是等价的

54
00:04:12,003 --> 00:04:16,010
同样地 如果你观察 θ1 的更新规则

55
00:04:16,010 --> 00:04:21,540
你会发现这里的这一项是

56
00:04:21,540 --> 00:04:25,020
和之前对参数θ1的更新项是等价的

57
00:04:25,020 --> 00:04:30,222
在这里我们只是用了新的符号x(i)1来表示我们的第一个特征

58
00:04:30,222 --> 00:04:37,605
现在我们有个更多的特征 那么就可以用与之前相同的更新规则

59
00:04:37,605 --> 00:04:43,560
我们可以用同样的规则来处理 θ2 等其它参数

60
00:04:43,560 --> 00:04:48,219
这张幻灯片的内容不少 请务必仔细理解

61
00:04:48,219 --> 00:04:52,020
如果觉得幻灯片上数学公式没看懂 尽管暂停视频

62
00:04:52,020 --> 00:04:55,446
请确保理解了再继续后面的学习

63
00:04:55,446 --> 00:05:00,440
如果你将这些算法都实现了

64
00:05:00,440 --> 00:05:51,300
那么你就可以直接应用到多元线性回归中了 【教育无边界字幕组】翻译：Yuens 校对：所罗门捷列夫 审核：Naplessss