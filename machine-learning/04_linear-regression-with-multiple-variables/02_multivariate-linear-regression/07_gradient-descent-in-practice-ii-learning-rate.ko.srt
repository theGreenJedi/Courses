1
00:00:00,450 --> 00:00:03,210
이번 비디오에서는, 
gradient descent에 대한

2
00:00:03,210 --> 00:00:05,070
유용한 정보를 알려주고자 합니다.

3
00:00:05,070 --> 00:00:08,650
이 번 비디오에서 주제는
learning rate 알파에 관한 것입니다.

4
00:00:09,860 --> 00:00:13,180
구체적으로, 여기 gradient descent 식이 있습니다.

5
00:00:13,180 --> 00:00:16,270
이 번 비디오에서 하고자 하는 것은

6
00:00:16,270 --> 00:00:19,050
debugging입니다.

7
00:00:19,050 --> 00:00:22,390
gradient descent가 잘 돌아가게 하기 위한 방법입니다.

8
00:00:22,390 --> 00:00:26,480
두 번째로는, learning rate 알파를 고르는 방법

9
00:00:26,480 --> 00:00:29,250
즉 어떤 값으로 시작할지에 대해 이야기해보겠습니다.

10
00:00:29,250 --> 00:00:32,770
gradient descent가 어떻게 돌아가는지 확인해 보도록 하죠

11
00:00:32,770 --> 00:00:34,150
정확히,

12
00:00:34,150 --> 00:00:38,219
gradient descent의 목적은
cost function J（θ）가 최소화되는

13
00:00:38,219 --> 00:00:42,553
θ의 값을 찾는 것입니다.

14
00:00:42,553 --> 00:00:47,483
cost function J(θ)를 gradient descent 할 때마다,

15
00:00:47,483 --> 00:00:49,750
그래프를 그리겠습니다.

16
00:00:49,750 --> 00:00:53,515
그래서 x축은 gradient descent의 동작 횟수이고

17
00:00:53,515 --> 00:00:58,659
gradient descent를 하면, 이처럼 그래프가 그려질 것입니다.

18
00:00:59,720 --> 00:01:02,960
x축은 반복 횟수라는 것을 잊지마세요.

19
00:01:02,960 --> 00:01:07,795
이전에, J(θ) 그래프를 그릴 때에는, x축,

20
00:01:07,795 --> 00:01:13,107
즉 수평축이 파라미터 벡터였지만, 이번에는 아닙니다.

21
00:01:13,107 --> 00:01:15,767
구체적으로 이 점은,

22
00:01:15,767 --> 00:01:20,570
gradient descent를 100번 돌렸을 때의 값입니다.

23
00:01:20,570 --> 00:01:25,240
100번 돌리고나서 얻은 θ에 대한 값과

24
00:01:25,240 --> 00:01:28,770
100번 반복했을 때 θ의 값을 구할 수 있습니다.

25
00:01:28,770 --> 00:01:32,630
cost function J(θ)를 보죠.

26
00:01:32,630 --> 00:01:35,630
100번 반복 후 나온 θ가 있을 때

27
00:01:35,630 --> 00:01:39,760
이 높이가 J(θ)의 값입니다.

28
00:01:39,760 --> 00:01:43,630
θ의 값도 gradient descent를 100번 돌려서 얻었습니다.

29
00:01:43,630 --> 00:01:49,620
그리고 여기에 이 점은

30
00:01:49,620 --> 00:01:53,810
gradient descent를 200번 돌린 후, 
나온 θ에 대한 J(θ)의 값입니다.

31
00:01:55,230 --> 00:01:59,353
결국, 이 그래프에서 보여주는 것은 
각 gradient decent를 할 때마다 나오는

32
00:01:59,353 --> 00:02:02,020
cost function의 값입니다.

33
00:02:02,020 --> 00:02:07,392
gradient가 잘 돌아간다면, J(θ)는

34
00:02:07,392 --> 00:02:11,671
매 반복마다 감소해야 합니다.

35
00:02:17,058 --> 00:02:21,774
그래프에서 찾을 수 있는 또 다른 유용한 정보는

36
00:02:21,774 --> 00:02:26,783
그림의 특정 구간을 보고 알 수 있습니다.

37
00:02:26,783 --> 00:02:31,350
대략 300번 반복하고 나서, 
300에서 400사이에서의 구간은

38
00:02:31,350 --> 00:02:35,720
J(θ)가 줄어들지 않는 것처럼 보입니다.

39
00:02:35,720 --> 00:02:38,540
결국 400번 이후에는

40
00:02:38,540 --> 00:02:41,520
선이 평평한 것처럼 보입니다.

41
00:02:41,520 --> 00:02:46,090
400번 반복후에는, gradient descent는 거의

42
00:02:46,090 --> 00:02:50,510
거의 수렴한 것입니다. 
cost function이 더 이상 줄어들지 않기 때문입니다.

43
00:02:50,510 --> 00:02:53,800
이렇게 그래프를 보면 gradient descent가

44
00:02:53,800 --> 00:02:55,829
수렴하였는지 아닌지 알 수 있습니다.

45
00:02:57,580 --> 00:03:01,630
gradient descent가 수렴하는데 필요한 반복 횟수는

46
00:03:01,630 --> 00:03:04,850
경우에 따라 다양합니다.

47
00:03:04,850 --> 00:03:09,220
어떤 경우에서는 gradient descent가 
30번 만에 수렴할 수도 있습니다.

48
00:03:09,220 --> 00:03:15,110
또 다른 경우에서는, 
gradient descent가 3000번 반복할 수도 있지만,

49
00:03:15,110 --> 00:03:20,110
다른 학습 알고리즘에서는 300만번 반복할수도 있습니다.

50
00:03:20,110 --> 00:03:24,048
결론은 gradient descent가 수렴하는데 얼마나 반복하는지가

51
00:03:24,048 --> 00:03:25,476
사전에 알기 어렵다는 것입니다.

52
00:03:25,476 --> 00:03:30,026
보통은 그래프를 그리고, 
반복 횟수의 증가에 따라

53
00:03:30,026 --> 00:03:34,430
cost function을 그려서, 
그래프를 보며

54
00:03:34,430 --> 00:03:37,725
gradient descent가 수렴하는지 아닌지를 봅니다.

55
00:03:37,725 --> 00:03:42,430
수렴하는지 아닌지 자동으로 
검사하는 방법(automatic convergence test)도 있습니다.

56
00:03:42,430 --> 00:03:47,560
즉 지금 gradient descent가 수렴하는지 
알려주는 알고리즘입니다.

57
00:03:47,560 --> 00:03:52,310
automatic convergence test의 대표적인 예가 있습니다.

58
00:03:52,310 --> 00:03:57,100
언제 수렴하는지 판단하는 기준은

59
00:03:57,100 --> 00:04:01,220
cost function J(θ)의 감소량이 
어떤 작은 값 ε보다 작을 때,

60
00:04:01,220 --> 00:04:05,340
즉 한 번 반복했을 때 0.001보다 작을 때입니다.

61
00:04:05,340 --> 00:04:10,460
하지만, 이러한 임계값을 결정하는 것도 꽤 어려운 일입니다

62
00:04:10,460 --> 00:04:13,840
그래서 gradient descent의 수렴을 확인하기위해

63
00:04:13,840 --> 00:04:18,110
저는 실제로 그래프를 그립니다.
왼쪽과 같이 말이죠.

64
00:04:18,110 --> 00:04:21,740
automatic convergence test는 잘 쓰지 않아요.

65
00:04:21,740 --> 00:04:25,370
그래프를 보면, gradient descent가 제대로 동작하지 않을 때

66
00:04:25,370 --> 00:04:28,730
사전에 주의해야 할 점도 알 수 있습니다.

67
00:04:28,730 --> 00:04:33,600
구체적인 예로, 매 반복 마다 J(θ)를 그릴 때

68
00:04:33,600 --> 00:04:38,280
그래프를 보면, J(θ)가 이렇게 증가합니다.

69
00:04:38,280 --> 00:04:43,110
이는 gradient descent가 제대로 동작하고 있지 않다고 
명백하게 알려주는 것입니다.

70
00:04:43,110 --> 00:04:47,250
이러한 그래프에서는, 더 작은 learning rate 알파를 
사용해야 한다는 것을 알 수 있습니다.

71
00:04:48,320 --> 00:04:52,885
실제로 J(θ)가 증가하는 일은, 보통

72
00:04:52,885 --> 00:04:58,370
어느 함수를 최소화 하려고 할 때 일어납니다.
이렇게 생긴 함수를요.

73
00:04:59,380 --> 00:05:02,545
그런데 만약 learning rate가 너무 크고, 
여기에서 시작한다면

74
00:05:02,545 --> 00:05:06,090
gradient descent는 최소값을 넘어서 여기로 가게 됩니다.

75
00:05:06,090 --> 00:05:07,450
그리고 learning rate는 여전히 크기 때문에

76
00:05:07,450 --> 00:05:11,525
또 다시 더 나가아서 이 곳으로 옵니다.
이런식으로 계속말이죠.

77
00:05:11,525 --> 00:05:15,075
우리가 원하는 것은 여기서 시작해서

78
00:05:15,075 --> 00:05:17,975
천천히 내려가는 것인데 말이죠. 그렇죠?

79
00:05:17,975 --> 00:05:20,096
하지만 learning rate가 너무 크다면

80
00:05:20,096 --> 00:05:24,284
gradient descent는 여전히 최소값을 지나칠 것입니다.

81
00:05:24,284 --> 00:05:26,617
그러면 상황은 점점 더 나빠지게 되고

82
00:05:26,617 --> 00:05:30,690
cost function J(θ)는 높은 값을 가지게 됩니다.

83
00:05:30,690 --> 00:05:34,140
결국, 이런 그래프가 그려지게 되죠.
이런 그래프가 나온다면

84
00:05:34,140 --> 00:05:38,660
알파를 작게하면 됩니다.

85
00:05:38,660 --> 00:05:41,820
물론, 코드의 오류가 없다는 가정하에 말이죠.

86
00:05:41,820 --> 00:05:46,700
하지만 보통은 알파의 값이 너무 커서 문제가 발생합니다.

87
00:05:49,020 --> 00:05:53,090
비슷하게 J(θ)가 이런 모양일 때도 있을 것입니다.

88
00:05:53,090 --> 00:05:56,890
잠시 내려가다가 올라가다가, 
다시 내려가다가 올라가다가를

89
00:05:56,890 --> 00:05:58,850
반복합니다.

90
00:05:58,850 --> 00:06:03,130
이런 경우에도 알파의 값을 작게하면 됩니다.

91
00:06:04,150 --> 00:06:05,400
여기에서 증명은 할 수는 없지만,

92
00:06:05,400 --> 00:06:09,560
cost function J가 있다는 전제 하에, 수학자의 말에 따르면,

93
00:06:09,560 --> 00:06:14,180
선형 회귀에서 learning rate 알파가 적절하게 작을 때

94
00:06:14,180 --> 00:06:19,030
J(θ)는 매 반복마다 감소한다고 합니다.

95
00:06:19,030 --> 00:06:21,979
그러므로 감소하지 않는다면, 알파가 너무 크다는 것이고,

96
00:06:21,979 --> 00:06:23,810
그럴때는 알파를 작게 하면 됩니다.

97
00:06:23,810 --> 00:06:26,430
물론, learning rate가 너무 작아서

98
00:06:26,430 --> 00:06:30,830
gradient descent가 천천히 수렴하는 일도 피하는게 좋습니다.

99
00:06:31,930 --> 00:06:36,760
알파가 너무 작으면, 이 점에서 시작했을 때

100
00:06:36,760 --> 00:06:40,930
아기가 기어가는 느낌을 받을 것입니다.

101
00:06:40,930 --> 00:06:47,100
수 차례 반복하고 나서야 최소값에 도달하게 됩니다.

102
00:06:47,100 --> 00:06:50,990
즉 알파가 너무 작으면, gradient descent는 천천히 진행하면서

103
00:06:50,990 --> 00:06:52,360
천천히 수렴하게 됩니다.

104
00:06:52,360 --> 00:06:55,510
정리하면, learning rate가 너무 작을때는

105
00:06:55,510 --> 00:06:59,845
천천히 수렴한다는 문제가 있고, 
learning rate가 너무 크면

106
00:06:59,845 --> 00:07:05,640
J(θ)가 매 반복마다 감소하지 않고 심하면 수렴하지 않습니다.

107
00:07:05,640 --> 00:07:11,490
learning rate가 너무 큰 경우에, 
천천히 수렴하는 문제가 발생할 수도 있습니다.

108
00:07:11,490 --> 00:07:15,220
하지만 대부분의 문제점은

109
00:07:15,220 --> 00:07:19,040
(θ)가 매 번 감소하지 않는다 일것입니다.

110
00:07:19,040 --> 00:07:23,810
이러한 문제들을 해결하기 위해, 매 반복마다

111
00:07:23,810 --> 00:07:27,810
J(θ)의 그래프를 그리는 것이 
무엇이 일어나는지 알아내는데 도움이 됩니다.

112
00:07:27,810 --> 00:07:31,620
구체적으로, 제가 gradient descent를 할 때는

113
00:07:31,620 --> 00:07:33,500
어느 범위 안에서 시도합니다.

114
00:07:33,500 --> 00:07:36,460
즉 gradient descent를 할 때

115
00:07:36,460 --> 00:07:38,670
알파의 범위는 0.001과 0.01, 등등으로요.

116
00:07:38,670 --> 00:07:41,550
각각 10배만큼 차이가 납니다.

117
00:07:41,550 --> 00:07:45,250
그리고 각 알파마다 매 반복 횟수에 따른

118
00:07:45,250 --> 00:07:47,290
J(θ)를 그리고,

119
00:07:47,290 --> 00:07:54,160
J(θ)가 빠르게 감소할 때의 알파 값을 선택합니다.

120
00:07:54,160 --> 00:07:57,180
사실 제가 할 때는 10배 단위로 하지 않습니다.

121
00:07:57,180 --> 00:08:00,970
이거는 10배씩 크기가 늘어나죠.

122
00:08:00,970 --> 00:08:03,679
제가 할 때는 이 범위로 합니다.

123
00:08:06,827 --> 00:08:09,985
등 등. 
0.001에서

124
00:08:09,985 --> 00:08:13,613
learning rate를 3배 증가시킨 0.003입니다.

125
00:08:13,613 --> 00:08:15,232
이 다음에는,

126
00:08:15,232 --> 00:08:20,627
0.003에서 대략 3배수인 0.01이 됩니다.

127
00:08:20,627 --> 00:08:25,512
결국, gradient descent는

128
00:08:25,512 --> 00:08:30,640
이전 값에서 3배만큼 증가하여 시험해보게 되죠.

129
00:08:30,640 --> 00:08:33,316
저 같은 경우

130
00:08:33,316 --> 00:08:37,078
가능한 가장 작은 값과 가장 큰 값을 찾습니다.

131
00:08:37,078 --> 00:08:40,966
그리고나서, 다음으로 가능한 가장 큰 값이나

132
00:08:40,966 --> 00:08:45,943
가장 큰 값에서 조금 작은 값을 선택합니다.

133
00:08:45,943 --> 00:08:51,780
저는 이렇게, 적절한 learning rate를 찾습니다.

134
00:08:51,780 --> 00:08:55,910
여러분들도 이렇게하여, 적절한 learning rate를 찾고,

135
00:08:55,910 --> 00:08:58,010
gradient descent를 구현할 수 있습니다.