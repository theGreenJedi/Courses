В предыдущем видео мы говорили о
форме гипотезы для линейной регрессии с несколькими
параметрами (переменными) В этом видео давайте поговорим о том, как
находить параметры этой гипотезы. В частности, давайте поговорим о
том, как использовать градиентный спуск для линейной регрессии с несколькими параметрами. Резюмируем наши обозначения.
Это – наша формальная гипотеза в многомерной линейной
регрессии, в которой мы приняли, что x0=1. Параметры этой модели – от тета 0 до тета n.
Но вместо того, чтобы обозначать их
как n отдельных параметров (что является допустимым), я буду обозначать их
одним параметром тета, где тета - n+1-размерный вектор. Итак, я буду считать
параметры этой модели вектором. Наша функция стоимости – J с параметрами
тета 0 по тета n, которая задана обычной суммой квадратов ошибок.
Опять, вместо того, чтобы считать J функцией n+1 чисел, я для
простоты записи буду считать ее
функцией параметра тета, где тета - вектор. Вот как выглядит градиентный спуск.
Мы будем в цикле обновлять каждый параметр тета j как тета j минус альфа умножить на
эту производную. И ещё раз мы просто записываем
это, как J от тета, где тета j обновлён как тета j минус скорость обучения
alpha умноженная на частную производную стоимостной
функции по параметру тета j. Давайте посмотрим, как это выглядит в случае
градиентного спуска, и, в частности, давайте посмотрим, как выглядит этот член
частных производных. Вот что у нас получилось для градиентного спуска, когда мы
имели N=1 свойство. У нас были два отдельных правила
обновления для параметров тета 0 и тета 1, и, хочется надеяться, они вам знакомы.
И этот член являлся, конечно, частной производной функции стоимости по параметру
тета 0, и аналогично у нас было другое правило обновления для
параметра тета 1. Есть одно маленькое различие. Когда у нас
раньше было только одно свойство, мы называли его x(i), но сейчас в нашем новом
обозначении мы, конечно, называли бы его x(i) с индексом 1, чтобы
обозначить наше свойство 1. Так как было, когда мы имели только одно
свойство. Давайте рассмотрим новый алгоритм, т.к. мы имеем
более одного свойства,где количество свойств n может быть
гораздо больше одного. У нас есть правило обновления
градиентного спуска и те из вас кто знает дифференциальное
исчисление, могут использовать определение функции затрат, J, чтобы посчитать ее частную производную по
параметру тета j, и вы увидите что эта производная как раз равна
выражению, которое я обвел синим. И, если вы выполните это, вы
получите работающую реализацию градиентного спуска для
многомерной линейной регрессии. Последнее, что я хочу показать на этом
слайде, это дать вам какое-то представление о том,
почему эти алгоритмы, старый и новый, - это примерно одно и то же, что это похожие алгоритмы, что оба они -
алгоритмы градиентного спуска. Давайте рассмотрим случай,
когда мы имеем два свойства или, возможно, более
двух свойств, так что у нас есть три правила обновления для
параметров тета 0, тета 1, тета 2 и, возможно, для других значений тета также. Если вы посмотрите на правило
обновления для тета 0, вы увидите, что правило обновления здесь такое же, как для случая который
у нас был раньше, когда n = 1. И причина, по которой они являются
эквивалентными в том, что по нашему
соглашению мы обозначаем x(i)0 = 1. Поэтому эти два выражения, которые
я обвел пурпурными прямоугольниками эквивалентны. Аналогично, если вы посмотрите на
правило обновления для тета 1, вы увидите, что это
выражение эквивалентно выражению, которое мы получили
ранее, или уравнению или правилу обновления, которое у нас ранее имелось
для тета 1, где мы просто используем новое обозначение x(i)1, чтобы
обозначить первое свойство, и т.к. у нас есть более одного свойства
мы можем использовать аналогичные правила обновления для других параметров, таких как тета 2 и так далее. Этот слайд очень важен,
поэтому я призываю вас, если нужно поставьте
видео на паузу и внимательно изучите все математические выкладки на
нем чтобы убедиться, что вы понимаете всё, что там написано. Но, если вы реализуете
алгоритм, записанный здесь, вы получите работающую
реализацию линейной регрессии с несколькими свойствами.