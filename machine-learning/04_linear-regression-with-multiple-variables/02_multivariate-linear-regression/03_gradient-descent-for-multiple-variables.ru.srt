1
00:00:00,220 --> 00:00:03,688
В предыдущем видео мы говорили о
форме гипотезы для

2
00:00:03,688 --> 00:00:07,246
линейной регрессии с несколькими
параметрами (переменными)

3
00:00:07,246 --> 00:00:11,912
В этом видео давайте поговорим о том, как
находить параметры этой гипотезы.

4
00:00:11,912 --> 00:00:15,175
В частности, давайте поговорим о
том, как использовать градиентный спуск для линейной регрессии с

5
00:00:15,175 --> 00:00:19,875
несколькими параметрами.

6
00:00:19,875 --> 00:00:24,802
Резюмируем наши обозначения.
Это – наша формальная

7
00:00:24,802 --> 00:00:31,509
гипотеза в многомерной линейной
регрессии, в которой мы приняли, что x0=1.

8
00:00:31,509 --> 00:00:37,505
Параметры этой модели – от тета 0 до тета n.
Но вместо того,

9
00:00:37,505 --> 00:00:42,385
чтобы обозначать их
как n отдельных параметров (что является

10
00:00:42,385 --> 00:00:51,175
допустимым), я буду обозначать их
одним параметром тета, где тета - n+1-размерный вектор.

11
00:00:51,175 --> 00:00:55,498
Итак, я буду считать
параметры этой модели

12
00:00:55,498 --> 00:00:58,674
вектором.

13
00:00:58,674 --> 00:01:03,507
Наша функция стоимости – J с параметрами
тета 0 по тета n, которая задана обычной

14
00:01:03,507 --> 00:01:08,983
суммой квадратов ошибок.
Опять, вместо того, чтобы считать J

15
00:01:08,983 --> 00:01:14,016
функцией n+1 чисел, я для
простоты

16
00:01:14,016 --> 00:01:22,275
записи буду считать ее
функцией параметра тета, где тета - вектор.

17
00:01:22,275 --> 00:01:26,897
Вот как выглядит градиентный спуск.
Мы будем в цикле обновлять каждый

18
00:01:26,897 --> 00:01:32,142
параметр тета j как тета j минус альфа умножить на
эту производную.

19
00:01:32,142 --> 00:01:37,868
И ещё раз мы просто записываем
это, как J от тета, где тета j обновлён как тета j

20
00:01:37,868 --> 00:01:41,840
минус скорость обучения
alpha умноженная

21
00:01:41,840 --> 00:01:47,840
на частную производную стоимостной
функции по параметру тета j.

22
00:01:47,840 --> 00:01:51,305
Давайте посмотрим, как это выглядит в случае
градиентного спуска, и, в частности, давайте

23
00:01:51,305 --> 00:01:55,985
посмотрим, как выглядит этот член
частных производных.

24
00:01:55,985 --> 00:02:01,383
Вот что у нас получилось для градиентного спуска, когда мы
имели N=1 свойство.

25
00:02:01,383 --> 00:02:06,782
У нас были два отдельных правила
обновления для параметров

26
00:02:06,782 --> 00:02:12,779
тета 0 и тета 1, и, хочется надеяться, они вам знакомы.
И этот член являлся, конечно, частной

27
00:02:12,779 --> 00:02:17,672
производной функции стоимости по параметру
тета 0, и аналогично у нас было

28
00:02:17,672 --> 00:02:21,891
другое правило обновления для
параметра тета 1.

29
00:02:21,891 --> 00:02:26,259
Есть одно маленькое различие. Когда у нас
раньше было только одно свойство,

30
00:02:26,259 --> 00:02:31,992
мы называли его x(i), но сейчас в нашем новом
обозначении мы, конечно, называли

31
00:02:31,992 --> 00:02:38,462
бы его x(i) с индексом 1, чтобы
обозначить наше свойство 1. Так как было, когда

32
00:02:38,462 --> 00:02:41,019
мы имели только одно
свойство.

33
00:02:41,019 --> 00:02:44,496
Давайте рассмотрим новый алгоритм, т.к. мы имеем
более одного свойства,где

34
00:02:44,496 --> 00:02:47,350
количество свойств n может быть
гораздо больше одного.

35
00:02:47,350 --> 00:02:53,158
У нас есть правило обновления
градиентного

36
00:02:53,158 --> 00:02:57,781
спуска и те из вас кто знает дифференциальное
исчисление, могут использовать определение

37
00:02:57,781 --> 00:03:03,312
функции затрат, J, чтобы посчитать ее частную производную по
параметру тета j, и вы увидите что эта

38
00:03:03,312 --> 00:03:08,119
производная как раз равна
выражению, которое я обвел

39
00:03:08,119 --> 00:03:10,665
синим.

40
00:03:10,665 --> 00:03:14,837
И, если вы выполните это, вы
получите работающую

41
00:03:14,837 --> 00:03:18,962
реализацию градиентного спуска для
многомерной линейной регрессии.

42
00:03:18,962 --> 00:03:21,572
Последнее, что я хочу показать на этом
слайде, это дать вам

43
00:03:21,572 --> 00:03:26,882
какое-то представление о том,
почему эти алгоритмы, старый и новый, -

44
00:03:26,882 --> 00:03:30,904
это примерно одно и то же, что это похожие алгоритмы, что оба они -
алгоритмы градиентного спуска.

45
00:03:30,904 --> 00:03:34,363
Давайте рассмотрим случай,
когда мы имеем

46
00:03:34,363 --> 00:03:37,488
два свойства или, возможно, более
двух свойств, так что у нас есть

47
00:03:37,488 --> 00:03:42,680
три правила обновления для
параметров тета 0, тета 1, тета 2 и, возможно, для других значений тета также.

48
00:03:42,680 --> 00:03:49,457
Если вы посмотрите на правило
обновления для тета 0, вы увидите, что правило

49
00:03:49,457 --> 00:03:55,300
обновления здесь такое же, как для случая который
у нас был раньше,

50
00:03:55,300 --> 00:03:57,350
когда n = 1.

51
00:03:57,350 --> 00:04:00,203
И причина, по которой они являются
эквивалентными в том,

52
00:04:00,203 --> 00:04:06,871
что по нашему
соглашению мы обозначаем x(i)0 = 1. Поэтому

53
00:04:06,871 --> 00:04:12,003
эти два выражения, которые
я обвел пурпурными прямоугольниками эквивалентны.

54
00:04:12,003 --> 00:04:16,010
Аналогично, если вы посмотрите на
правило обновления для

55
00:04:16,010 --> 00:04:21,540
тета 1, вы увидите, что это
выражение эквивалентно

56
00:04:21,540 --> 00:04:25,020
выражению, которое мы получили
ранее, или уравнению или

57
00:04:25,020 --> 00:04:30,222
правилу обновления, которое у нас ранее имелось
для тета 1, где мы просто используем

58
00:04:30,222 --> 00:04:37,605
новое обозначение x(i)1, чтобы
обозначить первое свойство, и т.к. у

59
00:04:37,605 --> 00:04:43,560
нас есть более одного свойства
мы можем использовать аналогичные правила обновления для других параметров, таких как тета 2 и так далее.

60
00:04:43,560 --> 00:04:48,219
Этот слайд очень важен,
поэтому я призываю вас,

61
00:04:48,219 --> 00:04:52,020
если нужно поставьте
видео на паузу и внимательно

62
00:04:52,020 --> 00:04:55,446
изучите все математические выкладки на
нем чтобы убедиться, что вы понимаете всё, что там написано.

63
00:04:55,446 --> 00:05:00,440
Но, если вы реализуете
алгоритм, записанный

64
00:05:00,440 --> 00:05:51,300
здесь, вы получите работающую
реализацию линейной регрессии с несколькими свойствами.