В этом видео я хочу поделиться практическими приемами по работе с методом градиентного спуска. В основном в этом видео мы будем говорить про скорость обучения - альфа. А именно, вот правило обновлений для градиентного спуска. В этом видео я расскажу вам про такую вещь, как отладку, и дам несколько советов относительно того, как заставить градиентный спуск работать корректно. Кроме того, я расскажу о том, как выбрать скорость обучения и о том, как это обычно делаю я. Вот что я обычно делаю для того, чтобы убедиться, что градиентный спуск работает корректно. Задача градиентного спуска - найти такое значение вектора тета, которое, в идеале минимизирует значение J(тета). Поэтому я часто рисую график изменения функции стоимости J по мере работы градиентного спуска. Ось X на графике - это номер итерации градиентного спуска, поэтому по мере того, как работает алгоритм, вы в идеале должны получить график какого-то такого вида. Заметьте, что ось X - это количество итераций. Раньше мы смотрели на графики J(тета), где ось X, горизонтальная ось, соответствовала вектору параметров тета, но не в этом случае. Если конкретно, то это точка соответствует сотой итерации градиентного спуска. И после ста итераций я получаю какое-то значение тета, некоторое значение вектора тета, и вычисляю функцию J(тета) для этого значения тета на сотой итерации, и высота этой точки - это значение J для того тета, которое я получил после ста итераций градиентного спуска. А вот эта точка соответствует значению J для тета, полученного на 200-й итерации градиентного спуска. Так что на этом графике показано значение вашей функции стоимости после некоторой итерации градиентного спуска. И если градиентный спуск работает правильно, но J(тета) должно уменьшаться. После каждой итерации. Чем еще может быть полезен этот график: если вы посмотрите на конкретную линию, которую я нарисовал, то увидите, что когда вы добрались до 300-й итерации, то на вот этом отрезке, между 300-й и 400-й итерацией, кажется, что J(тета) уже не сильно уменьшается. Так что к тому времени, когда вы дошли до 400 итераций, похоже, что кривая вышла на плато. Поэтому вот здесь, на 400 итерациях, судя по всему, градиентный спуск уже более или менее сошелся, потому что ваша функция затрат практически не уменьшается. Так что посмотреть на этот график может быть полезно для оценки того, сошелся ли градиентный спуск. Кстати говоря, количество итераций, которое нужно градиентному спуску, чтобы сойтись, в реальной жизни может быть абсолютно разным. В одной задаче градиентный спуск может сойтись после 30 итераций, в другой - после 3000 итераций. А для другого алгоритма обучения может потребоваться три миллиона итераций. Оказывается, что заранее очень сложно предсказать, сколько итераций потребуется градиентному спуску для сходимости, и бывает полезно построить такой вот график. График функции стоимости от количества итераций. Обычно, глядя на такие графики, я пытаюсь понять, сошелся ли градиентный спуск. Также можно придумать автоматический критерий сходимости: а именно, добавить в алгоритм оценку того, сошелся ли градиентный спуск. Вот, к примеру, типичный вариант автоматического критерия: процесс явно сходится, если функция стоимости J(тета) уменьшается меньше чем на некоторое маленькое значение эпсилон, скажем, 10^-3, за одну итерацию. Но для меня подобрать этот порог обычно довольно сложно. Поэтому для проверки сходимости градиентного спуска я чаще все-таки смотрю на такие графики, как в левой части слайда, а не полагаюсь на автоматические критерии сходимости. Еще, глядя на такой график, можно заранее увидеть, что градиентный спуск, возможно, работает некорректно. А именно, если рисовать J(тета) как функцию от количества итераций, то, если вы видите какую-то такую линию, где J(тета) на