이번 비디오에서는, 
gradient descent에 대한 유용한 정보를 알려주고자 합니다. 이 번 비디오에서 주제는
learning rate 알파에 관한 것입니다. 구체적으로, 여기 gradient descent 식이 있습니다. 이 번 비디오에서 하고자 하는 것은 debugging입니다. gradient descent가 잘 돌아가게 하기 위한 방법입니다. 두 번째로는, learning rate 알파를 고르는 방법 즉 어떤 값으로 시작할지에 대해 이야기해보겠습니다. gradient descent가 어떻게 돌아가는지 확인해 보도록 하죠 정확히, gradient descent의 목적은
cost function J（θ）가 최소화되는 θ의 값을 찾는 것입니다. cost function J(θ)를 gradient descent 할 때마다, 그래프를 그리겠습니다. 그래서 x축은 gradient descent의 동작 횟수이고 gradient descent를 하면, 이처럼 그래프가 그려질 것입니다. x축은 반복 횟수라는 것을 잊지마세요. 이전에, J(θ) 그래프를 그릴 때에는, x축, 즉 수평축이 파라미터 벡터였지만, 이번에는 아닙니다. 구체적으로 이 점은, gradient descent를 100번 돌렸을 때의 값입니다. 100번 돌리고나서 얻은 θ에 대한 값과 100번 반복했을 때 θ의 값을 구할 수 있습니다. cost function J(θ)를 보죠. 100번 반복 후 나온 θ가 있을 때 이 높이가 J(θ)의 값입니다. θ의 값도 gradient descent를 100번 돌려서 얻었습니다. 그리고 여기에 이 점은 gradient descent를 200번 돌린 후, 
나온 θ에 대한 J(θ)의 값입니다. 결국, 이 그래프에서 보여주는 것은 
각 gradient decent를 할 때마다 나오는 cost function의 값입니다. gradient가 잘 돌아간다면, J(θ)는 매 반복마다 감소해야 합니다. 그래프에서 찾을 수 있는 또 다른 유용한 정보는 그림의 특정 구간을 보고 알 수 있습니다. 대략 300번 반복하고 나서, 
300에서 400사이에서의 구간은 J(θ)가 줄어들지 않는 것처럼 보입니다. 결국 400번 이후에는 선이 평평한 것처럼 보입니다. 400번 반복후에는, gradient descent는 거의 거의 수렴한 것입니다. 
cost function이 더 이상 줄어들지 않기 때문입니다. 이렇게 그래프를 보면 gradient descent가 수렴하였는지 아닌지 알 수 있습니다. gradient descent가 수렴하는데 필요한 반복 횟수는 경우에 따라 다양합니다. 어떤 경우에서는 gradient descent가 
30번 만에 수렴할 수도 있습니다. 또 다른 경우에서는, 
gradient descent가 3000번 반복할 수도 있지만, 다른 학습 알고리즘에서는 300만번 반복할수도 있습니다. 결론은 gradient descent가 수렴하는데 얼마나 반복하는지가 사전에 알기 어렵다는 것입니다. 보통은 그래프를 그리고, 
반복 횟수의 증가에 따라 cost function을 그려서, 
그래프를 보며 gradient descent가 수렴하는지 아닌지를 봅니다. 수렴하는지 아닌지 자동으로 
검사하는 방법(automatic convergence test)도 있습니다. 즉 지금 gradient descent가 수렴하는지 
알려주는 알고리즘입니다. automatic convergence test의 대표적인 예가 있습니다. 언제 수렴하는지 판단하는 기준은 cost function J(θ)의 감소량이 
어떤 작은 값 ε보다 작을 때, 즉 한 번 반복했을 때 0.001보다 작을 때입니다. 하지만, 이러한 임계값을 결정하는 것도 꽤 어려운 일입니다 그래서 gradient descent의 수렴을 확인하기위해 저는 실제로 그래프를 그립니다.
왼쪽과 같이 말이죠. automatic convergence test는 잘 쓰지 않아요. 그래프를 보면, gradient descent가 제대로 동작하지 않을 때 사전에 주의해야 할 점도 알 수 있습니다. 구체적인 예로, 매 반복 마다 J(θ)를 그릴 때 그래프를 보면, J(θ)가 이렇게 증가합니다. 이는 gradient descent가 제대로 동작하고 있지 않다고 
명백하게 알려주는 것입니다. 이러한 그래프에서는, 더 작은 learning rate 알파를 
사용해야 한다는 것을 알 수 있습니다. 실제로 J(θ)가 증가하는 일은, 보통 어느 함수를 최소화 하려고 할 때 일어납니다.
이렇게 생긴 함수를요. 그런데 만약 learning rate가 너무 크고, 
여기에서 시작한다면 gradient descent는 최소값을 넘어서 여기로 가게 됩니다. 그리고 learning rate는 여전히 크기 때문에 또 다시 더 나가아서 이 곳으로 옵니다.
이런식으로 계속말이죠. 우리가 원하는 것은 여기서 시작해서 천천히 내려가는 것인데 말이죠. 그렇죠? 하지만 learning rate가 너무 크다면 gradient descent는 여전히 최소값을 지나칠 것입니다. 그러면 상황은 점점 더 나빠지게 되고 cost function J(θ)는 높은 값을 가지게 됩니다. 결국, 이런 그래프가 그려지게 되죠.
이런 그래프가 나온다면 알파를 작게하면 됩니다. 물론, 코드의 오류가 없다는 가정하에 말이죠. 하지만 보통은 알파의 값이 너무 커서 문제가 발생합니다. 비슷하게 J(θ)가 이런 모양일 때도 있을 것입니다. 잠시 내려가다가 올라가다가, 
다시 내려가다가 올라가다가를 반복합니다. 이런 경우에도 알파의 값을 작게하면 됩니다. 여기에서 증명은 할 수는 없지만, cost function J가 있다는 전제 하에, 수학자의 말에 따르면, 선형 회귀에서 learning rate 알파가 적절하게 작을 때 J(θ)는 매 반복마다 감소한다고 합니다. 그러므로 감소하지 않는다면, 알파가 너무 크다는 것이고, 그럴때는 알파를 작게 하면 됩니다. 물론, learning rate가 너무 작아서 gradient descent가 천천히 수렴하는 일도 피하는게 좋습니다. 알파가 너무 작으면, 이 점에서 시작했을 때 아기가 기어가는 느낌을 받을 것입니다. 수 차례 반복하고 나서야 최소값에 도달하게 됩니다. 즉 알파가 너무 작으면, gradient descent는 천천히 진행하면서 천천히 수렴하게 됩니다. 정리하면, learning rate가 너무 작을때는 천천히 수렴한다는 문제가 있고, 
learning rate가 너무 크면 J(θ)가 매 반복마다 감소하지 않고 심하면 수렴하지 않습니다. learning rate가 너무 큰 경우에, 
천천히 수렴하는 문제가 발생할 수도 있습니다. 하지만 대부분의 문제점은 (θ)가 매 번 감소하지 않는다 일것입니다. 이러한 문제들을 해결하기 위해, 매 반복마다 J(θ)의 그래프를 그리는 것이 
무엇이 일어나는지 알아내는데 도움이 됩니다. 구체적으로, 제가 gradient descent를 할 때는 어느 범위 안에서 시도합니다. 즉 gradient descent를 할 때 알파의 범위는 0.001과 0.01, 등등으로요. 각각 10배만큼 차이가 납니다. 그리고 각 알파마다 매 반복 횟수에 따른 J(θ)를 그리고, J(θ)가 빠르게 감소할 때의 알파 값을 선택합니다. 사실 제가 할 때는 10배 단위로 하지 않습니다. 이거는 10배씩 크기가 늘어나죠. 제가 할 때는 이 범위로 합니다. 등 등. 
0.001에서 learning rate를 3배 증가시킨 0.003입니다. 이 다음에는, 0.003에서 대략 3배수인 0.01이 됩니다. 결국, gradient descent는 이전 값에서 3배만큼 증가하여 시험해보게 되죠. 저 같은 경우 가능한 가장 작은 값과 가장 큰 값을 찾습니다. 그리고나서, 다음으로 가능한 가장 큰 값이나 가장 큰 값에서 조금 작은 값을 선택합니다. 저는 이렇게, 적절한 learning rate를 찾습니다. 여러분들도 이렇게하여, 적절한 learning rate를 찾고, gradient descent를 구현할 수 있습니다.