1
00:00:00,220 --> 00:00:03,688
En el video anterior, hablamos sobre
la formulación de la hipótesis para la

2
00:00:03,688 --> 00:00:07,246
regresión lineal con múltiples características
o con múltiples variables.

3
00:00:07,246 --> 00:00:11,912
En este video, vamos a hablar de cómo
ajustar los parámetros de esa hipótesis.

4
00:00:11,912 --> 00:00:15,175
En particular vamos a hablar de cómo
utilizar el gradiente de descenso para la

5
00:00:15,175 --> 00:00:19,875
regresión lineal con múltiples características.

6
00:00:19,875 --> 00:00:24,802
Para resumir rápidamente nuestra notación,
esta es nuestra hipótesis formal en

7
00:00:24,802 --> 00:00:31,509
la regresión lineal multivariable donde
hemos adoptado la convención de que x0 = 1.

8
00:00:31,509 --> 00:00:37,505
Los parámetros de este modelo van de theta0
hasta theta n, pero en lugar de pensar

9
00:00:37,505 --> 00:00:42,385
en ellos como parámetros separados de "n", lo
que es válido, en su lugar voy a pensar en

10
00:00:42,385 --> 00:00:51,175
los parámetros como theta, en donde theta
de aquí es un vector dimensional n+1.

11
00:00:51,175 --> 00:00:55,498
Entonces sólo voy a pensar en los
parámetros de este modelo

12
00:00:55,498 --> 00:00:58,674
como si fueran un vector.

13
00:00:58,674 --> 00:01:03,507
Nuestra función de costo es "J" de theta0 a través
de theta n que está dada por esta habitual

14
00:01:03,507 --> 00:01:08,983
suma de cuadrados del término de error. Pero, de nuevo
en lugar de pensar en "J" como una función

15
00:01:08,983 --> 00:01:14,016
de estos números n+1, voy a escribir
más frecuentemente "J" como una simple

16
00:01:14,016 --> 00:01:22,275
función del vector de parámetros theta
de manera que theta sea un vector.

17
00:01:22,275 --> 00:01:26,897
Así es como se ve el gradiente de descenso.
Vamos a actualizar varias veces cada

18
00:01:26,897 --> 00:01:32,142
parámetro theta j de acuerdo con theta j
menos alfa multiplicado por este término derivado.

19
00:01:32,142 --> 00:01:37,868
Y de nuevo escribimos esto simplemente como
"J" de theta, por lo que theta j se actualiza

20
00:01:37,868 --> 00:01:41,840
como theta j menos la tasa de aprendizaje
alfa multiplicado por la derivada,

21
00:01:41,840 --> 00:01:47,840
una derivada parcial de la función de costos con
respecto al parámetro theta j.

22
00:01:47,840 --> 00:01:51,305
Vamos a ver cómo se ve esto cuando
implementamos el gradiente de descenso y,

23
00:01:51,305 --> 00:01:55,985
en particular, vamos a ver cómo
se ve el término derivado parcial.

24
00:01:55,985 --> 00:02:01,383
Esto es lo que tenemos para el gradiente de descenso
para cuando teníamos la característica n=1.

25
00:02:01,383 --> 00:02:06,782
Teníamos dos reglas de actualización independientes para
los parámetros theta0 y theta1 y

26
00:02:06,782 --> 00:02:12,779
espero que esto te resulte familiar.
Y este término de aquí era por supuesto la

27
00:02:12,779 --> 00:02:17,672
derivada parcial de la función de costos
con respecto al parámetro de theta0,

28
00:02:17,672 --> 00:02:21,891
y del mismo modo teníamos una regla de
actualización diferente para el parámetro theta1.

29
00:02:21,891 --> 00:02:26,259
Hay una pequeña diferencia y es
que cuando antes teníamos solo una

30
00:02:26,259 --> 00:02:31,992
característica, la llamábamos x(i)
pero ahora en nuestra nueva notación

31
00:02:31,992 --> 00:02:38,462
por supuesto la llamaremos 
x superíndice i subíndice 1 para denotar nuestra única función.</u>

32
00:02:38,462 --> 00:02:41,019
Entonces eso era para cuando
teníamos sólo una característica.

33
00:02:41,019 --> 00:02:44,496
Ahora veamos el nuevo algoritmo para
el cual tenemos más de una característica,

34
00:02:44,496 --> 00:02:47,350
donde el número de características "n"
puede ser mucho mayor a 1.

35
00:02:47,350 --> 00:02:53,158
Obtenemos esta regla de actualización para el
gradiente de descenso y, tal vez para quienes

36
00:02:53,158 --> 00:02:57,781
saben cálculo, si se toma la
definición de la función de costos y

37
00:02:57,781 --> 00:03:03,312
la derivada parcial de la función de costos
"J" con respecto al parámetro

38
00:03:03,312 --> 00:03:08,119
theta j, encontrarás que la derivada
parcial es exactamente ese término al que

39
00:03:08,119 --> 00:03:10,665
le puse un recuadro azul alrededor.

40
00:03:10,665 --> 00:03:14,837
Y si lo implementas obtendrás
una implementación funcional del

41
00:03:14,837 --> 00:03:18,962
gradiente de descenso para 
la regresión lineal multivariable.

42
00:03:18,962 --> 00:03:21,572
Lo último que quiero hacer en
esta diapositiva es explicarte

43
00:03:21,572 --> 00:03:26,882
por qué estos nuevos y viejos algoritmos son
más o menos lo mismo o por qué

44
00:03:26,882 --> 00:03:30,904
ambos son algoritmos similares o por qué
ambos son algoritmos de gradiente de descenso.

45
00:03:30,904 --> 00:03:34,363
Vamos a considerar un caso
en donde tenemos dos características

46
00:03:34,363 --> 00:03:37,488
o tal vez más de dos características,
así que tenemos tres reglas de actualización para

47
00:03:37,488 --> 00:03:42,680
los parámetros theta0, theta1, theta2
y tal vez para otros valores de theta también.

48
00:03:42,680 --> 00:03:49,457
Si te fijas en la regla de actualización para
theta0, puedes encontrar que esta

49
00:03:49,457 --> 00:03:55,300
regla de actualización de aquí es la misma que
la regla de actualización que teníamos

50
00:03:55,300 --> 00:03:57,350
antes para el caso de n = 1.

51
00:03:57,350 --> 00:04:00,203
Y la razón por la que son
equivalentes es, por supuesto,

52
00:04:00,203 --> 00:04:06,871
porque en nuestra convención de notación
tuvimos esta convención x(i) 0 = 1, por lo que</u>

53
00:04:06,871 --> 00:04:12,003
estos dos términos a los que les puse 
recuadros color magenta son equivalentes.

54
00:04:12,003 --> 00:04:16,010
Del mismo modo, si ves la regla de actualización
para theta1, encontrarás que

55
00:04:16,010 --> 00:04:21,540
este término de aquí es equivalente al
término que teníamos antes,

56
00:04:21,540 --> 00:04:25,020
o a la ecuación o a la regla de actualización
que teníamos antes para theta1,

57
00:04:25,020 --> 00:04:30,222
en donde, por supuesto, sólo estamos utilizando
esta nueva notación x(i) subíndice 1 para denotar</u>

58
00:04:30,222 --> 00:04:37,605
nuestra nueva notación denotando nuestra primera característica y ahora que tenemos
más de una característica podemos tener

59
00:04:37,605 --> 00:04:43,560
reglas de actualización similares para los otros
parámetros como theta2 y así sucesivamente.

60
00:04:43,560 --> 00:04:48,219
Suceden muchas cosas en esta diapositiva
así que definitivamente te animo a que

61
00:04:48,219 --> 00:04:52,020
pongas pausa al video si lo necesitas
y revises a detalle todas las matemáticas que contiene

62
00:04:52,020 --> 00:04:55,446
para asegurarte de que entiendes
todo lo que está pasando aquí.

63
00:04:55,446 --> 00:05:00,440
Pero si implementas el algoritmo
escrito aquí, entonces tienes

64
00:05:00,440 --> 00:05:51,300
una implementación funcional de una
regresión lineal con múltiples características.