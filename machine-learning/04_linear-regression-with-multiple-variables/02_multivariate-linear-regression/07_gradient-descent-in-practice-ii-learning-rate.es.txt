En este video, quiero darte más consejos prácticos para conseguir que el gradiente de descenso funcione. Las ideas en este vídeo se centran alrededor del índice de aprendizaje «alfa». Específicamente, aquí está la regla de actualización del gradiente de descenso y lo que quiero hacer en este video es hablarte acerca de lo que pienso de la depuración y algunos consejos para asegurarte que funciona correctamente el gradiente de descenso y en segundo lugar, quiero hablarte sobre cómo elegir los índices de aprendizaje hacia afuera, y esta es la forma para elegirlos. Aquí hay algo que hago a menudo para asegurarme de que el gradiente de descenso está funcionando correctamente. El trabajo del gradiente de descenso es que te encuentre el valor de «theta», que como sabes, esperamos minimice la función de costo "J" de «theta». Por tanto, lo que hago a menudo es arrancar la función de costo de "J" de «theta» conforme se ejecuta el gradiente de descenso. Así es que, el eje x aquí es el número de iteración del gradiente de descenso y conforme el gradiente de descenso se ejecuta, espero que obtengas una gráfica que tal vez se parece a esto. Observa que el eje x es el número de iteraciones que previamente estábamos buscando en gráficas de "J" de «theta» donde el eje x, donde el eje horizontal, era el vector del parámetro «theta», pero esto no es en donde este está. Específicamente, lo que este punto es, voy a clasificar el gradiente de descenso para cien iteraciones. Y cualquiera que sea el valor que obtenga para «theta» después de un centenar de iteraciones y obtener, como sabes, algún valor de «theta» después de cien iteraciones y voy a evaluar la función coseno "J" de «theta» para el valor de «theta» que obtengo después de cien iteraciones y esta altura vertical es el valor de "J" de «theta» para el valor de «theta» que obtengo después de un centenar de iteraciones del gradiente de descenso y este punto de aquí, que corresponde al valor de "J" de «theta» para «theta» que obtengo después de haber ejecutado el gradiente de descenso para doscientas iteraciones. Así que lo que esta gráfica está mostrando, está mostrando el valor de tu función de costo después de cada iteración del gradiente de descenso. Y, si el gradiente de descenso está funcionando correctamente, entonces "J" de «theta» debe disminuir después de cada iteración. Y una cosa útil que este tipo de gráfica puede decirte también es que si te fijas en la figura específica que he dibujado, parece que para el momento en que has llegado a trescientas iteraciones, entre trescientas y cuatrocientas iteraciones, en este segmento, parece que "J" de «theta» disminuye mucho más. Así que para cuando alcances cuatrocientos iteraciones, parecerá que esta curva se ha aplanado hasta aquí. Y así, aquí en las cuatrocientas iteraciones, parece que el gradiente de descenso ha, más o menos convergido porque tu función de costo no bajará mucho más. Así que observar esta figura puede además ayudarte a juzgar si el gradiente de descenso converge o no. Por cierto, el número de iteraciones que el gradiente de descenso toma para converger en una aplicación física puede variar mucho. Así que tal vez, para una aplicación de gradiente de descenso puede converger después de solamente treinta iteraciones, y para una aplicación de gradiente de descenso diferente puede hacerlo a las 3,000 iteraciones. Para otro algoritmo de aprendizaje puede tardar tres millones de iteraciones. Resulta ser muy difícil saber de antemano cuantas iteraciones necesita el gradiente descendente para converger, y se logra por lo general tranzando este tipo de gráfica. Trazando la función de costo a medida que aumentamos el número de iteraciones. Por lo general al examinar estas gráficas intento saber si converge el gradiente de descenso. También es posible saber con la prueba automática de convergencia; es decir tener un algoritmo para intentar saber si el gradiente de descenso ha convergido y aquí está tal vez un ejemplo bastante típico de una prueba automática de convergencia, así, pruebas la clara convergencia si tu función de costo de "J" de «theta» disminuye en menos de un pequeño valor de «épsilon», algún pequeño valor de 10 a la -3 en una iteración, pero me parece que por lo general es bastante difícil elegir cual es este umbral. Así que, con el fin de comprobar que tu gradiente de descenso ha convergido, por lo general observo las gráficas como esta figura de la derecha en lugar de confiar en una prueba de convergencia automática. Observar esta clase de figura también puede decirte o darte una advertencia por adelantado si tal vez el gradiente de descenso no está funcionando correctamente. Específicamente, si aplicas "J" de «theta» como una función de un número de iteraciones, entonces, si ves una figura como esta, en donde "J" de «theta» está en realidad