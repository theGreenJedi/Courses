1
00:00:00,220 --> 00:00:03,688
בסרטון הקודם, דיברנו על צורת ההשערה של רגרסיה

2
00:00:03,688 --> 00:00:07,246
ליניארית עם תכונות מרובות או משתנים מרובים.

3
00:00:07,246 --> 00:00:11,912
בסרטון הזה, נדבר על איך מכיילים את הפרמטרים של ההשערה.

4
00:00:11,912 --> 00:00:15,175
בפרט נדבר על איך משתמשים בירידה במדרון - גְרַדְיֵינְט דִיסֶנְט - עבור רגרסיה

5
00:00:15,175 --> 00:00:19,875
ליניארית מרובת תכונות.

6
00:00:19,875 --> 00:00:24,802
חזרה קצרה על הסימונים שלנו, זו ההשערה הרשמית שלנו

7
00:00:24,802 --> 00:00:31,509
ברגרסיה ליניארית מרובת משתנים שבה אימצנו את ההסכמה ש-x0=1.

8
00:00:31,509 --> 00:00:37,505
הפרמטרים של המודל הם תטא-1 עד תטא-n, אבל במקום לחשוב

9
00:00:37,505 --> 00:00:42,385
על n הפרמטרים הנפרדים האלה, שגם זה אפשרי, אני במקום זה חושב על

10
00:00:42,385 --> 00:00:51,175
הפרמטרים כתטא שהוא וקטור תטא n+1-מימדי.

11
00:00:51,175 --> 00:00:55,498
אנחנו נחשוב על הפרמטרים של המודל הזה

12
00:00:55,498 --> 00:00:58,674
כאילו הם בעצמם וקטור.

13
00:00:58,674 --> 00:01:03,507
פונקצית העלות שלנו היא j של תטא-0 עד תטא-n היא אותו

14
00:01:03,507 --> 00:01:08,983
סכום של ריבועי השגיאה. אבל שוב במקום לחשוב על j כפונקציה

15
00:01:08,983 --> 00:01:14,016
של n+1 המספרים, אני בדרך כלל אכתוב j כפונקציה

16
00:01:14,016 --> 00:01:22,275
של וקטור הפרמטרים תטא ולכן תטא כאן הוא וקטור.

17
00:01:22,275 --> 00:01:26,897
הנה איך נראה אלגוריתם הירידה במדרון - גְרַדְיֵינְט דִיסֶנְט.
אנחנו הולכים לעדכן כל אחד מהפרמטרים

18
00:01:26,897 --> 00:01:32,142
תטא-j שוב ושוב לפי הנוסחה הזו, תטא-j פחות אלפא כפול הנגזרת כאן.

19
00:01:32,142 --> 00:01:37,868
וגם כאן אנחנו פשוט נכתוב את זה כ-J של תטא. טוב, אז כל פרמטר תטא-j מתעדכן

20
00:01:37,868 --> 00:01:41,840
לתטא-j פחות מקדם הלימוד אלפא כפול הנגזרת

21
00:01:41,840 --> 00:01:47,840
החלקית הזו של פונקצית העלות ביחס לפרמטר תטא-j.

22
00:01:47,840 --> 00:01:51,305
בואו נראה איך זה נראה כאשר אנו מיישמים את הירידה בשיפוע,

23
00:01:51,305 --> 00:01:55,985
ובפרט, בואו נראה איך נראית הנגזרת החלקית.

24
00:01:55,985 --> 00:02:01,383
הנה איך נראית הירידה בשיפוע עבור המקרה של תכונה אחת, N = 1.

25
00:02:01,383 --> 00:02:06,782
היו לנו שני כללי עדכון נפרדים עבור הפרמטרים תטא-0 ותטא-1,

26
00:02:06,782 --> 00:02:12,779
ואני מקווה שזה נראה לכם מוכר. הביטוי הזה כאן הוא כמובן

27
00:02:12,779 --> 00:02:17,672
הנגזרת החלקית של פונקצית העלות ביחס לפרמטר תטא-0,

28
00:02:17,672 --> 00:02:21,891
ובדומה לכך יש לנו כלל עדכון אחר עבור הפרמטר תטא-1.

29
00:02:21,891 --> 00:02:26,259
יש הבדל אחד קטן, שהוא שכשהיתה לנו רק תכונה

30
00:02:26,259 --> 00:02:31,992
אחת קראנו לה התכונה (x(i
אבל בסימון החדש שלנו

31
00:02:31,992 --> 00:02:38,462
היינו כמובן קוראים לה (x(i סימן תחתון 1 כדי
<u>לציין את התכונה האחת שלנו.</u>

32
00:02:38,462 --> 00:02:41,019
זה היה כאשר היתה לנו תכונה אחת בלבד.

33
00:02:41,019 --> 00:02:44,496
בואו נסתכל על האלגוריתם החדש בו יש לנו יותר ממאפיין אחד,

34
00:02:44,496 --> 00:02:47,350
שבו מספר התכונות n יכול להיות הרבה יותר גדול מאשר אחת.

35
00:02:47,350 --> 00:02:53,158
אנחנו מקבלים את כלל העדכון הזה עבור הירידה בשיפוע, ואולי עבור אלה מכם

36
00:02:53,158 --> 00:02:57,781
שיודעים חשבון דיפרנציאלי, אם תקח את הגדרת פונקצית העלות

37
00:02:57,781 --> 00:03:03,312
ותגזור נגזרת חלקית של פונקצית העלות J ביחס לפרמטר

38
00:03:03,312 --> 00:03:08,119
תטא-j, תראה שהנגזרת החלקית היא בדיוק הביטוי כאן

39
00:03:08,119 --> 00:03:10,665
שסביבו ציירתי את המלבן הכחול.

40
00:03:10,665 --> 00:03:14,837
ואם תיישמו את זה תקבלו יישום עובד של

41
00:03:14,837 --> 00:03:18,962
הירידה בשיפוע עבור רגרסיה לינארית מרובת-משתנים.

42
00:03:18,962 --> 00:03:21,572
הדבר האחרון שאני רוצה לעשות בשקופית הזו הוא לתת לכם תחושה של

43
00:03:21,572 --> 00:03:26,882
למה שני האלגוריתמים החדש והישן הם בערך אותו דבר או למה הם

44
00:03:26,882 --> 00:03:30,904
דומים או למה הם שניהם אלגוריתמים של ירידה בשיפוע.

45
00:03:30,904 --> 00:03:34,363
הבה נבחן מקרה בו יש לנו שתי תכונות

46
00:03:34,363 --> 00:03:37,488
או אולי יותר משתי תכונות, נניח שלושה כללי עדכון

47
00:03:37,488 --> 00:03:42,680
עבור הפרמטרים תטא-0, תטא-1, תטא-2 ואולי עוד ערכים של תטא.

48
00:03:42,680 --> 00:03:49,457
אם תסתכל על כלל העדכון עבור תטא-0, תגלה שהוא

49
00:03:49,457 --> 00:03:55,300
זהה לכלל העדכון שהיה לנו מקודם

50
00:03:55,300 --> 00:03:57,350
עבור המקרה של n=1.

51
00:03:57,350 --> 00:04:00,203
והסיבה שהם זהים היא, כמובן,

52
00:04:00,203 --> 00:04:06,871
כי לפי ההסכמות שלנו   x(i)<u> של 0 שווה ל-1., וזו</u>

53
00:04:06,871 --> 00:04:12,003
הסיבה ששני הביטויים האלה שציירתי סביבם את המרובע הסגול הם שקולים.

54
00:04:12,003 --> 00:04:16,010
בדומה, אם תסתכלו על כלל העדכון עבור תטא-1, תגלו

55
00:04:16,010 --> 00:04:21,540
שהביטוי הזה כאן הוא שווה ערך לביטוי שהיה לנו בעבר,

56
00:04:21,540 --> 00:04:25,020
או למשוואה בה השתמשנו בעבר עבור תטא-1,

57
00:04:25,020 --> 00:04:30,222
אלא שכאן כמובן אנחנו משתמשים בכיתוב החדש x(i)<u>1 כדי לציין</u>

58
00:04:30,222 --> 00:04:37,605
את התכונה הראשונה שלנו, ועכשיו שיש לנו יותר ממאפיין אחד יכולים להיות לנו

59
00:04:37,605 --> 00:04:43,560
כללי עדכון דומים עבור פרמטרים אחרים כמו תטא-2 וכן הלאה.

60
00:04:43,560 --> 00:04:48,219
יש הרבה חומר בשקף הזה אז אני בהחלט ממליץ לכם,

61
00:04:48,219 --> 00:04:52,020
אם אתם צריכים להשהות את הוידאו ולבחון בעיון את כל המתמטיקה בשקף הזה

62
00:04:52,020 --> 00:04:55,446
כדי לוודא שאתם מבינים מה קורה כאן.

63
00:04:55,446 --> 00:05:00,440
אבל בכל אופן, אם תיישמו את האלגוריתם שנכתב כאן תקבלו

64
00:05:00,440 --> 00:05:51,300
יישום עובד של רגרסיה ליניארית רבת-תכונות.