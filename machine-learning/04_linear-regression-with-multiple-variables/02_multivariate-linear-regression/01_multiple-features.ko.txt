이번 영상에서는 하나 이상의 변수나 features를 다루기 위한 더 강력한 선형 회귀에 대해 알아보겠습니다. 여기 보시죠. 우리가 앞서 배운 선형 회귀에서는 단일 feature를 가진 x가 있었죠. 그 x로 집의 크기를 나타냈구요. 그리고 이걸 집의 가격을 결정하는 요소로 썼고, 이 공식을 추론- hypothesis 이라 배웠습니다. 그런데 생각해보세요. 집의 가격을 결정하는 요소 - feature 또는 variable로 여태까지는 ‘집의 크기'만 갖고 있었죠. 그런데 우리가 '침실의 개수'나 집이 지어진 지 얼마나 오래되었는 지도 알고 있다면 가격을 결정하는 
더 많은 요소를 갖고 있는 것이겠지요. 먼저 notation - 변수명 지정을 먼저 얘기해보도록 하겠습니다. 이건 앞서 잠시 얘기 한 적이 있었어요. 이 변수들은 X1, X2 … 처럼 X에 숫자를 붙여서 변수로 사용하도록 하겠습니다. 총 네 개네요. 그리고 결과값인 '집의 가격'은 Y로 지정하도록 합니다. notation을 좀 더 보도록 하죠. 우리는 네 개의 features가 있네요. Features의 개수를 나타내는 변수는 소문자 n을 사용하겠습니다. 그래서 이 예제에서 우리는 총 n4가 있겠네요. 총 4개의 feature가 있으니. 그리고 n은 우리가 앞서 봐온 'M' 변수와는 달라요. 예제의 개수를 나타냅니다. 그래서 47행의 "M"은 테이블의 행 또는 training 예제의 개수를 
나타내는 것으로 보시면 됩니다. 그리고 또 이제 X i의 i는 training example의 입력 feature로 사용할 것입니다. 예제를 좀 더 구체화 하기 위해 x2가 두 번째 트레이닝 예제 - training example feature의 vector가 될 거에요. 그러면 x2는 1416, 3, 2, 40 이 되겠죠. 여기에 있는 네 개의 feature들이 두 번째 집의 가격 결정을 짓는 요소이니까요. 그래서 여기 변수명을 보면, x2로 지정되어 있지요. 이건 제 training set의 index에요. 이건 x의 2승이 아닙니다. index는 이 테이블의 두 번째 줄을 보시죠. 이것은 두 번째 training example을 참조하고 있는 게 보이시죠. X2랑 같이요. X2는 4차원 벡터예요. 사실상, 더 일반적으로, 이건 in-dimentional feature*에요. 이 notation으로, x2는 이제 벡터가 된 것입니다. xi 첨자 값으로 j를 이용해서 j값을 나타내겠습니다. 여기서 j는 training 예제의 feature 개수를 나타냅니다. 그래서, 구체적으로 x2의 첨자 3은 여기 x factor의 feature 의 수 3을 의미하고 이건 2와 같죠. 맞나요? 여기에 3이 있었죠, 
잠시 필기 좀 고치겠습니다. 그래서 x2 첨자의 3이 2와 같아집니다. 자, 우리는 이제 여러 개의 features 가 있네요. 그럼 이제 추론 - hypothesis이 어떤 모습으로 형성되야 할지 한 번 살펴봅시다. 앞서 봤던 추론의 형태는, x가 단일 feature였죠. 하지만 이제 우리는 여러 개의 features가 있으니까 단순하게 표현하지 않을 겁니다. 이제 이게 선형 회귀 추론의 형태를 하고 있는 걸 볼 수 있습니다. 이거는 θ 0 + θ 0 + θ 1 x1 + θ 2 x2 θ 0 + θ 1 x1 + θ 2 x2 + θ 3 x3 θ 0 + θ 1 x1 + θ 2 x2 + θ 3 x3 + θ 4 x4 에요. 그리고 우리가 4개features가 아닌, n개의 features가 있다면 n 개의 feature를 더해야 하겠지요. 다시 구체적으로 이 parameters의 특정 setting을 보겠습니다. X 80 + 0.1 X1 + 0.01x2 + 3x3 - 2x4. 우리는 h가 있었죠. 이건 추론 - hypothesis의 한 예로 볼 수 있어요. 그리고 추론은 가령 몇 천 달러가 나가는 집의 가격을 예측하는 거죠. 아시다시피, 집의 기본 가격은 80,000 + @ 일거에요. 그러니 이건 추가적으로, 1평 당 1평 당 몇 백 달러 1평 당 몇 백 달러 + 층 수 1평 당 몇 백 달러 + 층 수, 그리고 1평 당 몇 백 달러 + 층 수, 그리고 침실 개수 1평 당 몇 백 달러 + 층 수, 그리고 침실 개수 정도 정도 하겠죠. 왜냐면 x3 은 침실의 개수이고, 집 가격은 집이 오래되면 오래될 수록 떨어질테니까요. 여기에 추론의 형태를 다시 정리해 놓았어요 그리고 이제 제가 이 식을 좀 더 간편하게 notation해볼게요. notation의 편의를 위해서, x0이 1이라고 가정해봅시다. 구체적으로, 이건 각 각의 모든 예제 i, 여기에 벡터 xi 그리고 xi 0 는 1이랑 같은 것이겠죠. 이걸 추가적인 zero feature라고 정의해 볼 수 있습니다. 그리고 여기에는 앞서 n features 가 있었지요. 
왜냐면 x1, x2 x1, x2 ~ xn까지 추가적으로 1의 값 만을 갖는 zero feature 벡터를 정의하고 있으니까요. 자, 그래서 이렇게 여기의 feature 벡터 x는 n+1 차원이 될 것입니다. index값이 0인 백터가 됩니다. 이제 여기의 n+1 차원의 feature 벡터인데요. index를 0부터 줄 거에요. 그리고 이 parameter가 vector라고 하겠습니다. 자, 여기의 parameter가 있어요. 여기는 θ 0이고, θ1, θ2… θn까지, 그리고 이걸 다 하나의 parameter vector인 θ 0 θ0, θ1, θ2 … θn으로 θ0, θ1, θ2 … θn으로 모아보겠습니다. 이건 다른 zero index vector에요. 0부터 지정된 인덱스지요. 이건 다른 형식의 n + 1 차원 벡터입니다. 그래서, 이 추론 - hypothesis는 θ x0 θ x0+ θ1x1 θ x0+ θ1x1 + ... + θnXn 로 쓰여질 수 없는 거에요. 그리고 이 공식은 위에 있는 이것과 같다고 볼 수 있겠죠, 왜냐하면, 8개의 0는 1이랑 같으니까요. 아래에선 이 추론 형태를 이용해서 당신이 벡터의 내적에 대해 얼마나 알고 있느냐 여부에 따라, θt x로 θt x로 바꿔 쓸 수 있어요. θ transfers x는 θ0, θ0, θ1 θ0, θ1~ θn입니다. 그리고 바로 이것이 θ transpose 이고요. 이건 원래 하나의 매트릭스마다 n+1 x1 인거죠. 이걸 다른 말로 행 벡터라고도 합니다. 이걸로 vector x로 x0 x0, x1 x0, x1, xn까지 곱할 수 있습니다. 그리고 내적, θ transpose x는 이것과 같은 거에요. 이것은 편리한 방법으로 추론의 형태를 쓸 수 있도록 하죠. 파라미터 벡터 세터와 세터 벡터 x간의 내적처럼. 그리고 이런 notation을 통해 축약된 태로 쓸 수 있는 편의를 느낄 수 있습니다. 자, 여러 개의 features 를 갖고 있을 때의 
추론의 형태를 살펴보았습니다. 이건 다변량 선형 회귀 
multivariate linear regression 라고 부르기도 합니다. 그리고 용어 다변수량  - multivariable는 Y값을 예측하는 여러 개의 feature 혹은 변수를 표현하는 말입니다.