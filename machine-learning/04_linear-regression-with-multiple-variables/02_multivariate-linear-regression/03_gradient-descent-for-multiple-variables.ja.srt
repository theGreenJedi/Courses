1
00:00:00,220 --> 00:00:03,688
前回のビデオでは、線形回帰のうち、

2
00:00:03,688 --> 00:00:07,246
複数のフィーチャー、または変数に対する物を議論した。

3
00:00:07,246 --> 00:00:11,912
このビデオでは、その仮説にどうパラメータをフィットさせるかを議論しよう。

4
00:00:11,912 --> 00:00:15,175
特に、最急降下法（Gradient Descent）を複数フィーチャーの線形回帰に

5
00:00:15,175 --> 00:00:19,875
どう使うかを議論していきたい。

6
00:00:19,875 --> 00:00:24,802
我らのノーテーションを簡単に要約すると、これが複数変数における線形回帰の

7
00:00:24,802 --> 00:00:31,509
我らのフォーマルな仮説で、そこではx0=1のコンベンションを採用する。

8
00:00:31,509 --> 00:00:37,505
このモデルのパラメータはシータ0からシータnまでだが、

9
00:00:37,505 --> 00:00:42,385
これをn個の別々のパラメータと考える代わりに、そう考えてもいいのだが、その代わりにパラメータを

10
00:00:42,385 --> 00:00:51,175
シータというn+1次元のベクトルと考える事としよう。

11
00:00:51,175 --> 00:00:55,498
つまり、このモデルのパラメータ自身も

12
00:00:55,498 --> 00:00:58,674
ベクトルと考える。

13
00:00:58,674 --> 00:01:03,507
コスト関数はJのシータ0からシータnまでで、

14
00:01:03,507 --> 00:01:08,983
普通の誤差項の二乗和で与えられる。
だがJをこれらn+1個の数の

15
00:01:08,983 --> 00:01:14,016
関数と考える代わりに、より一般的にJを単なるパラメータベクトル、シータの

16
00:01:14,016 --> 00:01:22,275
関数とみなす。つまりここのシータはベクトル。

17
00:01:22,275 --> 00:01:26,897
最急降下法はこんな感じ。
各シータjを シータj - アルファ×この微分項 で、

18
00:01:26,897 --> 00:01:32,142
繰り返し何度も更新していく。

19
00:01:32,142 --> 00:01:37,868
そしてこれを、Jのシータと書く。
シータjが、シータj マイナスの

20
00:01:37,868 --> 00:01:41,840
学習率アルファ掛ける事の、コスト関数の微分、、、

21
00:01:41,840 --> 00:01:47,840
正確にはパラメータであるシータ jによる偏微分。

22
00:01:47,840 --> 00:01:51,305
最急降下法を実装する時にこれがどんな感じか、

23
00:01:51,305 --> 00:01:55,985
特に偏微分の項がどんな感じか見ていこう。

24
00:01:55,985 --> 00:02:01,383
N=1のフィーチャーの時に最急降下法を用いると、こうなる。

25
00:02:01,383 --> 00:02:06,782
パラメータ シータ0とシータ1に別々の2つのアップデートのルールがあった。

26
00:02:06,782 --> 00:02:12,779
これは今やお馴染みだろう。
そしてこの項はもちろん、

27
00:02:12,779 --> 00:02:17,672
パラメータ シータ0によるコスト関数の偏微分だ。

28
00:02:17,672 --> 00:02:21,891
同様に似たようなアップデートルールがシータ1についてもあった。

29
00:02:21,891 --> 00:02:26,259
ちょっとだけ違うのは、一つしかフィーチャーが無かった時は

30
00:02:26,259 --> 00:02:31,992
そのフィーチャーをx(i)と呼べたが、この新しいノーテーションでは、

31
00:02:31,992 --> 00:02:38,462
もちろんそれを、x(i)の1と、一つのフィーチャーを示すように呼ぶ事となる。

32
00:02:38,462 --> 00:02:41,019
つまりそれは一つしかフィーチャーを持たない場合だ。

33
00:02:41,019 --> 00:02:44,496
では新しいアルゴリズムを、フィーチャーが一つより多い場合について見てみよう。

34
00:02:44,496 --> 00:02:47,350
つまりフィーチャーの数nが1より大きい場合もありうる、というケース。

35
00:02:47,350 --> 00:02:53,158
最急降下法のアップデートルールはこうなる。
解析学が分かる人の為に言っておくと、

36
00:02:53,158 --> 00:02:57,781
コスト関数の定義をとってきて、

37
00:02:57,781 --> 00:03:03,312
そのコスト関数Jをパラメータ シータiに関して偏微分を取ると、

38
00:03:03,312 --> 00:03:08,119
その偏微分の項はここに青い箱で書いた項と

39
00:03:08,119 --> 00:03:10,665
正確に一致する事が分かる。

40
00:03:10,665 --> 00:03:14,837
そしてもしこれを実装すれば、それは多変量の線形回帰に対する

41
00:03:14,837 --> 00:03:18,962
最急降下法の、動く実装とあいなる。

42
00:03:18,962 --> 00:03:21,572
このスライドで行いたい最後の仕事は、

43
00:03:21,572 --> 00:03:26,882
この新しいアルゴリズムと古いアルゴリズムは同じ事というなんとなくの感覚を感じてもらいたい、という事。

44
00:03:26,882 --> 00:03:30,904
言い換えると両者が何故似たアルゴリズムなのか、どうしてどちらも最急降下法アルゴリズムなのかを感覚的に分かって欲しい。

45
00:03:30,904 --> 00:03:34,363
2つのフィーチャー、または2つ以上のフィーチャーがあるケースを

46
00:03:34,363 --> 00:03:37,488
考えてみよう。
例えばシータ0 シータ1 シータ2の、

47
00:03:37,488 --> 00:03:42,680
3つのアップデートルールがある。
さらなる別のシータもあっても良い。

48
00:03:42,680 --> 00:03:49,457
ここでシータ0のアップデートルールを見ると、

49
00:03:49,457 --> 00:03:55,300
これは前にやった、n=1の時のアップデートと

50
00:03:55,300 --> 00:03:57,350
同じ事に気付く。

51
00:03:57,350 --> 00:04:00,203
それらが等しい理由は、もちろん、

52
00:04:00,203 --> 00:04:06,871
我らの採用したノーテーションのコンベンションでは、x(i) 0 は1というコンベンションだから。

53
00:04:06,871 --> 00:04:12,003
そんな訳で、マゼンダの箱で描いたこれら二項は等価である。

54
00:04:12,003 --> 00:04:16,010
同じように、シータ1のアップデートルールを見ると、

55
00:04:16,010 --> 00:04:21,540
ここのこの項は以前のシータ1の時の物、

56
00:04:21,540 --> 00:04:25,020
つまり以前の方程式またはアップデートルールと等価だ。

57
00:04:25,020 --> 00:04:30,222
もちろん、この新しいノーテーション、x(i)の1を最初のフィーチャーを表すのに使っていて、

58
00:04:30,222 --> 00:04:37,605
今は一つより多いフィーチャーを扱っている。

59
00:04:37,605 --> 00:04:43,560
シータ2などにも似たようなアップデートルールを用いる事が出来る。

60
00:04:43,560 --> 00:04:48,219
このスライドではたくさんの事を説明したので、ビデオを一旦一時停止して、

61
00:04:48,219 --> 00:04:52,020
このスライドの全ての数式をゆっくりと見直して、

62
00:04:52,020 --> 00:04:55,446
ここにある事を全てしっかりと理解していることを確認する事を、激しく推奨する。

63
00:04:55,446 --> 00:05:00,440
だがもしここに書かれたアルゴリズムを実装したら、

64
00:05:00,440 --> 00:05:51,300
複数フィーチャーの線形回帰の、動く実装を得る事になるよ。