1
00:00:00,450 --> 00:00:03,210
在本段视频中 我想告诉大家

2
00:00:03,210 --> 00:00:05,070
一些关于梯度下降算法的实用技巧

3
00:00:05,070 --> 00:00:08,650
我将集中讨论

4
00:00:09,860 --> 00:00:13,180
学习率 α

5
00:00:13,180 --> 00:00:16,270
具体来说 这是梯度下降算法的

6
00:00:16,270 --> 00:00:19,050
更新规则

7
00:00:19,050 --> 00:00:22,390
这里我想要

8
00:00:22,390 --> 00:00:26,480
告诉大家

9
00:00:26,480 --> 00:00:29,250
如何调试

10
00:00:29,250 --> 00:00:32,770
也就是我认为应该如何确定

11
00:00:32,770 --> 00:00:34,150
梯度下降是正常工作的

12
00:00:34,150 --> 00:00:38,219
此外我还想告诉大家

13
00:00:38,219 --> 00:00:42,553
如何选择学习率 α

14
00:00:42,553 --> 00:00:47,483
也就是我平常

15
00:00:47,483 --> 00:00:49,750
如何选择这个参数

16
00:00:49,750 --> 00:00:53,515
我通常是怎样确定

17
00:00:53,515 --> 00:00:58,659
梯度下降正常工作的

18
00:00:59,720 --> 00:01:02,960
梯度下降算法所做的事情

19
00:01:02,960 --> 00:01:07,795
就是为你找到

20
00:01:07,795 --> 00:01:13,107
一个 θ 值

21
00:01:13,107 --> 00:01:15,767
并希望它能够最小化代价函数 J(θ)

22
00:01:15,767 --> 00:01:20,570
我通常会在

23
00:01:20,570 --> 00:01:25,240
梯度下降算法运行时

24
00:01:25,240 --> 00:01:28,770
绘出代价函数 J(θ) 的值

25
00:01:28,770 --> 00:01:32,630
这里的 x 轴是表示

26
00:01:32,630 --> 00:01:35,630
梯度下降算法的

27
00:01:35,630 --> 00:01:39,760
迭代步数

28
00:01:39,760 --> 00:01:43,630
你可能会得到

29
00:01:43,630 --> 00:01:49,620
这样一条曲线

30
00:01:49,620 --> 00:01:53,810
注意 这里的 x 轴

31
00:01:55,230 --> 00:01:59,353
是迭代步数

32
00:01:59,353 --> 00:02:02,020
在我们以前看到的

33
00:02:02,020 --> 00:02:07,392
J(θ) 曲线中

34
00:02:07,392 --> 00:02:11,671
x 轴 也就是横轴

35
00:02:17,058 --> 00:02:21,774
曾经用来表示参数 θ 但这里不是

36
00:02:21,774 --> 00:02:26,783
具体来说

37
00:02:26,783 --> 00:02:31,350
这一点的含义是这样的

38
00:02:31,350 --> 00:02:35,720
当我运行完100步的梯度下降迭代之后

39
00:02:35,720 --> 00:02:38,540
无论我得到

40
00:02:38,540 --> 00:02:41,520
什么 θ 值

41
00:02:41,520 --> 00:02:46,090
总之 100步迭代之后

42
00:02:46,090 --> 00:02:50,510
我将得到

43
00:02:50,510 --> 00:02:53,800
一个 θ 值

44
00:02:53,800 --> 00:02:55,829
根据100步迭代之后

45
00:02:57,580 --> 00:03:01,630
得到的这个 θ 值

46
00:03:01,630 --> 00:03:04,850
我将算出

47
00:03:04,850 --> 00:03:09,220
代价函数 J(θ) 的值

48
00:03:09,220 --> 00:03:15,110
而这个点的垂直高度就代表

49
00:03:15,110 --> 00:03:20,110
梯度下降算法

50
00:03:20,110 --> 00:03:24,048
100步迭代之后

51
00:03:24,048 --> 00:03:25,476
得到的 θ

52
00:03:25,476 --> 00:03:30,026
算出的 J(θ) 值

53
00:03:30,026 --> 00:03:34,430
而这个点

54
00:03:34,430 --> 00:03:37,725
则是梯度下降算法

55
00:03:37,725 --> 00:03:42,430
迭代200次之后

56
00:03:42,430 --> 00:03:47,560
得到的 θ

57
00:03:47,560 --> 00:03:52,310
算出的 J(θ) 值

58
00:03:52,310 --> 00:03:57,100
所以这条曲线

59
00:03:57,100 --> 00:04:01,220
显示的是

60
00:04:01,220 --> 00:04:05,340
梯度下降算法迭代过程中代价函数 J(θ) 的值

61
00:04:05,340 --> 00:04:10,460
如果梯度下降算法

62
00:04:10,460 --> 00:04:13,840
正常工作

63
00:04:13,840 --> 00:04:18,110
那么每一步迭代之后

64
00:04:18,110 --> 00:04:21,740
J(θ) 都应该下降

65
00:04:21,740 --> 00:04:25,370
这条曲线

66
00:04:25,370 --> 00:04:28,730
的一个用处在于

67
00:04:28,730 --> 00:04:33,600
它可以告诉你

68
00:04:33,600 --> 00:04:38,280
如果你看一下

69
00:04:38,280 --> 00:04:43,110
我画的这条曲线

70
00:04:43,110 --> 00:04:47,250
当你达到

71
00:04:48,320 --> 00:04:52,885
300步迭代之后

72
00:04:52,885 --> 00:04:58,370
也就是300步到400步迭代之间

73
00:04:59,380 --> 00:05:02,545
也就是曲线的这一段

74
00:05:02,545 --> 00:05:06,090
看起来 J(θ) 并没有下降多少

75
00:05:06,090 --> 00:05:07,450
所以当你

76
00:05:07,450 --> 00:05:11,525
到达400步迭代时

77
00:05:11,525 --> 00:05:15,075
这条曲线看起来已经很平坦了

78
00:05:15,075 --> 00:05:17,975
也就是说

79
00:05:17,975 --> 00:05:20,096
在这里400步迭代的时候

80
00:05:20,096 --> 00:05:24,284
梯度下降算法

81
00:05:24,284 --> 00:05:26,617
基本上已经收敛了

82
00:05:26,617 --> 00:05:30,690
因为代价函数并没有继续下降

83
00:05:30,690 --> 00:05:34,140
所以说 看这条曲线

84
00:05:34,140 --> 00:05:38,660
可以帮助你判断

85
00:05:38,660 --> 00:05:41,820
梯度下降算法是否已经收敛

86
00:05:41,820 --> 00:05:46,700
顺便说一下

87
00:05:49,020 --> 00:05:53,090
对于每一个特定的问题

88
00:05:53,090 --> 00:05:56,890
梯度下降算法所需的迭代次数

89
00:05:56,890 --> 00:05:58,850
可以相差很大

90
00:05:58,850 --> 00:06:03,130
也许对于某一个问题

91
00:06:04,150 --> 00:06:05,400
梯度下降算法

92
00:06:05,400 --> 00:06:09,560
只需要30步迭代就可以收敛

93
00:06:09,560 --> 00:06:14,180
然而换一个问题

94
00:06:14,180 --> 00:06:19,030
也许梯度下降算法就需要3000步迭代

95
00:06:19,030 --> 00:06:21,979
对于另一个机器学习问题

96
00:06:21,979 --> 00:06:23,810
则可能需要三百万步迭代

97
00:06:23,810 --> 00:06:26,430
实际上

98
00:06:26,430 --> 00:06:30,830
我们很难提前判断

99
00:06:31,930 --> 00:06:36,760
梯度下降算法

100
00:06:36,760 --> 00:06:40,930
需要多少步迭代才能收敛

101
00:06:40,930 --> 00:06:47,100
通常我们需要画出这类曲线

102
00:06:47,100 --> 00:06:50,990
画出代价函数随迭代步数数增加的变化曲线

103
00:06:50,990 --> 00:06:52,360
通常 我会通过看这种曲线

104
00:06:52,360 --> 00:06:55,510
来试着判断

105
00:06:55,510 --> 00:06:59,845
梯度下降算法是否已经收敛

106
00:06:59,845 --> 00:07:05,640
另外 也可以

107
00:07:05,640 --> 00:07:11,490
进行一些自动的收敛测试

108
00:07:11,490 --> 00:07:15,220
也就是说用一种算法

109
00:07:15,220 --> 00:07:19,040
来告诉你梯度下降算法

110
00:07:19,040 --> 00:07:23,810
是否已经收敛

111
00:07:23,810 --> 00:07:27,810
自动收敛测试

112
00:07:27,810 --> 00:07:31,620
一个非常典型的例子是

113
00:07:31,620 --> 00:07:33,500
如果代价函数 J(θ)

114
00:07:33,500 --> 00:07:36,460
的下降小于

115
00:07:36,460 --> 00:07:38,670
一个很小的值 ε

116
00:07:38,670 --> 00:07:41,550
那么就认为已经收敛

117
00:07:41,550 --> 00:07:45,250
比如可以选择

118
00:07:45,250 --> 00:07:47,290
1e-3

119
00:07:47,290 --> 00:07:54,160
但我发现

120
00:07:54,160 --> 00:07:57,180
通常要选择一个合适的阈值 ε 是相当困难的

121
00:07:57,180 --> 00:08:00,970
因此 为了检查

122
00:08:00,970 --> 00:08:03,679
梯度下降算法是否收敛

123
00:08:06,827 --> 00:08:09,985
我实际上还是

124
00:08:09,985 --> 00:08:13,613
通过看

125
00:08:13,613 --> 00:08:15,232
左边的这条曲线图

126
00:08:15,232 --> 00:08:20,627
而不是依靠自动收敛测试

127
00:08:20,627 --> 00:08:25,512
此外 这种曲线图

128
00:08:25,512 --> 00:08:30,640
也可以

129
00:08:30,640 --> 00:08:33,316
在算法没有正常工作时

130
00:08:33,316 --> 00:08:37,078
提前警告你

131
00:08:37,078 --> 00:08:40,966
具体地说

132
00:08:40,966 --> 00:08:45,943
如果代价函数 J(θ)

133
00:08:45,943 --> 00:08:51,780
随迭代步数

134
00:08:51,780 --> 00:08:55,910
的变化曲线是这个样子

135
00:08:55,910 --> 00:08:58,010
J(θ) 实际上在不断上升