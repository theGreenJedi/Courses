이제 여러분들은 변수가 여러개인 linear regression을 알고 있습니다. 이 번 비디오에서는 feature를 간단하게 선택하는 방법과 적절한 feature의 선택으로 강력한 학습 알고리즘을 만드는 방법에 대해 이야기해보겠습니다. 그리고 polynomial regression도 다루어 볼 건데, 선형 회귀를 이용하여 복잡한 비선형 함수에도 적용해 볼 것입니다. 집값 예측 예제를 보죠. 두 개의 feature가 있는데 그 두 개는 집의 frontage와 집의 depth입니다. 여기 판매하려는 집의 그림도 있어요. 그럼, frontage는 너비 즉 집 폭의 길이로 정의하고 집의 depth는 집의 세로길이입니다. 그래서, 이게 frontage이고, 이게 depth입니다. frontage와 depth라고 부릅니다. 선형 회귀 모델은 이렇게 만들 수 있습니다. frontage가 첫 번째 feature x_1이고, depth가 두 번째 feature인 x_2입니다.
하지만 선형 회귀를 할 때에는 feature x_1과 x_2를 사용할 필요는 없습니다. 여러분이 직접 새로운 feature를 만드는 것입니다. 그래서, 집값을 예측하고 싶을 때, feature x_1, x_2대신에 사용할 것은 집의 크기 즉, 이 집의 면적입니다. 그럼, 새로운 feature가 만들어졌고, 이 것은 feature x라고 부르겠습니다. x는 frontage * depth입니다. 이건 곱하기 기호입니다. frontage * depth는 집의 넓이이기 때문에 이 feature 하나만 가지고 가설 함수를 정의할 수 있습니다. 그렇죠? 사각형의 넓이는 길이의 곱이기 때문입니다. 즉, 문제를 어떻게 이해하느냐에 따라, 곧이곧대로 feature를 쓰는것보다는 때로는 새로운 feature로 더 나은 모델을 정의해도 좋습니다. feature를 선택하는 것과 밀접하게 관련된 것으로 다항 회귀(polynomial regression)가 있습니다. 이와 같은 집 값 data set이 있다고 해봅시다. 이 data set을 표현할 수 있는 몇 가지 다른 모델들이 있습니다. 그 중 하나는 이와 같은 2차식 모델입니다. 직선은 그다지 이 data set을 잘 표현할 수 없을 것 같습니다. 그래서 이와같은 2차식 모델로 표현할 수 있습니다. 이 모델은 가격을 사이즈의 2차 함수로 표현한 것입니다. 이 모델을 사용하여, 이와 같이 data를 표현할 수 있습니다. 하지만 이 2차식 모델은 사실 적합하지가 않습니다. 왜냐하면 이 함수는 결국 하강하기 때문입니다. 집 크기는 커지는데, 집 값은 작아질리는 없죠 그래서, 다른 다항식 모델을 사용해보겠습니다. 2차 함수 대신에, 3차식을 이용하여 표현하면, 이러한 모양의 모델이 나오고, 이번에 초록색 선은 data를 더 잘 표현합니다. 왜냐하면 이번에는 하강하지 않기 때문입니다. 그럼, 어떻게 data를 잘 표현하는 이런 모델을 만들 수 있을까요? multivariant linear regression의 구조를 이용하여, 알고리즘을 간단히 수정해 봅시다. 가설 함수가 있고, 이렇게 표현할 수 있습니다. h(x)는 h(x)는 θ_0 h(x)는 θ_0 + θ_1 x_1 + θ_2 x_2 + θ_3 x_3입니다. 이것을 3차식 모델로 표현하고 싶으면, 초록색 박스안에 있는 것처럼, 집 값은 θ_0 + θ_0 + θ_1*(집의 크기) + θ_2*(집의 크기의 제곱) 즉 이 항은 이 항과 똑같습니다. 그리고 + θ_3*(집의 크기의 세제곱은) 이 세 번째 항입니다. 두 식이 같아지도록 하기 위해서는, 첫 번째 feature x_1은 집의 크기로 치환하고, 두 번째 feature x_2는 집의 크기의 제곱으로 치환하고, 세 번째 x_3는 집의 크기의 세제곱으로 치환합니다. 이와 같이 세 개의 feature를 선형 회귀형태로 적용시키면, 이 모델을 삼차식으로 표현할 수 있습니다. 한 가지 더 강조하고 싶은 것은 이렇게 feature를 선택했을 때, eature scaling의 적용은 훨씬 더 중요해진다는 것입니다. 만약 집의 크기가 1에서 1000사이의 범위를 가진다면, 즉 1에서 1000 평방 피트라면, 집의 제곱 크기는 1에서 백만까지, 즉 1000의 제곱까지 됩니다. 그리고 세 번째 feature x 세제곱, 
아 정정할게요. 세 번째 featuren x_3는 집의 세제곱 크기라 1에서 10의 9승의 범위가 될것입니다. 결국 이 세 개의 feature는 매우 다른 값의 범위를 가지게 되므로, feature scaling을 적용시키는 것이 중요합니다. 그래야 gradient descent를 할 때 서로 비슷한 범위를 가질수 있습니다. 그럼, 마지막으로 feature를 다루는 다양한 방법을 보여주는 예제를 들어보겠습니다. 처음에, 이러한 2차식 모델은 이상적이지 않다고 말했는데, data를 표현하는데는 괜찮지만, 2차 함수는 하강하는 모양이고, 그런 모양은 우리가 원하는 모양이 아니기 때문이였습니다. 하강하는 모양이라면, 집의 크기가 폭등할 때, 집 값은 떨어지는 것으로 예측됩니다. 그래서 3차 식 모델을 소개했지만, 그것 말고도 다른 방법이 있습니다. 다른 괜찮은 방법 중 한 가지 예를 들어보면, 그 예는 집 값을 θ_0 + θ_1 * (크기) + θ_2*(크기의 제곱근)으로 나타내는 것입니다. 그렇죠? 제곱근 함수는 이런 모양입니다. 그리고, θ_0, θ_1, θ_2의 값이 있으면, 이러한 모델이 만들어질 것입니다. 이 곡선은 갈수록 이렇게 평평해지지만 하강하지는 않습니다. 결국, 문제를 다른 관점에서 본다면, 이 경우에서는 data의 모양을 제곱근 함수의 관점에서 본다면, feature를 가지고 더 나은 모델을 만들 수 있습니다. 이 번 비디오에서는, 다항 회귀에 대해 배워보았습니다. 어떻게 다항식을, 즉 2차 함수나, 3차 함수를 data에 맞게 표현하는지에 대해 배웠습니다. 그리고 어떻게 feature를 사용할지 여러 방법이 있다는 것도 배웠습니다. 예를들면, 집의 frontage와 depth를 사용하는 것보다는 둘을 곱하여, 집의 면적에 대한 feature를 만들었습니다. 여러개의 서로 다른 feature가 있을 때는, 어떤 feature를 사용해야하는지 혼란스러운 상황이 있을 수도 있습니다. 이 수업 이후에, 자동으로 어떤 feature를 사용할지 골라주는 알고리즘에 대해서 배울 것입니다. 그 알고리즘은 data를 보고, 자동으로 2차 함수나 3차 함수, 혹은 다른 함수들 중 알맞은 것을 선택합니다. 하지만, 이 알고리즘을 배우기전인 지금, 어떤 feature를 사용할지는 여러 방법이 있다는 것을 알아두었으면 합니다. 그리고, 다른 feature를 고안하여, data를 직선으로 표현하는게 아닌 복잡한 함수로 표현할 수 있습니다. 특히, 다항 함수나 때로는, 적절한 feature로 더 나은 모델을 새울 수 있습니다.