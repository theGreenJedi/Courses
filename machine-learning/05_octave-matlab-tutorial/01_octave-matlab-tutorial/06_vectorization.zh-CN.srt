1
00:00:00,320 --> 00:00:03,682
在这段视频中 我将介绍有关向量化的内容

2
00:00:03,682 --> 00:00:08,380
无论你是用Octave

3
00:00:08,380 --> 00:00:12,540
还是别的语言 比如MATLAB

4
00:00:12,540 --> 00:00:17,470
或者你正在用Python

5
00:00:17,470 --> 00:00:21,840
NumPy 或 Java C C++

6
00:00:21,840 --> 00:00:24,400
所有这些语言都具有

7
00:00:24,400 --> 00:00:29,250
各种线性代数库

8
00:00:29,250 --> 00:00:32,070
这些库文件都是内置的

9
00:00:32,070 --> 00:00:35,590
容易阅读和获取

10
00:00:35,590 --> 00:00:39,880
他们通常写得很好

11
00:00:39,880 --> 00:00:44,510
已经经过高度优化

12
00:00:44,510 --> 00:00:48,470
通常是数值计算方面的博士

13
00:00:48,470 --> 00:00:51,970
或者专业人士开发的

14
00:00:51,970 --> 00:00:53,170
而当你实现机器学习算法时

15
00:00:53,170 --> 00:00:58,120
如果你能

16
00:00:58,120 --> 00:01:03,080
好好利用这些

17
00:01:03,080 --> 00:01:07,330
线性代数库或者说

18
00:01:07,330 --> 00:01:08,540
数值线性代数库

19
00:01:08,540 --> 00:01:13,450
并联合调用它们

20
00:01:13,450 --> 00:01:18,170
而不是自己去做那些

21
00:01:18,170 --> 00:01:22,220
函数库可以做的事情

22
00:01:22,220 --> 00:01:26,090
如果是这样的话 那么

23
00:01:26,090 --> 00:01:30,310
通常你会发现 首先 这样更有效

24
00:01:30,310 --> 00:01:31,470
也就是说运行速度更快

25
00:01:33,090 --> 00:01:36,110
并且更好地利用

26
00:01:36,110 --> 00:01:40,010
你的计算机里可能有的一些并行硬件系统

27
00:01:40,010 --> 00:01:46,660
等等

28
00:01:46,660 --> 00:01:51,920
第二 这也意味着

29
00:01:51,920 --> 00:01:56,330
你可以用更少的代码来实现你需要的功能

30
00:01:56,330 --> 00:02:03,220
因此 实现的方式更简单

31
00:02:03,220 --> 00:02:07,221
代码出现问题的有可能性也就越小

32
00:02:07,221 --> 00:02:11,632
举个具体的例子

33
00:02:11,632 --> 00:02:17,590
与其自己写代码

34
00:02:17,590 --> 00:02:18,820
做矩阵乘法

35
00:02:18,820 --> 00:02:23,623
如果你只在Octave中

36
00:02:23,623 --> 00:02:25,360
输入 a乘以b

37
00:02:25,360 --> 00:02:30,160
就是一个非常有效的

38
00:02:30,160 --> 00:02:36,057
两个矩阵相乘的程序

39
00:02:36,057 --> 00:02:42,320
有很多例子可以说明

40
00:02:42,320 --> 00:02:44,740
如果你用合适的向量化方法来实现

41
00:02:44,740 --> 00:02:45,736
你就会有一个简单得多 也有效得多的代码

42
00:02:45,736 --> 00:02:51,100
让我们来看一些例子

43
00:02:51,100 --> 00:02:53,270
这是一个常见的线性回归假设函数

44
00:02:53,270 --> 00:02:58,543
如果

45
00:02:58,543 --> 00:03:03,557
你想要计算 h(x)

46
00:03:03,557 --> 00:03:08,570
注意到右边是求和

47
00:03:08,570 --> 00:03:13,411
那么你可以

48
00:03:13,411 --> 00:03:18,620
自己计算

49
00:03:18,620 --> 00:03:25,480
j =0 到 j = n 的和

50
00:03:25,480 --> 00:03:30,286
但换另一种方式来想想

51
00:03:30,286 --> 00:03:33,940
是把 h(x) 看作

52
00:03:33,940 --> 00:03:39,251
θ 转置乘以 x

53
00:03:39,251 --> 00:03:43,290
那么

54
00:03:43,290 --> 00:03:46,620
你就可以写成

55
00:03:46,620 --> 00:03:48,150
两个向量的内积

56
00:03:48,150 --> 00:03:52,077
其中 θ 就是

57
00:03:52,077 --> 00:03:54,620
θ0  θ1

58
00:03:54,620 --> 00:03:57,764
θ2 如果你有两个特征量

59
00:03:57,764 --> 00:04:02,604
如果 n 等于2 并且如果

60
00:04:02,604 --> 00:04:07,823
你把 x 看作

61
00:04:07,823 --> 00:04:13,250
x0 x1 x2

62
00:04:15,850 --> 00:04:17,590
这两种思考角度

63
00:04:17,590 --> 00:04:22,080
会给你两种不同的实现方式

64
00:04:22,080 --> 00:04:24,740
比如说

65
00:04:24,740 --> 00:04:27,820
这是未向量化的代码实现方式

66
00:04:27,820 --> 00:04:31,468
计算 h(x) 是未向量化的

67
00:04:31,468 --> 00:04:35,650
我的意思是 没有被向量化

68
00:04:35,650 --> 00:04:40,598
我们可能首先要初始化变量 prediction 的值为0.0

69
00:04:40,598 --> 00:04:42,620
而这个

70
00:04:42,620 --> 00:04:47,720
变量 prediction 的最终结果就是

71
00:04:47,720 --> 00:04:50,950
h(x) 然后

72
00:04:54,410 --> 00:04:59,000
变量 prediction 的最终结果就是

73
00:04:59,000 --> 00:05:02,500
j 取值 0 到 n+1

74
00:05:02,500 --> 00:05:06,540
变量prediction 每次就通过

75
00:05:06,540 --> 00:05:11,230
自身加上 theta(j) 乘以 x(j) 更新值

76
00:05:11,230 --> 00:05:17,540
这个就是算法的代码实现

77
00:05:17,540 --> 00:05:22,270
顺便我要提醒一下

78
00:05:22,270 --> 00:05:26,380
这里的向量

79
00:05:26,380 --> 00:05:30,390
我用的下标是 0

80
00:05:30,390 --> 00:05:32,880
所以我有 θ0 θ1

81
00:05:32,880 --> 00:05:35,800
θ2 但因为 MATLAB

82
00:05:35,800 --> 00:05:39,290
的下标从1开始 在 MATLAB 中 θ0

83
00:05:40,660 --> 00:05:43,590
我们可能会

84
00:05:43,590 --> 00:05:45,840
用 theta(1) 来表示

85
00:05:45,840 --> 00:05:48,260
这第二个元素

86
00:05:48,260 --> 00:05:55,070
最后就会变成

87
00:05:55,070 --> 00:05:59,575
theta(2) 而第三个元素

88
00:05:59,575 --> 00:06:03,410
最终可能就用

89
00:06:03,410 --> 00:06:08,377
theta(3) 表示 因为

90
00:06:08,377 --> 00:06:13,230
MATLAB 中的下标从1开始

91
00:06:14,850 --> 00:06:19,580
即使我们实际的

92
00:06:20,754 --> 00:06:24,120
θ 和 x 的下标从0开始

93
00:06:24,120 --> 00:06:28,110
这就是为什么

94
00:06:28,110 --> 00:06:35,500
这里我的 for 循环

95
00:06:35,500 --> 00:06:39,570
j 取值从 1 直到 n+1

96
00:06:39,570 --> 00:06:43,830
而不是

97
00:06:43,830 --> 00:06:48,540
从 0 到 n 清楚了吗？

98
00:06:48,540 --> 00:06:50,070
但这是一个

99
00:06:50,070 --> 00:06:54,220
未向量化的代码实现方式

100
00:06:54,220 --> 00:06:55,500
我们用一个 for 循环

101
00:06:55,500 --> 00:06:58,538
对 n 个元素进行加和

102
00:06:58,538 --> 00:07:03,458
作为比较 接下来是

103
00:07:03,458 --> 00:07:08,090
向量化的代码实现

104
00:07:08,090 --> 00:07:12,723
你把

105
00:07:12,723 --> 00:07:18,511
x 和 θ

106
00:07:18,511 --> 00:07:21,740
看做向量 而你只需要

107
00:07:21,740 --> 00:07:28,100
令变量 prediction 等于 theta转置

108
00:07:28,100 --> 00:07:29,840
乘以 x 你就可以这样计算

109
00:07:31,310 --> 00:07:34,920
与其写所有这些

110
00:07:34,920 --> 00:07:38,170
for 循环的代码

111
00:07:38,170 --> 00:07:43,610
你只需要一行代码

112
00:07:43,610 --> 00:07:50,080
这行代码

113
00:07:50,080 --> 00:07:54,920
右边所做的

114
00:07:54,920 --> 00:07:58,380
就是

115
00:07:58,380 --> 00:08:04,270
利用 Octave 的高度优化的数值

116
00:08:04,270 --> 00:08:06,580
线性代数算法来计算

117
00:08:06,580 --> 00:08:10,959
两个向量的内积

118
00:08:10,959 --> 00:08:17,040
θ 以及 x

119
00:08:17,040 --> 00:08:22,066
这样向量化的实现不仅仅是更简单

120
00:08:22,066 --> 00:08:27,570
它运行起来也将更加高效

121
00:08:27,570 --> 00:08:32,203
这就是 Octave 所做的

122
00:08:32,203 --> 00:08:37,288
而向量化的方法

123
00:08:37,288 --> 00:08:42,730
在其他编程语言中同样可以实现

124
00:08:42,730 --> 00:08:48,621
让我们来看一个 C++ 的例子

125
00:08:48,621 --> 00:08:53,964
这就是未向量化的代码实现的样子

126
00:08:53,964 --> 00:08:58,360
我们再次初始化变量 prediction 为 0.0

127
00:08:58,360 --> 00:09:01,352
然后我们现在有一个完整的

128
00:09:01,352 --> 00:09:04,010
从 j 等于 0 直到 n

129
00:09:04,010 --> 00:09:09,930
变量 prediction +=

130
00:09:09,930 --> 00:09:15,782
theta[j] 乘以 x[j]

131
00:09:15,782 --> 00:09:19,772
再一次 你有这样的自己写的 for 循环

132
00:09:19,772 --> 00:09:24,427
与此相反 使用一个比较好的

133
00:09:24,427 --> 00:09:29,481
C++ 数值线性代数库

134
00:09:29,481 --> 00:09:34,136
你就可以用这个方程

135
00:09:34,136 --> 00:09:38,150
来写这个函数

136
00:09:38,150 --> 00:09:44,142
与此相反 使用较好的

137
00:09:44,142 --> 00:09:49,504
C++ 数值线性代数库

138
00:09:49,504 --> 00:09:54,707
你可以写出

139
00:09:54,707 --> 00:10:00,700
像这样的代码

140
00:10:00,700 --> 00:10:06,080
因此取决于你的

141
00:10:06,080 --> 00:10:10,041
数值线性代数库的内容

142
00:10:10,041 --> 00:10:15,100
你可以有一个对象 (object)

143
00:10:15,100 --> 00:10:19,990
像这个

144
00:10:19,990 --> 00:10:23,760
C++ 对象

145
00:10:23,760 --> 00:10:30,680
theta 和一个 C++

146
00:10:30,680 --> 00:10:34,760
对象 向量 x

147
00:10:34,760 --> 00:10:36,470
你只需要用 theta.transpose ( )

148
00:10:36,470 --> 00:10:40,530
乘以 x

149
00:10:40,530 --> 00:10:42,940
而这次是让 C++ 来实现运算

150
00:10:42,940 --> 00:10:49,740
因此

151
00:10:49,740 --> 00:10:54,370
你只需要在 C++ 中将两个向量相乘

152
00:10:54,370 --> 00:10:59,213
根据你所使用的

153
00:10:59,213 --> 00:11:04,244
数值和线性代数库的使用细节的不同

154
00:11:04,244 --> 00:11:09,739
你最终使用的代码表达方式

155
00:11:09,739 --> 00:11:14,580
可能会有些许不同

156
00:11:14,580 --> 00:11:20,670
但是通过

157
00:11:22,240 --> 00:11:27,592
一个库来做内积

158
00:11:27,592 --> 00:11:32,800
你可以得到一段更简单

159
00:11:32,800 --> 00:11:35,920
更有效的代码

160
00:11:35,920 --> 00:11:41,050
现在 让我们来看一个更为复杂的例子

161
00:11:41,050 --> 00:11:44,610
提醒一下

162
00:11:44,610 --> 00:11:46,670
这是线性回归算法梯度下降的更新规则

163
00:11:47,980 --> 00:11:51,940
所以

164
00:11:51,940 --> 00:11:57,010
我们用这条规则对 j 等于 0 1 2 等等的所有值

165
00:11:57,010 --> 00:12:02,590
更新 对象 θj

166
00:12:02,590 --> 00:12:08,435
我只是

167
00:12:08,435 --> 00:12:12,870
用 θ0 θ1 θ2

168
00:12:12,870 --> 00:12:18,430
来写方程

169
00:12:18,430 --> 00:12:21,960
那就是假设我们有两个特征量

170
00:12:21,960 --> 00:12:26,560
所以 n等于2

171
00:12:26,560 --> 00:12:31,420
这些都是我们需要对

172
00:12:31,420 --> 00:12:36,000
θ0 θ1 θ2 进行更新

173
00:12:37,010 --> 00:12:41,630
你可能还记得

174
00:12:41,630 --> 00:12:44,030
在以前的视频中说过

175
00:12:44,030 --> 00:12:47,230
这些都应该是同步更新

176
00:12:47,230 --> 00:12:50,620
因此 让我们来看看

177
00:12:50,620 --> 00:12:53,839
我们是否可以拿出一个

178
00:12:53,839 --> 00:12:58,610
向量化的代码实现

179
00:12:58,610 --> 00:13:03,142
这里是和之前相同的三个方程

180
00:13:03,142 --> 00:13:08,710
只不过写得小一点而已

181
00:13:08,710 --> 00:13:13,210
你可以想象

182
00:13:13,210 --> 00:13:17,915
实现这三个方程的方式之一

183
00:13:17,915 --> 00:13:19,650
就是用

184
00:13:19,650 --> 00:13:23,140
一个 for 循环

185
00:13:23,140 --> 00:13:27,465
就是让 j 等于0

186
00:13:27,465 --> 00:13:31,500
等于 等于2

187
00:13:31,500 --> 00:13:34,220
来更新 θj

188
00:13:34,220 --> 00:13:37,860
但让我们

189
00:13:37,860 --> 00:13:41,820
用向量化的方式来实现 看看我们是否能够有一个更简单的方法

190
00:13:41,820 --> 00:13:45,110
基本上用三行代码

191
00:13:45,110 --> 00:13:47,700
或者一个 for 循环