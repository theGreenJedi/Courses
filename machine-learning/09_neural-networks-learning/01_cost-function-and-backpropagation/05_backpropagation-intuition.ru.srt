1
00:00:00,260 --> 00:00:03,120
В предыдущем ролике, мы говорили об алгоритме обратного распространения.

2
00:00:04,230 --> 00:00:05,090
Для большого числа людей,

3
00:00:05,220 --> 00:00:06,140
столкнувшихся с этим в первый раз,

4
00:00:06,460 --> 00:00:07,610
первым впечатлением часто бывает

5
00:00:08,070 --> 00:00:09,250
вау! это

6
00:00:09,380 --> 00:00:11,650
очень сложный алгоритм и там все эти

7
00:00:11,970 --> 00:00:12,990
различные шаги. И я

8
00:00:13,130 --> 00:00:13,980
не вполне уверен как из соединить

9
00:00:14,180 --> 00:00:15,130
вместе и это похоже на

10
00:00:15,400 --> 00:00:17,830
черный ящик, со всем этими сложными шагами.

11
00:00:18,130 --> 00:00:18,830
В случае, если у вас

12
00:00:18,870 --> 00:00:20,460
возникают подобные чувства, по отношению к алгоритму обратного распространения, это

13
00:00:20,860 --> 00:00:22,100
нормально.

14
00:00:22,740 --> 00:00:24,100
Алгоритм обратного распространения, возможно, к несчастью

15
00:00:24,970 --> 00:00:26,920
менее математически понятен, или

16
00:00:27,060 --> 00:00:28,520
менее математически простой алгоритм

17
00:00:28,860 --> 00:00:30,680
по сравнению с линейной регрессией или

18
00:00:31,130 --> 00:00:32,850
логистической регрессией, и Я

19
00:00:33,020 --> 00:00:35,560
,на самом деле, использовал алгоритм обратного распространения, вполне

20
00:00:36,080 --> 00:00:37,310
успешно много лет и

21
00:00:37,530 --> 00:00:39,130
даже сегодня, я иногда чувствую

22
00:00:39,510 --> 00:00:40,320
что не полностью

23
00:00:40,430 --> 00:00:41,790
понимаю,

24
00:00:42,130 --> 00:00:43,580
как лучше показать, что делает

25
00:00:43,830 --> 00:00:45,980
алгоритм обратного распространения. [?]

26
00:00:46,740 --> 00:00:47,850
Для тех из вас, что выполняет

27
00:00:48,250 --> 00:00:49,920
практические задания по программированию это будет

28
00:00:50,480 --> 00:00:51,970
по крайней мере, одним из

29
00:00:52,280 --> 00:00:53,710
нескольких различных шагов

30
00:00:53,810 --> 00:00:54,910
реализации алгоритма обратного распространения

31
00:00:55,200 --> 00:00:56,860
для использования в своих задачах.

32
00:00:57,910 --> 00:00:58,850
И что я хочу сделать

33
00:00:58,970 --> 00:01:00,170
в этом видеоролике, так это посмотреть

34
00:01:00,460 --> 00:01:01,750
чуть ближе на эти

35
00:01:02,190 --> 00:01:03,640
самые шаги алгоритма обратного распространения

36
00:01:04,160 --> 00:01:05,620
и попытаюсь дать вам немного

37
00:01:05,840 --> 00:01:07,450
большее понимание о том

38
00:01:07,930 --> 00:01:09,080
что за шаги совершаются в алгоритме обратного распространения

39
00:01:09,250 --> 00:01:10,590
и надеюсь убедить вас

40
00:01:10,790 --> 00:01:12,530
что, знаете ли, это достаточно неплохой алгоритм.

41
00:01:14,680 --> 00:01:16,240
Если даже после просмотра этого ролика

42
00:01:16,380 --> 00:01:18,000
алгоритм обратного распространения все еще кажется

43
00:01:18,760 --> 00:01:19,920
очень черным ящиком и вам кажется

44
00:01:20,160 --> 00:01:21,600
что в нем слишком много сложных шагов,

45
00:01:22,150 --> 00:01:23,230
малость магических для вас,

46
00:01:23,330 --> 00:01:24,740
это, на самом деле, нормально.

47
00:01:24,930 --> 00:01:26,760
И, кроме того,

48
00:01:27,050 --> 00:01:27,840
я использовал алгоритм обратного распространения ошибки

49
00:01:28,070 --> 00:01:31,590
много лет, иногда это сложный для понимания алгоритм.

50
00:01:32,310 --> 00:01:34,140
Но к счастью этот видеоролик должен немного помочь

51
00:01:36,410 --> 00:01:37,970
Чтобы лучше понять алгоритм обратного

52
00:01:38,190 --> 00:01:39,660
распространения, давайте

53
00:01:40,100 --> 00:01:42,290
посмотрим чуть подробнее что же делает алгоритм прямого распространения.

54
00:01:43,170 --> 00:01:44,420
Это нейронная сеть с двумя

55
00:01:44,770 --> 00:01:46,070
входами, которые не

56
00:01:46,390 --> 00:01:48,480
считая смещения, и

57
00:01:48,700 --> 00:01:50,300
и два скрытых нейрона в этом

58
00:01:50,500 --> 00:01:51,590
слое и два скрытых нейрона

59
00:01:52,030 --> 00:01:53,490
в следующем слое, и затем

60
00:01:53,640 --> 00:01:55,090
в завершение, выходной нейрон.

61
00:01:55,520 --> 00:01:57,800
И еще раз, они считаются 2,

62
00:01:57,920 --> 00:02:00,240
2, 2 не считая этих блоков смещения, сверху.

63
00:02:01,520 --> 00:02:03,170
Для того, чтобы проиллюстрировать прямое

64
00:02:03,430 --> 00:02:04,570
распространение, я собираюсь

65
00:02:04,690 --> 00:02:06,080
изобразить эту сеть немного по-другому.

66
00:02:08,040 --> 00:02:09,180
И, в частности, я собираюсь

67
00:02:09,370 --> 00:02:10,840
нарисовать эту нейронную сеть с

68
00:02:10,930 --> 00:02:12,620
узлами в виде этих, очень

69
00:02:12,920 --> 00:02:15,010
жирных эллипсов, так, чтобы я мог уместить в них текст.

70
00:02:15,840 --> 00:02:16,800
Когда мы производим прямое распространение,

71
00:02:17,600 --> 00:02:18,900
мы можем иметь некоторый конкретный образец данных

72
00:02:19,760 --> 00:02:21,190
скажем, некоторый (x(i),y(i))

73
00:02:21,610 --> 00:02:22,990
при этом, это будет

74
00:02:23,080 --> 00:02:24,550
тот x(i), который мы

75
00:02:24,740 --> 00:02:26,460
подаем на входной слой, таким образом,

76
00:02:29,110 --> 00:02:30,290
x(i) 1 и  x(i)2  это

77
00:02:30,440 --> 00:02:31,360
значения, которые мы устанавливаем на входном

78
00:02:31,510 --> 00:02:32,870
слое и затем мы

79
00:02:33,010 --> 00:02:34,350
распространяем их на

80
00:02:34,650 --> 00:02:36,210
первый скрытый слой нейронов,

81
00:02:36,360 --> 00:02:38,070
что мы делаем, так это вычисляем z(2)1 и

82
00:02:39,370 --> 00:02:42,900
z(2)2, они являются

83
00:02:43,770 --> 00:02:45,010
взвешенными суммами входов

84
00:02:45,260 --> 00:02:47,000
входного слоя и затем

85
00:02:47,230 --> 00:02:48,680
мы применяем сигмоид

86
00:02:48,940 --> 00:02:50,670
логистическую функцию и

87
00:02:51,940 --> 00:02:53,630
сигмоидная функция активации, примененная

88
00:02:54,050 --> 00:02:55,670
к значению z, дает нам

89
00:02:55,960 --> 00:02:57,520
эти значения активации.