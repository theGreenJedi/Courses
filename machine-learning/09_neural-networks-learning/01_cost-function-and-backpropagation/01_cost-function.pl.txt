W tym filmie zdefiniujemy tzw. funkcję kosztu. Pozwoli nam to dopasować najlepszą linię prostą do naszych danych. Przy regresji liniowej, mamy zbiór uczący, taki jak ten tu. Pamiętaj: m oznacza liczbę elementów zbioru uczącego - np. tutaj być może m = 47. Nasza hipoteza z kolei, której używamy do robienia prognoz, jest funkcją liniową. Wprowadźmy trochę terminologii: theta 0 oraz theta 1 są tzw. parametrami modelu. W tym filmie porozmawiamy o tym, jak wybrać wartości tych dwóch parametrów: theta 0 oraz theta 1. Wybierając różne wartości theta 0 oraz theta 1, otrzymujemy różne hipotezy, różne funkcje hipotezy. Wiem, że niektórzy z Was są już obeznani z tym, co pokażę na tym slajdzie, ale, dla przypomnienia, podam parę przykładów. Jeśli theta 0 równa się 1,5, a theta 1 równa się 0, wtedy funkcja hipotezy wygląda tak. Ponieważ funkcja hipotezy będzie równa h(x) równe 1,5 plus 0 razy x, co daje nam wartość stałą - 1,5. Jeśli theta0 = 0, theta1 = 0.5, hipoteza będzie wyglądać tak - powinna przechodzić przez punkt (2,1), i tak otrzymamy h(x). Tak naprawdę jest to h_theta(x), ale czasem będę pomijał theta, żeby było krócej. Tak więc h(x) będzie równe 0,5 razy x, co wygląda ten sposób. Wreszcie, jeśli theta0 równa się 1, a theta1 równa się 0,5, wtedy hipoteza będzie wyglądać tak: Zobaczmy: powinna przechodzić przez punkt (2,2). W ten sposób. Teraz to jest moje nowe h(x), czy też h indeks dolny theta (x). Pamiętaj: powiedziałem, że jest to h_theta(x), jednak będę czasem pisał skrótowo po prostu h(x). W przypadku regresji liniowej mamy zbiór uczący, np. taki, jak narysowałem tutaj. Chcemy znaleźć wartości dla parametrów theta0 i theta1 tak, żeby linia prosta dobrze pasowała do danych, tak jak np. ta linia. Tylko skąd wziąć wartości theta0 i theta1, które odpowiadają dobremu dopasowaniu do danych? Idea jest taka, żeby wybrać parametry theta0 i theta1 tak, aby h(x), czyli wartość przewidywana dla danego x, była przynajmniej zbliżona do wartości y w przypadku elementów naszego zbioru uczącego, naszych przykładów uczących. Tak więc w naszym zbiorze danych mamy przykłady, zawierające x - wielkość domu, a także znamy cenę, po jakiej został sprzedany. Tak więc spróbujmy wybrać wartości parametrów tak, abyśmy, mając dane wartości x ze zbioru uczącego, byli w stanie przewidzieć dość dokładnie wartości y. Sformalizujmy to trochę. Tak więc w regresji liniowej chcemy rozwiązać problem minimalizacji. Napiszę więc: "minimalizacja względem theta0 i theta1". I chcę, żeby to wyrażenie przyjęło niską wartość, prawda? Chcę, żeby różnica h(x) - y była niewielka. Mogę spróbować w tym celu minimalizować kwadrat różnicy wartości wyjściowej hipotezy oraz rzeczywistej ceny domu. OK? A teraz trochę więcej szczegółów. Pamiętasz, że używałem notacji (x(i), y(i)) na oznaczenie i-tego elementu zbioru uczącego? Tak więc teraz chcę zsumować po wszystkich elementach zbioru, tj. i = 1 do m kwadrat różnicy między prognozą mojej hipotezy, gdy na wejściu jest i-ty dom, oraz rzeczywistą ceną, po której sprzedano i-ty dom, i chcę zminimalizować tę sumę względem całego zbioru uczącego, sumę od i=1 do m, tej różnicy kwadratu błędu; kwadratu różnicy między przewidzianą ceną domu, a ceną, za którą naprawdę go sprzedano. Małe przypomnienie: w tym miejscu m oznacza liczbę elementów zbioru uczącego, ok? Tak więc m w tym miejscu jest liczbą przykładów treningowych. Ten krzyżyk jest skrótem i oznacza liczbę elementów zbioru uczącego, ok? Oprócz tego, aby późniejsze obliczenia były łatwiejsze, pomnożę całość przez 1/m, aby otrzymać wartość średnią i dodamy do tego jeszcze 2, a więc ostatecznie razy 1/2m. Ta dwójka z przodu sprawi, że łatwiej będzie obliczyć to wyrażenie, a minimalizacja połowy jakiejś wartości powinna dać takie same wartości theta0 i theta1, co minimalizacja całego wyrażenia. Tak dla pewności: to równanie jest jasne, prawda? To wyrażenie tutaj: h indeks dolny theta (x), znaczy to samo, co zawsze, prawda? To się równa to plus theta1 razy xi. A ten zapis, minimalizacja względem theta0 i theta1, oznacza: "Znajdź mi takie wartości theta0 i theta1, które sprawią, że to wyrażenie" "przyjmie minimalną wartość.", a to wyrażenie zależy od theta0 i theta1, prawda? Tak więc podsumowując: Zamykamy ten problem w ten sposób: "Znajdź mi wartości theta1 i theta0 takie, żeby" "ich średnia, a dokładniej 1/2m" "razy suma kwadratów błędów - różnic między prognozą dla wymiarów domów, pochodzących ze zbioru uczącego," "a rzeczywistą ceną domów o danej wielkości, przyjęła jak najmniejszą wartość." Tak więc to będzie moja funkcja celowa dla regresji liniowej. Przepiszmy to teraz na czysto: mam tutaj funkcję (zwykle zwaną funkcją kosztu), która jest opisana tym równaniem. I chcę zminimalizować względem theta0 i theta1 moją funkcję J(theta0, theta1). Ok, napiszmy to wyraźnie: to jest moja funkcja kosztu. Funkcja ta jest również zwana kwadratową funkcją błędu. Czasami nazywa się ją funkcją kwadratu błędu. Czemu podnosimy błąd do kwadratu? Okazuje się, że taka funkcja kosztu jest dobrym wyborem i działa dobrze w przypadku większości zagadnień regresji. Są też inne funkcje, które mogą dobrze działać, jednak funkcja błędu kwadratowego jest prawdopodobnie używana najczęściej, gdy stosujemy regresję. W dalszej części kursu będziemy rozmawiać też o alternatywnych funkcjach kosztu, jednak taki wybór funkcji powinien być senswony w przypadku większości problemów wymagających regresji. Okay. A więc to jest funkcja kosztu. Jak na razie widzieliśmy jedynie definicję matematyczną funkcji kosztu. Jeśli ta funkcja - J(theta0, theta1) - jeśli wydaje Ci się jakąś abstrakcją i nie masz jeszcze wyczucia, co ona robi, w następnych paru filmach mam zamiar wytłumaczyć dokładniej, co robi funkcja J. Spróbuję także wyrobić Ci lepszą intuicję odnośnie tego, co ona oblicza i dlaczego chcemy z tego skorzystać.