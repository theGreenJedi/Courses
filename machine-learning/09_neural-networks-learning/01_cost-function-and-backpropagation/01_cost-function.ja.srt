1
00:00:00,270 --> 00:00:01,380
ニューラルネットワークは

2
00:00:01,450 --> 00:00:03,610
こんにち使える中でもっとも強力な学習アルゴリズムの一つだ。

3
00:00:04,310 --> 00:00:05,490
このビデオと

4
00:00:05,590 --> 00:00:06,690
その後の一連のビデオで

5
00:00:06,760 --> 00:00:08,030
所与のトレーニングセットに対して

6
00:00:08,380 --> 00:00:09,920
ニューラルネットワークのパラメータを

7
00:00:10,630 --> 00:00:12,530
フィッティングする為の学習アルゴリズムの話をしていく。

8
00:00:13,460 --> 00:00:14,840
ほとんどの学習アルゴリズムの議論と同様、

9
00:00:15,020 --> 00:00:16,300
ニューラルネットワークのパラメータのフィッティングでも

10
00:00:16,450 --> 00:00:17,860
コスト関数から

11
00:00:17,960 --> 00:00:20,510
始める事にする。

12
00:00:21,170 --> 00:00:22,650
ニューラルネットワークの分類問題への応用に

13
00:00:23,270 --> 00:00:24,790
私はフォーカスしたいと

14
00:00:25,060 --> 00:00:26,590
思う。

15
00:00:27,660 --> 00:00:28,860
左に描いてあるような

16
00:00:29,120 --> 00:00:31,300
ネットワークがあったとする。

17
00:00:31,530 --> 00:00:32,510
そしてこんな感じのトレーニングセット、

18
00:00:32,710 --> 00:00:33,850
xiとyiのペアがm個あるようなのが

19
00:00:33,980 --> 00:00:36,550
あったとする。

20
00:00:37,920 --> 00:00:39,040
大文字のLを

21
00:00:39,450 --> 00:00:40,640
ネットワークに存在する

22
00:00:40,790 --> 00:00:42,460
レイヤーの総数を表すのに使う。

23
00:00:43,330 --> 00:00:44,550
例えば左に見える

24
00:00:44,810 --> 00:00:45,720
ネットワークの場合、

25
00:00:46,370 --> 00:00:47,920
大文字のLはイコール4だ。

26
00:00:48,020 --> 00:00:48,910
そしてsの下付き添字lで

27
00:00:49,180 --> 00:00:50,740
ユニットの総数を

28
00:00:51,260 --> 00:00:52,540
表す事にする。

29
00:00:52,730 --> 00:00:54,490
それはネットワークのレイヤーlに存在するニューロンの総数のうち、

30
00:00:54,770 --> 00:00:57,180
バイアスユニットは含めない数だ。

31
00:00:57,900 --> 00:00:59,440
例えば、s1と言えば、

32
00:00:59,580 --> 00:01:01,280
入力レイヤーの事で

33
00:01:01,370 --> 00:01:03,330
イコール 3ユニット。

34
00:01:04,140 --> 00:01:05,970
s2はこの例だと5ユニット。

35
00:01:06,900 --> 00:01:08,670
そして出力レイヤのs4は

36
00:01:09,940 --> 00:01:12,820
それはsLとも等しい。というのは大文字のLは4だから。

37
00:01:12,990 --> 00:01:14,290
この例における出力レイヤは

38
00:01:14,450 --> 00:01:16,230
4ユニットとなっている。

39
00:01:17,630 --> 00:01:19,880
2つの種類の分類問題を扱っていく事になる。

40
00:01:20,430 --> 00:01:21,780
一つ目はバイナリ分類、

41
00:01:22,970 --> 00:01:25,550
それはyの取りうる値は0か1のどちらかだけの場合。

42
00:01:26,240 --> 00:01:28,540
この場合は出力ユニットは一つだけとなる。

43
00:01:29,140 --> 00:01:30,260
この上にあるニューラルネットワークの場合は

44
00:01:30,510 --> 00:01:32,430
4つの出力ユニットがあるが、

45
00:01:32,570 --> 00:01:33,960
バイナリ分類なら、

46
00:01:34,120 --> 00:01:35,810
出力ユニットは一つだけで、

47
00:01:36,720 --> 00:01:38,360
それはhのxを計算する。

48
00:01:40,310 --> 00:01:41,550
そしてそのニューラルネットワークの

49
00:01:41,630 --> 00:01:42,960
出力は、つまりhのxは

50
00:01:43,140 --> 00:01:45,580
実数となる。

51
00:01:46,900 --> 00:01:47,980
そしてこのケースでは、

52
00:01:48,360 --> 00:01:50,240
出力ユニットの総数sLは

53
00:01:50,480 --> 00:01:51,880
ところでLは最後のレイヤのインデックスなので

54
00:01:52,300 --> 00:01:53,970
というのはLはそのネットワークの

55
00:01:54,240 --> 00:01:55,630
レイヤーの総数だからだが、

56
00:01:56,570 --> 00:01:57,960
すると出力レイヤに存在する

57
00:01:58,110 --> 00:02:00,060
ユニットの総数は、1に等しい。

58
00:02:01,040 --> 00:02:02,430
この場合、あとでノーテーションを簡素化する為、

59
00:02:02,950 --> 00:02:05,340
K=1とも書く事にする。

60
00:02:05,460 --> 00:02:06,560
つまりKもまた、

61
00:02:06,770 --> 00:02:08,240
出力レイヤの

62
00:02:08,700 --> 00:02:10,780
ユニットの総数を表す訳だ。

63
00:02:11,410 --> 00:02:12,980
2つ目のタイプの分類問題は

64
00:02:13,280 --> 00:02:15,160
マルチクラスの分類問題だ。

65
00:02:15,780 --> 00:02:18,020
それは、K個の異なる分類がある問題。

66
00:02:19,160 --> 00:02:20,760
前の例だと

67
00:02:21,070 --> 00:02:22,530
yがこんな表現のがあったよね。

68
00:02:23,080 --> 00:02:24,900
これは4つのクラスがある場合だった。

69
00:02:25,160 --> 00:02:27,050
このタイプでは、大文字のK個だけの

70
00:02:27,340 --> 00:02:29,530
出力ユニットがあり、我らの仮説は

71
00:02:30,350 --> 00:02:33,720
K次元のベクトルを出力することになる。

72
00:02:34,980 --> 00:02:36,230
そして出力ユニットの総数は

73
00:02:36,760 --> 00:02:38,390
Kと等しくなり、

74
00:02:39,000 --> 00:02:40,020
このKは通常は、

75
00:02:40,370 --> 00:02:41,620
3以上なのがこのタイプとなる。

76
00:02:41,820 --> 00:02:42,960
何故なら、

77
00:02:43,980 --> 00:02:45,340
もし2クラスしか無い時は

78
00:02:45,710 --> 00:02:46,560
one vs all法を使う必要が

79
00:02:46,690 --> 00:02:48,330
無いからだ。

80
00:02:48,720 --> 00:02:49,640
one vs all法を使わなくてはいけないのは

81
00:02:49,970 --> 00:02:50,950
Kが3以上の

82
00:02:51,110 --> 00:02:52,460
時だけなので

83
00:02:52,740 --> 00:02:54,250
2つしかクラスが無い時は

84
00:02:54,470 --> 00:02:56,100
一つの出力ユニットしか

85
00:02:56,180 --> 00:02:57,670
必要としない。

86
00:02:58,250 --> 00:03:00,870
ではここで、ニューラルネットワークのコスト関数を定義しよう。

87
00:03:03,880 --> 00:03:05,130
ニューラルネットワークで使う

88
00:03:05,240 --> 00:03:06,530
コスト関数は、

89
00:03:06,680 --> 00:03:08,300
ロジスティック回帰で使った物を

90
00:03:08,360 --> 00:03:09,340
一般化した物だ。

91
00:03:09,510 --> 00:03:11,500
ロジスティック回帰では

92
00:03:12,100 --> 00:03:13,440
コスト関数Jのシータを

93
00:03:13,510 --> 00:03:14,490
最小化したのだった。

94
00:03:15,270 --> 00:03:16,550
そのJのシータとは-1/mの

95
00:03:16,770 --> 00:03:17,760
このコスト関数に

96
00:03:18,720 --> 00:03:20,570
この追加の正規化の項を

97
00:03:21,300 --> 00:03:22,660
足した物だった。正規化の項は

98
00:03:22,850 --> 00:03:24,020
jが1からnまでの和だった。

99
00:03:24,700 --> 00:03:26,190
何故ならバイアス項である

100
00:03:26,270 --> 00:03:29,760
シータ0は正規化しないからだった。

101
00:03:31,030 --> 00:03:32,590
ニューラルネットワーク向けには

102
00:03:32,910 --> 00:03:34,490
コスト関数はこれを一般化したものとなる。

103
00:03:35,650 --> 00:03:37,060
そこでは基本的には

104
00:03:37,530 --> 00:03:39,360
単に一つのロジスティック回帰の出力ユニットしか無いのではなく、

105
00:03:39,650 --> 00:03:41,650
その代わりにK個のユニットがある訳だ。

106
00:03:42,590 --> 00:03:43,520
これがそのコスト関数となる。

107
00:03:44,770 --> 00:03:46,300
ニューラルネットワークはRのKのベクトルを

108
00:03:46,720 --> 00:03:47,920
出力する。

109
00:03:48,170 --> 00:03:48,830
ここで Kは1となる事もある、

110
00:03:49,200 --> 00:03:50,350
その時はバイナリ分類問題という事。

111
00:03:51,380 --> 00:03:52,240
h(x)の添字iという記法で

112
00:03:53,300 --> 00:03:56,470
i番目の出力を示す。

113
00:03:57,440 --> 00:03:59,860
つまり、h(x)はK次元ベクトルなので

114
00:04:00,840 --> 00:04:02,590
この添字iは単に

115
00:04:02,960 --> 00:04:04,400
ニューラルネットワークからの出力のベクトルから

116
00:04:05,200 --> 00:04:07,510
i番目を選び取るだけだ。

117
00:04:08,900 --> 00:04:10,050
コスト関数のJのシータは

118
00:04:10,180 --> 00:04:11,580
今や以下のようになる。

119
00:04:11,760 --> 00:04:13,790
-1/mに

120
00:04:13,940 --> 00:04:14,850
ロジスティック回帰の時と

121
00:04:15,420 --> 00:04:16,780
似たような項の和と

122
00:04:16,960 --> 00:04:18,990
なっている。だが、この和が

123
00:04:19,300 --> 00:04:20,360
kが1からKまで

124
00:04:21,020 --> 00:04:22,490
というのがあるのが違いか。

125
00:04:22,600 --> 00:04:23,650
その和は基本的には

126
00:04:23,720 --> 00:04:25,580
K個の出力ユニットに対する和だ。

127
00:04:26,060 --> 00:04:28,290
もし出力ユニットが4つあるとすると、

128
00:04:29,400 --> 00:04:30,740
それはつまりニューラルネットワークの

129
00:04:30,850 --> 00:04:32,530
最後のレイヤーが4つの出力ユニットを持つという事だが、

130
00:04:32,860 --> 00:04:34,420
するとその時この和は

131
00:04:34,700 --> 00:04:35,680
これはkが1から4までの

132
00:04:35,900 --> 00:04:37,140
基本的にはロジスティック回帰のアルゴリズムの

133
00:04:38,050 --> 00:04:40,550
コスト関数なんだが、

134
00:04:42,070 --> 00:04:43,640
しかしそのコスト関数を

135
00:04:43,750 --> 00:04:45,570
4つの出力ユニット分を一つずつ

136
00:04:45,890 --> 00:04:47,120
足していく所が違う。

137
00:04:47,800 --> 00:04:48,970
そして気づいたかもしれないが、

138
00:04:49,380 --> 00:04:50,700
これは特に、

139
00:04:51,400 --> 00:04:53,530
yのkとhのkに適用される。

140
00:04:53,740 --> 00:04:55,040
何故なら、基本的には

141
00:04:55,500 --> 00:04:57,020
我らはk番目の出力ユニットをとり、

142
00:04:57,780 --> 00:04:59,590
それをyのkと比較しているから。

143
00:04:59,810 --> 00:05:02,020
それはつまり、

144
00:05:02,210 --> 00:05:03,260
これらのベクトルの一つの

145
00:05:03,740 --> 00:05:05,110
コストがどうなっているべきか、という事。

146
00:05:06,280 --> 00:05:08,060
そして最後に、この二番目の項は

147
00:05:08,360 --> 00:05:09,490
正規化の項、

148
00:05:10,440 --> 00:05:12,970
これはロジスティック回帰と似てるね。

149
00:05:14,080 --> 00:05:15,640
この和の項はほんとうに難しそうに見えるね。

150
00:05:15,850 --> 00:05:17,370
でも実際にやってるのは

151
00:05:17,840 --> 00:05:19,460
これらの項、シータのijlの和を

152
00:05:19,950 --> 00:05:21,670
全てのijlの値に渡って

153
00:05:21,860 --> 00:05:23,340
取るだけだ。

154
00:05:23,410 --> 00:05:24,830
ただしバイアス項に

155
00:05:25,010 --> 00:05:26,340
対応する値は

156
00:05:26,710 --> 00:05:28,210
ロジスティック回帰の時同様

157
00:05:28,900 --> 00:05:30,000
除く。

158
00:05:30,900 --> 00:05:32,080
具体的には、i=0に

159
00:05:32,240 --> 00:05:33,590
対応する項は

160
00:05:34,300 --> 00:05:36,290
足さない。

161
00:05:36,780 --> 00:05:38,310
それはというと、

162
00:05:38,920 --> 00:05:40,010
ニューロンの活性化を計算してるとき

163
00:05:40,590 --> 00:05:41,930
こういう感じの項がある

164
00:05:42,280 --> 00:05:43,630
Θ(セータ) i0

165
00:05:43,810 --> 00:05:47,860
プラス セータのi1 x1 プラス、、、

166
00:05:48,160 --> 00:05:50,410
などなど。

167
00:05:50,520 --> 00:05:51,780
これが最初の隠れレイヤーだとすると

168
00:05:52,020 --> 00:05:53,310
ここに付くのは

169
00:05:53,490 --> 00:05:54,420
2となる。

170
00:05:55,250 --> 00:05:56,800
つまり、ここにある

171
00:05:57,230 --> 00:05:58,730
0に対応する値は

172
00:05:58,730 --> 00:06:00,110
x0とかa0の

173
00:06:00,260 --> 00:06:01,460
係数となっている、

174
00:06:02,210 --> 00:06:02,950
つまり、これは

175
00:06:03,120 --> 00:06:04,810
バイアスユニットの係数って事。

176
00:06:04,980 --> 00:06:06,020
そしてロジスティック回帰から

177
00:06:06,130 --> 00:06:07,680
類推出来るように、

178
00:06:07,890 --> 00:06:09,090
正規化の項では

179
00:06:09,160 --> 00:06:11,050
それらの項は足し合わせない。

180
00:06:11,160 --> 00:06:13,470
何故ならそれらは正規化したくないから。

181
00:06:13,670 --> 00:06:15,140
だから値0は含めていない。

182
00:06:15,360 --> 00:06:16,530
だがこれは単なる一つの慣習に過ぎず、

183
00:06:17,670 --> 00:06:18,670
別にiを0からslまで

184
00:06:18,840 --> 00:06:20,960
足し合わせたとしても、

185
00:06:21,200 --> 00:06:22,810
大差なく、

186
00:06:23,160 --> 00:06:24,720
だいたいうまく行く。

187
00:06:25,530 --> 00:06:26,760
だがこっちの慣習、

188
00:06:27,500 --> 00:06:28,790
つまりバイアス項は

189
00:06:29,070 --> 00:06:30,320
正規化しない流派の方がちょっとだけ普及してると思う。

190
00:06:32,960 --> 00:06:34,200
以上がネットワークをフィッティングするのに使用する

191
00:06:34,690 --> 00:06:36,270
コスト関数です。

192
00:06:36,790 --> 00:06:38,130
次のビデオでは、

193
00:06:38,480 --> 00:06:40,270
コスト関数を最適化する

194
00:06:40,570 --> 00:06:42,530
アルゴリズムに入っていきます。