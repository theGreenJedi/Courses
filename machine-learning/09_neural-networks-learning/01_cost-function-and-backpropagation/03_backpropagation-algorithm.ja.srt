1
00:00:00,090 --> 00:00:01,798
前回のビデオではニューラルネットワークの

2
00:00:01,857 --> 00:00:03,868
コスト関数について話した。

3
00:00:04,139 --> 00:00:07,079
このビデオでは、コスト関数の最小化を試みる

4
00:00:07,200 --> 00:00:09,062
アルゴリズムの話に入っていこう。

5
00:00:09,240 --> 00:00:12,735
特に、バックプロパゲーション（後に伝播）アルゴリズムについて話す。

6
00:00:13,834 --> 00:00:15,380
これは前回のビデオで書き下した

7
00:00:15,520 --> 00:00:17,905
コスト関数だ。

8
00:00:17,972 --> 00:00:19,438
今回やりたいのは

9
00:00:19,484 --> 00:00:21,161
Jのシータを最小化するような

10
00:00:21,246 --> 00:00:23,440
シータを探したい。

11
00:00:23,530 --> 00:00:25,782
最急降下法なりより進んだアルゴリズムなりを

12
00:00:25,832 --> 00:00:28,625
使うためには

13
00:00:28,675 --> 00:00:30,206
この入力であるパラメータ、シータをとり、

14
00:00:30,249 --> 00:00:31,598
Jのシータと

15
00:00:31,645 --> 00:00:33,487
これらの偏微分の項を

16
00:00:33,540 --> 00:00:34,965
計算するコードを

17
00:00:35,014 --> 00:00:37,364
書く必要がある。

18
00:00:37,425 --> 00:00:38,763
ニューラルネットワークのパラメータとは

19
00:00:38,790 --> 00:00:40,710
こんな物だったーー

20
00:00:40,760 --> 00:00:43,435
シータの上付き添字lに下付き添字ij。

21
00:00:43,492 --> 00:00:44,868
これは実数だった。

22
00:00:44,930 --> 00:00:47,185
つまりこれらが、計算しなくてはならない

23
00:00:47,249 --> 00:00:48,869
偏微分の項だ。

24
00:00:48,900 --> 00:00:50,077
コスト関数であるJのシータを

25
00:00:50,115 --> 00:00:51,840
計算するには、

26
00:00:51,883 --> 00:00:53,986
この上の式をただ使うだけで良い。

27
00:00:54,042 --> 00:00:55,617
だからこのビデオの残りのほとんどで

28
00:00:55,655 --> 00:00:56,850
フォーカスしたいのは、

29
00:00:56,897 --> 00:00:58,595
どうやってこれらの

30
00:00:58,636 --> 00:00:59,952
偏微分の項を

31
00:00:59,989 --> 00:01:01,994
計算出来るか、という方。

32
00:01:02,031 --> 00:01:03,812
一つしかトレーニング手本が

33
00:01:03,858 --> 00:01:05,512
無い場合から

34
00:01:05,556 --> 00:01:06,839
始めよう。

35
00:01:06,872 --> 00:01:09,385
トレーニングセットの全体が

36
00:01:09,432 --> 00:01:11,301
たった一つの手本から

37
00:01:11,351 --> 00:01:14,006
構成されている場合を思い浮かべてくれ。
ようするにそれはxとyのペアという事。

38
00:01:14,049 --> 00:01:15,591
x1とかy1とは書かずに

39
00:01:15,629 --> 00:01:16,375
ただこう書く事にする。

40
00:01:16,410 --> 00:01:17,665
一つのトレーニングの手本を

41
00:01:17,718 --> 00:01:19,980
x、yと書いて、この一つのトレーニング手本に対して

42
00:01:20,031 --> 00:01:21,423
どんな計算を行うのか

43
00:01:21,462 --> 00:01:24,332
順番に見ていこう。

44
00:01:25,754 --> 00:01:27,129
最初にやる事は

45
00:01:27,167 --> 00:01:29,175
フォワードプロパゲーション（前方に伝播）を

46
00:01:29,212 --> 00:01:31,773
与えられた入力に対して

47
00:01:31,813 --> 00:01:34,238
仮説が実際に何を出力するかを計算する為、適用する。

48
00:01:34,272 --> 00:01:36,734
具体的には、a(1)と呼ばれている物は

49
00:01:36,769 --> 00:01:39,025
この最初のレイヤーの

50
00:01:39,071 --> 00:01:41,541
アクティベーションの値、つまり入力の値だったのを思い出そう。

51
00:01:41,600 --> 00:01:43,452
だからそれをxに設定し、

52
00:01:43,505 --> 00:01:45,389
次にz(2)=シータ(1) a(1)を

53
00:01:45,435 --> 00:01:47,506
計算して、

54
00:01:47,552 --> 00:01:49,919
そこから a(2)イコールg、つまりsigmoid関数を

55
00:01:49,980 --> 00:01:52,250
z(2)に適用する。

56
00:01:52,310 --> 00:01:53,753
この結果が最初の中間レイヤーの

57
00:01:53,800 --> 00:01:56,115
アクティベーションとなる。

58
00:01:56,162 --> 00:01:58,208
つまりネットワークのレイヤー2に対応する。

59
00:01:58,241 --> 00:02:00,649
また、これらのバイアス項も足す。

60
00:02:01,315 --> 00:02:03,132
そこから伝播させる為に、

61
00:02:03,176 --> 00:02:04,966
もう2ステップさらに適用して

62
00:02:05,013 --> 00:02:08,328
a(3)と、、、a(4)を計算する。

63
00:02:08,360 --> 00:02:11,458
これはh(x)の

64
00:02:11,505 --> 00:02:14,089
出力でもある。

65
00:02:14,711 --> 00:02:18,103
以上がフォワードプロパゲーションの

66
00:02:18,145 --> 00:02:19,228
ベクトル化された実装だ。

67
00:02:19,276 --> 00:02:20,888
以上でまた、

68
00:02:20,938 --> 00:02:22,280
ニューラルネットワーク内の

69
00:02:22,345 --> 00:02:24,056
全てのアクティベーション値も

70
00:02:24,110 --> 00:02:25,948
計算出来る。

71
00:02:27,934 --> 00:02:29,608
次に、微分を計算する為に

72
00:02:29,650 --> 00:02:30,967
バックプロパゲーション（後方に伝播）と呼ばれる

73
00:02:31,026 --> 00:02:33,589
アルゴリズムを使っていく。

74
00:02:34,904 --> 00:02:37,765
バックプロパゲーションのアルゴリズムは、直感としては

75
00:02:37,807 --> 00:02:38,430
各ノードごとに

76
00:02:38,430 --> 00:02:41,065
デルタ項を計算していく。

77
00:02:41,126 --> 00:02:43,642
デルタには上付き添字のlに下付き添字のjがつく。

78
00:02:43,676 --> 00:02:45,130
それはある意味で

79
00:02:45,171 --> 00:02:46,310
レイヤーlにあるノードjの

80
00:02:46,361 --> 00:02:48,511
誤差を表す物だ。

81
00:02:48,552 --> 00:02:49,682
もういちど思い出すと

82
00:02:49,716 --> 00:02:52,313
aの添字のl、下の添え字のjは

83
00:02:52,355 --> 00:02:54,138
レイヤーlにあるj番目の

84
00:02:54,185 --> 00:02:56,182
アクティベーションだった。

85
00:02:56,224 --> 00:02:58,001
そして、このデルタの項は

86
00:02:58,045 --> 00:02:59,037
ある意味で、

87
00:02:59,082 --> 00:03:00,978
そのノードのアクティベーションの

88
00:03:01,012 --> 00:03:03,618
誤差を捕捉する、と考えられる。

89
00:03:03,650 --> 00:03:05,798
つまり、そのノードのアクティベーションの

90
00:03:05,823 --> 00:03:07,975
期待される値からどのくらいずれているかだ。

91
00:03:08,047 --> 00:03:09,670
具体的に見てみると、

92
00:03:10,270 --> 00:03:11,100
右のニューラルネットワークは

93
00:03:11,360 --> 00:03:12,700
４つのレイヤーを持ってる。

94
00:03:13,440 --> 00:03:15,710
だから、大文字のLは4。

95
00:03:16,060 --> 00:03:17,120
それぞれの出力のユニットに対し、いまこのデルタ項を計算しようとしている。

96
00:03:17,400 --> 00:03:19,130
で、この4つ目のレイヤーの、この j のユニットのデルタは以下に等しい、

97
00:03:23,380 --> 00:03:24,490
このユニットのアクティベーションの値から

98
00:03:24,720 --> 00:03:26,350
引くことの、

99
00:03:26,490 --> 00:03:28,650
この、ぼくたちのトレーニングセットの中の、実際に観測された値。

100
00:03:29,900 --> 00:03:32,420
そして、ここのこの項は、

101
00:03:32,580 --> 00:03:34,510
h(x)の下添字jとも

102
00:03:34,710 --> 00:03:38,040
書ける。でしょ？

103
00:03:38,330 --> 00:03:39,640
つまりこのデルタ項は

104
00:03:39,930 --> 00:03:40,900
単に我らの仮説の

105
00:03:41,290 --> 00:03:43,200
出力した値と、

106
00:03:43,370 --> 00:03:44,870
トレーニングセットでのyの値との

107
00:03:45,570 --> 00:03:46,900
差分でしか無い。

108
00:03:47,060 --> 00:03:48,610
ここでyの下添字jは

109
00:03:48,750 --> 00:03:49,910
トレーニング手本の

110
00:03:50,090 --> 00:03:53,340
ベクトルの値yのj番目の要素って意味だった。

111
00:03:56,200 --> 00:03:57,790
ところで、

112
00:03:57,970 --> 00:04:00,460
もしデルタ、a、yを

113
00:04:01,000 --> 00:04:02,350
ベクトルだとしても

114
00:04:02,520 --> 00:04:03,760
これはやはり成立し、

115
00:04:04,030 --> 00:04:05,890
これはベクトル化された実装となる。

116
00:04:06,010 --> 00:04:07,310
それは単に

117
00:04:07,690 --> 00:04:09,840
デルタ4に

118
00:04:10,700 --> 00:04:14,330
a4-yをセットする。

119
00:04:14,560 --> 00:04:15,820
ここで、デルタ4、a4、yは

120
00:04:16,540 --> 00:04:18,080
それぞれ

121
00:04:18,180 --> 00:04:19,860
ベクトルで

122
00:04:20,640 --> 00:04:22,040
その次元は

123
00:04:22,250 --> 00:04:24,150
ネットワークの出力ユニットの数に等しい。

124
00:04:25,210 --> 00:04:26,880
これで我らは今や

125
00:04:27,320 --> 00:04:28,670
ネットワークの誤差項であるところの

126
00:04:29,020 --> 00:04:30,170
デルタ4を計算した。

127
00:04:31,440 --> 00:04:32,950
次にやる事はネットワーク内の

128
00:04:33,620 --> 00:04:36,280
より手前のレイヤーのデルタ項を計算する事だ。

129
00:04:37,210 --> 00:04:38,690
これがデルタ3を計算する為の式だ。

130
00:04:39,010 --> 00:04:39,830
デルタ3 イコール

131
00:04:40,310 --> 00:04:42,050
シータ3の転置 掛ける デルタ4。

132
00:04:42,560 --> 00:04:44,190
そしてこの ドット掛け算 は、

133
00:04:44,390 --> 00:04:46,390
MATLABなんかにある、

134
00:04:47,580 --> 00:04:48,380
要素毎の積。

135
00:04:49,160 --> 00:04:50,760
シータ3転置 デルタ4、

136
00:04:51,020 --> 00:04:52,860
これはベクトル。

137
00:04:53,480 --> 00:04:55,080
g'(z3)は、これもベクトル。

138
00:04:55,800 --> 00:04:57,370
そしてこの ドット掛け算は

139
00:04:57,530 --> 00:04:59,670
これら2つのベクトルを要素ごとに掛け合わせた物。

140
00:05:01,460 --> 00:05:02,650
この項、g'(z3)は

141
00:05:02,740 --> 00:05:04,560
正式にはアクティベーション関数のgを

142
00:05:04,950 --> 00:05:06,420
入力値がz3の所で

143
00:05:06,720 --> 00:05:08,740
微分した値。

144
00:05:08,890 --> 00:05:10,620
微分した値。

145
00:05:10,760 --> 00:05:12,620
もし解析学を知ってるなら

146
00:05:12,710 --> 00:05:13,470
自分自身で実行してみて

147
00:05:13,850 --> 00:05:16,100
私が得たのと同じ答えになる事を確かめられるはずだが、

148
00:05:16,860 --> 00:05:19,690
現実的にはようするにどういう意味か、答えを教えちゃおう。

149
00:05:20,000 --> 00:05:21,260
実際にこのg'を計算する為にやるべき事は

150
00:05:21,460 --> 00:05:23,310
これらの微分項は

151
00:05:23,510 --> 00:05:25,660
a3 ドット掛ける 1-a3

152
00:05:26,010 --> 00:05:27,900
で、a3はアクティベーションの

153
00:05:28,160 --> 00:05:29,420
ベクトルだ。

154
00:05:30,150 --> 00:05:31,440
この1はベクトルで全要素が1を意味し、

155
00:05:31,600 --> 00:05:33,240
このa3も

156
00:05:34,020 --> 00:05:35,970
そのレイヤーのアクティベーションの値の

157
00:05:36,290 --> 00:05:38,850
ベクトルだ。

158
00:05:39,170 --> 00:05:40,210
次に似たような式を

159
00:05:40,540 --> 00:05:42,850
デルタ2にも適用する。

160
00:05:43,220 --> 00:05:45,230
それも似たような公式で

161
00:05:45,670 --> 00:05:47,410
計算出来る。

162
00:05:48,450 --> 00:05:49,950
今回はa2に

163
00:05:50,120 --> 00:05:53,850
なるだけ。

164
00:05:53,960 --> 00:05:55,020
ここで俺が本気出せば証明とかも出来るんだが

165
00:05:55,110 --> 00:05:56,400
気になるなら君が自分でやろう。

166
00:05:56,490 --> 00:05:57,520
解析学を知ってれば

167
00:05:58,240 --> 00:05:59,520
この式が数学的に

168
00:05:59,860 --> 00:06:02,010
アクティベーション関数であるところの

169
00:06:02,190 --> 00:06:03,570
gの微分と等しい事を示せるだろう。

170
00:06:04,040 --> 00:06:05,460
それがまさにg'の事だった。

171
00:06:05,910 --> 00:06:08,540
最後に、

172
00:06:09,270 --> 00:06:10,690
以上でおしまいで、デルタ1の項なんてのは

173
00:06:10,860 --> 00:06:13,650
存在しない。

174
00:06:13,720 --> 00:06:15,590
だって最初のレイヤーは

175
00:06:15,630 --> 00:06:16,940
入力レイヤに対応してるのだから、

176
00:06:17,000 --> 00:06:18,200
それって単にトレーニングセットで実際に観察される値なので

177
00:06:18,300 --> 00:06:20,380
それに関連した誤差も何も無い。

178
00:06:20,600 --> 00:06:22,080
ようするに、その値はまったく

179
00:06:22,120 --> 00:06:23,680
変更したいなんて思ってない訳だ。

180
00:06:24,320 --> 00:06:25,240
だからデルタ項は

181
00:06:25,510 --> 00:06:28,090
この例だとレイヤー2、3、4にしか無いって訳だ。

182
00:06:30,170 --> 00:06:32,120
バックプロパゲーションという名前は

183
00:06:32,170 --> 00:06:33,260
デルタ項を

184
00:06:33,350 --> 00:06:34,720
出力レイヤから

185
00:06:34,740 --> 00:06:36,190
計算しはじめて

186
00:06:36,370 --> 00:06:37,480
レイヤーを遡っていき

187
00:06:37,880 --> 00:06:39,670
隠れレイヤのデルタ項を

188
00:06:39,850 --> 00:06:41,050
計算していき、

189
00:06:41,180 --> 00:06:42,540
その次にさらにもう一歩戻ってデルタ2を計算して、、、

190
00:06:42,770 --> 00:06:44,070
という事からついた名前。

191
00:06:44,660 --> 00:06:46,060
つまりある意味で誤差を

192
00:06:46,280 --> 00:06:47,270
出力レイヤーからレイヤー3に、

193
00:06:47,650 --> 00:06:50,180
そこからさらに前にと伝播（プロパゲート）させていくから、バックプロパゲーションという名前な訳。

194
00:06:51,270 --> 00:06:53,120
最後に、微分はめちゃくちゃ大変だが

195
00:06:53,340 --> 00:06:56,510
凄い時間かかるが、

196
00:06:56,820 --> 00:06:58,100
単にこれらを一つ一つ

197
00:06:58,280 --> 00:07:00,130
計算していけば

198
00:07:00,680 --> 00:07:02,540
とても平凡なやり方で

199
00:07:02,810 --> 00:07:04,440
かなり面倒だけど数学的に示せるんだがーー

200
00:07:05,200 --> 00:07:07,410
本当に示せるんだけど、、、ほんとほんと。

201
00:07:07,560 --> 00:07:09,690
もし正規化の項を無視すれば

202
00:07:09,800 --> 00:07:11,080
我らが欲しい偏微分の項は

203
00:07:12,220 --> 00:07:14,650
正確にアクティベーションと

204
00:07:14,780 --> 00:07:17,690
これらのデルタ項で与えられる。

205
00:07:17,870 --> 00:07:20,630
これはラムダ無視してる、

206
00:07:20,780 --> 00:07:22,730
言い換えると正規化項を。

207
00:07:23,770 --> 00:07:24,630
ラムダが0の場合って

208
00:07:25,000 --> 00:07:25,170
事だ。

209
00:07:25,680 --> 00:07:27,130
この細かい事、正規化項については

210
00:07:27,470 --> 00:07:29,430
後で修正するが

211
00:07:29,620 --> 00:07:30,740
バックプロパゲーションを実行して

212
00:07:31,610 --> 00:07:32,820
これらのデルタ項を計算する事で

213
00:07:33,180 --> 00:07:34,240
全てのパラメータについて

214
00:07:34,530 --> 00:07:36,320
これらの偏微分の項を

215
00:07:36,380 --> 00:07:38,150
手早く計算出来る。

216
00:07:38,920 --> 00:07:40,020
たくさんの事が出てきたね。

217
00:07:40,570 --> 00:07:41,900
それらを全部合わせて

218
00:07:42,320 --> 00:07:43,660
パラメータに関する微分の計算を

219
00:07:44,120 --> 00:07:45,490
どう実装するのか

220
00:07:46,560 --> 00:07:48,590
議論しよう。

221
00:07:49,790 --> 00:07:50,770
しかもトレーニングセットが

222
00:07:51,000 --> 00:07:52,460
たくさんあるケースを、

223
00:07:52,830 --> 00:07:53,850
一つしか無い

224
00:07:54,100 --> 00:07:56,320
ケースではなく。
こんな風にやる。

225
00:07:57,290 --> 00:07:58,140
m個のトレーニング手本が

226
00:07:58,270 --> 00:07:59,750
あるとする。

227
00:07:59,900 --> 00:08:01,610
こんな感じ。

228
00:08:01,850 --> 00:08:02,600
最初にやる事は

229
00:08:03,220 --> 00:08:04,560
これらのデルタl下付き添字ijを、、、

230
00:08:05,100 --> 00:08:07,270
ところでこの三角の記号、

231
00:08:08,090 --> 00:08:09,990
これは実際はギリシャ文字のアルファベットで

232
00:08:10,310 --> 00:08:11,980
大文字のデルタだ。

233
00:08:12,050 --> 00:08:14,080
前のスライドにあったデルタは小文字のデルタ。

234
00:08:14,390 --> 00:08:16,810
この三角形は大文字のデルタ。

235
00:08:17,430 --> 00:08:18,490
これを全てのlijに対して

236
00:08:18,680 --> 00:08:21,930
0をセットする。

237
00:08:22,110 --> 00:08:23,850
最終的には、この大文字のデルタlijは

238
00:08:24,530 --> 00:08:25,830
偏微分の項、、、

239
00:08:26,860 --> 00:08:29,920
Jのシータの、シータlijに関する偏微分の項を

240
00:08:30,290 --> 00:08:31,570
計算するのに

241
00:08:32,380 --> 00:08:35,240
使う事になる。

242
00:08:35,430 --> 00:08:37,190
使う事になる。

243
00:08:39,040 --> 00:08:40,210
すぐに見る事となるが

244
00:08:40,480 --> 00:08:41,550
これらのデルタは

245
00:08:41,670 --> 00:08:43,700
これらの偏微分を計算する為に

246
00:08:43,950 --> 00:08:45,360
ちょっとずつ値を足していく為の

247
00:08:45,700 --> 00:08:47,130
アキュームレーターとして使う事になる。

248
00:08:49,570 --> 00:08:51,920
次にトレーニングセットをforループで回す。

249
00:08:52,150 --> 00:08:53,270
つまり、iが1からmまでのfor文、

250
00:08:53,610 --> 00:08:55,400
つまり

251
00:08:55,620 --> 00:08:57,270
i番目のイテレーションの時は

252
00:08:57,410 --> 00:08:59,180
トレーニング手本のxiとyiに関する計算をしてるという事。

253
00:09:00,480 --> 00:09:03,220
では、

254
00:09:03,720 --> 00:09:04,590
最初にやることは、

255
00:09:04,690 --> 00:09:06,120
a1、つまり入力レイヤーの

256
00:09:06,570 --> 00:09:07,830
アクティベーションに対し

257
00:09:08,190 --> 00:09:09,030
xiをセットする。

258
00:09:09,950 --> 00:09:11,800
それはi番目のトレーニング手本の

259
00:09:12,670 --> 00:09:15,070
入力を表すから。

260
00:09:15,340 --> 00:09:17,590
そして次に、フォワードプロパゲーションを適用して

261
00:09:17,730 --> 00:09:19,400
レイヤー2、レイヤー3と

262
00:09:19,790 --> 00:09:20,900
最後のレイヤーである所のレイヤーLまでを

263
00:09:21,170 --> 00:09:22,050
計算していく。

264
00:09:22,500 --> 00:09:25,190
次に、

265
00:09:25,570 --> 00:09:26,970
yiとラベルづけされた

266
00:09:27,280 --> 00:09:28,530
現在見ているトレーニング手本の

267
00:09:28,680 --> 00:09:29,870
出力を用いて、

268
00:09:30,340 --> 00:09:31,650
ここの出力の誤差を

269
00:09:31,950 --> 00:09:34,140
計算する。

270
00:09:34,480 --> 00:09:35,730
つまりデルタLは

271
00:09:35,880 --> 00:09:38,190
仮説の出力結果 引くことの

272
00:09:38,660 --> 00:09:39,870
ターゲットにしてる観測値。

273
00:09:41,840 --> 00:09:42,560
そしてバックプロパゲーションの

274
00:09:42,850 --> 00:09:44,550
アルゴリズムを用いて、

275
00:09:44,740 --> 00:09:46,020
デルタL-1、

276
00:09:46,220 --> 00:09:47,250
デルタL-2、、、と

277
00:09:47,350 --> 00:09:49,880
デルタ2までを計算する。

278
00:09:50,270 --> 00:09:51,380
ここでもデルタ1は求めない。

279
00:09:51,460 --> 00:09:54,380
何故なら入力レイヤに対応した誤差というのは想定しないから。

280
00:09:57,000 --> 00:09:58,160
そして最後に

281
00:09:58,340 --> 00:10:00,650
大文字のΔ(デルタ)の項を使う

282
00:10:01,190 --> 00:10:02,800
前の行で書いた

283
00:10:03,400 --> 00:10:05,670
偏微分の項を蓄積していく為に。

284
00:10:06,870 --> 00:10:07,870
ところで

285
00:10:07,960 --> 00:10:11,340
この式を眺めて見ると、これもベクトル化出来そうだ。

286
00:10:12,020 --> 00:10:13,040
具体的には

287
00:10:13,310 --> 00:10:14,860
デルタijを添字のijがインデックスの

288
00:10:15,000 --> 00:10:18,090
行列とみなすと、

289
00:10:19,220 --> 00:10:20,590
デルタlは

290
00:10:20,780 --> 00:10:22,040
行列としてこう書き直せる。

291
00:10:22,130 --> 00:10:24,100
デルタlは

292
00:10:24,350 --> 00:10:26,710
デルタl 足すことの

293
00:10:27,830 --> 00:10:29,370
小文字のデルタl+1に

294
00:10:29,640 --> 00:10:32,780
alの転置を掛けた物で更新する、と。

295
00:10:33,570 --> 00:10:35,380
以上がベクトル化したこれの実装で

296
00:10:35,520 --> 00:10:37,150
これは自動的に

297
00:10:37,590 --> 00:10:38,850
全てのi,jに対して値を更新してくれる。

298
00:10:39,010 --> 00:10:41,250
最後に、

299
00:10:41,500 --> 00:10:43,480
forループの中身を実行した後で

300
00:10:43,580 --> 00:10:45,350
forループの外に出るが、

301
00:10:46,330 --> 00:10:47,000
そこで以下を計算する。

302
00:10:47,440 --> 00:10:49,690
大文字のDを二つの場合、

303
00:10:50,020 --> 00:10:51,400
jが0の時とjが0以外の時に分けて

304
00:10:51,510 --> 00:10:52,750
以下のように

305
00:10:52,980 --> 00:10:54,890
計算していく。

306
00:10:56,080 --> 00:10:57,250
jが0の時とは

307
00:10:57,680 --> 00:10:58,730
バイアス項に対応するので

308
00:10:59,150 --> 00:11:00,030
だからこのケースでは

309
00:11:00,390 --> 00:11:01,320
追加の正規化項が

310
00:11:01,800 --> 00:11:03,320
無いのだ。

311
00:11:05,470 --> 00:11:06,850
最後に正式な証明は

312
00:11:07,180 --> 00:11:08,970
極めて複雑だが、

313
00:11:09,030 --> 00:11:10,410
頑張れば示せる事としては

314
00:11:10,640 --> 00:11:12,530
これらDの項をひとたび計算してしまえば、

315
00:11:13,510 --> 00:11:15,230
コスト関数の

316
00:11:15,640 --> 00:11:17,610
パラメータでの偏微分に

317
00:11:17,920 --> 00:11:19,230
ぴったりと一致する、

318
00:11:19,470 --> 00:11:20,890
という事。

319
00:11:21,040 --> 00:11:22,470
だからそれらを最急降下法にでも

320
00:11:22,610 --> 00:11:23,530
より高度な最適化アルゴリズムにでも

321
00:11:25,450 --> 00:11:25,450
使う事が出来る。

322
00:11:28,310 --> 00:11:29,360
というわけで。これが、バックプロパゲーション

323
00:11:29,990 --> 00:11:31,110
アルゴリズムだ。
それとニューラルネットワークの

324
00:11:31,470 --> 00:11:33,080
コスト関数の偏微分係数を

325
00:11:33,340 --> 00:11:34,710
計算する方法。

326
00:11:35,470 --> 00:11:36,330
これがまるで

327
00:11:36,470 --> 00:11:38,810
すごいたくさんのディティールとステップが数珠つなぎになった代物に見えることはよくわかってるんだけど。

328
00:11:39,460 --> 00:11:40,770
でも、プログラミングの

329
00:11:41,100 --> 00:11:43,010
宿題と、あとの

330
00:11:43,110 --> 00:11:44,580
ビデオで

331
00:11:44,720 --> 00:11:45,900
また、まとめ直すので

332
00:11:46,050 --> 00:11:46,830
すべてのピースの

333
00:11:47,260 --> 00:11:48,780
アルゴリズムを全部まとめて。

334
00:11:48,920 --> 00:11:50,550
きみがもしバックプロパゲーションを用いて

335
00:11:50,610 --> 00:11:51,760
ニューラルネットワークのコスト関数の

336
00:11:51,940 --> 00:11:53,460
パラメータによる偏微分係数を

337
00:11:53,890 --> 00:11:56,432
実装したくなったときに

338
00:11:56,574 --> 00:11:59,348
何しなくてはいけないかがはっきり分かるようにね。