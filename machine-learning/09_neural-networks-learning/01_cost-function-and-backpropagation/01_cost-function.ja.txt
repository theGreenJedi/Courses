ニューラルネットワークは こんにち使える中でもっとも強力な学習アルゴリズムの一つだ。 このビデオと その後の一連のビデオで 所与のトレーニングセットに対して ニューラルネットワークのパラメータを フィッティングする為の学習アルゴリズムの話をしていく。 ほとんどの学習アルゴリズムの議論と同様、 ニューラルネットワークのパラメータのフィッティングでも コスト関数から 始める事にする。 ニューラルネットワークの分類問題への応用に 私はフォーカスしたいと 思う。 左に描いてあるような ネットワークがあったとする。 そしてこんな感じのトレーニングセット、 xiとyiのペアがm個あるようなのが あったとする。 大文字のLを ネットワークに存在する レイヤーの総数を表すのに使う。 例えば左に見える ネットワークの場合、 大文字のLはイコール4だ。 そしてsの下付き添字lで ユニットの総数を 表す事にする。 それはネットワークのレイヤーlに存在するニューロンの総数のうち、 バイアスユニットは含めない数だ。 例えば、s1と言えば、 入力レイヤーの事で イコール 3ユニット。 s2はこの例だと5ユニット。 そして出力レイヤのs4は それはsLとも等しい。というのは大文字のLは4だから。 この例における出力レイヤは 4ユニットとなっている。 2つの種類の分類問題を扱っていく事になる。 一つ目はバイナリ分類、 それはyの取りうる値は0か1のどちらかだけの場合。 この場合は出力ユニットは一つだけとなる。 この上にあるニューラルネットワークの場合は 4つの出力ユニットがあるが、 バイナリ分類なら、 出力ユニットは一つだけで、 それはhのxを計算する。 そしてそのニューラルネットワークの 出力は、つまりhのxは 実数となる。 そしてこのケースでは、 出力ユニットの総数sLは ところでLは最後のレイヤのインデックスなので というのはLはそのネットワークの レイヤーの総数だからだが、 すると出力レイヤに存在する ユニットの総数は、1に等しい。 この場合、あとでノーテーションを簡素化する為、 K=1とも書く事にする。 つまりKもまた、 出力レイヤの ユニットの総数を表す訳だ。 2つ目のタイプの分類問題は マルチクラスの分類問題だ。 それは、K個の異なる分類がある問題。 前の例だと yがこんな表現のがあったよね。 これは4つのクラスがある場合だった。 このタイプでは、大文字のK個だけの 出力ユニットがあり、我らの仮説は K次元のベクトルを出力することになる。 そして出力ユニットの総数は Kと等しくなり、 このKは通常は、 3以上なのがこのタイプとなる。 何故なら、 もし2クラスしか無い時は one vs all法を使う必要が 無いからだ。 one vs all法を使わなくてはいけないのは Kが3以上の 時だけなので 2つしかクラスが無い時は 一つの出力ユニットしか 必要としない。 ではここで、ニューラルネットワークのコスト関数を定義しよう。 ニューラルネットワークで使う コスト関数は、 ロジスティック回帰で使った物を 一般化した物だ。 ロジスティック回帰では コスト関数Jのシータを 最小化したのだった。 そのJのシータとは-1/mの このコスト関数に この追加の正規化の項を 足した物だった。正規化の項は jが1からnまでの和だった。 何故ならバイアス項である シータ0は正規化しないからだった。 ニューラルネットワーク向けには コスト関数はこれを一般化したものとなる。 そこでは基本的には 単に一つのロジスティック回帰の出力ユニットしか無いのではなく、 その代わりにK個のユニットがある訳だ。 これがそのコスト関数となる。 ニューラルネットワークはRのKのベクトルを 出力する。 ここで Kは1となる事もある、 その時はバイナリ分類問題という事。 h(x)の添字iという記法で i番目の出力を示す。 つまり、h(x)はK次元ベクトルなので この添字iは単に ニューラルネットワークからの出力のベクトルから i番目を選び取るだけだ。 コスト関数のJのシータは 今や以下のようになる。 -1/mに ロジスティック回帰の時と 似たような項の和と なっている。だが、この和が kが1からKまで というのがあるのが違いか。 その和は基本的には K個の出力ユニットに対する和だ。 もし出力ユニットが4つあるとすると、 それはつまりニューラルネットワークの 最後のレイヤーが4つの出力ユニットを持つという事だが、 するとその時この和は これはkが1から4までの 基本的にはロジスティック回帰のアルゴリズムの コスト関数なんだが、 しかしそのコスト関数を 4つの出力ユニット分を一つずつ 足していく所が違う。 そして気づいたかもしれないが、 これは特に、 yのkとhのkに適用される。 何故なら、基本的には 我らはk番目の出力ユニットをとり、 それをyのkと比較しているから。 それはつまり、 これらのベクトルの一つの コストがどうなっているべきか、という事。 そして最後に、この二番目の項は 正規化の項、 これはロジスティック回帰と似てるね。 この和の項はほんとうに難しそうに見えるね。 でも実際にやってるのは これらの項、シータのijlの和を 全てのijlの値に渡って 取るだけだ。 ただしバイアス項に 対応する値は ロジスティック回帰の時同様 除く。 具体的には、i=0に 対応する項は 足さない。 それはというと、 ニューロンの活性化を計算してるとき こういう感じの項がある Θ(セータ) i0 プラス セータのi1 x1 プラス、、、 などなど。 これが最初の隠れレイヤーだとすると ここに付くのは 2となる。 つまり、ここにある 0に対応する値は x0とかa0の 係数となっている、 つまり、これは バイアスユニットの係数って事。 そしてロジスティック回帰から 類推出来るように、 正規化の項では それらの項は足し合わせない。 何故ならそれらは正規化したくないから。 だから値0は含めていない。 だがこれは単なる一つの慣習に過ぎず、 別にiを0からslまで 足し合わせたとしても、 大差なく、 だいたいうまく行く。 だがこっちの慣習、 つまりバイアス項は 正規化しない流派の方がちょっとだけ普及してると思う。 以上がネットワークをフィッティングするのに使用する コスト関数です。 次のビデオでは、 コスト関数を最適化する アルゴリズムに入っていきます。