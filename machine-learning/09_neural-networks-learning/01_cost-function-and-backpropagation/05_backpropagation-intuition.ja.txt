前回の動画では、バックプロパゲーションについて話した。 多くの人にとっては 最初に見たら 第一印象は 「うげぇ。こりゃすげー複雑な アルゴリズムだ！
そしてこんなたくさんの ステップがあるなんて！」と。 そして「私、実はそれらがどう組み合わせて使うのか良く分かってないんだぁ、、、」とか、 さらに「これらの込み入ったステップは なんだかブラックボックスみたいだ！」と。 あなたがバックプロパゲーションを、 もしそんな風に感じていたとしても、 実は問題ありません。 バックプロパゲーションは残念な事に 線形回帰やロジスティック回帰に比べると 数学的にクリーンという訳でも 数学的にシンプルなアルゴリズムという訳でも ありません。 でも実の所、私も長年、ひょっとしたら今日ですら、 バックプロパゲーションが何やってるのか いまいち直感的に理解出来てないなぁ、と 思う事はあるけれど、 使う分には問題なく とてもしっかりと 使えてきました。 プログラミングの宿題ちゃんとやりさえすれば 少なくとも機械的に どうやってバックプロパゲーションを実装するかを 順番に見ていく事には なるから。 そうすれば自分で動かす事は出来るようになるよ。 そしてこのビデオでやりたい事は バックプロパゲーションの 手順を、もうちょっとだけ 機械的に見てみたい。 そうする事で、もうちょっと感覚的に バックプロパゲーションを機械的に行う手順は どんな感じかを伝えたい。 そうする事で これは少なくとも良さそうなアルゴリズムだな、と思ってもらえたら幸い。 このビデオ見た後でも バックプロパゲーションがまだ すげーブラックボックスちっくで 複雑でたくさんの手順が有り過ぎる！と思った君に、 ちょっとだけ魔法をかけてあげよう。 実際の所、それでも問題無いです！ もしそう感じてたとしても、 私はバックプロパゲーションを長年使ってきたよ。 たまに理解の難しい事のあるアルゴリズムだけどね。 でも願わくばこの動画が バックプロパゲーションを理解する助けにならんことを！ まずフォワードプロパゲーションが何をしてるのか 別の角度から詳しく見ていこう。 ここにニューラルネットワークがある。 2つの入力ユニットがある。 バイアスユニットを数えない。 そして2つの隠れユニットがこのレイヤーにあり さらに2つの隠れユニットが その次のレイヤーにある。 そして最後に出力ユニット。 そしてこれらのカウント、 2、2、2は、このトップのバイアスユニットを数えていない。 フォーワードプロパゲーションを わかりやすく説明するために このネットワークをちょっと違う風に描いてみよう。 特に、このニューラルネットのノードを 大きな楕円で描くことで、 その中にテキストを 書けるようにする。 フォワードプロパゲーションを実行する時は ある手本に対して それを手本x(i)、y(i)としよう、 そしてこのx(i)こそが 入力レイヤーに 食わせる物だ。 つまり、この x(i)の1とx(i)の2の2つの値が 入力レイヤーに セットする値で、 そしてその値を最初の 隠れレイヤーへとフォワードプロパゲートするには、 z(2)の1とz(2)の2を計算し、 ところで、これらは 入力ユニットからの入力の 重み付け和だ。で、 ロジスティック関数のsigmoid関数を 適用するーーー sigmoidアクティベーション関数を zの値に適用すると、 これらのアクティベーションの値が得られる。 つまり以上で、a(2)の1と a(2)の2が得られる。 次にそれをまたフォワードプロパゲートして、 ここのz(3)の1に sigmoidのロジスティック関数、 アクティベーション関数を適用し、 そしてa(3)の1を得る。 同様に、 z(4)の1を得て、そこに アクティベーション関数を適用してa(4)の1を得るまで 続ける。これこそが、 ネットワークの最終的な出力の値だ。 ちょっとスペース開ける為に この矢印を消す。 そしてこの計算が 実際に何をやってるのか見てみよう。 この隠れユニットにフォーカスしてみると、 このウェイト、 マゼンダで示したが ウェイト、 シータ(2)10だとしよう。 インデックスはそんな重要でも無いんだが。 で、こんな風に、 ここは赤で シータ(2)11、 そしてここのウェイトは 緑、、、というよりはシアンで描いたのが シータ(2)12。 以上がz(3)1を z(3)1を計算する方法。 このウェイトに この値を掛ける。 つまり シータ(2)10 掛ける 1、 足すことの 赤のウェイト 掛ける この値、 つまり シータ(2)11 掛ける a(2)1。 そして最後にこのシアン、 掛けるこの値。 つまり、足すことの シータ(2)12 掛ける a(2)1。 これがフォワードプロパゲーションだ！ そしてこのビデオの後半で見ると 明らかになるが、 バックプロパゲーションでやる事も とてもこれと 似たプロセスだったりする。 違いは計算の流れが ネットワークの左から右へと 流れる代わりに、 そこでは計算は ネットワークの 左から右へと流れる。 そしてこれととても似た計算を用いて、 二枚のスライドで ちゃんと説明する。 バックプロパゲーションが 何やってるのかより良く理解する為に、 コスト関数を見てみよう。 一つしか出力ユニットが無い時の コスト関数を。 もし一つよりも多くの 出力ユニットがある時は 単に出力ユニットのインデックスについて 足し合わせてやれば良い。 もし出力ユニットが一つしかなければ これがコスト関数。 フォワードプロパゲーションとバックプロパゲーションを一度に一つの手本データずつに対して行う。 だから一つの手本、 x(i)、y(i)にフォーカスしよう。 そして出力ユニットは一つのケースについてフォーカスしよう。 つまりここのy(i)は 単なる実数。 さらに正規化は無視しよう。 つまりラムダは0。 そうすればこの最後の正規化の項は消えるからね。 今、この和の中を 見てみれば、 i番目のトレーニング手本に対応する コストの項は つまりx(i)とy(i)に対応した コストは この式で与えられる。 これがi番目のトレーニング手本に対応した コストという事になる。 そしてこのコスト関数が やってるのは 誤差の二乗に似た感じの役割。 だからこの複雑な式を 見る代わりに i番目に対応したコストを ニューラルネットワークの出力と 実際の値との 差分の二乗みたいなもんだと 考えても良い。 ロジスティック回帰の時と同様に 実際にはこちらの logを使ったちょびっと複雑なコスト関数の方がいいんだけど、 感覚的に理解する、という点では 誤差の二乗のコスト関数なんだと 考えて 差し支えない。 つまり このコストのiは どのくらいネットワークが うまく手本iを予測しているかを測ってる。 どのくらい出力が 実際の観測値、y(i)と近いかを。 ではバックプロパゲーションが何をやってるかを見ていこう。 有用な直感的な説明としては、 バックプロパゲーションはこれらの デルタ 上付き添字l 下付き添字j の項を計算している、というのがある。 これらをアクティベーションの値の 「誤差」と 考える事が出来る、 l番目のレイヤーの j番目のユニットの アクティベーション。 より正式には 解析学が得意な人向け だろうけど、 より正式には、デルタ項が 実際になんなのかというと、これだ！ それらはz(l)jによる 偏微分係数。 このzは入力の重み付き和を 計算したもので、 これらによるコスト関数の偏微分となる。 具体的にはコスト関数は ラベルyと このh(x)の、 ネットワークの 出力した値の関数。 そしてもしこのネットワークの中に入って それらのz(l)jの値を ちょっとずらしたら、 これらの値がニューラルネットに影響を与えて 最終的にはコスト関数も変わる。 もう一度言っておくと これはほんと解析得意な人向けの話。 偏微分に慣れ親しんでて、快適に使える人向け。 これらのデルタの項は 我らの計算している、これらの中間項による コスト関数の 偏微分となっている。 つまりそれらは、 これらの中間の値の計算に影響を与えて、 ニューラルネットワークの最終出力、h(x)に影響を与えて、 結果として全体のコストに影響を与える為に どれだけ ニューラルネットワークのウェイトを 変更すれば良いかの 指標となっている。 この最後の部分の 偏微分の直感が いまいちピンと来て無くても、心配ご無用。 ここから後は 偏微分なんて計算出来なくても やっていけるから。 でも、バックプロパゲーションが何をやっているのか もうちょっと詳しく見てみよう。 出力レイヤについては、 最初にセットされるデルタ項だが、 それはデルタ(4)1をy(i)、、、 もしこのトレーニング手本のiについて フォワードプロパゲーションを行い、 さらにバックプロパゲーションを行うとすると、 y(i) - a(4)1となる。 つまり、それは本当に誤差だ。 実際の値であるyから、引くことの 予言された 値なのだから。 こんな風にデルタ(4)1を 計算する。 次に、これらの値を後ろ（バック）へと伝播（プロパゲート）させていく。 すぐに説明する。 で、一つ前のデルタ項を計算する事になる。 結局、デルタ(3)1とデルタ(3)2 になる。 そしてその後に、 これをさらに後ろへと 伝播させていき、 デルタ(2)1とデルタ(2)2を 計算する事になる。 ここまでくると、バックプロパゲーションの計算は かなりフォワードプロパゲーションのアルゴリズムを 実行するのに似通ってくる。後ろにやるだけ。 それの意味する所はこうだ。 どうやってこのデルタ(2)2まで 来たのか見てみよう。 デルタ(2)2がある。 フォワードプロパゲーションみたいに 幾つかのウェイトにラベルをつけよう。 このウェイトはシアンの色で、 シータ(2)の1 2で、 そしてこの下の ウェイトは、赤で書こう、 これを以後、シータ(2)の2 2と 呼ぼう。 さて、デルタ(2)の2が どう計算されるか このノードはどう計算されるのか？
見てみよう。 計算する為に やる事は この値を取って このウェイトを掛ける、 そしてそれを足し合わせる事の この値掛けるそのウェイト。 つまりそれは本当に これらのデルタの値の重み付き和だ。 対応するエッジの重みで重みづけする。 具体的に書き下してみよう。 このデルタ(2)2はイコール、 シータ(2)の1 2ーー これはマゼンダ色で描いた重みーー 掛ける事のデルタ(3)1、 足すことの 次は赤の奴。 シータ(2)の2 2掛ける デルタ(3) 2。 つまり本当に、文字通り、 この赤のウェイト掛けるこの値、足すことの このマゼンダのウェイト掛けることのこの値。 それがこの値、デルタを計算する方法。 もう一つ見てみよう。この値を見てみる。 この値はどうやったら得られるか？ それは似た感じで、 このウェイトをとりあえず 緑で表しておくと、 このウェイトを シータ(3)の1 2とすると、 デルタ(3)2は その場合、 イコール 緑のウェイト、シータ(3) 1 2 掛ける デルタ(4) 1だ。 ところで、 ここまでの所、 デルタの値を隠れユニットにだけ 書いてきた。 そしてバイアスのユニットには書いて来なかった。 バックプロパゲーションのアルゴリズムを どう定義するか、または どう実装するかによって、 これらのバイアスユニットに対する デルタの値を計算する事に なったりする。 バイアスユニットはいつも 値、+1を出力する。 それがまさにバイアスユニットという物だ。 そしてその値を変更する方法は 存在しない。 だからバックプロパゲーションの実装方法によるが、 私の普段の実装方法だと、 これらのデルタの値は 計算してる。でも、 ただその結果を捨てていて、 使っていない。 何故ならそれらは結局のところ 微分を計算するのに必要な計算に含まれていないから。 さて、以上でバックプロパゲーションが 何をしているか、感覚的な理解が ちょっとでも与えられたら幸い。 もしこれらを見てもなお、 全部魔法のようで あまりにもブラックボックスに感じられた場合は あとのビデオ、 「Putting It Together」（ビデオのタイトル）の中で、 バックプロパゲーションが何をしているかについて、さらなる直感を提供したいと思っている。 でも残念なお知らせがある。 これは、可視化したり、 それが本当の所何をやっているのかを理解するのは難しいアルゴリズムだ。 だが良い知らせもある。 私の推測するところによると、 たくさんの人がとてもうまく このアルゴリズムを使えていて、 実装してみると、 とても効率的な学習アルゴリズムだ、 たとえ中がどうなってるのか、正確に何が起こるかを 可視化するのが難しいにせよ。