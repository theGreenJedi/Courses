이 비디오에서 우리는 비용함수라고 
불리는 것에 대해 배워보겠습니다. 비용함수를 사용하면 주어진 데이터에 가장 가까운 일차함수 그래프를 알아 낼 수 있습니다 선형증가에서, 
우리는 훈련집합에서 제가 저번에 보여드렸던 기호m은 훈련예시의 개수이고, 
그래서 m은 47이다 라는 것을 배웠습니다. 그리고 우리의 가설의 형태에 대해서도요. 우리가 선형함수를 사용해 예측했던 것도 있었습니다. 조금 더 전문용어를 소개해드리자면, 
이 Θ0과 Θ1은 이 Θ0과 Θ1은 모형들의 
파라메터를 나타내고 있습니다. 우리가 이 비디오에서 
얘기하고자 하는 것은 어떻게 이 2개의 파라메터, 
Θ0과 Θ1를 고를 것이냐는 것 입니다. 파라메터 Θ0과 Θ1에서 
다른 선택을 하면 다른 가설, 
다른 가설함수들을 가지게 됩니다. 여러분 중 몇 분은 슬라이드에 나올 내용에 
익숙할 것이라는 것을 알고 있지만, 복습한다는 느낌으로 다시 봐 주시 길 바랍니다.
여기 몇가지 예가 있습니다. 만약 Θ0이 1.5이고 Θ1이 0이라면 가설함수는 이런 식으로 
생기게 됩니다. 가설 함수는 h = 1.5 + 0x, 
즉 정수값 1.5에 수렴합니다. 즉 정수값 1.5에 수렴합니다. 만약 Θ0이 0이고 Θ1은 0.5이면, 
가설은 이런 식으로 생기게 되고, 이 함수는 2, 1 값을 지나가는 h(x)가 됩니다. hΘ(x)로도 표현할 수 있지만, 
가끔 저는 간단히 하기위해 Θ를 생략합니다. 그래서 h(x)는 이런 식으로 
생긴 0.5x 일차함수가 됩니다 그리고 마지막으로, 
Θ0이 1이고 Θ1은 0.5이면, 우리는 이렇게 생긴 
가설을 갖게 됩니다. 한번 봅시다. 
이 가설은 2, 2값을 지나가게 됩니다. 그래서 이것은 x의 백터이고, 
또는 새로운 hΘx가 됩니다. hΘx나 이 기호의 약어인 hx든 당신이 원하는 방법으로
기억하면 됩니다. 선형회귀에서, 만약 이런 식으로 제가 표시한 것처럼
 훈련집합을 가지고 있다고 합시다. 우리가 하려는 것은 파라메터 Θ0과 Θ1의 값들을 
이용해서 이러한 직선을 구해, 여기에 그린 이 직선처럼 
직선이 자료와 얼마나 잘 일치하는지를 보는 것입니다. 그래서, 우리가 어떻게 값들, 
Θ0, Θ1이 데이터와 얼마나 잘 일치하는지를 알 수 있을까요? 훈련집합의 예시에서 
파라메터 Θ0과 Θ1을 고르는 것은, hx, 즉 x값과 같다고 예측한 값은 y값과 가장 비슷하지 않은 값입니다. 그래서 이 훈련집합에서, 
우리는 몇몇 숫자들의 예시를 가지고있고, x값이 집을 선택하게 되고, 
이것을 통해 우리는 실제로 어떤 가격에 팔리는지에 
대해 알 수 있습니다. 그래서, 파라메터 값을 선택해서, 최소한 훈련집합에서, 
x값이 주어진다면 우리는 y값에 대한 예측을 할 수 있습니다. 이것을 공식화해봅시다. 그렇다면 회귀에서, 
우리가 하려는 것은, 저는 최소화문제를 풀려고 합니다. 그래서 저는 최소값을 Θ0, Θ1이라고 적겠습니다. 그리고 저는 이 값을 작게 만들려고 합니다, 맞죠? 저는 h(x)와 y간의 차이를 작게 만들고 싶습니다. 그리고 저는 가설의 결과값과 
실제 집의 가격의 차이의 차이의 제곱을 최소화할 것입니다. 좋습니다. 
자세하게 보겠습니다. (x(i), y(i)) 기호가 
i번째 훈련 예시를 나타내는 것을 기억하실 겁니다. 그래서 실제 훈련집합에서 훈련집합에서 i=1부터 m까지의 차이의 제곱의 합계를 구하려고 하는 것입니다. 이 값은 i번의 집의 입력 값에 대한 
제 가설을 예측한 것입니다. 거기에 실제 제가 파려고 하는 
집의 실제 가격을 빼 줍니다. 그리고 저는 훈련집합의 합을 
최소화하고 싶고, 이 차의 제곱의 1부터 m까지 합은, 예측된 집 가격과 
실제 팔리는 가격의 차의 제곱과 같습니다. 그래서 표기법에 대해서 다시 한번 상기시켜주자면,
 여기에 있는 m값은 훈련집합의 크기입니다, 맞죠? 그래서 m은 훈련 예시들의 개수입니다. 여기 있는 #은 훈련 예시들의 수의 
약어를 나타냅니다, 알겠죠? 그리고 수학을 조금 더 쉽게 해보자면, 이 합에 1/m을 곱해보겠습니다. 제 평균값을 최소화하기 위해 
1/2m을 최소화해보겠습니다. 맨 앞에 1/2값을 놓는 것은 좀 더 쉬워 보이게 되고, 
어떤 값의 반을 최소화하는 것은 Θ0, Θ1값을 최소화 하는 것과 
같은 값의 과정을 보여줄 것입니다. 그리고 확실히 하기 위해서, 
방정식에 대해 확실히 이해했죠? 여기에 있는 표현은 hΘ(x), 우리가 자주 쓰던 표현입니다. 이 값은 Θ0 + Θ1x와 같습니다. 그리고 이 표기법은, Θ0, Θ1를 최소화 하는 것은, Θ0과 Θ1값을 찾아야 하고, Θ0값과 Θ1값에 최소화되는 것이 달려있습니다. 요약해보겠습니다. 우리는 Θ0, Θ1값을 찾기 위해서 평균값인 1/2m에, 
훈련집합에 대한 내 예측과, 최소화된 훈련집합에서 실제 집들의 값의 차의 합계의 제곱을 곱한 값으로 접근했습니다. 그래서 이것이 선형회귀에 대한 
전반적인 목적 함수입니다. 그리고 좀 더 명확하게 하기 위해 다시 써본다면, 비용 함수를 알아내기 위한 공식, 제가 여기에 적은 이런 공식이 될 것입니다. 그리고 제가 원하는 것은 Θ0, Θ1을 최소화 하는 것입니다. 함수 j(Θ0, Θ1), 여기에 방금 적은 함수입니다. 이것이 바로 비용함수입니다. 그래서 이 비용함수는 
오차함수의 제곱 이라고도 불립니다. 가끔은 오차요인 제곱 함수 라고도 불리며, 왜 우리가 이 부분을 
제곱해야 하는지를 나타냅니다. 이 오차요인 제곱함수는 합리적인 선택이며 대부분의 회귀 프로그램
문제에서 잘 작동합니다. 다른 비용함수들도 꽤 잘 작동합니다. 그러나 오차함수의 제곱이 가장 통상적으로 회귀문제에서 사용되는 방법입니다. 나중에 우리는 대안 비용 함수들에 대해서도 
이야기하게 될 텐데, 우리가 계속해서 얘기했던 방법이 대부분의 선형 회귀 문제에서 
꽤 합리적인 선택입니다. 좋아요. 그래서 이것이 바로 비용 함수입니다. 지금까지 우리는 비용함수의 
수학적인 정의에 대해서 알아봤습니다. j Θ0, Θ1함수에 대해서 약간 추상적이고, 아직 이게 무엇을 하는지 여전히 
잘 이해하지 못했을 수 있습니다만, 다음 비디오에서, 
다음 몇 개의 비디오에서 저는 “j”라는 함수가 하는 것에 대해 
좀 더 깊게 설명하고, 컴퓨터에서 어떻게 사용하며, 
왜 우리가 이것을 사용해야 하는지에 대해 
더 나은 직감을 주도록 하겠습니다.