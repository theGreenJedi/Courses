В предыдущем ролике, мы говорили об алгоритме обратного распространения. Для большого числа людей, столкнувшихся с этим в первый раз, первым впечатлением часто бывает вау! это очень сложный алгоритм и там все эти различные шаги. И я не вполне уверен как из соединить вместе и это похоже на черный ящик, со всем этими сложными шагами. В случае, если у вас возникают подобные чувства, по отношению к алгоритму обратного распространения, это нормально. Алгоритм обратного распространения, возможно, к несчастью менее математически понятен, или менее математически простой алгоритм по сравнению с линейной регрессией или логистической регрессией, и Я ,на самом деле, использовал алгоритм обратного распространения, вполне успешно много лет и даже сегодня, я иногда чувствую что не полностью понимаю, как лучше показать, что делает алгоритм обратного распространения. [?] Для тех из вас, что выполняет практические задания по программированию это будет по крайней мере, одним из нескольких различных шагов реализации алгоритма обратного распространения для использования в своих задачах. И что я хочу сделать в этом видеоролике, так это посмотреть чуть ближе на эти самые шаги алгоритма обратного распространения и попытаюсь дать вам немного большее понимание о том что за шаги совершаются в алгоритме обратного распространения и надеюсь убедить вас что, знаете ли, это достаточно неплохой алгоритм. Если даже после просмотра этого ролика алгоритм обратного распространения все еще кажется очень черным ящиком и вам кажется что в нем слишком много сложных шагов, малость магических для вас, это, на самом деле, нормально. И, кроме того, я использовал алгоритм обратного распространения ошибки много лет, иногда это сложный для понимания алгоритм. Но к счастью этот видеоролик должен немного помочь Чтобы лучше понять алгоритм обратного распространения, давайте посмотрим чуть подробнее что же делает алгоритм прямого распространения. Это нейронная сеть с двумя входами, которые не считая смещения, и и два скрытых нейрона в этом слое и два скрытых нейрона в следующем слое, и затем в завершение, выходной нейрон. И еще раз, они считаются 2, 2, 2 не считая этих блоков смещения, сверху. Для того, чтобы проиллюстрировать прямое распространение, я собираюсь изобразить эту сеть немного по-другому. И, в частности, я собираюсь нарисовать эту нейронную сеть с узлами в виде этих, очень жирных эллипсов, так, чтобы я мог уместить в них текст. Когда мы производим прямое распространение, мы можем иметь некоторый конкретный образец данных скажем, некоторый (x(i),y(i)) при этом, это будет тот x(i), который мы подаем на входной слой, таким образом, x(i) 1 и  x(i)2  это значения, которые мы устанавливаем на входном слое и затем мы распространяем их на первый скрытый слой нейронов, что мы делаем, так это вычисляем z(2)1 и z(2)2, они являются взвешенными суммами входов входного слоя и затем мы применяем сигмоид логистическую функцию и сигмоидная функция активации, примененная к значению z, дает нам эти значения активации.