1
00:00:00,280 --> 00:00:04,180
在之前的视频里，我们介绍了反向传播算法

2
00:00:04,180 --> 00:00:06,470
对许多人来说，

3
00:00:06,470 --> 00:00:11,390
第一次看到它的印象是，这是一个非常复杂的算法

4
00:00:11,390 --> 00:00:14,700
并且这里有很多步骤，人们很难搞清楚是怎么统一起来

5
00:00:14,700 --> 00:00:17,940
看起来像是一个复杂的黑箱

6
00:00:17,940 --> 00:00:21,973
如果你也是这么觉得，其实很正常

7
00:00:21,973 --> 00:00:26,950
反向传播，很大程度上数学步骤比较复杂

8
00:00:26,950 --> 00:00:28,900
并不是一个简单的算法

9
00:00:28,900 --> 00:00:32,550
比起线性回归和逻辑回归而言。

10
00:00:32,550 --> 00:00:36,770
我实际上使用反向传播算法许多年，

11
00:00:36,770 --> 00:00:37,330
也很成功。

12
00:00:37,330 --> 00:00:41,800
但即使是今天，有时候我还是感觉不太好把握，

13
00:00:41,800 --> 00:00:45,620
或者忽然觉得迷茫。

14
00:00:45,620 --> 00:00:49,780
因此，对于即将做编程的同学，

15
00:00:49,780 --> 00:00:52,850
你们不用担心，我们会有数学的具体步骤，

16
00:00:52,850 --> 00:00:55,240
它将帮助你来一步一步完成。

17
00:00:55,240 --> 00:00:57,880
所以，你将能够自主独立实现。

18
00:00:57,880 --> 00:01:02,670
在这个视频，我要做的，

19
00:01:02,670 --> 00:01:07,230
是再一步一步介绍这个算法，让你更有体会。

20
00:01:07,230 --> 00:01:11,160
这些机械的步骤，将使你信服，

21
00:01:11,160 --> 00:01:12,910
让你认为这是一个非常合理的算法。

22
00:01:13,940 --> 00:01:19,960
如果这个视频之后，

23
00:01:19,960 --> 00:01:22,390
你还是觉得这个算法非常复杂，

24
00:01:22,390 --> 00:01:25,540
其实也没有太大关系。

25
00:01:25,540 --> 00:01:30,283
正如之前所说，对我而言有时候也很难。

26
00:01:30,283 --> 00:01:34,697
但，希望这个视频可以有所帮助。

27
00:01:34,697 --> 00:01:37,896
为了更好地理解反向传播算法，

28
00:01:37,896 --> 00:01:42,544
我们来看看正向传播

29
00:01:42,544 --> 00:01:47,938
这个神经网络有两个输入层单元，当然不算上偏置单元

30
00:01:47,938 --> 00:01:53,310
两个隐层单元，有两层

31
00:01:53,310 --> 00:01:55,630
还有一个输出单元

32
00:01:55,630 --> 00:02:01,650
当然，我们都不算偏置单元

33
00:02:01,650 --> 00:02:04,360
为了更好地展示前向传播

34
00:02:04,360 --> 00:02:06,640
我这次要用另外一种画法

35
00:02:08,030 --> 00:02:11,780
特别地，我要把每个神经元

36
00:02:11,780 --> 00:02:15,870
画的更扁平一些，所以我可以在里面写字

37
00:02:15,870 --> 00:02:19,750
当进行前向传播算法的时候，我们可能有一些特别的例子

38
00:02:19,750 --> 00:02:22,660
比如，xi，yi

39
00:02:22,660 --> 00:02:27,140
我们将把它输入到这个网络当中

40
00:02:27,140 --> 00:02:33,046
所以，xi1和xi2将是我们对输入层的设置

41
00:02:33,046 --> 00:02:37,833
当我们进入第一个隐层，

42
00:02:37,833 --> 00:02:41,460
我们会计算z(2)1和z(2)2

43
00:02:41,460 --> 00:02:46,780
那么，这些是我们要的值

44
00:02:46,780 --> 00:02:50,380
然后我们来用冲击函数计算

45
00:02:50,380 --> 00:02:55,490
它作用与z值

46
00:02:55,490 --> 00:02:57,850
这里是激励值

47
00:02:57,850 --> 00:03:00,970
所以我们有a(2)1和a(2)2

48
00:03:00,970 --> 00:03:05,640
之后我们把这些值赋予给z(3)1

49
00:03:05,640 --> 00:03:08,490
然后使用sigmoid函数

50
00:03:08,490 --> 00:03:11,720
我们会得到a(3)1

51
00:03:11,720 --> 00:03:17,910
类似的，我们一直得到z(4)1

52
00:03:17,910 --> 00:03:19,210
再次计算，

53
00:03:19,210 --> 00:03:23,710
我们有a(4)1，这是最后的结果

54
00:03:24,910 --> 00:03:27,800
我们擦掉这些箭头，来得到更多空间

55
00:03:27,800 --> 00:03:32,320
如果你仔细看我们的计算过程，

56
00:03:32,320 --> 00:03:35,500
我们可以说，

57
00:03:35,500 --> 00:03:37,550
我们要加上这个权重

58
00:03:37,550 --> 00:03:44,780
(2)1 0，这里的编号不重要

59
00:03:44,780 --> 00:03:49,290
这个方向，我用红色高亮

60
00:03:49,290 --> 00:03:53,840
是theta(2)11以及权重

61
00:03:53,840 --> 00:03:59,118
这里用青色标注theta(2)12

62
00:03:59,118 --> 00:04:04,532
所以，z(3)1是

63
00:04:04,532 --> 00:04:11,620
z(3)1等于这个值

64
00:04:11,620 --> 00:04:15,530
所以我们有(2)10x1

65
00:04:15,530 --> 00:04:20,500
然后加上红色标注的权值

66
00:04:20,500 --> 00:04:25,740
得到theta(2)11乘以a(2)1

67
00:04:25,740 --> 00:04:30,680
最后我们再用青色来乘

68
00:04:30,680 --> 00:04:39,320
也就是加上theta(2)12乘以a(2)1

69
00:04:39,320 --> 00:04:41,084
那么这就是前向传播

70
00:04:41,084 --> 00:04:44,839
这我们之前看到过

71
00:04:44,839 --> 00:04:49,840
而反向传播做的很类似

72
00:04:49,840 --> 00:04:54,330
除了这些计算从左到右，

73
00:04:54,330 --> 00:04:59,610
现在是从右到左

74
00:04:59,610 --> 00:05:02,460
同时计算流程相似

75
00:05:02,460 --> 00:05:06,560
我用两页PPT来描述这个过程

76
00:05:06,560 --> 00:05:10,630
首先来看其支付函数

77
00:05:10,630 --> 00:05:15,410
这是只有一个输出单元时候的支付函数

78
00:05:15,410 --> 00:05:17,320
如果有多个

79
00:05:17,320 --> 00:05:21,670
那就需要编号并且求和

80
00:05:21,670 --> 00:05:25,200
如果只有一个，用这个函数就行

81
00:05:25,200 --> 00:05:30,410
我们在一个例子里做前向和后向传播

82
00:05:30,410 --> 00:05:35,010
来关注一个例子x(i)和y(i)

83
00:05:35,010 --> 00:05:37,210
并且来看输出值

84
00:05:37,210 --> 00:05:40,400
所以y(i)是一个实数

85
00:05:40,400 --> 00:05:43,420
我们现在不考虑标准化，所以lambda为0

86
00:05:43,420 --> 00:05:47,360
所以最后一项去掉

87
00:05:47,360 --> 00:05:49,667
如果你来看这个求和公式

88
00:05:49,667 --> 00:05:54,208
你会发现，这个支付项

89
00:05:54,208 --> 00:05:58,627
和我们的训练数据x(i)和y(i)有关

90
00:05:58,627 --> 00:06:01,690
这由我们的表达式给出

91
00:06:01,690 --> 00:06:06,729
所以，正如下述所写的

92
00:06:06,729 --> 00:06:11,230
支付函数所做的和这个箭头相似

93
00:06:11,230 --> 00:06:14,010
我们不看这个复杂的表达式

94
00:06:14,010 --> 00:06:18,040
如果你考虑支付，

95
00:06:18,040 --> 00:06:22,750
这里就是我们的插值

96
00:06:22,750 --> 00:06:25,830
和之前逻辑回归很像，

97
00:06:25,830 --> 00:06:28,330
我们用了Log

98
00:06:28,330 --> 00:06:32,060
但，从直觉上来说，

99
00:06:32,060 --> 00:06:34,880
这其实就是平方误差函数

100
00:06:34,880 --> 00:06:38,820
所以cost(i)描述了

101
00:06:38,820 --> 00:06:40,820
这个网络的表现，对于特定的结果i

102
00:06:40,820 --> 00:06:45,630
那么到底这个计算结果和真实值y(i)多接近呢

103
00:06:45,630 --> 00:06:48,510
我们来看反向传播在做什么

104
00:06:48,510 --> 00:06:53,640
一个很有用的例子就是反向传播

105
00:06:53,640 --> 00:06:56,690
计算了deltai下标j

106
00:06:56,690 --> 00:07:02,080
这是我们的理解方法，

107
00:07:02,080 --> 00:07:05,969
我们在l层得到单元j

108
00:07:07,120 --> 00:07:10,060
正式一点说，

109
00:07:10,060 --> 00:07:12,690
这个对于熟悉微积分的人来说更恰当

110
00:07:12,690 --> 00:07:15,950
所以，这就是delta项

111
00:07:15,950 --> 00:07:19,050
它就是一个偏微分，针对z,l,j

112
00:07:19,050 --> 00:07:23,440
这是权重，

113
00:07:23,440 --> 00:07:25,929
针对这些量的偏微分，所得到的支付函数

114
00:07:27,000 --> 00:07:31,230
所以，具体来说，

115
00:07:31,230 --> 00:07:34,958
这个h x输出值，

116
00:07:34,958 --> 00:07:37,800
如果我们走进这个神经网络，

117
00:07:37,800 --> 00:07:41,480
并且只稍微改变一下zl j值

118
00:07:41,480 --> 00:07:45,610
那么这就会改变我们的输出

119
00:07:45,610 --> 00:07:49,080
也会改变我们的支付函数

120
00:07:49,080 --> 00:07:53,030
同样，还是针对那些微积分比较好的同学

121
00:07:53,030 --> 00:07:56,560
如果你适应偏微分

122
00:07:56,560 --> 00:08:00,830
这些就是对支付函数的偏微分，

123
00:08:00,830 --> 00:08:04,529
针对中间变量

124
00:08:06,000 --> 00:08:10,430
并且，他们衡量了我们要如果改变网络的权值

125
00:08:10,430 --> 00:08:15,870
当然，这是为了影响我们的计算结果

126
00:08:15,870 --> 00:08:19,190
所以，为了改变计算结果h(x)

127
00:08:19,190 --> 00:08:21,570
以及对整个支付函数的影响

128
00:08:21,570 --> 00:08:25,190
上下的这个偏微分的理解，

129
00:08:25,190 --> 00:08:26,780
如果你不能理解

130
00:08:26,780 --> 00:08:28,540
不要太担心

131
00:08:28,540 --> 00:08:32,350
我们可以撇开它来谈

132
00:08:32,350 --> 00:08:35,950
我们就来看看到底反向传播算法做了什么

133
00:08:35,950 --> 00:08:40,366
首先，设置这个delta项

134
00:08:40,366 --> 00:08:45,610
delta(4) 1正如y(i)我们对前向传播算法

135
00:08:45,610 --> 00:08:49,880
和后向传播对训练数据i的做法一样。

136
00:08:49,880 --> 00:08:52,750
这表达的是y(i)减去a(4)1

137
00:08:52,750 --> 00:08:54,440
所以就是误差，对吧

138
00:08:54,440 --> 00:08:57,640
这就是真实结果和

139
00:08:57,640 --> 00:09:01,910
我们预测结果的误差，所以我们结算delta(4)1

140
00:09:01,910 --> 00:09:06,920
接下来，我们来把这些值反向传播回去

141
00:09:06,920 --> 00:09:10,360
我会马上解释，

142
00:09:10,360 --> 00:09:11,040
这最后就是计算前向的结果

143
00:09:11,040 --> 00:09:13,020
我们会得到delta(3)1

144
00:09:13,020 --> 00:09:14,520
delta3(2)

145
00:09:14,520 --> 00:09:19,982
然后，我们进一步往前，

146
00:09:19,982 --> 00:09:25,464
得到delta(2)1和delta(2)2

147
00:09:25,464 --> 00:09:29,680
现在

148
00:09:29,680 --> 00:09:33,290
看起来就像是又重演前向传播

149
00:09:33,290 --> 00:09:34,230
只不过我们现在反过来做了，这就是我之前所说的

150
00:09:34,230 --> 00:09:37,490
我们来看看最后我们如何得到delta(2)2

151
00:09:37,490 --> 00:09:40,442
所以我们得到delta(2)2

152
00:09:40,442 --> 00:09:45,060
和前向传播类似，

153
00:09:45,060 --> 00:09:47,590
这个权值，我用青色来表示

154
00:09:47,590 --> 00:09:54,040
加入它是theta(2)12

155
00:09:54,040 --> 00:09:57,280
然后，我用红色来高亮

156
00:09:57,280 --> 00:10:02,990
这个我们说是theta(2) 22

157
00:10:02,990 --> 00:10:06,690
如果，我们来看delta(2)2

158
00:10:06,690 --> 00:10:09,440
如何计算。

159
00:10:09,440 --> 00:10:12,040
结果，我们发现

160
00:10:12,040 --> 00:10:19,080
我们就把这个值乘以它权值，并加上这个值乘以权值

161
00:10:19,080 --> 00:10:23,450
所以，就是一个加权求和

162
00:10:23,450 --> 00:10:25,590
权值是每一条边的强度

163
00:10:25,590 --> 00:10:31,351
所以，我们来看delta(2)2

164
00:10:31,351 --> 00:10:38,100
theta(2)12是delta(3)1

165
00:10:38,100 --> 00:10:41,533
加上 我们红色标注的东西

166
00:10:41,533 --> 00:10:46,700
theta(2)2乘以delta(3)2

167
00:10:46,700 --> 00:10:50,440
所以，这个红色值乘以这个值

168
00:10:50,440 --> 00:10:53,530
加上品红色的权值

169
00:10:53,530 --> 00:10:56,880
恩，

170
00:10:56,880 --> 00:10:59,980
另一个例子，我们来看这个值怎么求

171
00:10:59,980 --> 00:11:01,300
如果得到呢

172
00:11:01,300 --> 00:11:03,531
恩，一样的步骤

173
00:11:03,531 --> 00:11:08,224
如果这个权值，我用绿色来描述

174
00:11:08,224 --> 00:11:12,826
它等于delta(3)12

175
00:11:12,826 --> 00:11:20,282
然后我们有delta(3)2将等于它的绿色权值

176
00:11:20,282 --> 00:11:24,515
theta(3)12乘以delta(4)1

177
00:11:24,515 --> 00:11:28,950
顺便一提，我只写隐层单元

178
00:11:28,950 --> 00:11:33,640
忽略了偏置单元

179
00:11:33,640 --> 00:11:36,660
这要看你如何定义算法

180
00:11:36,660 --> 00:11:40,500
或者你如何应用

181
00:11:40,500 --> 00:11:44,980
你也可能要用这些单元

182
00:11:44,980 --> 00:11:48,700
这些偏置单元总是为1

183
00:11:48,700 --> 00:11:51,190
所以他们就是1，我们不会改变他们

184
00:11:51,190 --> 00:11:53,790
所以，要看你的应用思路

185
00:11:53,790 --> 00:11:55,380
以及使用方法

186
00:11:55,380 --> 00:11:57,710
我们计算完了这些值

187
00:11:57,710 --> 00:11:59,760
我们扔掉它，

188
00:11:59,760 --> 00:12:03,910
因为我们最后得到的不过是

189
00:12:03,910 --> 00:12:05,710
计算导数的一个部分

190
00:12:05,710 --> 00:12:08,790
希望这就可以给一个更好的直观体会

191
00:12:08,790 --> 00:12:12,500
关于反向传播算法

192
00:12:12,500 --> 00:12:14,760
如果仍然感觉很迷茫，

193
00:12:14,760 --> 00:12:19,658
像是黑箱，在下一个视频

194
00:12:19,658 --> 00:12:23,290
我会再把他们总结起来

195
00:12:23,290 --> 00:12:27,690
但，这是一个很难讲解的算法

196
00:12:27,690 --> 00:12:29,540
难以可视化

197
00:12:29,540 --> 00:12:31,260
但，

198
00:12:31,260 --> 00:12:35,480
幸运的是很多人都在成功使用它

199
00:12:35,480 --> 00:12:40,130
如果你使用这个算法

200
00:12:40,130 --> 00:12:43,580
它将是非常有效的，尽管它内部的机制很难可视化。