Redes neurais são um dos mais poderosos algoritmos de aprendizagem que temos atualmente. Neste e em alguns dos próximos vídeos, gostaria de começar falando sobre um algoritmo de aprendizagem para ajustar os parâmetros de uma rede neural dado um conjunto de treinamento. Tal como na discussão da maioria dos algoritmos de aprendizagem, vamos iniciar falando sobre a função de custo para ajustar os parâmetros da rede. Vou focar na aplicação de redes neurais nos problemas de classificação. Então, suponha que temos uma rede como a exibida à esquerda. E suponha que temos um  conjunto de treinamento como este, sendo os pares x(i), y(i) do exemplo de treinamento M. Vou usar L maiúscula para denotar o número total de camadas nesta rede. Então, para a rede mostrada à esquerda, teríamos L maiúscula igual a 4. Vou usar o S subscrito de L para denotar o número de unidades, que é o número de neurônios. Sem contar a unidade de viés em sua camada L da rede. Então, por exemplo, temos o S um, que é igual ali, igual a três unidades, S dois em meu exemplo tem cinco unidades. E a camada resultante S quatro, que também é igual a sL, L maiúscula é igual a quatro. A camada resultante em meu exemplo, tem quatro unidades. Vamos considerar dois tipos de problemas de classificação. O primeiro é a classificação Binária, onde os rótulos y podem ser ou 0 ou 1. Neste caso, teremos 1 unidade resultante, então esta unidade da Rede Neural no topo tem 4 unidades resultantes, mas se tivéssemos classificação binária, teríamos apenas uma unidade resultante que computa h(x). E o resultado da rede neural seria h(x) tendo um número real. E neste caso, o número de unidades resultantes, s L, onde L é, de novo, o índice da camada final. Portanto este é o número de camadas que temos na rede então o número de unidades que temos na camada resultante será igual a 1. Neste caso, para simplificar a notação mais adiante, eu também vou definir K = 1 então você pode pensar em K também denotando o número de unidades da camada resultante. No segundo tipo de problema de classificação consideraremos que vamos ter problemas de classificação de classe múltipla onde poderemos ter K classes distintas. Então nosso exemplo inicial tem sua representação para y se tivermos 4 classes, e neste caso teremos um K maiúsculo de unidades resultantes e nossa hipótese ou vetores resultantes que possuem K dimensões. E o número unidades resultantes será igual a K. E geralmente teremos K maior ou igual a 3 neste caso, porque se tivermos duas classes, então não precisamos usar o método um versus todos. Usamos o método um versus todos apenas se tivermos um K maior ou igual a V classes, então, tendo apenas duas classes nós precisaremos usar apenas uma unidade resultante. Agora, vamos definir a função custo da nossa rede neural. A função custo que usamos para rede neural será uma generalização da que usamos para regressão logística. Para regressão logística costumamos minimizar a função custo J(teta) que tem menos 1/m desta função custo e então somar este extra termo de regularização aqui, onde ele era uma somatória de J = 1 até n, porque não regularizamos o termo de viés θ₀. Para uma rede neural, nossa função custo será uma generalização desta. Onde ao invés de termos basicamente apenas um, que é a unidade resultante de compressão, podemos ter K delas, ao invés. Aqui está nossa função de custo. Nossa nova rede agora retorna vetores em R K
onde R pode ser igual a 1 se tivermos um problema de classificação binária. Usarei a notação h(x) indexada de i subscritor para denotar o i-ésimo resultado. Isto é, h(x) é um vetor de dimensão k, logo o subscritor i apenas seleciona o resultado do i-ésimo elemento do vetor que é retornado pela minha rede neural. Minha função J(θ) será agora como segue: -1 sobre M da somatória de um termo semelhante ao que temos para regressão logística, exceto que temos que somar de K igual a 1 até K. Esta somatória é basicamente uma soma sobre o meu resultante K. Uma unidade. Então, se tivermos quatro unidades, isto é, se a camada final da minha rede neural tem quatro unidades resultantes, então esta é a soma de k igual a um até quatro para basicamente a função de custo do algoritmo de regressão logística, mas, somando esta função de custo em cada uma das minhas quatro unidades resultantes. E assim, note que particularmente isso se aplica para Yk Hk, porque estamos basicamente falando de "K sobrescrito " unidades, e comparando com o valor de Yk o qual é um dos vetores dizendo qual seria o custo. E finalmente, o segundo termo aqui é o termo de regularização, semelhante ao que temos para regressão logística. Este termo da somatória parece realmente complicado, mas, tudo que ele faz é somar várias vezes os termos teta J i l para todos os valores de i, de j e de l. Exceto que não somamos os termos correspondentes destes valores de viés como se tivéssemos regressão logística. Completamente, não somamos os termos correspondentes onde i é igual a 0. Isso porque quando computamos a ativação de um neurônio, temos termos como esses. Teta i 0. Mais teta i 1, x 1 mais ... assim por diante. Onde eu acho que pondo um dois aqui, este é o primeiro acesso ali. E então os valores com zero ali, que corresponde a algo que multiplicado com um x0 ou um a0. Então isso é um tipo bem parecido com a unidade de viés e por analogia com o que fizemos em regressão logística, não queremos somar sobre aqueles termos nosso termo de regularização porque não queremos regularizá-los então atribua o valor zero para eles. Mas isso é só uma convenção possível, e mesmo se você for somar sobre i igual a 0 até sL, vai funcionar quase a mesma coisa e não fará uma grande diferença. Mas talvez esta convenção de não regularizar o termo de viés seja um pouco mais comumente usada. Então, esta é a função de custo que usaremos para nossa rede neural. No próximo vídeo, começaremos a falar sobre um algoritmo para tentar otimizar a função custo.