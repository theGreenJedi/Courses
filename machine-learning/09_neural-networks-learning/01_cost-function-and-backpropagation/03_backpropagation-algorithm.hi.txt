पिछले विडीओ में, हमने बात की न्यूरल नेटवर्क के एक कॉस्ट फ़ंक्शन के बारे में. इस विडीओ में, चलिए बात करते हैं एक अल्गोरिद्म के बारे में, कॉस्ट फ़ंक्शन को मिनिमाईज़ / न्यूनतम करने के लिए. विशेषत:, हम बात करेंगे बैक प्रॉपगेशन अल्गोरिद्म की. यह है कोस्ट फ़ंक्शन जो हमने लिखा था पिछले विडीओ में. क्या करना चाहेंगे हम कि कोशिश करेंगे जानने कि पेरमिटर्स थेटा न्यूनतम करने के लिए जे ऑफ़ थीटा. करने के लिए प्रयोग ग्रेडीयंट डिसेंट का या कोई एक एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म. हमें क्या करने की आवश्यकता है तब कि कोड लिखें जो ले सकता है इस इनपुट पेरामिटर थीटा को और कम्प्यूट करता है जे ऑफ़ थीटा तथा इन पारशियल डेरिवेटिव टर्म्ज़ को. याद करें, कि पेरमिटर्स इन चीजों के न्यूरल नेटवर्क में थीटा सूपर स्क्रिप्ट एल सब स्क्रिप्ट आइ जे, वह है रियल नंबर तथा इसलिए, ये पार्शल डेरिवेटिव टर्म्ज़ हैं जो हमें कम्प्यूट करनी हैं. कम्प्यूट करने के लिए कॉस्ट फ़ंक्शन जे ऑफ थीटा, हम सिर्फ़ इस्तेमाल करते हैं यह फ़ॉर्म्युला उपेर यहाँ और इसलिए, मैं करना चाहता हूँ इस विडीओ के काफ़ी हिस्से में यह है कि केंद्रित करूँ बात करने पर कि कैसे हम कम्प्यूट कर सकते हैं इन पार्शल डेरिवेटिव टर्म्ज़ को. चलिए शुरू करते हैं बात करते हुए उस केस की जब हमारे पास है केवल एक ट्रेनिंग इग्ज़ैम्पल, अत: कल्पना कीजिए, अगर आप चाहें कि हमारा पूरा ट्रेनिंग सेट बना है केवल एक ट्रेनिंग इग्ज़ैम्पल से जो है जोड़ा एक्स वाय. मैं नहीं लिखूँगा एक्स 1 वाय 1 सिर्फ़ लिखूँगा यह. लिखते हैं एक ट्रेनिंग इग्ज़ैम्पल को एक्स वाय की तरह और धीरे धीरे जाते हैं कैल्क्युलेशन्स के क्रम से जो हम करेंगे इस एक ट्रेनिंग इग्ज़ैम्पल के साथ. पहली चीज हम करते हैं कि हम अप्लाई करते हैं फ़ॉर्वर्ड प्रॉपगेशन कम्प्यूट करने के लिए कि क्या एक हायपॉथिसस वास्तव में आउट्पुट देती है दी होने पर इनपुट. वस्तुत:, हमने रेफ़र किया था ए (1) को ऐक्टिवेशन वैल्यूज़ की तरह इस पहली लेअर की जो वहाँ इनपुट थी. अत:, मैं उसको सेट करूँगा एक्स पर और तब हम कम्प्यूट करेंगे ज़ी (2) बराबर थीटा(1) ए(1) और ए(2) बराबर है जी, सिग्मोईड ऐक्टिवेशन फ़ंक्शन अप्लाई करने पर ज़ी(2) को और यह हमें देगा हमारे ऐक्टिवेशनज़ पहली बीच की / मिडल लेअर के लिए. वह है नेटवर्क की लेअर दो के लिए और हम भी जोड़ते हैं बाइयस टर्म्ज़. आगे हम अप्लाई करते हैं 2 और स्टेप्स इस फ़ॉर्वर्ड प्रॉपगेशन के कम्प्यूट करने के लिए ए(3) और ए(4) जो वे भी हैं अप्वर्ड्ज़ हायपॉथिसस एच ऑफ़ एक्स के. तो, यह है हमारा वेक्टराइज्ड इम्प्लमेंटेशन फ़ॉर्वर्ड प्रॉपगेशन का. और यह हमें कम्प्यूट करने देता है ऐक्टिवेशन वैल्यूज़ सारे न्यूरोंस के लिए हमारे न्यूरल नेटवर्क में. आगे, कम्प्यूट करने के लिए डिरिवटिव्ज़, हम प्रयोग करेंगे एक अल्गोरिद्म जिसे कहते हैं बैक प्रॉपगेशन. अनुभव / इंटूइशन बैक प्रॉपगेशन अल्गोरिद्म की है कि प्रत्येक नोड के लिए हम कम्प्यूट करेंगे टर्म डेल्टा सूपर स्क्रिप्ट एल सब स्क्रिप्ट जे जो किसी प्रकार दर्शाएगा एरर नोड जे की लेअर एल में. अत:, याद करें कि एक सूपर स्क्रिप्ट एल सब स्क्रिप्ट जे जो ऐक्टिवेशन करती है नोड जे का लेअर एल में और इसलिए, यह डेल्टा टर्म है एक तरह से जो कैप्चर करेंगी हमारी एरर ऐक्टिवेशन में उस न्यूरल ड्यूओ के. तो, कैसे हम आशा करते कि ऐक्टिवेशन उस नोड की थोड़ी अलग होती. वस्तुत:, लेने पर इग्ज़ैम्पल न्यूरल नेटवर्क जो हमारे पास है दाईं तरफ़ जिसमें है चार लेअर्ज़. और इसलिए कैपिटल एल बराबर है 4 के. प्रत्येक आउट्पुट यूनिट के लिए, हम कम्प्यूट करेंगे यह डेल्टा टर्म. अत: , डेल्टा जे यूनिट के लिए चौथी लेअर में बराबर है सिर्फ़ ऐक्टिवेशन उस यूनिट का माइनस जो थी वास्तविक वैल्यू ओब्ज़र्व्ड़ इन हमारे ट्रेनिंग इग्ज़ैम्पल मैं. अत: यह टर्म यहाँ भी लिखी जा सकती है एच ऑफ़ एक्स सब स्क्रिप्ट जे, सही. इसलिए यह डेल्टा टर्म सिर्फ़ अंतर है उसमें कि क्या है एक हायपॉथिसस की आउट्पुट और क्या थी वैल्यू वाय की हमारे ट्रेनिंग सेट में जबकि वाय सब स्क्रिप्ट जे है जे एथ एलेमेंट वेक्टर वैल्यू वाय का हमारे लेबल्ड ट्रेनिंग सेट में. और वैसे तो, यदि आप सोचें डेल्टा ए और वाय वेक्टर्ज़ की तरह तब आप ले सकते हैं उनको भी और बना सकते हैं एक वेक्टराइज्ड इम्प्लमेंटेशन इसका, जो सिर्फ़ है डेल्टा 4 सेट होता है ए 4 माइनस वाय. जहाँ यहाँ, प्रत्येक ये डेल्टा 4 ए 4 और वाय, प्रत्येक इनमें से हैं एक वेक्टर जिसकी डिमेन्शन है बराबर आउट्पुट यूनिट्स की संख्या के हमारे नेटवर्क में. तो हमने अभी कम्प्यूट किया है एरर टर्म का डेल्टा 4 हमारे नेटवर्क के लिए. क्या करेंगे हम आगे कि कम्प्यूट करेंगे डेल्टा टर्म पहले की लेअर्स के लिए हमारे नेटवर्क में. यह है फ़ॉर्म्युला कम्प्यूट करने के लिए डेल्टा 3 डेल्टा 3 बराबर है थीटा 3 ट्रांसपोज़ टाइम्ज़ डेल्टा 4. और यह डॉट टाइम्ज़, यह एलेमेंट बाय का मल्टिप्लिकेशन ऑपरेशन है जो हम जानते हैं मैटलैब से. अत: थीटा 3 ट्रान्स्पोज़ डेल्टा 4, वह एक वेक्टर है; जी प्राइम ज़ी 3 वह भी वेक्टर है और इसलिए डॉट टाइम्ज़ है एलेमेंट वाय के मल्टिप्लिकेशन में इन दोनो वेक्टर्ज़ का. यह टर्म जी प्राइम ज़ी 3 की, वह नियमानुसार है वास्तव में डेरिवेटिव ऐक्टिवेशन का फ़ंक्शन जी का जो इवैल्यूएट किया है इनपुट वैल्यूज़ पर, जो दी गईं हैं ज़ी 3 द्वारा. अगर आप कैल्क्युलुस जानते हैं, आप स्वयं कोशिश कर सकते हैं इसे हल करने की और देख सकते हैं कि आप इसे सरलीकृत कर सकते हैं उसी उत्तर पर जो मुझे मिला है. लेकिन मैं आपको बताऊँगा कि व्यावहारिक रूप में उसका क्या मतलब है. क्या करते हैं आप कि कम्प्यूट करते हैं है यह जी प्राइम, ये डेरिवेटिव टर्म्ज़ है सिर्फ़ ए 3 डॉट टाइम्ज़ 1 माइनस ए 3 जहाँ ए 3 है वेक्टर ऐक्टिवेशन्स का. 1 है वेक्टर वन्स का और ए 3 है फिर से ऐक्टिवेशन वेक्टर ऐक्टिवेशन वैल्यूज़ का उस लेअर की. आगे फिर आप अप्लाई करते हैं एक वैसा ही फ़ॉर्म्युला कम्प्यूट करने के लिए डेल्टा 2 जहाँ फिर से वह किया जा सकता है कम्प्यूट वैसे ही फ़ॉर्म्युला से. सिर्फ़ अब यह है ए 2 वैसे ही और मैं प्रमाणित कर सकता हूँ यहाँ लेकिन आप वास्तव में, यह सम्भव है प्रमाणित करना इसे यदि आप जानते हैं कैल्क्युलुस कि यह इक्स्प्रेशन बराबर है गणितीय रूप में, डेरिवेटिव जी फ़ंक्शन के जो है ऐक्टिवेशन फ़ंक्शन, जिसे मैं डिनोट कर रहा हूँ ज़ी प्राइम से. और अंत में, बस इतना ही और यहाँ कोई डेल्टा 1 टर्म नहीं है, क्योंकि पहली लेअर कॉरेस्पॉंड करती है इनपुट लेअर को और वह है सिर्फ़ फ़ीचर्ज़ जो हमने देखे अपने ट्रेनिंग सेट्स में, इसलिए उसमें कोईं एरर नहीं है उसके साथ. ऐसा नहीं है, आप जानते हैं, कि हम नहीं वास्तव में चाहते बदलना उन वैल्यूज़ को. और इसलिए हमारे पास है डेल्टा टर्म्ज़ सिर्फ़ लेअर्ज़ 2 , 3 के लिए इस उदाहरण में. यह नाम बैक प्रॉपगेशन आता है उस तथ्य से कि हम शुरू करते हैं कम्प्यूट करना डेल्टा टर्म आउट्पुट लेअर से और तब हम जाते हैं पीछे एक लेअर और कम्प्यूटर करते हैं डेल्टा टर्म्ज़ तीसरी हिडन लेअर के लिए और तब हम जाते हैं पीछे एक और स्टेप कम्प्यूट करने के लिए डेल्टा 2 और इस तरह, हम एक प्रकार से बैक प्रापगेट कर रहे हैं एरर को आउट्पुट लेअर से लेअर 3 पर वहाँ से लेअर दो पर और इसलिए इसका नाम है बैक प्रॉपगेशन. अंत में, डेरिवेशन है काफ़ी कॉम्प्लिकेटेड, काफ़ी पेचीदा लेकिन यदि आप सिर्फ़ करते हैं ये कुछ स्टेप्स कॉम्प्यूटेशन के तो यह सम्भव है प्रमाणित करना जबकि यह स्पष्टत: कुछ कॉम्प्लिकेटेड / पेचीदा गणितीय प्रूफ़ है. यह सम्भव हैं प्रमाणित करना कि यदि आप अनदेखा करते हैं रेग्युलराइज़ेशन तब पार्शल डेरिवेटिव टर्म्ज़ जो आपको चाहिएँ वे वास्तव में सटीक मिल जाती हैं ऐक्टिवेशन्स और इन डेल्टा टर्म्ज़ से. यह है अनदेखा करना लैम्डा को या दूसरे शब्दों में रेग्यूलराईज़ेशन टर्म लैम्डा होगी बराबर 0 के. हम बाद में यह डिटेल जोड़ देंगे रेग्यूलराईज़ेशन टर्म के बारे में, लेकिन करने से बैक प्रॉपगेशन और कम्प्यूट करने से ये डेल्टा टर्म्ज़, आप कर सकते हैं, आप जानते हैं, काफ़ी जल्दी से कम्प्यूट ये पार्शल डेरिवेटिव टर्म्ज़ आपके सब पेरमिटर्स के लिए. तो यह है काफ़ी सारी विस्तृत जानकारी. चलिए सब लेते हैं और रखते हैं सब एक साथ बात करने के लिए कि कैसे इम्प्लमेंट करते हैं बैक प्रॉपगेशन कम्प्यूट करने के लिए डेरिवेटिव्स विद रिस्पेक्ट टु पेरमिटर्स. और उस केस में जब हमारे पास है एक बड़ा ट्रेनिंग सेट, न कि केवल एक ट्रेनिंग सेट एक इग्ज़ैम्पल का, यह करते हैं हम. मान लीजिए हमारे पास है एक ट्रेनिंग सेट एम इग्ज़ैम्पल्ज़ का उस जैसा जो यहाँ दिखाया है. पहली चीज हम करेंगे कि हम सेट करेंगे ये डेल्टा एल सब स्क्रिप्ट आई जे. तो यह तिकोना चिन्ह? वह है वास्तव में कैपिटल ग्रीक ऐल्फ़बेट डेल्टा. चिन्ह जो हमारे पास पिछली स्लाइड पर था वह था लोअर केस डेल्टा. तो त्रिकोण है कैपिटल डेल्टा. हम इसे सेट करेंगे बराबर ज़ीरो के एल आइ जे की सारी वैल्यूज़ के लिए. अंत में, यह कैपिटल डेल्टा एल आइ जे इस्तेमाल होगा कम्प्यूट करने के लिए पार्शल डेरिवेटिव टर्म, पार्शल डेरिवेटिव विद रिस्पेक्ट टु एल आइ जे ऑफ़ जे ऑफ़ थीटा. अत: जैसे कि हम देखेंगे एक सेकंड में, ये डेल्टाज़ होंगे प्रयोग संकलनकर्ता / एक्कमुलेटर की तरह जो जो धीरे धीरे चीज़ों को जोड़ेगा करने के लिए कम्प्यूट ये पार्शल डेरिवेटिव्स. आगे, हम हमारे ट्रेनिंग सेट में से लूप रूप में जाएँगे. अत: हम कहेंगे आइ बराबर है 1 से एम तक और इसलिए आइटरेशन संख्या आइ के लिए हम काम करेंगे ट्रेनिंग इग्ज़ैम्पल एक्स आइ, वाय आइ के साथ. अत: पहला काम जो हम करेंगे वह है सेट करना ए 1 को जो है ऐक्टिवेशन्स इनपुट लेअर की, उसे सेट करना बराबर एक्स1 करेंगे इनपुट की तरह हमारे इग्ज़ैम्पल आइ के लिए, और तब हम करेंगे फ़ॉर्वर्ड प्रॉपगेशन कम्प्यूट करने के लिए ऐक्टिवेशन्ज़ लेअर दो के लिए, लेअर तीन और ऐसे आगे भी अंतिम लेअर तक, लेअर कैपिटल एल तक. फिर, हम करेंगे प्रयोग आउट्पुट लेबल वाय आइ इस निश्चित इग्ज़ैम्पल से जिसको हम देख रहे हैं कम्प्यूट करने के लिए एरर टर्म डेल्टा एल के लिए आउट्पुट लेअर के लिए. अत: डेल्टा एल है जो एक हायपॉथिसस आउट्पुट माइनस जो टार्गट लेबल था. और तब हम करेंगे प्रयोग बैक प्रॉपगेशन अल्गोरिद्म का करने के लिए कम्प्यूट डेल्टा एल माइनस 1, डेल्टा एल माइनस 2, और ऐसे ही डेल्टा 2 तक और एक बार फिर अब यहाँ डेल्टा 1 नहीं है क्योंकि हम नहीं जोड़ते एक एरर टर्म इनपुट लेअर के साथ. और अंत में, हम करेंगे प्रयोग इन कैपिटल डेल्टा टर्म्ज़ का संकलित करने के लिए ये पार्शल डेरिवेटिव टर्म्ज़ जो हमने लिखीं थी पिछली लाइन पर. और वैसे तो, यदि आप देखें इस इक्स्प्रेशन को, इसको भी वेक्टराइज़ करना सम्भव है. वास्तव में, अगर आप सोचें डेल्टा आइ जे को एक मेट्रिक्स की तरह, जिसको इंडेक्स किया है सब स्क्रिप्ट आइ जे से. तब, यदि डेल्टा एल है एक मेट्रिक्स हम दोबारा लिख सकते हैं इसे डेल्टा एल की तरह, यह हो जाता है अपडेट डेल्टा एल जमा लोअर केस डेल्टा एल जमा एक टाइम्ज़ ए एल ट्रान्स्पोज़. तो यह है एक वेक्टराइज्ड इम्प्लमेंटेशन इसका जो अपने आप करता है एक अप्डेट सारी वैल्यूज़ आइ और जे के लिए. अंत में, होने के बाद एक्सेक्यूट बॉडी फ़ोर-लूप की हम तब बाहर आते हैं फ़ोर-लूप से और हम कम्प्यूट करते हैं निम्नलिखित. हम कम्प्यूट करते हैं कैपिटल डी इस प्रकार और हमारे पास है दो अलग केसेज़ जे बराबर है ज़ीरो के लिए और जे नहीं है बराबर ज़ीरो के लिए. जे बराबर है ज़ीरो के केस कॉरेस्पॉंड करता हैं बाइयस टर्म को इसलिए जब जे बराबर है ज़ीरो के, वह कारण है कि हम मिस कर रहे है एक अतिरिक्त रेग्यूलराईज़ेशन टर्म. अत: में, यद्यपि, फ़ोर्मल प्रूफ़ काफ़ी पेचीदा है आप क्या दिखा सकते हैं कि एक बार आपने कम्प्यूट कर लिए ये डी टर्म्ज़, वह ही है पार्शल डेरिवेटिव कॉस्ट फ़ंक्शन का विद रिस्पेक्ट टु प्रत्येक आपका पेरामिटर और इसलिए आप कर सकते हैं प्रयोग उन्हें ग्रेडीयंट डिसेंट में या किसी एक एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म में. तो यह है बैक प्रॉपगेशन अल्गोरिद्म और कैसे आप कम्प्यूट करते हैं डेरिवेटिवज़ आपके कॉस्ट फ़ंक्शन के एक न्यूरल नेटवर्क के लिए. मैं जानता हूँ कि ऐसा लग रहा है कि यह एक बहुत विस्तृत जानकारी थी और यह था बहुत सारे स्टेप्स एक साथ. लेकिन दोनो प्रोग्रामिंग असायन्मेंटस राइट आउट में और बाद में इस विडीओ में, हम देंगे आपको एक सार इसका ताकि आपके पास हो सके सारे हिस्से अल्गोरिद्म के एक साथ ताकि आप जान पाएँ कि आपको वास्तव में क्या चाहिए इम्प्लमेंट करने के लिए यदि आप करना चाहते है इम्प्लमेंट बैक प्रॉपगेशन कम्प्यूट करने के लिए डेरिवेटिव आपके न्यूरल नेटवर्क के कॉस्ट फ़ंक्शन विद रिस्पेक्ट टू उन पेरामिटर्स के.