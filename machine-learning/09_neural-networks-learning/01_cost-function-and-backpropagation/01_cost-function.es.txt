Las redes neuronales son uno de los algoritmos de aprendizaje más potentes que tenemos hoy. En este video y en los siguientes, me gustaría comenzar a hablar sobre un algoritmo de aprendizaje para establecer los parámetros de la red neuronal dada en el paquete de entrenamiento. En cuanto a la discusión de la mayoría de los algoritmos de aprendizaje, vamos a comenzar hablando de la función de costo para ajustar los parámetros de la red. Voy a enfocarme en la aplicación de las redes neuronales para los problemas de clasificación. Entonces, supongamos que tenemos una red como la que se muestra en la izquierda. Y supongamos que tenemos un paquete de entrenamiento como este con pares xi y yi de ejemplos de entrenamiento m. Voy a usar la L mayúscula para denotar el número total de capas en esta red. Así, para la red que se muestra a la izquierda, tendríamos que L mayúscula es igual a 4. Y voy a usar s subíndice l para denotar el número de unidades, que es un número de neuronas, sin contar la unidad de oscilación en la capa L de la red. Entonces, por ejemplo, tendríamos un S1 que es la capa de entrada igual a la unidad S3, S2 en mi ejemplo son cinco unidades. Y la capa de salida S4, Que también es igual SL, porque la L mayúscula es igual a cuatro. La capa de salida en mi ejemplo de la izquierda tiene cuatro unidades. Vamos a considerar dos tipos de problemas de clasificación. La primera es la clasificación binaria, en donde las etiquetas "y" son cero o uno. En este caso, tendríamos una unidad de salida. Entonces, esta red neuronal en la parte superior tiene cuatro unidades de salida, pero si tuviéramos una clasificación binaria, sólo tendríamos una unidad de salida que calculará h de x. Y las salidas de la red neuronal serían h de x, que sería un número real. Y, en este caso, el número de unidades de salida, SL, donde L nuevamente es el índice de la capa final, porque es el número de capas que tenemos en la red. Entonces, el número de unidades que tenemos en la capa de salida va a ser igual a uno. En este caso, para simplificar la notación más tarde, también voy a establecer que k es igual a 1. Entonces, puedes considerar que k también denota el número de unidades en la capa de salida. El segundo problema de tipo de clasificación que consideraremos será el problema de clasificación multiclase, en el que podemos tener k distintas clases. Entonces, el ejemplo anterior tenía esta representación para "y" cuando teníamos cuatro clases y, en este caso, tendremos K mayúscula unidades de salida y nuestras hipótesis mostrarán vectores de salida que tienen K dimensiones. Y el número de unidades de salida serán igual a K. Por lo general, tendremos una K mayor o igual a tres en este caso, porque si tuviéramos dos clases, entonces no necesitaríamos utilizar el método de todos contra uno. Sólo debemos usar el método todos contra uno si tenemos una K mayor o igual a tres clases. Entonces, como sólo tenemos dos clases, sólo necesitamos utilizar una unidad de salida. Ahora, vamos a definir la función de costo para nuestra función de costo para nuestra red neuronal. La función de costo que usaremos para la red neuronal será una generalización de la que usamos para la regresión logística. 
Para la regresión logística solíamos minimizar la función de costo j de «theta» que era menos 1 sobre m de esta función de costo, y después se suma este término de regularización adicional, que era la suma de j igual a 1 hasta n, porque no regularizamos el término de oscilación «theta» cero. Para una red neuronal, nuestra función de costo va a ser una generalización de ésta. Donde, en lugar de tener básicamente sólo una unidad de salida de regresión logística, podemos tener K de éstas. Entonces, esta es nuestra función de costo. Ahora la red neuronal muestra vectores en RK en donde K podría ser igual a 1 si tenemos el problema de clasificación binaria. Voy a utilizar la notación h de x subíndice i, para denotar la salida “i-nésima”. Eso es h de x es un vector de K dimensiones. Y así, el subíndice i sólo selecciona el elemento “i-nésimo” del vector mostrado por mi red neuronal. Mi función de costo, j de «theta» ahora va a ser la siguiente, es menos uno sobre m de la suma de un término similar a lo que teníamos en la regresión logística. 
Excepto que tenemos que esta suma de K es igual a uno hasta K. La sumatoria es básicamente una suma sobre mi unidad de salida K. Entonces, si tengo cuatro unidades superiores, esto es que la capa final de mi red neuronal tiene cuatro unidades de salida, entonces, esta es una suma desde K igual a uno hasta cuatro de básicamente la función de costo de los algoritmos de regresión logística, pero sumando esta función de costo sobre cada una de mis cuatro unidades de salida. Entonces, notarás que esto aplica particularmente a YK, HK, porque básicamente estamos tomando la unidad superior K y comparándola con el valor de YK, que, como sabes, es el de esos vectores para indicar cuál es la causa. Y, por último, el segundo término aquí es el término de regularización similar a lo que teníamos para la regresión logística. Estas sumatorias de términos de ven muy complicadas, y siempre es complicado hacer una suma con estos términos, «theta» j  i  l para todos los valores de  i  j y l. 
Excepto que no sumamos los términos correspondiente a estos valores de oscilación como lo hicimos para la progresión logística. En concreto, no sumamos los términos correspondientes a cuando i es igual a cero. Esto se debe a que cuando calculamos la activación de la neurona, tenemos términos como estos, como sabes «theta», i 0 más «theta», i 1, x1 más, y así sucesivamente, donde supongo que podríamos tener un 2 si esta es la primera capa oculta, de forma que los valores con el 0 ahí, que corresponden a algo que se multiplica en un x0 o un a0 y así, esto es como una unidad de oscilación y, por analogía con lo que hicimos para la progresión logística, no sumamos esos términos en nuestro término de regularización porque no queremos regularizarlos y encadenar los valores a 0. Pero esta sólo es una convención posible, e incluso si fueras a sumar sobre, tu sabes, i es igual a 0 hasta SL, funcionará más o menos igual y no haría una gran diferencia. Pero tal vez esta convención de no regularizar el término de oscilación sólo es ligeramente más común. Entonces, esta es la función de costo que vamos a utilizar para llenar tu propia red. En el siguiente video empezaremos a hablar sobre un algoritmo para intentar optimizar la función de costo.