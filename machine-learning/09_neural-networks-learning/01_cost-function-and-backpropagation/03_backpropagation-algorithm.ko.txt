이전 비디오에서는 우리는 신경망의 cost function을 배웠습니다. 이번 비디오에서는 이 cost function을 최소화 하는 알고리즘에 대해 말씀드리겠습니다. 이른바 역전파(back propagation) 알고리즘 입니다. 이것은 이전 비디오에서 설명드린 cost function 입니다. 우리는 J(Θ)를 최소화 하기 위해 매개변수 Θ를 찾아야 하고, 그러기 위해서는 경사도 하강법(gradient descent)이나 다른 최적화된 알고리즘을 사용해야 합니다. 그러므로 우리가 해야 할 것은 입력 매개변수 Θ를 이용해서 J(Θ)를 계산하고 이들을 편미분하는 코드를 쓰는 것입니다. 이들 신경망의 매개변수 Θij^(l)는 실수(R)인 것을 기억하세요. 그리고 이들은 우리가 계산해야 하는 편미분 대상입니다. cost function J(Θ)를 계산하기 위해서 위에 적은 공식을 사용하겠습니다. 그리고 이 비디오의 남은 시간에서는 이들 편미분을 어떻게 계산할 수 있는지 설명하는데 초점을 맞추겠습니다. 자, 단 하나의 훈련예제가 있는 케이스 부터 설명을 시작하겠습니다. 전체 훈련세트가 (x,y)로 구성된 하나의 훈련 예제로 되어 있다고 상상해 보세요. 저는 훈련예제를 (x1,y1)가 아닌, (x,y)로 적을 것 입니다. 이 훈련 예제로 수행 할 계산 순서를 살펴 보겠습니다 첫번째로 해야할 일은 입력 x에 대한 가설(=출력)을 계산하기 위해 앞으로 전파시키는 것 입니다. 구체적으로 설명하면, a(1)은 여기서 입력 입력인 첫번째 레이어의 활성값 입니다. a(1)에 x를 입력하고 Θ(1)과 a(1)를 곱해 z(2)를 계산합니다. 그리고 a(2) = g(z(2)) 즉 z(2)에 sigmoid 함수를 적용시킵니다. 이렇게 하면 활성값 a(2)를 얻을 수 있습니다. 이것은 신경망의 두번째 레에어에 존재합니다. 그리고 또 bias a0^(2) = 1 을 추가해야 합니다. 그 다음 우리는 a(3)과 a(4)를 계산하기 위해 이런 순방향 전파를 2번 더 적용해야 합니다. 또한 a(4)는 가설 h(x)와 같습니다. 이것이 벡터화되어 구현된 순방향 전파 알고리즘 입니다. 그리고 이것은 신경망 내 모든 뉴런의 활성화 값을 계산하는 것을 가능하게 해줍니다. 다음으로, 미분을 계산하기 위해 역 전파라고 불리는 알고리즘을 사용할 것입니다. 역 전파 알고리즘을 직관적으로 보면 각 노드에 대해 δj^(l)을 계산하는 것입니다. 여기서 δj^(l)의 의미는 l 레이어의 j노드에 에러가 있는지 나타내는 것입니다. 다시 정리해보면 aj^(l)은 l 레이어에 있는 j유닛의 활성화를 의미하고, δj^(l)은 활성화된 노드 안의 오류를 잡는다는 의미 입니다. 그래서 우리는 노드의 활성화가 약간 다를 수 있기를 바랍니다. 구체적으로, 오른쪽에있는 4 개의 레이어를 갖는 신경망을 예로 들겠습니다. L = 4이고, 각 출력단에 대해서 δj^(l)을 계산하겠습니다. 4번째 레이어에 있는 δj^(4)는 활성 단위 aj^(4)에서 yi를 뺀것과 같고, 여기서 yi의 값은 예제에서 0으로 하겠습니다. 그리고 이 항은 (hΘ(x))j로 쓸수 있습니다. 그렇죠? 이 δ(델타)항은 가설들의 출력항 중 하나와 훈련 세트의 y벡터에서 j번째 요소인 yj 사이에 대한 차이입니다. 그런데 여러분이 이 표현식을 보면 만약 δ, a, y를 벡터로 생각하면, 여러분은 그것들을 가져 와서 벡터화 시켜 구현할 수 있습니다. 그냥 δ^(4) = a^(4) - y 가 됩니다. 여기서 이들 δ^(4), a^(4), y는 각각 신경망의 출력단의 수와 같은 차원(dimension)의 벡터가 됩니다. 그래서 우리는 방금 신경망에서 δ^(4) 에러항을 계산했습니다. 우리가 다음에 할 것은 이전 레이어의 δ을 계산하는 것입니다. 계산 공식은 다음과 같습니다. δ^(3)는 Θ^(3)의 전치행렬에 δ^(4)를 곱하고, ' .* '의 의미는 MATLAB에서 봤던 행렬의 요소끼리의 곱셈입니다. Θ^(3)의 전치행렬과 δ^(4)의 곱은 벡터가 되고, g'(z3)도 벡터입니다. 따라서, ' .* '은 두 벡터들 간의 요소 별 곱셈이 됩니다. g'(z3) 항은 미분된 활성화 함수 g에 입력값 z3가 입력되어 평가 되었습니다. 여러분이 계산법을 안다면, 여러분 스스로 풀어 보시고 제가 얻은것과 같이 간략하게 되는지 보시기 바랍니다. 하지만 지금은 g'의 미분 결과를 알려드리겠습니다. 이 미분항은 a^(3).* (1-a^(3))이 됩니다. 여기서 a^(3)는 활성화 벡터입니다. 1도 [1;1;1;..] 처럼 생긴 벡터이고, 앞에 또 나온 a^(3)도 세번째 레이어의 활성화 값 입니다. 다음으로 여러분은 유사한 공식을 δ^(2)를 계산하는데 적용할 것이고, g'(z2)도 비슷하게 a^(2).* (1-a^(2))로 계산될 것입니다. 그리고 여기서는 하지 않을 것이지만, 여러분이 계산하는 방법을 안다면 이 표현은 미분된 활성화 함수인 g'와 수학적으로 동일하다는 것을 증명할 수 있습니다. 마지막으로 끝입니다. δ^(1)항은 없습니다. 그 이유는 첫번째 레이어는 입력 레이어이기 때문입니다. 그리고 그건 우리 훈련 세트의 feature(=x값) 입니다. 따라서 오류가 있을 수 없습니다. 여러분도 알다시피 우리는 이들 값을 변경하는 것을 원치 않습니다. 즉, 이 예제에서 δ항은 2, 3, 4 레이어에만 존재합니다. 역전파라는 이름은 δ의 계산을 출력단부터 시작해서 세번째 히든 레이어의 δ^(3)의 계산을 거쳐 δ^(2)까지 거꾸로 계산해 나가는 데서 이름 지어졌습니다. 역전파의 결과로 인하여 출력 레이어의 오류는 레이어 3으로 레이어 2로 거꾸로 계산해 나갑니다. 마지막으로 미분은 놀라울 정도로 복잡합니다. 그러나 여러분이 몇 단계의 계산을 수행하면 수학적으로 증명하는 것이 가능합니다. 만약 여러분이 정규화 하는것을 무시한다면 편미분 항은 활성화 항과 δ항으로 주어진 다는 것으로 증명할 수 있습니다. 이것은 람다(λ)를 생략하거나 혹은 정규화 변수 λ값이 0과 같다는 것입니다. 우리는 추후에 정규화 항을 디테일하게 고칠 것 입니다. 역 전파를 수행하고 이러한 델타 항을 계산함으로써 여러분은 모든 변수에 대해 이 편미분 항을 매우 빠르게 계산할 수 있습니다. 자, 여기까지 많은 것을 설명했습니다. 이제는 모든 훈련세트를 가져와서 그것들을 집어넣을 때 어떻게 매개변수에 대한 미분값을 계산하는 역전파를 구현하는 방법에 대해 이야기 해 보겠습니다. 우리는 한 예제만 있는 훈련세트가 아닌 거대한 훈련세트를 가지고 있는 경우 입니다. 자, 여기 우리가 살펴볼 것이 있습니다. 여기 보이는 것처럼 m개 예제로 구성된 훈련 세트를 가지고 있다고 가정해 보세요. 첫번째로 우리가 해야할 것은 델타 Δij^(l)을 정하는 것입니다. 여기 삼각형 기호가 보이시나요? 이것은 실제 그리스의 알파벳 델타입니다. 우리가 지금까지 사용한 δ기호는 소문자 델타 입니다. 그래서 삼각형 Δ은 대문자 델타 입니다. 우리는 이 델타를 모든 l, i, j값에 대해 0과 같게 할 것입니다. 결국엔 이 델타 Δij^(l)는 J(Θ)의 편미분항을 계산하는데 사용될 것입니다. 두번째로 우리가 살펴볼 것은 이들 델타들은 축적자로서 사용될 것입니다. 축적자는 편미분을 계산하기 위해서 값들을 더하는 저장소 입니다. 다음엔 우리는 훈련세트 전체에 대한 루프를 돌 것입니다. i=1부터 m까지 반복하면서 훈련예제 (xi,yi)를 가지고 학습시킬 것 입니다. 따라서 그래서 첫번째 루프에서 우리는 입력 레이어의 활성화된 a(1)에 i번째 훈련예제 x(i)를 대입할 것입니다. 그리고 우리는 레이어2, 레이어3,…, 최종 레이어 L까지의 활성화를 계산하기 위해 순방향 전파를 수행할 것입니다. 그 다음에 출력 레이블인 yi를 에러항인 δ^(L)을 계산하기 위해 위와 같이 사용할 것입니다. 그래서 δ^(L)은 가설의 출력인 a(L)에서 yi를 뺀 값입니다. 그리고 우리는 역전파 알고리즘을 δ^(L-1), δ^(L-2),…, δ^(2)까지 계산하는데 사용합니다. 다시한번 설명하지만, δ^(1)은 입력 레이어 이기 때문에 에러와 관련 없습니다. 마지막으로 우리는 이전 줄에서 적었던 편미분항을 축적하기 위해서 델타 Δij^(l)를 사용할 것입니다. 그런데 여러분이 이 표현식을 보면 이것도 역시 벡터화 시킬 수 있는 걸 알수 있습니다. 구체적으로 설명드리면 Δij를 매트릭스로 생각하고 아래첨자 ij로 요소를 표현한다면 매트릭스 Δ^(l)의 값은 Δ^(l)와 δ^(l+1)과 a(l)의 전치행렬의 곱을 더한 값으로 업데이트 될 것입니다. 이것은 모든 i,j의 값에 대해서 자동으로 업데이트 되는 벡터를 구현한 것입니다. 결국 이 for 루프의 바디 내용을 실행한 후, 루프 밖을 나와서 다음을 계산합니다. 우리는 대문자 D를 다음과 같이 계산합니다. 그리고 j<>0, j=0인 두개의 다른 케이스를 얻습니다. j=0인 케이스는 bias항에 대응됩니다. 그래서 j=0일때, 추가적인 정규화 항이 없는지에 대한 이유입니다. 마지막으로, 공식 증명은 꽤 복잡하지만, 여러분들이 일단 D 항을 계산해보면 이는 각 매개 변수에 대한 cost function의 편미분 이므로, 여러분은 이것들을 경사도하강법이나 다른 고급 알고리즘에서 사용할 수 있습니다. 여기까지 역전파 알고리즘과 어떻게 신경망의 cost function을 미분하는지 설명 드렸습니다. 오늘 다룬 이 내용이 많은 내용을 담고있고, 많은 단계가 서로 얽혀 있다는것을 저도 알고 있습니다. 그러나 프로그래밍 과제와 이 비디오의 뒷부분에서 이 알고리즘에 대한 요약해 드리겠습니다. 그래서 모든 종류의 알고리즘을 함께 다룰 수 있을 것입니다. 그래서 여러분이 신경망의 cost function과 관련된 매개변수를 계산하는 미분을 계산하기 위해 역전파를 구현하는 방법을 정확하게 알수 있게 해드리겠습니다.