1
00:00:00,620 --> 00:00:03,800
이 비디오에서 우리는 비용함수라고 
불리는 것에 대해 배워보겠습니다.

2
00:00:03,800 --> 00:00:07,480
비용함수를 사용하면 주어진 데이터에 가장 가까운 일차함수 그래프를 알아 낼 수 있습니다

3
00:00:10,310 --> 00:00:13,820
선형증가에서, 
우리는 훈련집합에서

4
00:00:13,820 --> 00:00:18,870
제가 저번에 보여드렸던 기호m은 훈련예시의 개수이고, 
그래서 m은 47이다 라는 것을 배웠습니다.

5
00:00:18,870 --> 00:00:20,989
그리고 우리의 가설의 형태에 대해서도요.

6
00:00:22,210 --> 00:00:25,360
우리가 선형함수를 사용해 예측했던 것도 있었습니다.

7
00:00:26,430 --> 00:00:31,240
조금 더 전문용어를 소개해드리자면, 
이 Θ0과 Θ1은

8
00:00:31,240 --> 00:00:37,260
이 Θ0과 Θ1은 모형들의 
파라메터를 나타내고 있습니다.

9
00:00:37,260 --> 00:00:42,560
우리가 이 비디오에서 
얘기하고자 하는 것은

10
00:00:42,560 --> 00:00:47,550
어떻게 이 2개의 파라메터, 
Θ0과 Θ1를 고를 것이냐는 것 입니다.

11
00:00:47,550 --> 00:00:51,100
파라메터 Θ0과 Θ1에서 
다른 선택을 하면

12
00:00:51,100 --> 00:00:55,250
다른 가설, 
다른 가설함수들을 가지게 됩니다.

13
00:00:55,250 --> 00:00:58,170
여러분 중 몇 분은 슬라이드에 나올 내용에 
익숙할 것이라는 것을 알고 있지만,

14
00:00:58,170 --> 00:01:02,110
복습한다는 느낌으로 다시 봐 주시 길 바랍니다.
여기 몇가지 예가 있습니다.

15
00:01:02,110 --> 00:01:05,990
만약 Θ0이 1.5이고 Θ1이 0이라면

16
00:01:05,990 --> 00:01:08,870
가설함수는 이런 식으로 
생기게 됩니다.

17
00:01:10,070 --> 00:01:17,610
가설 함수는 h = 1.5 + 0x, 
즉 정수값 1.5에 수렴합니다.

18
00:01:17,610 --> 00:01:22,533
즉 정수값 1.5에 수렴합니다.

19
00:01:22,533 --> 00:01:26,600
만약 Θ0이 0이고 Θ1은 0.5이면, 
가설은 이런 식으로 생기게 되고,

20
00:01:26,600 --> 00:01:31,420
이 함수는 2, 1 값을 지나가는

21
00:01:31,420 --> 00:01:34,850
h(x)가 됩니다.

22
00:01:34,850 --> 00:01:40,150
hΘ(x)로도 표현할 수 있지만, 
가끔 저는 간단히 하기위해 Θ를 생략합니다.

23
00:01:40,150 --> 00:01:45,570
그래서 h(x)는 이런 식으로 
생긴 0.5x 일차함수가 됩니다

24
00:01:45,570 --> 00:01:49,830
그리고 마지막으로, 
Θ0이 1이고 Θ1은 0.5이면,

25
00:01:49,830 --> 00:01:53,280
우리는 이렇게 생긴 
가설을 갖게 됩니다.

26
00:01:53,280 --> 00:01:59,670
한번 봅시다. 
이 가설은 2, 2값을 지나가게 됩니다.

27
00:01:59,670 --> 00:02:04,640
그래서 이것은 x의 백터이고, 
또는 새로운 hΘx가 됩니다.

28
00:02:04,640 --> 00:02:08,618
hΘx나 이 기호의 약어인 hx든

29
00:02:08,618 --> 00:02:12,095
당신이 원하는 방법으로
기억하면 됩니다.

30
00:02:13,917 --> 00:02:19,330
선형회귀에서, 만약 이런 식으로 제가 표시한 것처럼
 훈련집합을 가지고 있다고 합시다.

31
00:02:19,330 --> 00:02:24,880
우리가 하려는 것은 파라메터 Θ0과 Θ1의 값들을 
이용해서 이러한 직선을 구해,

32
00:02:24,880 --> 00:02:29,960
여기에 그린 이 직선처럼 
직선이 자료와 얼마나

33
00:02:29,960 --> 00:02:33,500
잘 일치하는지를 보는 것입니다.

34
00:02:34,590 --> 00:02:37,190
그래서, 우리가 어떻게 값들, 
Θ0, Θ1이 데이터와 얼마나

35
00:02:37,190 --> 00:02:40,650
잘 일치하는지를 알 수 있을까요?

36
00:02:42,540 --> 00:02:46,460
훈련집합의 예시에서 
파라메터 Θ0과 Θ1을 고르는 것은,

37
00:02:46,460 --> 00:02:51,190
hx, 즉 x값과 같다고 예측한 값은

38
00:02:51,190 --> 00:02:55,730
y값과 가장 비슷하지 않은 값입니다.

39
00:02:55,730 --> 00:02:59,908
그래서 이 훈련집합에서, 
우리는 몇몇 숫자들의 예시를 가지고있고,

40
00:02:59,908 --> 00:03:04,000
x값이 집을 선택하게 되고, 
이것을 통해 우리는 실제로

41
00:03:04,000 --> 00:03:07,350
어떤 가격에 팔리는지에 
대해 알 수 있습니다.

42
00:03:07,350 --> 00:03:11,100
그래서, 파라메터 값을 선택해서,

43
00:03:11,100 --> 00:03:13,830
최소한 훈련집합에서, 
x값이 주어진다면

44
00:03:13,830 --> 00:03:19,040
우리는 y값에 대한 예측을 할 수 있습니다.

45
00:03:19,040 --> 00:03:20,980
이것을 공식화해봅시다.

46
00:03:20,980 --> 00:03:23,700
그렇다면 회귀에서, 
우리가 하려는 것은,

47
00:03:23,700 --> 00:03:27,430
저는 최소화문제를 풀려고 합니다.

48
00:03:27,430 --> 00:03:34,319
그래서 저는 최소값을 Θ0, Θ1이라고 적겠습니다.

49
00:03:34,319 --> 00:03:39,620
그리고 저는 이 값을 작게 만들려고 합니다, 맞죠?

50
00:03:39,620 --> 00:03:42,960
저는 h(x)와 y간의 차이를 작게 만들고 싶습니다.

51
00:03:42,960 --> 00:03:47,770
그리고 저는 가설의 결과값과 
실제 집의 가격의 차이의

52
00:03:47,770 --> 00:03:51,226
차이의 제곱을 최소화할 것입니다.

53
00:03:51,226 --> 00:03:54,600
좋습니다. 
자세하게 보겠습니다.

54
00:03:54,600 --> 00:03:59,328
(x(i), y(i)) 기호가 
i번째 훈련 예시를

55
00:03:59,328 --> 00:04:02,380
나타내는 것을 기억하실 겁니다.

56
00:04:02,380 --> 00:04:07,480
그래서 실제 훈련집합에서

57
00:04:07,480 --> 00:04:10,666
훈련집합에서 i=1부터 m까지의

58
00:04:10,666 --> 00:04:16,040
차이의 제곱의 합계를 구하려고 하는 것입니다.

59
00:04:16,040 --> 00:04:21,261
이 값은 i번의 집의 입력 값에 대한 
제 가설을 예측한 것입니다.

60
00:04:22,560 --> 00:04:25,530
거기에 실제 제가 파려고 하는 
집의 실제 가격을 빼 줍니다.

61
00:04:25,530 --> 00:04:29,630
그리고 저는 훈련집합의 합을 
최소화하고 싶고,

62
00:04:29,630 --> 00:04:34,240
이 차의 제곱의 1부터 m까지 합은,

63
00:04:34,240 --> 00:04:37,160
예측된 집 가격과 
실제 팔리는 가격의

64
00:04:37,160 --> 00:04:40,550
차의 제곱과 같습니다.

65
00:04:40,550 --> 00:04:46,950
그래서 표기법에 대해서 다시 한번 상기시켜주자면,
 여기에 있는 m값은 훈련집합의 크기입니다, 맞죠?

66
00:04:46,950 --> 00:04:50,570
그래서 m은 훈련 예시들의 개수입니다.

67
00:04:50,570 --> 00:04:57,750
여기 있는 #은 훈련 예시들의 수의 
약어를 나타냅니다, 알겠죠?

68
00:04:57,750 --> 00:05:01,270
그리고 수학을 조금 더 쉽게 해보자면,

69
00:05:01,270 --> 00:05:05,950
이 합에 1/m을 곱해보겠습니다.

70
00:05:05,950 --> 00:05:09,380
제 평균값을 최소화하기 위해 
1/2m을 최소화해보겠습니다.

71
00:05:09,380 --> 00:05:14,450
맨 앞에 1/2값을 놓는 것은

72
00:05:14,450 --> 00:05:18,730
좀 더 쉬워 보이게 되고, 
어떤 값의 반을 최소화하는 것은

73
00:05:18,730 --> 00:05:23,130
Θ0, Θ1값을 최소화 하는 것과 
같은 값의 과정을 보여줄 것입니다.

74
00:05:24,300 --> 00:05:27,640
그리고 확실히 하기 위해서, 
방정식에 대해 확실히 이해했죠?

75
00:05:27,640 --> 00:05:31,452
여기에 있는 표현은

76
00:05:31,452 --> 00:05:36,560
hΘ(x), 우리가 자주 쓰던 표현입니다.

77
00:05:37,890 --> 00:05:42,668
이 값은 Θ0 + Θ1x와 같습니다.

78
00:05:42,668 --> 00:05:48,050
그리고 이 표기법은, Θ0, Θ1를 최소화 하는 것은,

79
00:05:48,050 --> 00:05:53,140
Θ0과 Θ1값을 찾아야 하고,

80
00:05:53,140 --> 00:05:57,620
Θ0값과 Θ1값에 최소화되는 것이 달려있습니다.

81
00:05:57,620 --> 00:05:58,710
요약해보겠습니다.

82
00:05:58,710 --> 00:06:03,380
우리는 Θ0, Θ1값을 찾기 위해서

83
00:06:03,380 --> 00:06:07,210
평균값인 1/2m에, 
훈련집합에 대한 내 예측과,

84
00:06:07,210 --> 00:06:11,240
최소화된 훈련집합에서 실제 집들의 값의 차의

85
00:06:11,240 --> 00:06:15,250
합계의 제곱을 곱한 값으로 접근했습니다.

86
00:06:15,250 --> 00:06:20,709
그래서 이것이 선형회귀에 대한 
전반적인 목적 함수입니다.

87
00:06:22,080 --> 00:06:27,250
그리고 좀 더 명확하게 하기 위해 다시 써본다면,

88
00:06:27,250 --> 00:06:29,790
비용 함수를 알아내기 위한 공식,

89
00:06:31,240 --> 00:06:35,930
제가 여기에 적은 이런 공식이 될 것입니다.

90
00:06:37,040 --> 00:06:45,289
그리고 제가 원하는 것은 Θ0, Θ1을 최소화 하는 것입니다.

91
00:06:45,289 --> 00:06:51,770
함수 j(Θ0, Θ1),

92
00:06:51,770 --> 00:06:52,430
여기에 방금 적은 함수입니다.

93
00:06:53,730 --> 00:06:56,540
이것이 바로 비용함수입니다.

94
00:06:59,380 --> 00:07:04,960
그래서 이 비용함수는 
오차함수의 제곱 이라고도 불립니다.

95
00:07:06,850 --> 00:07:11,190
가끔은 오차요인 제곱 함수 라고도 불리며,

96
00:07:11,190 --> 00:07:15,730
왜 우리가 이 부분을 
제곱해야 하는지를 나타냅니다.

97
00:07:15,730 --> 00:07:19,660
이 오차요인 제곱함수는 합리적인 선택이며

98
00:07:19,660 --> 00:07:22,990
대부분의 회귀 프로그램
문제에서 잘 작동합니다.

99
00:07:22,990 --> 00:07:25,740
다른 비용함수들도 꽤 잘 작동합니다.

100
00:07:25,740 --> 00:07:29,860
그러나 오차함수의 제곱이 가장 통상적으로

101
00:07:29,860 --> 00:07:30,935
회귀문제에서 사용되는 방법입니다.

102
00:07:30,935 --> 00:07:34,980
나중에 우리는 대안 비용 함수들에 대해서도 
이야기하게 될 텐데,

103
00:07:34,980 --> 00:07:39,085
우리가 계속해서 얘기했던 방법이

104
00:07:39,085 --> 00:07:41,030
대부분의 선형 회귀 문제에서 
꽤 합리적인 선택입니다.

105
00:07:42,340 --> 00:07:43,030
좋아요.

106
00:07:43,030 --> 00:07:44,280
그래서 이것이 바로 비용 함수입니다.

107
00:07:45,340 --> 00:07:50,840
지금까지 우리는 비용함수의 
수학적인 정의에 대해서 알아봤습니다.

108
00:07:50,840 --> 00:07:54,310
j Θ0, Θ1함수에 대해서

109
00:07:54,310 --> 00:07:56,260
약간 추상적이고,

110
00:07:56,260 --> 00:07:58,885
아직 이게 무엇을 하는지 여전히 
잘 이해하지 못했을 수 있습니다만,

111
00:07:58,885 --> 00:08:03,210
다음 비디오에서, 
다음 몇 개의 비디오에서

112
00:08:03,210 --> 00:08:07,930
저는 “j”라는 함수가 하는 것에 대해 
좀 더 깊게 설명하고,

113
00:08:07,930 --> 00:08:11,730
컴퓨터에서 어떻게 사용하며, 
왜 우리가 이것을 사용해야 하는지에 대해 
더 나은 직감을 주도록 하겠습니다.