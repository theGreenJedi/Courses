पिछले विडीओ में, हमने बात की बैक प्रॉपगेशन अल्गोरिद्म की. बहुत से लोग इसे देखने पर पहली बार, उनकी पहली धारना अधिकतर होती है कि अरे यह काफ़ी पेचीदा अल्गोरिद्म है, और उसमें ये सारे स्टेप्स हैं, और मुझे विश्वास नहीं कि कैसे वे एक साथ जुड़ते हैं. और यह एक प्रकार से ब्लैक बॉक्स हैं इन सब पेचीदा स्टेप्स का. यदि ऐसे आप महसूस कर रहे हैं बैक प्रॉपगेशन के बारे में, वह वास्तव में सही है. बैक प्रॉपगेशन शायद दुर्भाग्यवश कम शुद्ध है गणित के अनुसार, या गणित के अनुसार कम सरल अल्गोरिद्म है, तुलना में लिनियर रिग्रेशन या लोजिस्टिक रिग्रेशन के. और मैंने वास्तव में प्रयोग किया है बैक प्रॉपगेशन, आप जानते हैं काफ़ी सफलतापूर्वक काफ़ी वर्षों से. और आज भी, मुझे कभी कभी यह महसूस होता है कि मुझे पूरे तरह से पता नहीं है कि यह क्या कर रहा है, या अनुभव / इंटूइशन कि क्या करता है बैक प्रॉपगेशन. यदि, आप में से वे जो कर रहे हैं प्रोग्रैमिंग एक्सर्सायज़ेज़, वह कम से कम यंत्रवत रूप से आपको स्टेप थ्रू करवा देगा विभिन्न स्टेप्स से कि कैसे करना है इम्प्लमेंट बैक प्रॉपगेशन. अतः आप करवा पाएँगे इसे अपने लिए काम. और इस विडीओ में मैं क्या करना चाहता हूँ थोड़ा और दृष्टि डालें यांत्रिक स्टेप्स पर बैक प्रॉपगेशन के, और कोशिश करूँगा आपको देने की थोड़ा अनुभव कि क्या हैं यंत्रवत स्टेप्स जो बैक प्रॉपगेशन कर रहा है इस आशा से कि आपको विश्वास आ सके कि, आप जानते हैं, यह कम से कम एक तर्क संगत अल्गोरिद्म हैं. यदि इस विडीओ के बाद भी आपको बैक प्रॉपगेशन अभी भी लगे एक ब्लैक बॉक्स और एक प्रकार से बहुत से पेचीदा स्टेप्स और थोड़ा बहुत जादुई, वह वास्तव में ठीक है. और  यद्यपि मैंने प्रयोग किया है बैक प्रॉपगेशन बहुत वर्षों से, कभी कभी यह एक कठिन अल्गोरिद्म है समझने में, लेकिन आशा है इस विडीओ से थोड़ी सहायता होगी. बेहतर समझने के लिए बैक प्रॉपगेशन को, चलिए देखते हैं थोड़ा और नज़दीक से कि फ़ॉर्वर्ड प्रॉपगेशन क्या कर रहा है. यह एक न्यूरल नेटवर्क हैं दो इनपुट यूनिट्स की बाइयस यूनिट को न गिनते हुए, और दो हिडन यूनिट्स इस लेअर में, और दो हिडन यूनिट्स अगली लेअर में. और तब, अंत में, एक आउट्पुट यूनिट. फिर से, ये गिने जाएँगे दो, दो, दो, न गिनते हुए ऊपर के बाइयस यूनिट्स को. ख़ुलासा करने के लिए फ़ॉर्वर्ड प्रॉपगेशन का, मैं बनाऊँगा इस नेटवर्क को थोड़ा अलग ढंग से. और विशेषत: मैं बनाऊँगा इस न्यूरल नेटवर्क को इन नोड्ज़ के साथ जो बनाएँ हैं ये बड़े इलिप्सीज़ कि तरह, ताकि मैं उनमें टेक्स्ट लिख सकूँ. जब करते हैं फ़ॉर्वर्ड परोपगेशन, हमारे पास शायद होगा एक विशेष इग्ज़ैम्पल. मान लीजिए इग्ज़ैम्पल एक्स आइ कॉमा वाय आइ. और यह होगा यह एक्स आइ जो हम फ़ीड करेंगे इस इनपुट लेअर को. तो यह शायद होगा एक्स आइ 1 और एक्स आइ 2 हैं वैल्यूज़ जो हम सेट करते हैं इनपुट लेअर के लिए. और जब हम फ़ॉर्वर्ड प्रापगेट करते हैं इस पहली हिडन लेअर को यहाँ, हम क्या करते हैं कि कम्प्यूट करते हैं ज़ी (2) 1 और ज़ी (2) 2. तो ये हैं वेटेड सम्ज़ इनपूट यूनिट्स की इन्पुट्स के. और फिर हम अप्लाई करते हैं सिग्मोईड लजिस्टिक फ़ंक्शन का. और सिग्मोईड ऐक्टिवेशन फ़ंक्शन अप्लाई करते हैं ज़ी वैल्यू को. यहाँ हैं ऐक्टिवेशन वैल्यूज़. तो यह देता है हमें ए(2) 1 और ए(2) 2. और तब हम फ़ॉर्वर्ड प्रापगेट करते हैं फिर से यहाँ पहुँचने के लिए ज़ी (3) 1 पर. अप्लाई करें सिग्मोईड लजिस्टिक फ़ंक्शन का, ऐक्टिवेशन फ़ंक्शन, पाने के लिए एक ए (3) 1. और इसी प्रकार, आगे जब तक हमें मिलता है ज़ी (4) 1. अप्लाई करें ऐक्टिवेशन फ़ंक्शन. यह हमें देता हैं ए (4) 1, जो है अंतिम आउट्पुट वैल्यू न्यूरल नेटवर्क की. चलिए मिटा देते हैं इस ऐरो को मुझे थोड़ी और जगह देने के लिए. और अगर आप देखें कि यह कॉम्प्यूटेशन वास्तव में क्या कर रही है. केंद्रित करते हुए इस हिडन यूनिट पर, मान लीजिए. हमें यह वेट जोड़ना है. जो दिखाया गया है मजेंटा में वह है मेरा वेट थीटा (2) 1 0, इंडेक्सिंग महत्व पूर्ण नहीं है. और इस प्रकार यहाँ पर, जो मैं अंकित कर रहा हूँ लाल से, वह है थीटा (2) 1 1 और यह वेट यहाँ, जो मैं बना रहा हूँ हरे नीले रंग में, है थीटा (2) 1 2. तो हम जैसे कम्प्यूट करते हैं यह वैल्यू, ज़ी (3) 1 की, वह है, ज़ी (3) 1 बराबर है यह मजेंटा वेट टाइम्ज़ ये वैल्यू. तो वह है थीटा (2) 10 x 1. और फिर प्लस यह रेड वेट टाइम्ज़ यह वैल्यू, तो वह है थीटा (2) 11 टाइम्ज़ ए(2)1. और अंत में यह हरानीला वेट टाइम्ज़ यह वैल्यू, जो है इसलिए प्लस थीटा(2)12 टाइम्ज़ ए(2)1. और इसलिए वह है फ़ॉर्वर्ड प्रॉपगेशन. और यह समझ आता है, जैसे हमें आगे देखेंगे इस विडीओ में, बैक प्रॉपगेशन करता है एक प्रक्रिया जो बिलकुल इस जैसी है. सिवाय इसके कि बजाय कॉम्प्यूटेशन करने के जो बाएँ  से दाएँ जाती है इस नेटवर्क के, कॉम्प्यूटेशनज़ जाती है दाएँ से बाएँ इस नेटवर्क के. और करते हुए कॉम्प्यूटेशन इसके जैसे. और मैं बताऊँगा दो स्लाइड्ज़ में मेरा ठीक-ठीक मतलब क्या है उससे. बेहतर समझने के लिए क्या कर रहा है बैक प्रॉपगेशन, चलिए देखते हैं कॉस्ट फ़ंक्शन को. यह सिर्फ़ कॉस्ट फ़ंक्शन जो हमारे पास था जब हमारे पास केवल एक आउट्पुट यूनिट थी. यदि हमारे पास होंगे एक से अधिक आउट्पुट यूनिट्स, हम सिर्फ़ करेंगे समेशन आप जानते हैं आउट्पुट यूनिट्स पर इंडेक्स करते हुए के से वहाँ. यदि आपके पास है केवल एक आउट्पुट यूनिट तब यह है एक कॉस्ट फ़ंक्शन. और हम करते हैं फ़ॉर्वर्ड प्रॉपगेशन तथा बैक प्रॉपगेशन एक इग्ज़ैम्पल पर एक समय में. अत: चलिए केंद्रित करते हैं ध्यान सिर्फ़ एक इग्ज़ैम्पल पर, एक्स(आइ) वाय(आइ) और केंद्रित करते हैं ध्यान केवल एक आउट्पुट यूनिट होने पर. तो वाय(आइ) सिर्फ़ एक रियल नम्बर है. और चलिए अनदेखा करते हैं रेग्युलराइज़ेशन को, अत: लैम्डा बराबर है 0. और यह अंतिम टर्म, वह रेग्यूलराईज़ेशन टर्म, वह हट जाती है. अब यदि आप देखें समेशन के अंदर, आपको मिलेगी कॉस्ट टर्म, ट्रेनिंग इग्ज़ैम्पल से जुड़ी, वह है कॉस्ट ट्रेनिंग इग्ज़ैम्पल एक्स (आइ), वाय(आइ) से जुड़ी. जो मिलेगी इस इक्स्प्रेशन से. अत:, कॉस्ट इग्ज़ैम्पल आइ की लिखी जाएगी इस प्रकार. और यह कॉस्ट फ़ंक्शन क्या करता है कि यह एक रोल अदा करता है बिलकुल इस जैसे. अत:, इस पेचीदा इक्स्प्रेशन को देखने के बजाय, अगर आप सोच सकते हैं कॉस्ट आइ को लगभग स्क्वेर अंतर का जो न्यूरल नेटवर्क आउट्पुट करता है उसमें और असली वैल्यू में है. जैसे लजिस्टिक रेग्रेशन में, हम वास्तव में पसंद करते हैं एक थोड़ा अधिक पेचीदा कॉस्ट फ़ंक्शन लॉग प्रयोग करते हुए. लेकिन इंट्यूशन उद्देश्य के लिए, आप निसंकोच सोच सकते हैं कॉस्ट फ़ंक्शन को एक प्रकार से स्क्वेर्ड एरर कॉस्ट फ़ंक्शन. और इसलिए यह कॉस्ट(आइ) मापता है कि नेटवर्क कितना अच्छी तरह से काम कर रहा है सही प्रिडिक्ट करते हुए इग्ज़ैम्पल आइ को. कितनी नज़दीक है आउट्पुट ऐक्चूअल अब्ज़र्व्ड लेबल वाय (आइ) के? चलिए देखते हैं कि बैक प्रॉपगेशन क्या कर रहा है. एक लाभदायक अनुमान है कि बैक प्रॉपगेशन कम्प्यूट कर रहा है ये डेल्टा सूपर स्क्रिप्ट एल सब स्क्रिप्ट जे टर्म्ज़. और हम उनको सोच सकते हैं एरर टर्म की तरह ऐक्टिवेशन वैल्यू के जो हमें मिले यूनिट जे के लिए लेअर, लेअर संख्या एल में. और विधिवत रूप में, और शायद यह होगा केवल आप में से उनके लिए जो जानकारी रखते हैं कैल्क्युलस की. अधिक विधिवत रूप से, डेल्टा टर्म्ज़ वास्तव में क्या हैं , वे हैं पार्शल डेरिवेटिव विद रिस्पेक्ट टु ज़ी,आइ,जे, मतलब कि यह वेटेड सम इन्पुट्स का, जो हम प्रयोग कर रहे थे कम्प्यूट करने के लिए ये ज़ी टर्म्ज़. कॉस्ट फ़ंक्शन के पार्शल डेरिवेटिव विद रिस्पेक्ट टु इन सब के. अत: वास्तव में, कॉस्ट फ़ंक्शन है फ़ंक्शन लेबल वाय और इस वैल्यू का, एच ऑफ़ एक्स आउट्पुट वैल्यू इस न्यूरल नेटवर्क की. और अगर हम अंदर जा सकते न्यूरल नेटवर्क के और बदल सकते उन ज़ी आइ जे वैल्यूज़ को थोड़ा बहुत, तब उससे ये वैल्यूज़ प्रभावित होंगी जो न्यूरल नेटवर्क आउट्पुट करता है. और वह समाप्त होगा कॉस्ट फ़ंक्शन बदलने में. और फिर से वास्तव, यह केवल उनके लिए जो एक्स्पर्ट हैं कैल्क्युलस में. अगर आपको पता है पार्शल डेरिवेटिव्स के बारे में, क्या हैं ये डेल्टा टर्म्ज़ कि ये हैं पार्शल डेरिवेटिव कॉस्ट फ़ंक्शन के, विद रिस्पेक्ट टु ये बीच की टर्म्ज़ जो हम कम्प्यूट कर रहे थे. और इसलिए, ये हैं एक माप कि कितना हम बदलना चाहते हैं न्यूरल नेटवर्क के वेट्स को, प्रभावित करने के लिए कॉम्प्यूटेशन की इन बीच की वैल्यूज़ को. ताकि प्रभावित कर सकें अंतिम आउट्पुट को न्यूरल नेटवर्क की एच(एक्स) और इसलिए प्रभावित कर सके समस्त कॉस्ट को. यदि यह आख़िरी हिस्सा इस पार्शल डेरिवेटिव अनुमान का, यदि यह समझ नहीं आ रहा है, चिंता मत करें इस बाक़ी सब के बारे में, हम काम कर सकते हैं बिना बात किए पार्शल डेरिवेटिव के बारे में. लेकिन चलिए देखते हैं थोड़ा और विस्तार से कि बैक प्रॉपगेशन क्या कर रहा है. आउट्पुट लेअर के लिए, चलिए सेट करते हैं यह डेल्टा टर्म, डेल्टा (4) 1, वाय(आइ) यदि हम कर रहे हैं फ़ॉर्वर्ड प्रॉपगेशन और बैक्वर्ड प्रॉपगेशन इस ट्रेनिंग इग्ज़ैम्पल आइ पर. तो यह है वाय(आइ) माइनस ए(4) 1. तो यह वास्तव में एरर है, सही है? यह है अंतर बीच का असली वैल्यू वाय का माइनस जो थी वैल्यू प्रिडिक्टेड, और इसलिए हम कम्प्यूट करेंगे डेल्टा(4) 1 इस प्रकार. आगे हम क्या करेंगे, प्रापगेट करेंगे इन वैल्यूज़ को बैक्वर्ड्ज़ / पीछे की तरफ़. मैं यह समझाता हूँ एक सेकंड में, और हम पहुँचेंगे कम्प्यूट करने के लिए डेल्टा टर्म्ज़ पिछली लेअर के लिए. हमें मिलेगा डेल्टा(3) 1, डेल्टा (3) 2. और तब हम उसे और प्रापगेट करेंगे बैक्वर्ड्ज़ / पीछे की तरफ़. और पहुँचेगे कम्प्यूट करने के लिए डेल्टा(2) 1 तथा डेल्टा(2) 2. अब, बैक प्रॉपगेशन कैल्क्युलेशन काफ़ी कुछ मिलती है फ़ॉर्वर्ड कैल्क्युलेशन अल्गोरिद्म से, लेकिन जैसे करना इसे पीछे की तरफ़ से. यह मेरा मतलब ऐसे है. चलिए देखते हैं कैसे हमें मिलती हैं यह वैल्यू डेल्टा(2) 2 की. तो हमारे पास है डेल्टा (2)2. और फ़ॉर्वर्ड प्रॉपगेशन के जैसे, मैं लिख लेता हूँ कुछ वेट्स. तो यह वेट, जो मैं लिख रहा हूँ हरेनीले में. मान लीजिए यह वेट है थीटा(2)1 2, और यह नीचे यहाँ पर, जो मैं अंकित कर रहा हूँ लाल से, वह होगा मान लीजिए थीटा (2) ऑफ 2 2. अत: यदि हम देखें कि कैसे डेल्टा(2) 2, कम्प्यूट होता है, कैसे यह कम्प्यूट होता है इस नोड के साथ. ऐसा हुआ कि हम क्या करेंगे, हम लेंगे इस वैल्यू को और गुणा करेंगे इसे इस वेट से, और जमा करेंगे इसे इस वैल्यू में गुना उस वेट से. तो यह वास्तव में है एक वेटेड सम इन डेल्टा वैल्यूज़ का, जो वेटेड है उससे सम्बंधित एज की स्ट्रेन्ग्थ से. अत: पूर्णतया, मैं इसे भर लेता हूँ, यह डेल्टा(2)2 होगा बराबर थीटा(2)1 2 जो मजेंटा में है टाइम्ज़ डेल्टा(3)1. प्लस, यह जो मेरे पास लाल में है, वह है थीटा(2)2 टाइम्ज़ डेल्टा(3)2. अत: यह है वास्तव में अक्षरश: यह रेड वेट टाइम्ज़ यह वैल्यू, प्लस यह मजेंटा वेट टाइम्ज़ यह वैल्यू. और इस प्रकार हमें मिलती है वह वैल्यू डेल्टा की. और एक और इग्ज़ैम्पल की तरह, देखते हैं इस वैल्यू को. कैसे मिलती है हमें वह वैल्यू? यह भी बिलकुल वैसी ही प्रक्रिया है. यदि यह वेट, जो मैं अंकित कर रहा हूँ हरे से, यदि यह वेट बराबर है, मान लीजिए, डेल्टा(3) 1 2. तब हमें मिलेगा वह डेल्टा(3) 2 जो होगा बराबर उस हरे वेट के, थीटा (3) 12 टाइम्ज़ डेल्टा(4) 1. और वैसे तो, अभी तक मैं लिख रहा था डेल्टा वैल्यूज़ केवल हिडन यूनिट्स के लिए, लेकिन छोड़ते हुए बाइयस यूनिट्स को. निर्भर करता है कि आप कैसे परिभाषित करते हैं बैक प्रॉपगेशन अल्गोरिद्म, या निर्भर करता है कैसे आप इम्प्लमेंट करते हैं इसे, आप जानते हैं, आप अंतत: इम्प्लमेंट करते हैं कुछ जो कम्प्यूट करता है डेल्टा वैल्यूज़ इन बाइयस यूनिट्स के लिए भी. बाइयस यूनिट्स हमेशा आउट्पुट करती हैं वैल्यू प्लस वन की, और वे हैं जो वे हैं, और हमारे पास कोई तरीक़ा नहीं है उस वैल्यू को बदलने का. और इसलिए, आपके बैक प्रॉपगेशन की इम्प्लमेंटेशन के आधार पर, जैसे मैं इसे अक्सर इम्प्लमेंट करता हूँ, मैं अंतत: कम्प्यूट करता हूँ यह डेल्टा वैल्यूज़, लेकिन हम सिर्फ़ उन्हें छोड़ देते हैं, हम उन्हें प्रयोग में नहीं लाते. क्योकिं वे हिस्सा नहीं बनते कैल्क्युलेशन के जो चाहिए होता है कम्प्यूट करने के लिए डेरिवेटिव. तो आशा है उससे आप को मिलता है थोड़ा बेहतर अनुमान कि क्या कर रहा है बैक प्रॉपगेशन. यदि यह सारा अभी भी एक प्रकार से जादुई प्रतीत हो रहा है, एक प्रकार से ब्लैक बॉक्स सा, एक बाद के विडीओ में, पुटिंग ईंट टुगेदर विडीओ में, में कोशिश करूँगा थोड़ा और अनुमान देने की कि बैक प्रॉपगेशन क्या कर रहा है. लेकिन दुर्भाग्य से यह एक कठिन अल्गोरिद्म है उसे विजुएलाईज़ / एक रूप देने और समझने के लिए कि यह वास्तव में क्या कर रहा है. लेकिन सौभाग्य से मैं कर रहा हूँ, शायद बहुत से और लोग भी कर रहे हैं इसका प्रयोग सफलतापूर्वक कई वर्षों से. और यदि आप इम्प्लमेंट करते हैं अल्गोरिद्म को तो आपको मिल सकता है एक बहुत उपयोगी लर्निंग अल्गोरिद्म. यद्यपि उसकी अंदर के कार्यप्रणाली थोड़ी कठिन है समझने के लिए.