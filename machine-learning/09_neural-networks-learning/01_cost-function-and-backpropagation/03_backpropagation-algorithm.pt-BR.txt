No vídeo anterior falamos da função de custo para a rede neural. Neste vídeo, vamos começar a apresentar o algoritmo que tenta minimizar a função de custo. Especificamente, vamos falar do algoritmo de retro-propagação. Aqui está a função de custo que escrevemos no vídeo anterior. O que queremos fazer é tentar achar parâmetros Θ que minimizem J(Θ). Para usar a Descida de Gradiente ou um dos algoritmos de otimização avançados, precisamos escrever um procedimento que receba como entrada os parâmetros Θ e compute J(Θ) e estas derivadas parciais. Lembre-se que os parâmetros na rede neural destes elementos, Θ expoente l índice ij, são números reais, então estas são as derivadas parciais que precisamos calcular. Para computar a função de custo J(Θ), simplesmente usamos esta fórmula aqui, então o que eu quero fazer na maior parte deste vídeo é focar em explicar como podemos computar estas derivadas parciais. Vamos começar falando sobre do caso em que temos apenas um exemplo de treinamento, então imagine, se quiser, que todo o conjunto de treino tem apenas um exemplo de treinamento, que é um par (x, y). Não vou escrever (x1, y1), apenas escrever isso, escrever um exemplo de treinamento como x,y. E vamos fazer o passo a passo da sequência de cálculos que faríamos com este único exemplo de treinamento. A primeira coisa que fazemos é aplicarmos propagação adiante para calcular a hipótese que resulta da entrada x. Lembre que a coluna a(1) guarda os valores de ativação desta primeira camada que é a de entrada. Então eu vou atribuir x a isso e em seguida vamos computar z(2) recebe Θ(1) a(1) e a(2) recebe g, a função sigmóide de ativação aplicada a z(2). E isto nos dá nossas ativações para a primeira camada intermediária. Ou seja, para a camada dois da rede. E também adicionamos os termos de bias. Em seguida aplicamos mais duas iterações dessa propagação adiante para computar a(3) e a(4), que também é o resultado de nossa hipótese h(x). Então, esta é nossa implementação vetorizada da propagação frontal, e ela nos permite computar os valores de ativação para todos os neurônios da nossa rede neural. Em seguida, para calcular as derivadas, vamos usar um algoritmo chamado retro-propagação. A ideia do algoritmo da retro-propagação é para cada nó computar o termo δ sobrescrito l índice j, que, de alguma forma, irá representar o erro do nó j na camada l. Então, lembre que a sobrescrito l índice j é a ativação da unidade j na camada l e então, este termo δ irá, de certa forma, capturar o erro na ativação daquele neurônio. então, como desejamos que a ativação daquele nó seja ligeiramente diferente. Na prática, tome o exemplo da rede neural que temos à direita, que tem quatro camadas. Então L maiúsculo é igual a 4.  Para cada unidade de saída, vamos computar este termo δ. Então, δ para a unidade j da quarta camada é igual a simplesmente a ativação daquela unidade menos o que era o valor real 0 em nosso exemplo de treinamento. Então, este termo aqui pode ser escrito também como h(x) índice j, correto, então este termo δ é somente a diferença entre o que a hipótese resultou e o que era o valor de y em nosso conjunto de treino onde y índice j é o elemento j do vetor de valores y em nosso conjunto de treino rotulado. Falando nisso, olhando pensar em δ, a e y como vetores então você pode criar uma implementação vetorizada da algoritmo, que é simplesmente δ(4) recebe a(4) menos y, onde aqui, cada um desses δ(4), a(4) e y, cada um deles é um vetor cuja dimensão é igual ao número de unidades de saída em nossa rede. Então, agora computamos o termo de erro δ(4) da nossa rede. O que faremos em seguida é computar os termos δ para as camadas anteriores de nossa rede. Aqui está uma fórmula para computar δ(3): δ(3) recebe Θ(3) transposta vezes δ(4), e este ".*", isso é a operação de multiplicação elemento-a-elemento que conhecemos do MATLAB. Então, Θ(3) transposta vezes δ(4) é um vetor; g'(z(3)) também é um vetor, e ".*" é a multiplicação elemento-a-elemento destes dois vetores. Este termo g'(z(3)), é formalmente a derivada da função de ativação g, calculada para os valores de entrada dados por z(3). Se você Cálculo, você pode concluir por conta própria que é possível simplificar essa derivada para a mesma resposta que eu obtive. Mas eu vou simplificar e te dar o resultado. O que você precisa para computar g', esta derivada, é só a(3) .* (1 - a(3)) a(3) .* (1 - a(3)) , onde a(3) é o vetor de ativações, 1 é o vetor de uns e e a(3) é novamente a ativação, o vetor de valores de ativação para aquela camada. Em seguida se aplica uma fórmula similar para computar δ(2) onde, isso pode ser calculado usando a mesma fórmula. Apenas dessa vez é a(2), assim, e eu não demonstro aqui, mas é possível demonstrar, se souber cálculo, que esta expressão é igual matematicamente à derivada da função g, da função de ativação, que estou representando por g'. E, finalmente, é isso, e não há um termo δ(1), porque a primeira camada corresponde à camada de entrada que são as variáveis que observamos em nossos conjuntos de treino, e não há nenhum erro associado a elas. Não é o caso de tentar alterar estes valores. Então temos termos δ somente para as camadas 2, 3 e 4 para este exemplo. O nome retro-propagação vem do fato de que começamos por computar o termo δ para a camada de saída e então voltamos uma camada e computamos os termos δ para a terceira camada intermediária, depois voltamos mais um passo para computar δ(2) e assim por diante, estamos como que propagando para trás os erros, da camada de saída para a 3, e para a 2, daí o nome retro-propagação. Finalmente, a demonstração é surpreendentemente complicada, mas se você executar estes poucos passos computacionais, é possível provar, através de métodos matemáticos complicados, é possível provar que, se você ignorar a regularização, então as derivadas parciais que você quer são dadas exatamente pelas ativações e estes termos δ. Isso é ignorando λ, ou, alternativamente o termo de regularização λ será igual a zero. Vamos resolver este detalhe sobre o termo de regularização mais tarde, mas então executando a retro-propagação e computando estes termos δ, podemos, bem rapidamente, computar estas derivadas parciais para todos os parâmetros. Bem. isso foi um bocado de detalhes. Vamos pegar tudo que vimos e colocar todas as partes juntas para falar sobre como implementar a retro-propagação para computar as derivadas relativas aos seus parâmetros. E para os casos em que temos um conjunto de treino grande, não apenas um conjunto de um só exemplo, fazemos o seguinte. Suponha que temos um conjunto de treino de m exemplos, como o exibido aqui. A primeira coisa que vamos fazer é atribuir a estes Δ(l) índice ij - este símbolo triangular, é a letra grega δ maiúscula, o símbolo no slide anterior era o δ minúsculo. Então, o triângulo Δ é o δ maiúsculo. Vamos atribuir o valor zero a ele para todos os valores de l, i e j. Eventualmente, este Δ(l) índice ij vai ser usado para computar a derivada parcial, a derivada parcial relativa a Θ(l) índice ij de J(Θ). Logo, vamos ver em um minuto, estes Δ vão ser usados como acumuladores que irão lentamente somar elementos para computar estas derivadas parciais. Em seguida, vamos iterar por todo o nosso conjunto de treino. Então, digamos que para i igual a 1 até m, logo para a i-ésima iteração, vamos trabalhar com o exemplo de treinamento x(i), y(i). Então coisa que vamos fazer é atribuir a a(1), que são as ativações da camanda de entrada, atribuir a a(1) o valor x(i), que são as entradas do i-ésimo exemplo de treino, em seguida vamos executar a propagação adiante para computar as ativações das camadas dois, três e assim por diante até a camada final, a camada L maiúsculo. Em seguida, vamos usar o resultado de saída esperado y(i) deste exemplo específico que estamos processando para computar o termo de erro δ(L) para a camada de saída. Então δ(L) é a hipótese resultante menos o resultado esperado. Em seguida, aplicamos o algoritmo de retro-propagação para computar δ(L - 1), δ(L - 2) e assim por diante, voltando até δ(2). E, novamente, não há δ(1) porque não associamos um erro à camada de entrada. Por fim, vamos usar estes termos Δ maiúsculo para acumular as derivadas parciais que escrevemos na linha acima. Falando nisso, olhando para esta expressão, é possível vetorizar isto também. na prática, se pensarmos em Δ índice ij como uma matriz, indexada por i e j, então, se Δ(l) é uma matriz, podemos reescrever isso como Δ(l) é atualizado para Δ(l) mais δ(l mais um) mais a(l) transposta. Esta é uma implementação vetorizada desta atribuição, que automaticamente atualiza Δ para todos os valores de i e j. Finalmente, após executar o corpo da iteração saímos dela e computamos o seguinte. Computamos D maiúsculo desse jeito, e temos dois casos distintos: para j =0  e para j ≠ 0. O caso "j = 0" corresponde ao termo de bias, então quando j =0 essa é a razão para não termos esse termos extra de regularização. Por fim, ainda que a prova formal seja bem complicada, é possível demonstrar que, uma vez que se tenham computado estes termos D, eles são exatamente as parcial da função de custo com respeito a cada parâmetro, então eles podem ser utilizados ou na descida de gradiente ou em um dos algoritmos avançados de otimização. Esse é o algoritmo de retro-propagação e como se computam as derivadas de sua função de custo par uma rede neural. Eu entendo que isso foi um bocado de detalhes e muitos passos encadeados, Mas nas instruções do exercício de programação e mais adiante neste vídeo, vamos dar um sumário de tudo, para que possamos colocar todas as partes do algoritmo juntas e para que você entenda exatamente o que precisa implementar se quiser implementar a retro-propagação para computar as derivadas da função de custo relativas àqueles parâmetros de sua rede neural.
Tradução: Roberto Bruno | Revisão: Eduardo Bonet