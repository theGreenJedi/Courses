前回のビデオではニューラルネットワークの コスト関数について話した。 このビデオでは、コスト関数の最小化を試みる アルゴリズムの話に入っていこう。 特に、バックプロパゲーション（後に伝播）アルゴリズムについて話す。 これは前回のビデオで書き下した コスト関数だ。 今回やりたいのは Jのシータを最小化するような シータを探したい。 最急降下法なりより進んだアルゴリズムなりを 使うためには この入力であるパラメータ、シータをとり、 Jのシータと これらの偏微分の項を 計算するコードを 書く必要がある。 ニューラルネットワークのパラメータとは こんな物だったーー シータの上付き添字lに下付き添字ij。 これは実数だった。 つまりこれらが、計算しなくてはならない 偏微分の項だ。 コスト関数であるJのシータを 計算するには、 この上の式をただ使うだけで良い。 だからこのビデオの残りのほとんどで フォーカスしたいのは、 どうやってこれらの 偏微分の項を 計算出来るか、という方。 一つしかトレーニング手本が 無い場合から 始めよう。 トレーニングセットの全体が たった一つの手本から 構成されている場合を思い浮かべてくれ。
ようするにそれはxとyのペアという事。 x1とかy1とは書かずに ただこう書く事にする。 一つのトレーニングの手本を x、yと書いて、この一つのトレーニング手本に対して どんな計算を行うのか 順番に見ていこう。 最初にやる事は フォワードプロパゲーション（前方に伝播）を 与えられた入力に対して 仮説が実際に何を出力するかを計算する為、適用する。 具体的には、a(1)と呼ばれている物は この最初のレイヤーの アクティベーションの値、つまり入力の値だったのを思い出そう。 だからそれをxに設定し、 次にz(2)=シータ(1) a(1)を 計算して、 そこから a(2)イコールg、つまりsigmoid関数を z(2)に適用する。 この結果が最初の中間レイヤーの アクティベーションとなる。 つまりネットワークのレイヤー2に対応する。 また、これらのバイアス項も足す。 そこから伝播させる為に、 もう2ステップさらに適用して a(3)と、、、a(4)を計算する。 これはh(x)の 出力でもある。 以上がフォワードプロパゲーションの ベクトル化された実装だ。 以上でまた、 ニューラルネットワーク内の 全てのアクティベーション値も 計算出来る。 次に、微分を計算する為に バックプロパゲーション（後方に伝播）と呼ばれる アルゴリズムを使っていく。 バックプロパゲーションのアルゴリズムは、直感としては 各ノードごとに デルタ項を計算していく。 デルタには上付き添字のlに下付き添字のjがつく。 それはある意味で レイヤーlにあるノードjの 誤差を表す物だ。 もういちど思い出すと aの添字のl、下の添え字のjは レイヤーlにあるj番目の アクティベーションだった。 そして、このデルタの項は ある意味で、 そのノードのアクティベーションの 誤差を捕捉する、と考えられる。 つまり、そのノードのアクティベーションの 期待される値からどのくらいずれているかだ。 具体的に見てみると、 右のニューラルネットワークは ４つのレイヤーを持ってる。 だから、大文字のLは4。 それぞれの出力のユニットに対し、いまこのデルタ項を計算しようとしている。 で、この4つ目のレイヤーの、この j のユニットのデルタは以下に等しい、 このユニットのアクティベーションの値から 引くことの、 この、ぼくたちのトレーニングセットの中の、実際に観測された値。 そして、ここのこの項は、 h(x)の下添字jとも 書ける。でしょ？ つまりこのデルタ項は 単に我らの仮説の 出力した値と、 トレーニングセットでのyの値との 差分でしか無い。 ここでyの下添字jは トレーニング手本の ベクトルの値yのj番目の要素って意味だった。 ところで、 もしデルタ、a、yを ベクトルだとしても これはやはり成立し、 これはベクトル化された実装となる。 それは単に デルタ4に a4-yをセットする。 ここで、デルタ4、a4、yは それぞれ ベクトルで その次元は ネットワークの出力ユニットの数に等しい。 これで我らは今や ネットワークの誤差項であるところの デルタ4を計算した。 次にやる事はネットワーク内の より手前のレイヤーのデルタ項を計算する事だ。 これがデルタ3を計算する為の式だ。 デルタ3 イコール シータ3の転置 掛ける デルタ4。 そしてこの ドット掛け算 は、 MATLABなんかにある、 要素毎の積。 シータ3転置 デルタ4、 これはベクトル。 g'(z3)は、これもベクトル。 そしてこの ドット掛け算は これら2つのベクトルを要素ごとに掛け合わせた物。 この項、g'(z3)は 正式にはアクティベーション関数のgを 入力値がz3の所で 微分した値。 微分した値。 もし解析学を知ってるなら 自分自身で実行してみて 私が得たのと同じ答えになる事を確かめられるはずだが、 現実的にはようするにどういう意味か、答えを教えちゃおう。 実際にこのg'を計算する為にやるべき事は これらの微分項は a3 ドット掛ける 1-a3 で、a3はアクティベーションの ベクトルだ。 この1はベクトルで全要素が1を意味し、 このa3も そのレイヤーのアクティベーションの値の ベクトルだ。 次に似たような式を デルタ2にも適用する。 それも似たような公式で 計算出来る。 今回はa2に なるだけ。 ここで俺が本気出せば証明とかも出来るんだが 気になるなら君が自分でやろう。 解析学を知ってれば この式が数学的に アクティベーション関数であるところの gの微分と等しい事を示せるだろう。 それがまさにg'の事だった。 最後に、 以上でおしまいで、デルタ1の項なんてのは 存在しない。 だって最初のレイヤーは 入力レイヤに対応してるのだから、 それって単にトレーニングセットで実際に観察される値なので それに関連した誤差も何も無い。 ようするに、その値はまったく 変更したいなんて思ってない訳だ。 だからデルタ項は この例だとレイヤー2、3、4にしか無いって訳だ。 バックプロパゲーションという名前は デルタ項を 出力レイヤから 計算しはじめて レイヤーを遡っていき 隠れレイヤのデルタ項を 計算していき、 その次にさらにもう一歩戻ってデルタ2を計算して、、、 という事からついた名前。 つまりある意味で誤差を 出力レイヤーからレイヤー3に、 そこからさらに前にと伝播（プロパゲート）させていくから、バックプロパゲーションという名前な訳。 最後に、微分はめちゃくちゃ大変だが 凄い時間かかるが、 単にこれらを一つ一つ 計算していけば とても平凡なやり方で かなり面倒だけど数学的に示せるんだがーー 本当に示せるんだけど、、、ほんとほんと。 もし正規化の項を無視すれば 我らが欲しい偏微分の項は 正確にアクティベーションと これらのデルタ項で与えられる。 これはラムダ無視してる、 言い換えると正規化項を。 ラムダが0の場合って 事だ。 この細かい事、正規化項については 後で修正するが バックプロパゲーションを実行して これらのデルタ項を計算する事で 全てのパラメータについて これらの偏微分の項を 手早く計算出来る。 たくさんの事が出てきたね。 それらを全部合わせて パラメータに関する微分の計算を どう実装するのか 議論しよう。 しかもトレーニングセットが たくさんあるケースを、 一つしか無い ケースではなく。
こんな風にやる。 m個のトレーニング手本が あるとする。 こんな感じ。 最初にやる事は これらのデルタl下付き添字ijを、、、 ところでこの三角の記号、 これは実際はギリシャ文字のアルファベットで 大文字のデルタだ。 前のスライドにあったデルタは小文字のデルタ。 この三角形は大文字のデルタ。 これを全てのlijに対して 0をセットする。 最終的には、この大文字のデルタlijは 偏微分の項、、、 Jのシータの、シータlijに関する偏微分の項を 計算するのに 使う事になる。 使う事になる。 すぐに見る事となるが これらのデルタは これらの偏微分を計算する為に ちょっとずつ値を足していく為の アキュームレーターとして使う事になる。 次にトレーニングセットをforループで回す。 つまり、iが1からmまでのfor文、 つまり i番目のイテレーションの時は トレーニング手本のxiとyiに関する計算をしてるという事。 では、 最初にやることは、 a1、つまり入力レイヤーの アクティベーションに対し xiをセットする。 それはi番目のトレーニング手本の 入力を表すから。 そして次に、フォワードプロパゲーションを適用して レイヤー2、レイヤー3と 最後のレイヤーである所のレイヤーLまでを 計算していく。 次に、 yiとラベルづけされた 現在見ているトレーニング手本の 出力を用いて、 ここの出力の誤差を 計算する。 つまりデルタLは 仮説の出力結果 引くことの ターゲットにしてる観測値。 そしてバックプロパゲーションの アルゴリズムを用いて、 デルタL-1、 デルタL-2、、、と デルタ2までを計算する。 ここでもデルタ1は求めない。 何故なら入力レイヤに対応した誤差というのは想定しないから。 そして最後に 大文字のΔ(デルタ)の項を使う 前の行で書いた 偏微分の項を蓄積していく為に。 ところで この式を眺めて見ると、これもベクトル化出来そうだ。 具体的には デルタijを添字のijがインデックスの 行列とみなすと、 デルタlは 行列としてこう書き直せる。 デルタlは デルタl 足すことの 小文字のデルタl+1に alの転置を掛けた物で更新する、と。 以上がベクトル化したこれの実装で これは自動的に 全てのi,jに対して値を更新してくれる。 最後に、 forループの中身を実行した後で forループの外に出るが、 そこで以下を計算する。 大文字のDを二つの場合、 jが0の時とjが0以外の時に分けて 以下のように 計算していく。 jが0の時とは バイアス項に対応するので だからこのケースでは 追加の正規化項が 無いのだ。 最後に正式な証明は 極めて複雑だが、 頑張れば示せる事としては これらDの項をひとたび計算してしまえば、 コスト関数の パラメータでの偏微分に ぴったりと一致する、 という事。 だからそれらを最急降下法にでも より高度な最適化アルゴリズムにでも 使う事が出来る。 というわけで。これが、バックプロパゲーション アルゴリズムだ。
それとニューラルネットワークの コスト関数の偏微分係数を 計算する方法。 これがまるで すごいたくさんのディティールとステップが数珠つなぎになった代物に見えることはよくわかってるんだけど。 でも、プログラミングの 宿題と、あとの ビデオで また、まとめ直すので すべてのピースの アルゴリズムを全部まとめて。 きみがもしバックプロパゲーションを用いて ニューラルネットワークのコスト関数の パラメータによる偏微分係数を 実装したくなったときに 何しなくてはいけないかがはっきり分かるようにね。