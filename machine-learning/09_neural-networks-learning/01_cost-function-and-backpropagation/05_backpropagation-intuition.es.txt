En el video anterior, hablamos sobre el algoritmo de retropropagación. Para mucha gente que lo esté viendo por primera vez, la primera impresión a menudo es ¡Vaya!, este es un algoritmo en verdad muy complicado y mira todos estos diferentes pasos. No estoy muy seguro cómo encajan entre sí y es como si fuera una caja negra con todos estos pasos complicados. En caso de que así sea como tú te sientes respecto a la retropropagación, no te preocupes, está bien. La retropropagación quizás sea por desgracia un algoritmo matemáticamente menos limpio o menos simple en comparación con la regresión lineal o la regresión logística y yo en realidad he usado la retropropagación, ¿sabes?, con bastante éxito durante muchos años e incluso hoy en día, algunas veces todavía siento como que no tengo un sentido muy claro de qué es lo está haciendo, en su mayoría es intuición, acerca de qué está haciendo la propagación de fondo. Para aquellos de ustedes que estén haciendo los ejercicios de programación que al menos mecánicamente los guiarán a través de los diferentes pasos sobre cómo implementar la retropropagación, pues bien, ustedes serán capaces de hacer que esto funcione para si mismos. y lo que quiero hacer en este video es poner un poco más de atención sobre los pasos mecánicos de la retropropagación y tratar de darles una intuición un poco mejor acerca de qué es lo que están haciendo los pasos mecánicos de la retropropagación para con suerte convencerlos de que, bueno ya sabes, al menos de qué es un algoritmo razonable. En caso de que aún después de este video, la retropropagación todavía te parezca que es una caja muy negra y, que bueno, ya sabes, sean pasos demasiado complicados e incluso un tanto mágicos para ti, no te preocupes, todo está bien. Y aún cuando, ya sabes, he utilizado la retropropagación durante muchos años, a veces es un algoritmo difícil de entender. Pero espero que este video te ayude un poco. Con el fin de comprender mejor la retropropagación, echemos otro vistazo a qué está haciendo la propagación hacia adelante. Aquí se encuentra la red neuronal con dos unidades de entrada que no está tomando en cuenta la unidad de oscilación, y dos unidades ocultas en esta capa y dos unidades ocultas en la siguiente capa y luego tendríamos una unidad de salida. Y otra vez, estas cuentan 2, 2, 2 no cuentan estas unidades de oscilación en la parte superior. Con el fin de ilustrar la propagación hacia adelante, voy a dibujar esta red un poco diferente. Y en particular, voy a dibujar esta red neuronal con los nodos dibujados como estas elipses muy gordas, de modo que pueda escribir texto en ellas. Cuando se realiza la propagación hacia adelante, puede que tengamos algún ejemplo concreto, digamos algún ejemplo con x(i) coma y(i) y será esta x(i) que introdujimos en la capa de entrada, para que esto pueda ser, x(i)1 y x(i)2 (i) que son los valores con los que establecemos la capa de entrada y cuando los propagamos hacia adelante en dirección a la primera capa oculta, lo que que hacemos es calcular z(2) 1 y z(2) 2, y así, éstos son la suma ponderada de las entradas de las unidades de entrada y luego aplicamos el sigmoide de la función logística y la función de activación sigmoidea aplicada al valor de z, y nos da estos valores de activación Bien, esto nos da a(2)1 y a(2) 2 y luego propagamos hacia adelante otra vez para obtener, ya sabes, aquí, z(3) 1, aplicamos el sigmoideo de la función logística, la función de activación a esto, para obtener 3,1 y asimismo, de este modo, hasta que lleguemos a z(4) 1, aplicamos la la función de activación a esto y nos da a(4) 1 que es el mejor valor de salida de la red neuronal. Voy a borrar esta flecha para darme un poco de espacio, y si miras lo que éste cálculo realmente está haciendo, enfocándonos en esta unidad oculta digamos que tenemos este peso, que se muestra en magenta, es mi peso «theta» 2(1)0 la indexación no es importante, y de esta manera aquí, que supongo estoy resaltando en rojo, eso es «theta» 2(1) 1 y este peso de aquí, que estoy dibujando en verde, en turquesa, es «theta» 2(1) 2, entonces, la forma en la que se calcula el valor de z(3) 1 es z(3) 1 que es igual a este peso en magenta, tantas veces este valor, así que «theta» 2(1) 0 veces 1, y entonces + este peso en rojo por veces este valor, y así, eso es «theta» 2(1) 1 veces a(2) 1, y finalmente este turquesa rojo tantas veces este valor, que es, por lo tanto, + «theta» 2(1) 2 veces a(2) 1. Y bien, eso es la propagación hacia adelante. Y resulta que, como veremos más adelante en este video, lo que la retropropagación está haciendo es, realizar un proceso muy similar a esto, excepto que en lugar de los cálculos fluyan de izquierda a derecha en esta red, el flujo de cálculos va de derecha a izquierda en la red y utiliza un cálculo muy similar a este, y diré en dos diapositivas más exactamente lo que quiero decir con esto. Para entender mejor lo que está haciendo la retropropagación, vamos ver la función de costo. Sólo es la función de costo que teníamos para cuando tuviéramos sólo una unidad de salida Si tenemos más de una unidad de salida, sólo tenemos una suma total, ya sabes, mayor a las unidades de salida indexadas mediante k allí, pero si sólo hay una unidad de salida entonces esta es una función de costos y hacemos una propagación hacia adelante y una retropropagación en un ejemplo a la vez. Así que, vamos a centrarnos en un único ejemplo x(i)y(i) y enfoquémonos en el caso en el que se tiene una unidad de salida como y(i) aquí que sólo es un número real, e ignoremos la regularización, y así lambda que sea igual a cero y este término final, este término de regularización desaparece. Ahora, si observamos esta suma total, descubrirás que el término del costo asociado con el ejemplo de entrenamiento “i-nésimo”, es decir el costo asociado con el ejemplo de entrenamiento x(i)y(i), que va a ser dado por esta expresión, en el que el costo es, digamos, del ejemplo de entrenamiento i que está escrito así. Y lo que esta función de costo hace es que juega un papel similar al error cuadrático. Bueno, en lugar de ver esta complicada expresión, si quieres puedes considerar el coseno de i para que sea aproximadamente, ya sabes, la diferencia cuadrada entre o las salidas de la red neuronal versus lo que es el valor real. Al igual que en la regresión logística en realidad podemos preferir usar esta función de costo ligeramente más complicada utilizando el registro, aunque para el propósito de la intuición, tómate la libertad de pensar en la función de costos como una clase de error cuadrático de la función de costo, y entonces este coseno de i mide cuán bien lo está haciendo la red para predecir correctamente el ejemplo i. ¿Cuán cerca está la salida al valor real asignado a y(i) ? Echemos un vistazo a lo que está haciendo la retropropagación. Una intuición útil es que la retropropagación está calculando éstos términos «delta» superíndice l subíndice j, y no podemos pensar en ellos como el "error" del valor de activación que obtuvimos para la unidad j en la capa, en la capa “i-nésima”. De manera más formal, y esto quizá es sólo para aquellos de ustedes que están familiarizados con el cálculo, más formalmente, lo que los términos de «delta» son en realidad es lo siguiente: son una derivada parcial con respecto a j z (l), es decir esta suma ponderada de las entradas que estamos calculando para los términos de z, la derivada parcial con respecto a estos elementos de la función de costo. Bien, en concreto, la función de costo es una función del valor asignado a "y" y del valor, de esta h de la salida de x mediante nuestra red neuronal. Y si pudiéramos ir hacia adentro de la red neuronal y simplemente cambiar estos valores de z(l) j un poco, entonces eso afectaría a estos valores que salen de la red neuronal Y así, eso terminará por cambiar la función de costo. Y de nuevo, esto en realidad sólo es para aquellos de ustedes que sean expertos en cálculo. Si estás familiarizado con, o si te sientes cómodo con las derivadas parciales. ¿Qué son estos términos de «delta»?, son, resulta que son la derivada parcial de la función de costos con respecto a estos términos intermedios que estamos calculando. Así que, ellos son la medición de cuánto nos gustaría cambiar los pesos de la red neuronal para afectar estos valores intermedios del cálculo, de modo que la afectación de la salida final de la red neuronal "h" de "x" afecta, por lo tanto, los costos totales. En el caso de esta última parte de esta intuición de la derivada parcial, en caso de que esto no haya tenido sentido, no te preocupes por eso, el resto podemos hacerlo sin realmente hablar de derivadas parciales pero vamos a ver con más detalle qué está haciendo la retropropagación. Para la capa de salida, primero se define este término de «delta», decimos «delta» 4(1), como y(i) si es que estamos haciendo una propagación hacia adelante y una retropropagación de este ejemplo de entrenamiento. Dice que es y(i) menos a(4) 1, así que es realmente el error, es la diferencia entre el valor real de "y" menos lo que era el valor predicho. Así que, vamos a calcular «delta» 4(1) o algo similar. A continuación vamos a propagar estos valores hacia atrás. Explico esto en un momento y termino de calcular los términos «delta» de la capa anterior. Vamos a concluir con «delta» 3(1); «delta» 3(2); y luego vamos a propagar esto más hacia atrás y a terminar de calcular «delta» 2(1) y «delta» 2(2). Ahora, el cálculo de la retropropagación es como hacer correr el algoritmo de propagación hacia adelante, pero a la inversa. Bien, esto es lo que quiero decir. Echemos un vistazo a cómo hemos llegado a este valor de «delta» 2(2) Y entonces, tenemos que «delta» es 2(2) y similar a la propagación hacia adelante, déjenme asignar valores a un par de los pesos. Así que este peso debería estar en turquesa, digamos ese peso es «theta» 2 de 1, 2 y este peso aquí abajo, permítanme resaltar esto en rojo. Esto va a ser, digamos, «theta» 2 de 2,2. Así que si vemos cómo se calcula «delta» 2(2). ¿Cómo se calcula para esta notación? Pues resulta que lo que vamos a hacer es que vamos a tomar este valor y a multiplicarlo por este peso y a agregarlo a este valor multiplicado por ese peso. Bien, en realidad es una suma ponderada de las nuevos, de estos valores nuevos de «delta», ponderados por la fuerza del contorno correspondiente. Bueno, en concreto, déjame llenar esto. Esta «delta» 2,2 va a ser igual a «theta» 2(1)2, que es ese peso en magenta, veces «delta» (3)1 + y entonces lo que tengo en rojo es «theta» 2(2) 2 veces «delta» 3(2) Así que, en realidad es, literalmente este peso en rojo veces este valor + este peso en magenta veces este valor y así es cómo terminamos con ese valor de «delta». Y nada más como otro ejemplo, echemos un vistazo a este valor. ¿Cómo hemos llegado a ese valor? Bueno, es un proceso similar, si este peso, que voy a resaltar en verde, si este peso es igual a, digamos, «delta» 3(1) 2, entonces tenemos que «delta» 3(2) será igual a ese peso en verde, «theta»  3(1)2 veces «delta» 4(1). Por cierto, hasta ahora he estado escribiendo sólo los valores de «delta» para las unidades ocultas y, aunque no, aunque excluyo las unidades de oscilación. Dependiendo de cómo definas el algoritmo de retropropagación o dependiendo de cómo lo implementes, ya sabes, puedes terminar implementando algo que calcule los valores de «delta» para estas unidades de oscilación también. La unidad de oscilación son siempre los valores de salida + uno y son justo lo que son y no hay forma de que cambiemos su valor, así que, dependiendo de en tu implementación de la retropropagación, la manera en la que yo usualmente lo pongo en práctica, lo concluyo calculando estos valores de «delta», aunque sólo los desechamos y no los utilizamos, porque no terminan siendo parte del cálculo necesario para calcular las derivadas. Bien, pues ojalá esto te de un un poco de intuición acerca de lo que hace la retropropagación. En caso de que todo esto todavía te parezca muy mágico y una especie de caja negra, en un video posterior, en el video de resumen, voy a tratar de dar un poco más de intuición sobre lo está haciendo la retropropagación. Pero, por desgracia, este es, ya sabes un algoritmo que es difícil tratar de visualizarlo y de comprender qué es lo que realmente está haciendo. Aunque por fortuna, ya sabes, a menudo, supongo claro, muchas personas lo han estado usando con mucho éxito durante muchos años y si infieres el algoritmo, puedes tener un algoritmo de aprendizaje muy eficaz, incluso si resulta difícil visualizar las funciones internas que expliquen exactamente cómo funciona.