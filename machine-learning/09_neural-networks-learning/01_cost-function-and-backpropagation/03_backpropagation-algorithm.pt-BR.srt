1
00:00:00,090 --> 00:00:01,798
No vídeo anterior falamos 

2
00:00:01,857 --> 00:00:03,868
da função de custo para a rede neural.

3
00:00:04,139 --> 00:00:07,079
Neste vídeo, vamos começar a apresentar o algoritmo

4
00:00:07,200 --> 00:00:09,062
que tenta minimizar a função de custo.

5
00:00:09,240 --> 00:00:12,735
Especificamente, vamos falar do algoritmo de retro-propagação.

6
00:00:13,834 --> 00:00:15,380
Aqui está a função de custo que

7
00:00:15,520 --> 00:00:17,905
escrevemos no vídeo anterior.

8
00:00:17,972 --> 00:00:19,438
 O que queremos fazer é

9
00:00:19,484 --> 00:00:21,161
tentar achar parâmetros Θ

10
00:00:21,246 --> 00:00:23,440
que minimizem J(Θ).

11
00:00:23,530 --> 00:00:25,782
Para usar a Descida de Gradiente ou

12
00:00:25,832 --> 00:00:28,625
um dos algoritmos de otimização avançados,

13
00:00:28,675 --> 00:00:30,206
precisamos escrever 

14
00:00:30,249 --> 00:00:31,598
um procedimento que receba como

15
00:00:31,645 --> 00:00:33,487
entrada os parâmetros Θ

16
00:00:33,540 --> 00:00:34,965
e compute J(Θ)

17
00:00:35,014 --> 00:00:37,364
e estas derivadas parciais.

18
00:00:37,425 --> 00:00:38,763
Lembre-se que os parâmetros

19
00:00:38,790 --> 00:00:40,710
na rede neural destes elementos,

20
00:00:40,760 --> 00:00:43,435
Θ expoente l índice ij,

21
00:00:43,492 --> 00:00:44,868
são números reais,

22
00:00:44,930 --> 00:00:47,185
então estas são as derivadas parciais

23
00:00:47,249 --> 00:00:48,869
que precisamos calcular.

24
00:00:48,900 --> 00:00:50,077
Para computar a

25
00:00:50,115 --> 00:00:51,840
função de custo J(Θ),

26
00:00:51,883 --> 00:00:53,986
simplesmente usamos esta fórmula aqui,

27
00:00:54,042 --> 00:00:55,617
então o que eu quero fazer

28
00:00:55,655 --> 00:00:56,850
na maior parte deste vídeo é

29
00:00:56,897 --> 00:00:58,595
focar em explicar

30
00:00:58,636 --> 00:00:59,952
como podemos computar estas

31
00:00:59,989 --> 00:01:01,994
derivadas parciais.

32
00:01:02,031 --> 00:01:03,812
Vamos começar falando sobre

33
00:01:03,858 --> 00:01:05,512
do caso em que temos apenas

34
00:01:05,556 --> 00:01:06,839
um exemplo de treinamento,

35
00:01:06,872 --> 00:01:09,385
então imagine, se quiser, que todo

36
00:01:09,432 --> 00:01:11,301
o conjunto de treino tem apenas um

37
00:01:11,351 --> 00:01:14,006
exemplo de treinamento, que é um par (x, y).

38
00:01:14,049 --> 00:01:15,591
Não vou escrever (x1, y1),

39
00:01:15,629 --> 00:01:16,375
apenas escrever isso,

40
00:01:16,410 --> 00:01:17,665
escrever um exemplo de treinamento

41
00:01:17,718 --> 00:01:19,980
como x,y. E vamos fazer o passo a passo

42
00:01:20,031 --> 00:01:21,423
da sequência de cálculos

43
00:01:21,462 --> 00:01:24,332
que faríamos com este único exemplo de treinamento.

44
00:01:25,754 --> 00:01:27,129
A primeira coisa que fazemos é

45
00:01:27,167 --> 00:01:29,175
aplicarmos propagação adiante para

46
00:01:29,212 --> 00:01:31,773
calcular a hipótese 

47
00:01:31,813 --> 00:01:34,238
que resulta da entrada x.

48
00:01:34,272 --> 00:01:36,734
Lembre que a coluna a(1)

49
00:01:36,769 --> 00:01:39,025
guarda os valores de ativação

50
00:01:39,071 --> 00:01:41,541
desta primeira camada que é a de entrada.

51
00:01:41,600 --> 00:01:43,452
Então eu vou atribuir x a isso

52
00:01:43,505 --> 00:01:45,389
e em seguida vamos computar

53
00:01:45,435 --> 00:01:47,506
z(2) recebe Θ(1) a(1)

54
00:01:47,552 --> 00:01:49,919
e a(2) recebe g, a função sigmóide

55
00:01:49,980 --> 00:01:52,250
de ativação aplicada a z(2).

56
00:01:52,310 --> 00:01:53,753
E isto nos dá nossas

57
00:01:53,800 --> 00:01:56,115
ativações para a primeira camada intermediária.

58
00:01:56,162 --> 00:01:58,208
Ou seja, para a camada dois da rede.

59
00:01:58,241 --> 00:02:00,649
E também adicionamos os termos de bias.

60
00:02:01,315 --> 00:02:03,132
Em seguida aplicamos mais duas iterações

61
00:02:03,176 --> 00:02:04,966
dessa propagação adiante

62
00:02:05,013 --> 00:02:08,328
para computar a(3) e a(4),

63
00:02:08,360 --> 00:02:11,458
que também é o resultado

64
00:02:11,505 --> 00:02:14,089
de nossa hipótese h(x).

65
00:02:14,711 --> 00:02:18,103
Então, esta é nossa implementação vetorizada da

66
00:02:18,145 --> 00:02:19,228
propagação frontal,

67
00:02:19,276 --> 00:02:20,888
e ela nos permite computar

68
00:02:20,938 --> 00:02:22,280
os valores de ativação

69
00:02:22,345 --> 00:02:24,056
para todos os neurônios

70
00:02:24,110 --> 00:02:25,948
da nossa rede neural.

71
00:02:27,934 --> 00:02:29,608
Em seguida, para calcular

72
00:02:29,650 --> 00:02:30,967
as derivadas, vamos usar

73
00:02:31,026 --> 00:02:33,589
um algoritmo chamado retro-propagação.

74
00:02:34,904 --> 00:02:37,765
A ideia do algoritmo da retro-propagação

75
00:02:37,807 --> 00:02:38,430
é para cada nó

76
00:02:38,430 --> 00:02:41,065
computar o termo

77
00:02:41,126 --> 00:02:43,642
δ sobrescrito l índice j,

78
00:02:43,676 --> 00:02:45,130
que, de alguma forma, 

79
00:02:45,171 --> 00:02:46,310
irá representar o erro

80
00:02:46,361 --> 00:02:48,511
do nó j na camada l.

81
00:02:48,552 --> 00:02:49,682
Então, lembre que

82
00:02:49,716 --> 00:02:52,313
a sobrescrito l índice j

83
00:02:52,355 --> 00:02:54,138
é a ativação da

84
00:02:54,185 --> 00:02:56,182
unidade j na camada l

85
00:02:56,224 --> 00:02:58,001
e então, este termo δ

86
00:02:58,045 --> 00:02:59,037
irá, de certa forma,

87
00:02:59,082 --> 00:03:00,978
capturar o erro

88
00:03:01,012 --> 00:03:03,618
na ativação daquele neurônio.

89
00:03:03,650 --> 00:03:05,798
então, como desejamos que a ativação

90
00:03:05,823 --> 00:03:07,975
daquele nó seja ligeiramente diferente.

91
00:03:08,047 --> 00:03:09,670
Na prática, tome o exemplo da

92
00:03:10,270 --> 00:03:11,100
rede neural que temos

93
00:03:11,360 --> 00:03:12,700
à direita, que tem quatro camadas.

94
00:03:13,440 --> 00:03:15,710
Então L maiúsculo é igual a 4.  Para cada

95
00:03:16,060 --> 00:03:17,120
unidade de saída, vamos computar este termo δ.

96
00:03:17,400 --> 00:03:19,130
Então, δ para a unidade j da quarta camada é

97
00:03:23,380 --> 00:03:24,490
igual a simplesmente a ativação daquela

98
00:03:24,720 --> 00:03:26,350
unidade menos o que

99
00:03:26,490 --> 00:03:28,650
era o valor real 0 em nosso exemplo de treinamento.

100
00:03:29,900 --> 00:03:32,420
Então, este termo aqui pode

101
00:03:32,580 --> 00:03:34,510
ser escrito também como

102
00:03:34,710 --> 00:03:38,040
h(x) índice j, correto,

103
00:03:38,330 --> 00:03:39,640
então este termo δ é somente

104
00:03:39,930 --> 00:03:40,900
a diferença entre o que

105
00:03:41,290 --> 00:03:43,200
a hipótese resultou e o que

106
00:03:43,370 --> 00:03:44,870
era o valor de y

107
00:03:45,570 --> 00:03:46,900
em nosso conjunto de treino

108
00:03:47,060 --> 00:03:48,610
onde y índice j é o

109
00:03:48,750 --> 00:03:49,910
elemento j do vetor 

110
00:03:50,090 --> 00:03:53,340
de valores y em nosso conjunto de treino rotulado.

111
00:03:56,200 --> 00:03:57,790
Falando nisso, olhando

112
00:03:57,970 --> 00:04:00,460
pensar em δ, a e y

113
00:04:01,000 --> 00:04:02,350
como vetores então você

114
00:04:02,520 --> 00:04:03,760
pode criar

115
00:04:04,030 --> 00:04:05,890
uma implementação vetorizada da

116
00:04:06,010 --> 00:04:07,310
algoritmo, que é simplesmente

117
00:04:07,690 --> 00:04:09,840
δ(4) recebe

118
00:04:10,700 --> 00:04:14,330
a(4) menos y, onde

119
00:04:14,560 --> 00:04:15,820
aqui, cada um desses δ(4),

120
00:04:16,540 --> 00:04:18,080
a(4) e y, cada um

121
00:04:18,180 --> 00:04:19,860
deles é um vetor cuja

122
00:04:20,640 --> 00:04:22,040
dimensão é igual ao

123
00:04:22,250 --> 00:04:24,150
número de unidades de saída em nossa rede.

124
00:04:25,210 --> 00:04:26,880
Então, agora computamos o

125
00:04:27,320 --> 00:04:28,670
termo de erro δ(4)

126
00:04:29,020 --> 00:04:30,170
da nossa rede.

127
00:04:31,440 --> 00:04:32,950
O que faremos em seguida é computar

128
00:04:33,620 --> 00:04:36,280
os termos δ para as camadas anteriores de nossa rede.

129
00:04:37,210 --> 00:04:38,690
Aqui está uma fórmula para computar δ(3):

130
00:04:39,010 --> 00:04:39,830
δ(3) recebe

131
00:04:40,310 --> 00:04:42,050
Θ(3) transposta vezes δ(4),

132
00:04:42,560 --> 00:04:44,190
e este ".*", isso é a

133
00:04:44,390 --> 00:04:46,390
operação de multiplicação elemento-a-elemento

134
00:04:47,580 --> 00:04:48,380
que conhecemos do MATLAB.

135
00:04:49,160 --> 00:04:50,760
Então, Θ(3) transposta vezes δ(4)

136
00:04:51,020 --> 00:04:52,860
é um vetor; 

137
00:04:53,480 --> 00:04:55,080
g'(z(3)) também é um vetor,

138
00:04:55,800 --> 00:04:57,370
e ".*" é a multiplicação

139
00:04:57,530 --> 00:04:59,670
elemento-a-elemento destes dois vetores.

140
00:05:01,460 --> 00:05:02,650
Este termo g'(z(3)),

141
00:05:02,740 --> 00:05:04,560
é formalmente a 

142
00:05:04,950 --> 00:05:06,420
derivada da função de

143
00:05:06,720 --> 00:05:08,740
ativação g, calculada para

144
00:05:08,890 --> 00:05:10,620
os valores de entrada dados por z(3).

145
00:05:10,760 --> 00:05:12,620
Se você Cálculo, você pode concluir

146
00:05:12,710 --> 00:05:13,470
por conta própria que é possível simplificar

147
00:05:13,850 --> 00:05:16,100
essa derivada para a mesma resposta que eu obtive.

148
00:05:16,860 --> 00:05:19,690
Mas eu vou simplificar e te dar o resultado.

149
00:05:20,000 --> 00:05:21,260
O que você precisa para computar g',

150
00:05:21,460 --> 00:05:23,310
esta derivada, é só

151
00:05:23,510 --> 00:05:25,660
a(3) .* (1 - a(3))

152
00:05:26,010 --> 00:05:27,900
a(3) .* (1 - a(3)) , onde 

153
00:05:28,160 --> 00:05:29,420
a(3) é o vetor de ativações,

154
00:05:30,150 --> 00:05:31,440
1 é o vetor de uns e

155
00:05:31,600 --> 00:05:33,240
e a(3) é novamente

156
00:05:34,020 --> 00:05:35,970
a ativação,

157
00:05:36,290 --> 00:05:38,850
o vetor de valores de ativação para aquela camada.

158
00:05:39,170 --> 00:05:40,210
Em seguida se aplica uma fórmula

159
00:05:40,540 --> 00:05:42,850
similar para computar δ(2)

160
00:05:43,220 --> 00:05:45,230
onde, isso pode ser

161
00:05:45,670 --> 00:05:47,410
calculado usando a mesma fórmula.

162
00:05:48,450 --> 00:05:49,950
Apenas dessa vez é a(2),

163
00:05:50,120 --> 00:05:53,850
assim, e eu

164
00:05:53,960 --> 00:05:55,020
não demonstro aqui, 

165
00:05:55,110 --> 00:05:56,400
mas é possível demonstrar,

166
00:05:56,490 --> 00:05:57,520
se souber cálculo, 

167
00:05:58,240 --> 00:05:59,520
que esta expressão é igual

168
00:05:59,860 --> 00:06:02,010
matematicamente à derivada da

169
00:06:02,190 --> 00:06:03,570
função g, da função

170
00:06:04,040 --> 00:06:05,460
de ativação, que estou representando

171
00:06:05,910 --> 00:06:08,540
por g'. E, finalmente, 

172
00:06:09,270 --> 00:06:10,690
é isso, e não há

173
00:06:10,860 --> 00:06:13,650
um termo δ(1), porque a primeira

174
00:06:13,720 --> 00:06:15,590
camada corresponde à

175
00:06:15,630 --> 00:06:16,940
camada de entrada que são as

176
00:06:17,000 --> 00:06:18,200
variáveis que observamos em nossos

177
00:06:18,300 --> 00:06:20,380
conjuntos de treino, e não há nenhum erro associado a elas.

178
00:06:20,600 --> 00:06:22,080
Não é o caso de

179
00:06:22,120 --> 00:06:23,680
tentar alterar estes valores.

180
00:06:24,320 --> 00:06:25,240
Então temos termos δ

181
00:06:25,510 --> 00:06:28,090
somente para as camadas 2, 3 e 4 para este exemplo.

182
00:06:30,170 --> 00:06:32,120
O nome retro-propagação vem do

183
00:06:32,170 --> 00:06:33,260
fato de que começamos 

184
00:06:33,350 --> 00:06:34,720
por computar o termo δ 

185
00:06:34,740 --> 00:06:36,190
para a camada de saída e então

186
00:06:36,370 --> 00:06:37,480
voltamos uma camada

187
00:06:37,880 --> 00:06:39,670
e computamos os termos δ 

188
00:06:39,850 --> 00:06:41,050
para a terceira camada intermediária, 

189
00:06:41,180 --> 00:06:42,540
depois voltamos mais um passo para computar

190
00:06:42,770 --> 00:06:44,070
δ(2) e assim por diante, estamos como que

191
00:06:44,660 --> 00:06:46,060
propagando para trás os erros,

192
00:06:46,280 --> 00:06:47,270
da camada de saída para a 3,

193
00:06:47,650 --> 00:06:50,180
e para a 2, daí o nome retro-propagação.

194
00:06:51,270 --> 00:06:53,120
Finalmente, a demonstração é

195
00:06:53,340 --> 00:06:56,510
surpreendentemente complicada,

196
00:06:56,820 --> 00:06:58,100
mas se você executar estes poucos

197
00:06:58,280 --> 00:07:00,130
passos computacionais, é possível

198
00:07:00,680 --> 00:07:02,540
provar, através de métodos

199
00:07:02,810 --> 00:07:04,440
matemáticos complicados,

200
00:07:05,200 --> 00:07:07,410
é possível provar que, 

201
00:07:07,560 --> 00:07:09,690
se você ignorar a regularização, então as

202
00:07:09,800 --> 00:07:11,080
derivadas parciais que você quer

203
00:07:12,220 --> 00:07:14,650
são dadas exatamente pelas

204
00:07:14,780 --> 00:07:17,690
ativações e estes termos δ.

205
00:07:17,870 --> 00:07:20,630
Isso é ignorando λ, ou,

206
00:07:20,780 --> 00:07:22,730
alternativamente o termo de

207
00:07:23,770 --> 00:07:24,630
regularização λ será

208
00:07:25,000 --> 00:07:25,170
igual a zero.

209
00:07:25,680 --> 00:07:27,130
Vamos resolver este detalhe sobre

210
00:07:27,470 --> 00:07:29,430
o termo de regularização mais tarde, 

211
00:07:29,620 --> 00:07:30,740
mas então executando a retro-propagação

212
00:07:31,610 --> 00:07:32,820
e computando estes termos δ,

213
00:07:33,180 --> 00:07:34,240
podemos, bem rapidamente,

214
00:07:34,530 --> 00:07:36,320
computar estas derivadas

215
00:07:36,380 --> 00:07:38,150
parciais para todos os parâmetros.

216
00:07:38,920 --> 00:07:40,020
Bem. isso foi um bocado de detalhes.

217
00:07:40,570 --> 00:07:41,900
Vamos pegar tudo que vimos e colocar

218
00:07:42,320 --> 00:07:43,660
todas as partes juntas para falar sobre

219
00:07:44,120 --> 00:07:45,490
como implementar a retro-propagação 

220
00:07:46,560 --> 00:07:48,590
para computar as derivadas relativas aos seus parâmetros.

221
00:07:49,790 --> 00:07:50,770
E para os casos em que temos

222
00:07:51,000 --> 00:07:52,460
um conjunto de treino grande,

223
00:07:52,830 --> 00:07:53,850
não apenas um conjunto de

224
00:07:54,100 --> 00:07:56,320
um só exemplo, fazemos o seguinte.

225
00:07:57,290 --> 00:07:58,140
Suponha que temos um conjunto

226
00:07:58,270 --> 00:07:59,750
de treino de m exemplos, como o 

227
00:07:59,900 --> 00:08:01,610
exibido aqui.

228
00:08:01,850 --> 00:08:02,600
A primeira coisa que vamos fazer é

229
00:08:03,220 --> 00:08:04,560
atribuir a estes Δ(l) índice ij - 

230
00:08:05,100 --> 00:08:07,270
este símbolo triangular,

231
00:08:08,090 --> 00:08:09,990
é a letra grega

232
00:08:10,310 --> 00:08:11,980
δ maiúscula, o símbolo no slide

233
00:08:12,050 --> 00:08:14,080
anterior era o δ minúsculo.

234
00:08:14,390 --> 00:08:16,810
Então, o triângulo Δ é o δ maiúsculo.

235
00:08:17,430 --> 00:08:18,490
Vamos atribuir o valor zero

236
00:08:18,680 --> 00:08:21,930
a ele para todos os valores de l, i e j.

237
00:08:22,110 --> 00:08:23,850
Eventualmente, este Δ(l) índice ij

238
00:08:24,530 --> 00:08:25,830
vai ser usado para

239
00:08:26,860 --> 00:08:29,920
computar a derivada parcial,

240
00:08:30,290 --> 00:08:31,570
a derivada parcial relativa

241
00:08:32,380 --> 00:08:35,240
a Θ(l) índice ij

242
00:08:35,430 --> 00:08:37,190
de J(Θ).

243
00:08:39,040 --> 00:08:40,210
Logo, vamos ver em

244
00:08:40,480 --> 00:08:41,550
um minuto, estes Δ vão ser

245
00:08:41,670 --> 00:08:43,700
usados como acumuladores que

246
00:08:43,950 --> 00:08:45,360
irão lentamente somar elementos

247
00:08:45,700 --> 00:08:47,130
para computar estas derivadas parciais.

248
00:08:49,570 --> 00:08:51,920
Em seguida, vamos iterar por todo o nosso conjunto de treino.

249
00:08:52,150 --> 00:08:53,270
Então, digamos que para i igual

250
00:08:53,610 --> 00:08:55,400
a 1 até m, logo para

251
00:08:55,620 --> 00:08:57,270
a i-ésima iteração, vamos

252
00:08:57,410 --> 00:08:59,180
trabalhar com o exemplo de treinamento x(i), y(i).

253
00:09:00,480 --> 00:09:03,220
Então

254
00:09:03,720 --> 00:09:04,590
coisa que vamos fazer é

255
00:09:04,690 --> 00:09:06,120
atribuir a a(1), que são as

256
00:09:06,570 --> 00:09:07,830
ativações da camanda de entrada,

257
00:09:08,190 --> 00:09:09,030
atribuir a a(1) o valor

258
00:09:09,950 --> 00:09:11,800
x(i), que são as entradas

259
00:09:12,670 --> 00:09:15,070
do i-ésimo exemplo de treino, em seguida

260
00:09:15,340 --> 00:09:17,590
vamos executar a propagação adiante para

261
00:09:17,730 --> 00:09:19,400
computar as ativações das

262
00:09:19,790 --> 00:09:20,900
camadas dois, três e assim

263
00:09:21,170 --> 00:09:22,050
por diante até a camada

264
00:09:22,500 --> 00:09:25,190
final, a camada L maiúsculo. 

265
00:09:25,570 --> 00:09:26,970
Em seguida, vamos usar o resultado de

266
00:09:27,280 --> 00:09:28,530
saída esperado y(i) deste

267
00:09:28,680 --> 00:09:29,870
exemplo específico que estamos

268
00:09:30,340 --> 00:09:31,650
processando para computar o termo

269
00:09:31,950 --> 00:09:34,140
de erro δ(L) para a camada de saída.

270
00:09:34,480 --> 00:09:35,730
Então δ(L) é a hipótese

271
00:09:35,880 --> 00:09:38,190
resultante menos o

272
00:09:38,660 --> 00:09:39,870
resultado esperado.

273
00:09:41,840 --> 00:09:42,560
Em seguida, aplicamos o

274
00:09:42,850 --> 00:09:44,550
algoritmo de retro-propagação para

275
00:09:44,740 --> 00:09:46,020
computar δ(L - 1),

276
00:09:46,220 --> 00:09:47,250
δ(L - 2) e

277
00:09:47,350 --> 00:09:49,880
assim por diante, voltando até δ(2). 

278
00:09:50,270 --> 00:09:51,380
E, novamente, não há δ(1) porque

279
00:09:51,460 --> 00:09:54,380
não associamos um erro à camada de entrada.

280
00:09:57,000 --> 00:09:58,160
Por fim, vamos 

281
00:09:58,340 --> 00:10:00,650
usar estes termos Δ maiúsculo

282
00:10:01,190 --> 00:10:02,800
para acumular as derivadas parciais

283
00:10:03,400 --> 00:10:05,670
que escrevemos na linha acima.

284
00:10:06,870 --> 00:10:07,870
Falando nisso, olhando

285
00:10:07,960 --> 00:10:11,340
para esta expressão, é possível vetorizar isto também.

286
00:10:12,020 --> 00:10:13,040
na prática, se pensarmos em

287
00:10:13,310 --> 00:10:14,860
Δ índice ij como uma matriz,

288
00:10:15,000 --> 00:10:18,090
indexada por i e j,

289
00:10:19,220 --> 00:10:20,590
então, se Δ(l) é uma

290
00:10:20,780 --> 00:10:22,040
matriz, podemos reescrever isso

291
00:10:22,130 --> 00:10:24,100
como Δ(l) é

292
00:10:24,350 --> 00:10:26,710
atualizado para Δ(l) mais 

293
00:10:27,830 --> 00:10:29,370
δ(l mais um) mais

294
00:10:29,640 --> 00:10:32,780
a(l) transposta.

295
00:10:33,570 --> 00:10:35,380
Esta é uma implementação vetorizada

296
00:10:35,520 --> 00:10:37,150
desta atribuição, que automaticamente

297
00:10:37,590 --> 00:10:38,850
atualiza Δ para todos os valores de i e j.

298
00:10:39,010 --> 00:10:41,250
Finalmente, após

299
00:10:41,500 --> 00:10:43,480
executar o corpo da

300
00:10:43,580 --> 00:10:45,350
iteração saímos dela e

301
00:10:46,330 --> 00:10:47,000
computamos o seguinte.

302
00:10:47,440 --> 00:10:49,690
Computamos D maiúsculo 

303
00:10:50,020 --> 00:10:51,400
desse jeito, e temos

304
00:10:51,510 --> 00:10:52,750
dois casos distintos:

305
00:10:52,980 --> 00:10:54,890
para j =0  e para j ≠ 0.

306
00:10:56,080 --> 00:10:57,250
O caso "j = 0"

307
00:10:57,680 --> 00:10:58,730
corresponde ao termo de bias,

308
00:10:59,150 --> 00:11:00,030
então quando j =0

309
00:11:00,390 --> 00:11:01,320
essa é a razão para não termos

310
00:11:01,800 --> 00:11:03,320
esse termos extra de regularização.

311
00:11:05,470 --> 00:11:06,850
Por fim, ainda que a prova formal

312
00:11:07,180 --> 00:11:08,970
seja bem complicada, é possível

313
00:11:09,030 --> 00:11:10,410
demonstrar que, uma vez

314
00:11:10,640 --> 00:11:12,530
que se tenham computado estes termos D,

315
00:11:13,510 --> 00:11:15,230
eles são exatamente as

316
00:11:15,640 --> 00:11:17,610
parcial da função

317
00:11:17,920 --> 00:11:19,230
de custo com respeito a cada

318
00:11:19,470 --> 00:11:20,890
parâmetro, então eles podem

319
00:11:21,040 --> 00:11:22,470
ser utilizados ou na descida

320
00:11:22,610 --> 00:11:23,530
de gradiente ou em um dos algoritmos

321
00:11:25,450 --> 00:11:25,450
avançados de otimização.

322
00:11:28,310 --> 00:11:29,360
Esse é o algoritmo de

323
00:11:29,990 --> 00:11:31,110
retro-propagação e como se computam

324
00:11:31,470 --> 00:11:33,080
as derivadas de sua função

325
00:11:33,340 --> 00:11:34,710
de custo par uma rede neural.

326
00:11:35,470 --> 00:11:36,330
Eu entendo que isso foi 

327
00:11:36,470 --> 00:11:38,810
um bocado de detalhes e muitos passos encadeados,

328
00:11:39,460 --> 00:11:40,770
Mas nas instruções do

329
00:11:41,100 --> 00:11:43,010
exercício de programação e mais

330
00:11:43,110 --> 00:11:44,580
adiante neste vídeo, vamos dar

331
00:11:44,720 --> 00:11:45,900
um sumário de tudo, para que

332
00:11:46,050 --> 00:11:46,830
possamos colocar todas as partes

333
00:11:47,260 --> 00:11:48,780
do algoritmo juntas e para

334
00:11:48,920 --> 00:11:50,550
que você entenda exatamente o que

335
00:11:50,610 --> 00:11:51,760
precisa implementar se quiser

336
00:11:51,940 --> 00:11:53,460
implementar a retro-propagação para computar

337
00:11:53,890 --> 00:11:56,432
as derivadas da função de custo

338
00:11:56,574 --> 00:11:59,348
relativas àqueles parâmetros de sua rede neural.
Tradução: Roberto Bruno | Revisão: Eduardo Bonet