1
00:00:00,270 --> 00:00:01,380
Las redes neuronales son uno

2
00:00:01,450 --> 00:00:03,610
de los algoritmos de aprendizaje más potentes que tenemos hoy.

3
00:00:04,310 --> 00:00:05,490
En este video y

4
00:00:05,590 --> 00:00:06,690
en los siguientes, me gustaría

5
00:00:06,760 --> 00:00:08,030
comenzar a hablar sobre un algoritmo

6
00:00:08,380 --> 00:00:09,920
de aprendizaje para establecer los parámetros

7
00:00:10,630 --> 00:00:12,530
de la red neuronal dada en el paquete de entrenamiento.

8
00:00:13,460 --> 00:00:14,840
En cuanto a la discusión de la mayoría

9
00:00:15,020 --> 00:00:16,300
de los algoritmos de aprendizaje, vamos

10
00:00:16,450 --> 00:00:17,860
a comenzar hablando de la

11
00:00:17,960 --> 00:00:20,510
función de costo para ajustar los parámetros de la red.

12
00:00:21,170 --> 00:00:22,650
Voy a enfocarme

13
00:00:23,270 --> 00:00:24,790
en la aplicación de las redes

14
00:00:25,060 --> 00:00:26,590
neuronales para los problemas de clasificación.

15
00:00:27,660 --> 00:00:28,860
Entonces, supongamos que tenemos una

16
00:00:29,120 --> 00:00:31,300
red como la que se muestra en la izquierda.

17
00:00:31,530 --> 00:00:32,510
Y supongamos que tenemos un paquete de

18
00:00:32,710 --> 00:00:33,850
entrenamiento como este con

19
00:00:33,980 --> 00:00:36,550
pares xi y yi de ejemplos de entrenamiento m.

20
00:00:37,920 --> 00:00:39,040
Voy a usar la L mayúscula

21
00:00:39,450 --> 00:00:40,640
para denotar el

22
00:00:40,790 --> 00:00:42,460
número total de capas en esta red.

23
00:00:43,330 --> 00:00:44,550
Así, para la red que se muestra

24
00:00:44,810 --> 00:00:45,720
a la izquierda, tendríamos que

25
00:00:46,370 --> 00:00:47,920
L mayúscula es igual a 4.

26
00:00:48,020 --> 00:00:48,910
Y voy a usar

27
00:00:49,180 --> 00:00:50,740
s subíndice l para denotar

28
00:00:51,260 --> 00:00:52,540
el número de unidades, que es

29
00:00:52,730 --> 00:00:54,490
un número de neuronas, sin contar

30
00:00:54,770 --> 00:00:57,180
la unidad de oscilación en la capa L de la red.

31
00:00:57,900 --> 00:00:59,440
Entonces, por ejemplo, tendríamos

32
00:00:59,580 --> 00:01:01,280
un S1 que

33
00:01:01,370 --> 00:01:03,330
es la capa de entrada igual a la unidad S3,

34
00:01:04,140 --> 00:01:05,970
S2 en mi ejemplo son cinco unidades.

35
00:01:06,900 --> 00:01:08,670
Y la capa de salida S4,

36
00:01:09,940 --> 00:01:12,820
Que también es igual SL, porque la L mayúscula es igual a cuatro.

37
00:01:12,990 --> 00:01:14,290
La capa de salida en mi

38
00:01:14,450 --> 00:01:16,230
ejemplo de la izquierda tiene cuatro unidades.

39
00:01:17,630 --> 00:01:19,880
Vamos a considerar dos tipos de problemas de clasificación.

40
00:01:20,430 --> 00:01:21,780
La primera es la clasificación binaria,

41
00:01:22,970 --> 00:01:25,550
en donde las etiquetas "y" son cero o uno.

42
00:01:26,240 --> 00:01:28,540
En este caso, tendríamos una unidad de salida.

43
00:01:29,140 --> 00:01:30,260
Entonces, esta red neuronal en la parte superior

44
00:01:30,510 --> 00:01:32,430
tiene cuatro unidades de salida, pero si

45
00:01:32,570 --> 00:01:33,960
tuviéramos una clasificación binaria, sólo

46
00:01:34,120 --> 00:01:35,810
tendríamos una unidad de salida

47
00:01:36,720 --> 00:01:38,360
que calculará h de x.

48
00:01:40,310 --> 00:01:41,550
Y las salidas de la

49
00:01:41,630 --> 00:01:42,960
red neuronal serían h

50
00:01:43,140 --> 00:01:45,580
de x, que sería un número real.

51
00:01:46,900 --> 00:01:47,980
Y, en este caso, el número

52
00:01:48,360 --> 00:01:50,240
de unidades de salida, SL, donde

53
00:01:50,480 --> 00:01:51,880
L nuevamente es el índice

54
00:01:52,300 --> 00:01:53,970
de la capa final, porque es

55
00:01:54,240 --> 00:01:55,630
el número de capas que tenemos en la red.

56
00:01:56,570 --> 00:01:57,960
Entonces, el número de unidades que

57
00:01:58,110 --> 00:02:00,060
tenemos en la capa de salida va a ser igual a uno.

58
00:02:01,040 --> 00:02:02,430
En este caso, para simplificar la notación

59
00:02:02,950 --> 00:02:05,340
más tarde, también voy a establecer que k es igual a 1.

60
00:02:05,460 --> 00:02:06,560
Entonces, puedes considerar que k

61
00:02:06,770 --> 00:02:08,240
también denota el número

62
00:02:08,700 --> 00:02:10,780
de unidades en la capa de salida.

63
00:02:11,410 --> 00:02:12,980
El segundo problema de tipo de clasificación

64
00:02:13,280 --> 00:02:15,160
que consideraremos será el problema de clasificación

65
00:02:15,780 --> 00:02:18,020
multiclase, en el que podemos tener k distintas clases.

66
00:02:19,160 --> 00:02:20,760
Entonces, el ejemplo anterior tenía

67
00:02:21,070 --> 00:02:22,530
esta representación para "y"

68
00:02:23,080 --> 00:02:24,900
cuando teníamos cuatro clases y,

69
00:02:25,160 --> 00:02:27,050
en este caso, tendremos K mayúscula

70
00:02:27,340 --> 00:02:29,530
unidades de salida y nuestras hipótesis

71
00:02:30,350 --> 00:02:33,720
mostrarán vectores de salida que tienen K dimensiones.

72
00:02:34,980 --> 00:02:36,230
Y el número de unidades de salida

73
00:02:36,760 --> 00:02:38,390
serán igual a K.

74
00:02:39,000 --> 00:02:40,020
Por lo general, tendremos

75
00:02:40,370 --> 00:02:41,620
una K mayor o igual

76
00:02:41,820 --> 00:02:42,960
a tres en este caso, porque

77
00:02:43,980 --> 00:02:45,340
si tuviéramos dos clases, entonces

78
00:02:45,710 --> 00:02:46,560
no necesitaríamos

79
00:02:46,690 --> 00:02:48,330
utilizar el método de todos contra uno.

80
00:02:48,720 --> 00:02:49,640
Sólo debemos usar el método

81
00:02:49,970 --> 00:02:50,950
todos contra uno si

82
00:02:51,110 --> 00:02:52,460
tenemos una K mayor o

83
00:02:52,740 --> 00:02:54,250
igual a tres clases. Entonces, como

84
00:02:54,470 --> 00:02:56,100
sólo tenemos dos clases, sólo

85
00:02:56,180 --> 00:02:57,670
necesitamos utilizar una unidad de salida.

86
00:02:58,250 --> 00:03:00,870
Ahora, vamos a definir la función de costo para nuestra función de costo para nuestra red neuronal.

87
00:03:03,880 --> 00:03:05,130
La función de costo que usaremos para

88
00:03:05,240 --> 00:03:06,530
la red neuronal será una

89
00:03:06,680 --> 00:03:08,300
generalización de la

90
00:03:08,360 --> 00:03:09,340
que usamos para la regresión

91
00:03:09,510 --> 00:03:11,500
logística. 
Para la regresión logística

92
00:03:12,100 --> 00:03:13,440
solíamos minimizar la

93
00:03:13,510 --> 00:03:14,490
función de costo j de «theta»

94
00:03:15,270 --> 00:03:16,550
que era menos 1 sobre

95
00:03:16,770 --> 00:03:17,760
m de esta función de costo,

96
00:03:18,720 --> 00:03:20,570
y después se suma este término

97
00:03:21,300 --> 00:03:22,660
de regularización adicional, que era la

98
00:03:22,850 --> 00:03:24,020
suma de j igual a

99
00:03:24,700 --> 00:03:26,190
1 hasta n, porque no regularizamos

100
00:03:26,270 --> 00:03:29,760
el término de oscilación «theta» cero.

101
00:03:31,030 --> 00:03:32,590
Para una red neuronal, nuestra función

102
00:03:32,910 --> 00:03:34,490
de costo va a ser una generalización de ésta.

103
00:03:35,650 --> 00:03:37,060
Donde, en lugar de tener básicamente

104
00:03:37,530 --> 00:03:39,360
sólo una unidad de salida de regresión logística,

105
00:03:39,650 --> 00:03:41,650
podemos tener K de éstas.

106
00:03:42,590 --> 00:03:43,520
Entonces, esta es nuestra función de costo.

107
00:03:44,770 --> 00:03:46,300
Ahora la red neuronal muestra vectores

108
00:03:46,720 --> 00:03:47,920
en RK en donde K podría

109
00:03:48,170 --> 00:03:48,830
ser igual a 1 si

110
00:03:49,200 --> 00:03:50,350
tenemos el problema de clasificación binaria.

111
00:03:51,380 --> 00:03:52,240
Voy a utilizar la notación

112
00:03:53,300 --> 00:03:56,470
h de x subíndice i, para denotar la salida “i-nésima”.

113
00:03:57,440 --> 00:03:59,860
Eso es h de x es un vector de K dimensiones.

114
00:04:00,840 --> 00:04:02,590
Y así, el subíndice i sólo

115
00:04:02,960 --> 00:04:04,400
selecciona el elemento “i-nésimo”

116
00:04:05,200 --> 00:04:07,510
del vector mostrado por mi red neuronal.

117
00:04:08,900 --> 00:04:10,050
Mi función de costo, j de

118
00:04:10,180 --> 00:04:11,580
«theta» ahora va

119
00:04:11,760 --> 00:04:13,790
a ser la siguiente, es menos uno

120
00:04:13,940 --> 00:04:14,850
sobre m de la suma

121
00:04:15,420 --> 00:04:16,780
de un término similar a lo que

122
00:04:16,960 --> 00:04:18,990
teníamos en la regresión logística. 
Excepto que

123
00:04:19,300 --> 00:04:20,360
tenemos que esta suma de K

124
00:04:21,020 --> 00:04:22,490
es igual a uno hasta K. La

125
00:04:22,600 --> 00:04:23,650
sumatoria es básicamente una

126
00:04:23,720 --> 00:04:25,580
suma sobre mi unidad de salida K.

127
00:04:26,060 --> 00:04:28,290
Entonces, si tengo cuatro unidades superiores,

128
00:04:29,400 --> 00:04:30,740
esto es que la capa final de mi

129
00:04:30,850 --> 00:04:32,530
red neuronal tiene cuatro unidades

130
00:04:32,860 --> 00:04:34,420
de salida, entonces, esta es

131
00:04:34,700 --> 00:04:35,680
una suma desde

132
00:04:35,900 --> 00:04:37,140
K igual a uno hasta cuatro de

133
00:04:38,050 --> 00:04:40,550
básicamente la función de costo de

134
00:04:42,070 --> 00:04:43,640
los algoritmos de regresión logística, pero sumando

135
00:04:43,750 --> 00:04:45,570
esta función de costo sobre cada una

136
00:04:45,890 --> 00:04:47,120
de mis cuatro unidades de salida.

137
00:04:47,800 --> 00:04:48,970
Entonces, notarás que

138
00:04:49,380 --> 00:04:50,700
esto aplica particularmente

139
00:04:51,400 --> 00:04:53,530
a YK, HK, porque

140
00:04:53,740 --> 00:04:55,040
básicamente estamos tomando la unidad

141
00:04:55,500 --> 00:04:57,020
superior K y comparándola

142
00:04:57,780 --> 00:04:59,590
con el valor de YK, que,

143
00:04:59,810 --> 00:05:02,020
como sabes, es el

144
00:05:02,210 --> 00:05:03,260
de esos vectores para

145
00:05:03,740 --> 00:05:05,110
indicar cuál es la causa.

146
00:05:06,280 --> 00:05:08,060
Y, por último, el segundo término

147
00:05:08,360 --> 00:05:09,490
aquí es el término

148
00:05:10,440 --> 00:05:12,970
de regularización similar a lo que teníamos para la regresión logística.

149
00:05:14,080 --> 00:05:15,640
Estas sumatorias de términos de ven muy

150
00:05:15,850 --> 00:05:17,370
complicadas, y siempre es complicado hacer

151
00:05:17,840 --> 00:05:19,460
una suma con estos términos,

152
00:05:19,950 --> 00:05:21,670
«theta» j  i  l para

153
00:05:21,860 --> 00:05:23,340
todos los valores de  i  j

154
00:05:23,410 --> 00:05:24,830
y l. 
Excepto que

155
00:05:25,010 --> 00:05:26,340
no sumamos los términos

156
00:05:26,710 --> 00:05:28,210
correspondiente a estos valores de oscilación

157
00:05:28,900 --> 00:05:30,000
como lo hicimos para la progresión logística.

158
00:05:30,900 --> 00:05:32,080
En concreto, no sumamos

159
00:05:32,240 --> 00:05:33,590
los términos correspondientes a

160
00:05:34,300 --> 00:05:36,290
cuando i es igual a cero.

161
00:05:36,780 --> 00:05:38,310
Esto se debe a que

162
00:05:38,920 --> 00:05:40,010
cuando calculamos la activación

163
00:05:40,590 --> 00:05:41,930
de la neurona, tenemos términos

164
00:05:42,280 --> 00:05:43,630
como estos, como sabes «theta», i 0

165
00:05:43,810 --> 00:05:47,860
más «theta», i 1,

166
00:05:48,160 --> 00:05:50,410
x1 más, y así

167
00:05:50,520 --> 00:05:51,780
sucesivamente, donde supongo

168
00:05:52,020 --> 00:05:53,310
que podríamos tener un 2

169
00:05:53,490 --> 00:05:54,420
si esta es la primera capa oculta,

170
00:05:55,250 --> 00:05:56,800
de forma que los valores con

171
00:05:57,230 --> 00:05:58,730
el 0 ahí, que corresponden a

172
00:05:58,730 --> 00:06:00,110
algo que se multiplica en un

173
00:06:00,260 --> 00:06:01,460
x0 o un a0 y

174
00:06:02,210 --> 00:06:02,950
así, esto es como

175
00:06:03,120 --> 00:06:04,810
una unidad de oscilación y, por

176
00:06:04,980 --> 00:06:06,020
analogía con lo que hicimos

177
00:06:06,130 --> 00:06:07,680
para la progresión logística, no

178
00:06:07,890 --> 00:06:09,090
sumamos esos términos en

179
00:06:09,160 --> 00:06:11,050
nuestro término de regularización porque no

180
00:06:11,160 --> 00:06:13,470
queremos regularizarlos y

181
00:06:13,670 --> 00:06:15,140
encadenar los valores a 0.

182
00:06:15,360 --> 00:06:16,530
Pero esta sólo es una convención posible,

183
00:06:17,670 --> 00:06:18,670
e incluso si fueras a

184
00:06:18,840 --> 00:06:20,960
sumar sobre, tu sabes, i es igual a 0 hasta

185
00:06:21,200 --> 00:06:22,810
SL, funcionará

186
00:06:23,160 --> 00:06:24,720
más o menos igual y no haría una gran diferencia.

187
00:06:25,530 --> 00:06:26,760
Pero tal vez esta convención

188
00:06:27,500 --> 00:06:28,790
de no regularizar el término de oscilación

189
00:06:29,070 --> 00:06:30,320
sólo es ligeramente más común.

190
00:06:32,960 --> 00:06:34,200
Entonces, esta es la función de costo

191
00:06:34,690 --> 00:06:36,270
que vamos a utilizar para llenar tu propia red.

192
00:06:36,790 --> 00:06:38,130
En el siguiente video empezaremos

193
00:06:38,480 --> 00:06:40,270
a hablar sobre un algoritmo para

194
00:06:40,570 --> 00:06:42,530
intentar optimizar la función de costo.