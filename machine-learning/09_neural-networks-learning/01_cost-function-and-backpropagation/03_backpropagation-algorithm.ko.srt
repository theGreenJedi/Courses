1
00:00:00,090 --> 00:00:01,798
이전 비디오에서는 우리는 신경망의

2
00:00:01,857 --> 00:00:03,868
cost function을 배웠습니다.

3
00:00:04,139 --> 00:00:07,079
이번 비디오에서는 이 cost function을

4
00:00:07,200 --> 00:00:09,062
최소화 하는 알고리즘에 대해 말씀드리겠습니다.

5
00:00:09,240 --> 00:00:12,735
이른바 역전파(back propagation) 알고리즘 입니다.

6
00:00:13,834 --> 00:00:15,380
이것은 이전 비디오에서 설명드린

7
00:00:15,520 --> 00:00:17,905
cost function 입니다.

8
00:00:17,972 --> 00:00:19,438
우리는 J(Θ)를

9
00:00:19,484 --> 00:00:21,161
최소화 하기 위해

10
00:00:21,246 --> 00:00:23,440
매개변수 Θ를 찾아야 하고,

11
00:00:23,530 --> 00:00:25,782
그러기 위해서는 경사도 하강법(gradient descent)이나

12
00:00:25,832 --> 00:00:28,625
다른 최적화된 알고리즘을 사용해야 합니다.

13
00:00:28,675 --> 00:00:30,206
그러므로 우리가 해야 할 것은

14
00:00:30,249 --> 00:00:31,598
입력 매개변수 Θ를 이용해서

15
00:00:31,645 --> 00:00:33,487
J(Θ)를 계산하고

16
00:00:33,540 --> 00:00:34,965
이들을 편미분하는

17
00:00:35,014 --> 00:00:37,364
코드를 쓰는 것입니다.

18
00:00:37,425 --> 00:00:38,763
이들 신경망의

19
00:00:38,790 --> 00:00:40,710
매개변수 Θij^(l)는

20
00:00:40,760 --> 00:00:43,435
실수(R)인 것을

21
00:00:43,492 --> 00:00:44,868
기억하세요.

22
00:00:44,930 --> 00:00:47,185
그리고 이들은 우리가 계산해야 하는

23
00:00:47,249 --> 00:00:48,869
편미분 대상입니다.

24
00:00:48,900 --> 00:00:50,077
cost function J(Θ)를

25
00:00:50,115 --> 00:00:51,840
계산하기 위해서

26
00:00:51,883 --> 00:00:53,986
위에 적은 공식을 사용하겠습니다.

27
00:00:54,042 --> 00:00:55,617
그리고 이 비디오의

28
00:00:55,655 --> 00:00:56,850
남은 시간에서는

29
00:00:56,897 --> 00:00:58,595
이들 편미분을

30
00:00:58,636 --> 00:00:59,952
어떻게 계산할 수 있는지 설명하는데

31
00:00:59,989 --> 00:01:01,994
초점을 맞추겠습니다.

32
00:01:02,031 --> 00:01:03,812
자, 단 하나의

33
00:01:03,858 --> 00:01:05,512
훈련예제가 있는 케이스 부터

34
00:01:05,556 --> 00:01:06,839
설명을 시작하겠습니다.

35
00:01:06,872 --> 00:01:09,385
전체 훈련세트가 (x,y)로 구성된

36
00:01:09,432 --> 00:01:11,301
하나의 훈련 예제로

37
00:01:11,351 --> 00:01:14,006
되어 있다고 상상해 보세요.

38
00:01:14,049 --> 00:01:15,591
저는 훈련예제를

39
00:01:15,629 --> 00:01:16,375
(x1,y1)가 아닌,

40
00:01:16,410 --> 00:01:17,665
(x,y)로 적을 것 입니다.

41
00:01:17,718 --> 00:01:19,980
이 훈련 예제로

42
00:01:20,031 --> 00:01:21,423
수행 할

43
00:01:21,462 --> 00:01:24,332
계산 순서를 살펴 보겠습니다

44
00:01:25,754 --> 00:01:27,129
첫번째로 해야할 일은

45
00:01:27,167 --> 00:01:29,175
입력 x에 대한 가설(=출력)을

46
00:01:29,212 --> 00:01:31,773
계산하기 위해

47
00:01:31,813 --> 00:01:34,238
앞으로 전파시키는 것 입니다.

48
00:01:34,272 --> 00:01:36,734
구체적으로 설명하면,

49
00:01:36,769 --> 00:01:39,025
a(1)은 여기서 입력 입력인

50
00:01:39,071 --> 00:01:41,541
첫번째 레이어의 활성값 입니다.

51
00:01:41,600 --> 00:01:43,452
a(1)에 x를 입력하고

52
00:01:43,505 --> 00:01:45,389
Θ(1)과 a(1)를 곱해

53
00:01:45,435 --> 00:01:47,506
z(2)를 계산합니다.

54
00:01:47,552 --> 00:01:49,919
그리고 a(2) = g(z(2))

55
00:01:49,980 --> 00:01:52,250
즉 z(2)에 sigmoid 함수를 적용시킵니다.

56
00:01:52,310 --> 00:01:53,753
이렇게 하면

57
00:01:53,800 --> 00:01:56,115
활성값 a(2)를 얻을 수 있습니다.

58
00:01:56,162 --> 00:01:58,208
이것은 신경망의 두번째 레에어에 존재합니다.

59
00:01:58,241 --> 00:02:00,649
그리고 또 bias a0^(2) = 1 을 추가해야 합니다.

60
00:02:01,315 --> 00:02:03,132
그 다음 우리는

61
00:02:03,176 --> 00:02:04,966
a(3)과 a(4)를 계산하기 위해

62
00:02:05,013 --> 00:02:08,328
이런 순방향 전파를 2번 더 적용해야 합니다.

63
00:02:08,360 --> 00:02:11,458
또한 a(4)는

64
00:02:11,505 --> 00:02:14,089
가설 h(x)와 같습니다.

65
00:02:14,711 --> 00:02:18,103
이것이 벡터화되어 구현된

66
00:02:18,145 --> 00:02:19,228
순방향 전파 알고리즘 입니다.

67
00:02:19,276 --> 00:02:20,888
그리고 이것은

68
00:02:20,938 --> 00:02:22,280
신경망 내 모든 뉴런의

69
00:02:22,345 --> 00:02:24,056
활성화 값을 계산하는 것을

70
00:02:24,110 --> 00:02:25,948
가능하게 해줍니다.

71
00:02:27,934 --> 00:02:29,608
다음으로,

72
00:02:29,650 --> 00:02:30,967
미분을 계산하기 위해

73
00:02:31,026 --> 00:02:33,589
역 전파라고 불리는 알고리즘을 사용할 것입니다.

74
00:02:34,904 --> 00:02:37,765
역 전파 알고리즘을

75
00:02:37,807 --> 00:02:38,430
직관적으로 보면

76
00:02:38,430 --> 00:02:41,065
각 노드에 대해

77
00:02:41,126 --> 00:02:43,642
δj^(l)을 계산하는 것입니다.

78
00:02:43,676 --> 00:02:45,130
여기서 δj^(l)의 의미는

79
00:02:45,171 --> 00:02:46,310
l 레이어의 j노드에

80
00:02:46,361 --> 00:02:48,511
에러가 있는지 나타내는 것입니다.

81
00:02:48,552 --> 00:02:49,682
다시 정리해보면

82
00:02:49,716 --> 00:02:52,313
aj^(l)은

83
00:02:52,355 --> 00:02:54,138
l 레이어에 있는 j유닛의

84
00:02:54,185 --> 00:02:56,182
활성화를 의미하고,

85
00:02:56,224 --> 00:02:58,001
δj^(l)은

86
00:02:58,045 --> 00:02:59,037
활성화된 노드 안의

87
00:02:59,082 --> 00:03:00,978
오류를 잡는다는

88
00:03:01,012 --> 00:03:03,618
의미 입니다.

89
00:03:03,650 --> 00:03:05,798
그래서 우리는 노드의 활성화가

90
00:03:05,823 --> 00:03:07,975
약간 다를 수 있기를 바랍니다.

91
00:03:08,047 --> 00:03:09,670
구체적으로, 오른쪽에있는

92
00:03:10,270 --> 00:03:11,100
4 개의 레이어를 갖는

93
00:03:11,360 --> 00:03:12,700
신경망을 예로 들겠습니다.

94
00:03:13,440 --> 00:03:15,710
L = 4이고,

95
00:03:16,060 --> 00:03:17,120
각 출력단에 대해서 δj^(l)을 계산하겠습니다.

96
00:03:17,400 --> 00:03:19,130
4번째 레이어에 있는 δj^(4)는

97
00:03:23,380 --> 00:03:24,490
활성 단위 aj^(4)에서 yi를 뺀것과 같고,

98
00:03:24,720 --> 00:03:26,350
여기서 yi의 값은

99
00:03:26,490 --> 00:03:28,650
예제에서 0으로 하겠습니다.

100
00:03:29,900 --> 00:03:32,420
그리고 이 항은

101
00:03:32,580 --> 00:03:34,510
(hΘ(x))j로

102
00:03:34,710 --> 00:03:38,040
쓸수 있습니다. 그렇죠?

103
00:03:38,330 --> 00:03:39,640
이 δ(델타)항은

104
00:03:39,930 --> 00:03:40,900
가설들의

105
00:03:41,290 --> 00:03:43,200
출력항 중 하나와

106
00:03:43,370 --> 00:03:44,870
훈련 세트의

107
00:03:45,570 --> 00:03:46,900
y벡터에서

108
00:03:47,060 --> 00:03:48,610
j번째 요소인

109
00:03:48,750 --> 00:03:49,910
yj 사이에 대한

110
00:03:50,090 --> 00:03:53,340
차이입니다.

111
00:03:56,200 --> 00:03:57,790
그런데 여러분이 이 표현식을 보면

112
00:03:57,970 --> 00:04:00,460
만약 δ, a, y를

113
00:04:01,000 --> 00:04:02,350
벡터로 생각하면,

114
00:04:02,520 --> 00:04:03,760
여러분은

115
00:04:04,030 --> 00:04:05,890
그것들을 가져 와서

116
00:04:06,010 --> 00:04:07,310
벡터화 시켜

117
00:04:07,690 --> 00:04:09,840
구현할 수 있습니다.

118
00:04:10,700 --> 00:04:14,330
그냥 δ^(4) = a^(4) - y 가 됩니다.

119
00:04:14,560 --> 00:04:15,820
여기서

120
00:04:16,540 --> 00:04:18,080
이들 δ^(4), a^(4), y는

121
00:04:18,180 --> 00:04:19,860
각각 신경망의

122
00:04:20,640 --> 00:04:22,040
출력단의 수와 같은

123
00:04:22,250 --> 00:04:24,150
차원(dimension)의 벡터가 됩니다.

124
00:04:25,210 --> 00:04:26,880
그래서 우리는

125
00:04:27,320 --> 00:04:28,670
방금 신경망에서

126
00:04:29,020 --> 00:04:30,170
δ^(4) 에러항을 계산했습니다.

127
00:04:31,440 --> 00:04:32,950
우리가 다음에 할 것은

128
00:04:33,620 --> 00:04:36,280
이전 레이어의 δ을 계산하는 것입니다.

129
00:04:37,210 --> 00:04:38,690
계산 공식은 다음과 같습니다.

130
00:04:39,010 --> 00:04:39,830
δ^(3)는 Θ^(3)의 전치행렬에

131
00:04:40,310 --> 00:04:42,050
δ^(4)를 곱하고,

132
00:04:42,560 --> 00:04:44,190
' .* '의 의미는

133
00:04:44,390 --> 00:04:46,390
MATLAB에서 봤던

134
00:04:47,580 --> 00:04:48,380
행렬의 요소끼리의 곱셈입니다.

135
00:04:49,160 --> 00:04:50,760
Θ^(3)의 전치행렬과

136
00:04:51,020 --> 00:04:52,860
δ^(4)의 곱은 벡터가 되고,

137
00:04:53,480 --> 00:04:55,080
g'(z3)도 벡터입니다.

138
00:04:55,800 --> 00:04:57,370
따라서, ' .* '은

139
00:04:57,530 --> 00:04:59,670
두 벡터들 간의 요소 별 곱셈이 됩니다.

140
00:05:01,460 --> 00:05:02,650
g'(z3) 항은

141
00:05:02,740 --> 00:05:04,560
미분된 활성화 함수 g에

142
00:05:04,950 --> 00:05:06,420
입력값 z3가

143
00:05:06,720 --> 00:05:08,740
입력되어

144
00:05:08,890 --> 00:05:10,620
평가 되었습니다.

145
00:05:10,760 --> 00:05:12,620
여러분이 계산법을 안다면,

146
00:05:12,710 --> 00:05:13,470
여러분 스스로 풀어 보시고

147
00:05:13,850 --> 00:05:16,100
제가 얻은것과 같이 간략하게 되는지 보시기 바랍니다.

148
00:05:16,860 --> 00:05:19,690
하지만 지금은 g'의 미분 결과를 알려드리겠습니다.

149
00:05:20,000 --> 00:05:21,260
이 미분항은

150
00:05:21,460 --> 00:05:23,310
a^(3).* (1-a^(3))이

151
00:05:23,510 --> 00:05:25,660
됩니다.

152
00:05:26,010 --> 00:05:27,900
여기서 a^(3)는

153
00:05:28,160 --> 00:05:29,420
활성화 벡터입니다.

154
00:05:30,150 --> 00:05:31,440
1도 [1;1;1;..] 처럼 생긴

155
00:05:31,600 --> 00:05:33,240
벡터이고,

156
00:05:34,020 --> 00:05:35,970
앞에 또 나온 a^(3)도

157
00:05:36,290 --> 00:05:38,850
세번째 레이어의 활성화 값 입니다.

158
00:05:39,170 --> 00:05:40,210
다음으로 여러분은

159
00:05:40,540 --> 00:05:42,850
유사한 공식을 δ^(2)를

160
00:05:43,220 --> 00:05:45,230
계산하는데 적용할 것이고,

161
00:05:45,670 --> 00:05:47,410
g'(z2)도 비슷하게

162
00:05:48,450 --> 00:05:49,950
a^(2).* (1-a^(2))로

163
00:05:50,120 --> 00:05:53,850
계산될 것입니다.

164
00:05:53,960 --> 00:05:55,020
그리고 여기서는 하지 않을 것이지만,

165
00:05:55,110 --> 00:05:56,400
여러분이

166
00:05:56,490 --> 00:05:57,520
계산하는 방법을 안다면

167
00:05:58,240 --> 00:05:59,520
이 표현은

168
00:05:59,860 --> 00:06:02,010
미분된 활성화 함수인 g'와

169
00:06:02,190 --> 00:06:03,570
수학적으로

170
00:06:04,040 --> 00:06:05,460
동일하다는 것을

171
00:06:05,910 --> 00:06:08,540
증명할 수 있습니다.

172
00:06:09,270 --> 00:06:10,690
마지막으로 끝입니다.

173
00:06:10,860 --> 00:06:13,650
δ^(1)항은 없습니다.

174
00:06:13,720 --> 00:06:15,590
그 이유는 첫번째 레이어는

175
00:06:15,630 --> 00:06:16,940
입력 레이어이기 때문입니다.

176
00:06:17,000 --> 00:06:18,200
그리고 그건 우리 훈련 세트의 feature(=x값) 입니다.

177
00:06:18,300 --> 00:06:20,380
따라서 오류가 있을 수 없습니다.

178
00:06:20,600 --> 00:06:22,080
여러분도 알다시피 우리는

179
00:06:22,120 --> 00:06:23,680
이들 값을 변경하는 것을 원치 않습니다.

180
00:06:24,320 --> 00:06:25,240
즉, 이 예제에서 δ항은

181
00:06:25,510 --> 00:06:28,090
2, 3, 4 레이어에만 존재합니다.

182
00:06:30,170 --> 00:06:32,120
역전파라는 이름은

183
00:06:32,170 --> 00:06:33,260
δ의 계산을

184
00:06:33,350 --> 00:06:34,720
출력단부터 시작해서

185
00:06:34,740 --> 00:06:36,190
세번째 히든 레이어의

186
00:06:36,370 --> 00:06:37,480
δ^(3)의 계산을 거쳐

187
00:06:37,880 --> 00:06:39,670
δ^(2)까지

188
00:06:39,850 --> 00:06:41,050
거꾸로 계산해

189
00:06:41,180 --> 00:06:42,540
나가는 데서

190
00:06:42,770 --> 00:06:44,070
이름 지어졌습니다.

191
00:06:44,660 --> 00:06:46,060
역전파의 결과로 인하여

192
00:06:46,280 --> 00:06:47,270
출력 레이어의 오류는 레이어 3으로

193
00:06:47,650 --> 00:06:50,180
레이어 2로 거꾸로 계산해 나갑니다.

194
00:06:51,270 --> 00:06:53,120
마지막으로 미분은

195
00:06:53,340 --> 00:06:56,510
놀라울 정도로 복잡합니다.

196
00:06:56,820 --> 00:06:58,100
그러나 여러분이

197
00:06:58,280 --> 00:07:00,130
몇 단계의 계산을 수행하면

198
00:07:00,680 --> 00:07:02,540
수학적으로 증명하는 것이

199
00:07:02,810 --> 00:07:04,440
가능합니다.

200
00:07:05,200 --> 00:07:07,410
만약 여러분이

201
00:07:07,560 --> 00:07:09,690
정규화 하는것을 무시한다면

202
00:07:09,800 --> 00:07:11,080
편미분 항은 활성화 항과

203
00:07:12,220 --> 00:07:14,650
δ항으로 주어진 다는 것으로

204
00:07:14,780 --> 00:07:17,690
증명할 수 있습니다.

205
00:07:17,870 --> 00:07:20,630
이것은

206
00:07:20,780 --> 00:07:22,730
람다(λ)를 생략하거나

207
00:07:23,770 --> 00:07:24,630
혹은 정규화 변수 λ값이

208
00:07:25,000 --> 00:07:25,170
0과 같다는 것입니다.

209
00:07:25,680 --> 00:07:27,130
우리는 추후에 정규화 항을

210
00:07:27,470 --> 00:07:29,430
디테일하게 고칠 것 입니다.

211
00:07:29,620 --> 00:07:30,740
역 전파를 수행하고

212
00:07:31,610 --> 00:07:32,820
이러한 델타 항을 계산함으로써

213
00:07:33,180 --> 00:07:34,240
여러분은 모든 변수에 대해

214
00:07:34,530 --> 00:07:36,320
이 편미분 항을

215
00:07:36,380 --> 00:07:38,150
매우 빠르게 계산할 수 있습니다.

216
00:07:38,920 --> 00:07:40,020
자, 여기까지 많은 것을 설명했습니다.

217
00:07:40,570 --> 00:07:41,900
이제는 모든 훈련세트를 가져와서

218
00:07:42,320 --> 00:07:43,660
그것들을 집어넣을 때

219
00:07:44,120 --> 00:07:45,490
어떻게 매개변수에 대한 미분값을 계산하는

220
00:07:46,560 --> 00:07:48,590
역전파를 구현하는 방법에 대해 이야기 해 보겠습니다.

221
00:07:49,790 --> 00:07:50,770
우리는 한 예제만 있는 훈련세트가 아닌

222
00:07:51,000 --> 00:07:52,460
거대한 훈련세트를

223
00:07:52,830 --> 00:07:53,850
가지고 있는 경우 입니다.

224
00:07:54,100 --> 00:07:56,320
자, 여기 우리가 살펴볼 것이 있습니다.

225
00:07:57,290 --> 00:07:58,140
여기 보이는 것처럼

226
00:07:58,270 --> 00:07:59,750
m개 예제로 구성된 훈련 세트를

227
00:07:59,900 --> 00:08:01,610
가지고 있다고 가정해 보세요.

228
00:08:01,850 --> 00:08:02,600
첫번째로 우리가 해야할 것은

229
00:08:03,220 --> 00:08:04,560
델타 Δij^(l)을 정하는 것입니다.

230
00:08:05,100 --> 00:08:07,270
여기 삼각형 기호가 보이시나요?

231
00:08:08,090 --> 00:08:09,990
이것은 실제 그리스의

232
00:08:10,310 --> 00:08:11,980
알파벳 델타입니다.

233
00:08:12,050 --> 00:08:14,080
우리가 지금까지 사용한 δ기호는 소문자 델타 입니다.

234
00:08:14,390 --> 00:08:16,810
그래서 삼각형 Δ은 대문자 델타 입니다.

235
00:08:17,430 --> 00:08:18,490
우리는 이 델타를 모든 l, i, j값에 대해

236
00:08:18,680 --> 00:08:21,930
0과 같게 할 것입니다.

237
00:08:22,110 --> 00:08:23,850
결국엔 이 델타 Δij^(l)는

238
00:08:24,530 --> 00:08:25,830
J(Θ)의

239
00:08:26,860 --> 00:08:29,920
편미분항을

240
00:08:30,290 --> 00:08:31,570
계산하는데

241
00:08:32,380 --> 00:08:35,240
사용될

242
00:08:35,430 --> 00:08:37,190
것입니다.

243
00:08:39,040 --> 00:08:40,210
두번째로 우리가 살펴볼 것은

244
00:08:40,480 --> 00:08:41,550
이들 델타들은

245
00:08:41,670 --> 00:08:43,700
축적자로서 사용될 것입니다.

246
00:08:43,950 --> 00:08:45,360
축적자는 편미분을 계산하기 위해서

247
00:08:45,700 --> 00:08:47,130
값들을 더하는 저장소 입니다.

248
00:08:49,570 --> 00:08:51,920
다음엔 우리는 훈련세트 전체에 대한 루프를 돌 것입니다.

249
00:08:52,150 --> 00:08:53,270
i=1부터 m까지

250
00:08:53,610 --> 00:08:55,400
반복하면서

251
00:08:55,620 --> 00:08:57,270
훈련예제 (xi,yi)를 가지고

252
00:08:57,410 --> 00:08:59,180
학습시킬 것 입니다.

253
00:09:00,480 --> 00:09:03,220
따라서

254
00:09:03,720 --> 00:09:04,590
그래서 첫번째 루프에서

255
00:09:04,690 --> 00:09:06,120
우리는 입력 레이어의

256
00:09:06,570 --> 00:09:07,830
활성화된 a(1)에

257
00:09:08,190 --> 00:09:09,030
i번째 훈련예제

258
00:09:09,950 --> 00:09:11,800
x(i)를 대입할 것입니다.

259
00:09:12,670 --> 00:09:15,070
그리고

260
00:09:15,340 --> 00:09:17,590
우리는 레이어2, 레이어3,…,

261
00:09:17,730 --> 00:09:19,400
최종 레이어 L까지의

262
00:09:19,790 --> 00:09:20,900
활성화를 계산하기 위해

263
00:09:21,170 --> 00:09:22,050
순방향 전파를

264
00:09:22,500 --> 00:09:25,190
수행할 것입니다.

265
00:09:25,570 --> 00:09:26,970
그 다음에 출력 레이블인 yi를

266
00:09:27,280 --> 00:09:28,530
에러항인 δ^(L)을

267
00:09:28,680 --> 00:09:29,870
계산하기 위해

268
00:09:30,340 --> 00:09:31,650
위와 같이

269
00:09:31,950 --> 00:09:34,140
사용할 것입니다.

270
00:09:34,480 --> 00:09:35,730
그래서 δ^(L)은

271
00:09:35,880 --> 00:09:38,190
가설의 출력인 a(L)에서

272
00:09:38,660 --> 00:09:39,870
yi를 뺀 값입니다.

273
00:09:41,840 --> 00:09:42,560
그리고 우리는

274
00:09:42,850 --> 00:09:44,550
역전파 알고리즘을

275
00:09:44,740 --> 00:09:46,020
δ^(L-1),

276
00:09:46,220 --> 00:09:47,250
δ^(L-2),…,

277
00:09:47,350 --> 00:09:49,880
δ^(2)까지 계산하는데 사용합니다.

278
00:09:50,270 --> 00:09:51,380
다시한번 설명하지만, δ^(1)은

279
00:09:51,460 --> 00:09:54,380
입력 레이어 이기 때문에 에러와 관련 없습니다.

280
00:09:57,000 --> 00:09:58,160
마지막으로 우리는

281
00:09:58,340 --> 00:10:00,650
이전 줄에서 적었던

282
00:10:01,190 --> 00:10:02,800
편미분항을 축적하기 위해서

283
00:10:03,400 --> 00:10:05,670
델타 Δij^(l)를 사용할 것입니다.

284
00:10:06,870 --> 00:10:07,870
그런데 여러분이 이 표현식을 보면

285
00:10:07,960 --> 00:10:11,340
이것도 역시 벡터화 시킬 수 있는 걸 알수 있습니다.

286
00:10:12,020 --> 00:10:13,040
구체적으로 설명드리면

287
00:10:13,310 --> 00:10:14,860
Δij를 매트릭스로 생각하고

288
00:10:15,000 --> 00:10:18,090
아래첨자 ij로 요소를 표현한다면

289
00:10:19,220 --> 00:10:20,590
매트릭스 Δ^(l)의 값은

290
00:10:20,780 --> 00:10:22,040
Δ^(l)와

291
00:10:22,130 --> 00:10:24,100
δ^(l+1)과

292
00:10:24,350 --> 00:10:26,710
a(l)의 전치행렬의

293
00:10:27,830 --> 00:10:29,370
곱을 더한 값으로

294
00:10:29,640 --> 00:10:32,780
업데이트 될 것입니다.

295
00:10:33,570 --> 00:10:35,380
이것은 모든 i,j의 값에 대해서

296
00:10:35,520 --> 00:10:37,150
자동으로 업데이트 되는

297
00:10:37,590 --> 00:10:38,850
벡터를 구현한 것입니다.

298
00:10:39,010 --> 00:10:41,250
결국 이 for 루프의

299
00:10:41,500 --> 00:10:43,480
바디 내용을 실행한 후,

300
00:10:43,580 --> 00:10:45,350
루프 밖을 나와서

301
00:10:46,330 --> 00:10:47,000
다음을 계산합니다.

302
00:10:47,440 --> 00:10:49,690
우리는 대문자 D를

303
00:10:50,020 --> 00:10:51,400
다음과 같이 계산합니다.

304
00:10:51,510 --> 00:10:52,750
그리고 j<>0, j=0인

305
00:10:52,980 --> 00:10:54,890
두개의 다른 케이스를 얻습니다.

306
00:10:56,080 --> 00:10:57,250
j=0인 케이스는

307
00:10:57,680 --> 00:10:58,730
bias항에 대응됩니다.

308
00:10:59,150 --> 00:11:00,030
그래서 j=0일때,

309
00:11:00,390 --> 00:11:01,320
추가적인 정규화 항이

310
00:11:01,800 --> 00:11:03,320
없는지에 대한 이유입니다.

311
00:11:05,470 --> 00:11:06,850
마지막으로, 공식 증명은

312
00:11:07,180 --> 00:11:08,970
꽤 복잡하지만,

313
00:11:09,030 --> 00:11:10,410
여러분들이

314
00:11:10,640 --> 00:11:12,530
일단 D 항을 계산해보면

315
00:11:13,510 --> 00:11:15,230
이는 각 매개 변수에 대한

316
00:11:15,640 --> 00:11:17,610
cost function의 편미분 이므로,

317
00:11:17,920 --> 00:11:19,230
여러분은

318
00:11:19,470 --> 00:11:20,890
이것들을

319
00:11:21,040 --> 00:11:22,470
경사도하강법이나

320
00:11:22,610 --> 00:11:23,530
다른 고급 알고리즘에서

321
00:11:25,450 --> 00:11:25,450
사용할 수 있습니다.

322
00:11:28,310 --> 00:11:29,360
여기까지

323
00:11:29,990 --> 00:11:31,110
역전파 알고리즘과

324
00:11:31,470 --> 00:11:33,080
어떻게 신경망의 cost function을

325
00:11:33,340 --> 00:11:34,710
미분하는지 설명 드렸습니다.

326
00:11:35,470 --> 00:11:36,330
오늘 다룬 이 내용이 많은 내용을 담고있고,

327
00:11:36,470 --> 00:11:38,810
많은 단계가 서로 얽혀 있다는것을 저도 알고 있습니다.

328
00:11:39,460 --> 00:11:40,770
그러나 프로그래밍 과제와

329
00:11:41,100 --> 00:11:43,010
이 비디오의 뒷부분에서

330
00:11:43,110 --> 00:11:44,580
이 알고리즘에 대한 요약해 드리겠습니다.

331
00:11:44,720 --> 00:11:45,900
그래서 모든 종류의 알고리즘을

332
00:11:46,050 --> 00:11:46,830
함께 다룰 수 있을 것입니다.

333
00:11:47,260 --> 00:11:48,780
그래서 여러분이 신경망의 cost function과

334
00:11:48,920 --> 00:11:50,550
관련된 매개변수를 계산하는 미분을

335
00:11:50,610 --> 00:11:51,760
계산하기 위해

336
00:11:51,940 --> 00:11:53,460
역전파를 구현하는 방법을

337
00:11:53,890 --> 00:11:56,432
정확하게 알수 있게

338
00:11:56,574 --> 00:11:59,348
해드리겠습니다.