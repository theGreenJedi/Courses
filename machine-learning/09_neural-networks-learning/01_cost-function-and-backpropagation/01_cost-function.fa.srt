1
00:00:00,620 --> 00:00:03,800
توی این ویدیو چیزی رو مطرح میکنیم
که به نام تابع هزینه نامیده شده ،

2
00:00:03,800 --> 00:00:07,480
این بهمون اجازه میده که بهترین شکل ممکن 
برای انطباق خط مستقیم روی داده مون رو بدست بیاریم .

3
00:00:10,310 --> 00:00:13,820
در رگرسیون خطی ، مجموعه آموزشی رو داریم 
که اینجا نشون دادم ، یادتون بیاد که

4
00:00:13,820 --> 00:00:18,870
علامت ام ، تعداد نمومه های آموزشی بودن ،
پس ممکنه ام برابر با 47 باشه .

5
00:00:18,870 --> 00:00:20,989
و شکل فرضیه مون ،

6
00:00:22,210 --> 00:00:25,360
که برای پیش بینی ها استفاده میکنیم ،
تابع خطی هست .

7
00:00:26,430 --> 00:00:31,240
برای معرفی کمی تخصصی تر ،
این تتا صفر و

8
00:00:31,240 --> 00:00:37,260
تتا یک ، ثبت کننده چیزی 
هستند که اونارو پارامترهای مالدو می نامم .

9
00:00:37,260 --> 00:00:42,560
و چیزیکه میخوایم توی این ویدیو
دربارش حرف زنیم چگونگی

10
00:00:42,560 --> 00:00:47,550
انتخاب کردن مقادیر این دو پارامتر هست ،
تتا 0 و تتا 1 .

11
00:00:47,550 --> 00:00:51,100
با انتخاب متفاوت پارامترهای 
تتا 0 و تتا 1 ،

12
00:00:51,100 --> 00:00:55,250
فرضیه متفاوتی رو بدست میاریم ،
توابع فرضیه متفاوتی رو .

13
00:00:55,250 --> 00:00:58,170
میدونم بعضی از شماها 
احتمالا تا حالا آشنایید دارین با

14
00:00:58,170 --> 00:01:02,110
کاری که توی این اسلاید میخوام بکنم ،
ولی فقط برای یادآوری ، اینجا چندتا نمونه داریم .

15
00:01:02,110 --> 00:01:05,990
اگر تتا 9 برابر 1.5 و تتا 1 برابر با 0 باشه ،

16
00:01:05,990 --> 00:01:08,870
پس تابع فرضیه چیزی 
شبیه این میشه .

17
00:01:10,070 --> 00:01:17,610
چون تابع فرضیه تون خواهد بود 
اچ از ایکس برابر با 1.5 بعلاوه 0 ضربدر

18
00:01:17,610 --> 00:01:22,533
ایکس که این مقدار تابع 
ثابته که منطبق بر روی 1.5 هست .

19
00:01:22,533 --> 00:01:26,600
اگر تتا 0 برابر با 0 ، تتا 1 برابر با 0.5 ، پس 
فرضیه چیزی شبیه این خواهد بود ، و

20
00:01:26,600 --> 00:01:31,420
باید از این نقطه 2 و 1 بگذره، خب

21
00:01:31,420 --> 00:01:34,850
همونطور که میدونین حالا دارین اچ ایکس .

22
00:01:34,850 --> 00:01:40,150
یا درواقع اچ تتا ایکس ، ولی بعضی وقتا 
بخاطر مختصر کردن تتا رو حذف میکنم .

23
00:01:40,150 --> 00:01:45,570
پس اچ ایکس برابر با فقط 0.5 در 
ایکس خواهد بود ، که مثل اینه

24
00:01:45,570 --> 00:01:49,830
و درآخر ، اگر تتا صفر برابر با یک باشه ،
و تتا یک برابر با 0.5 ،

25
00:01:49,830 --> 00:01:53,280
پس فرضیه ایی بدست میاریم که 
شبیه این خواهد بود .

26
00:01:53,280 --> 00:01:59,670
بزارین ببینیم ،
از نقطه دو - دو خواهد گذشت .

27
00:01:59,670 --> 00:02:04,640
بدین ترتیب ، و این بردار ایکس جدیدمه ،
یا اچ تتا ایکس جدیدمه .

28
00:02:04,640 --> 00:02:08,618
هر راهی که یادتون میاد ، گفتم این اچ 
تتا ایکس هست ، ولی

29
00:02:08,618 --> 00:02:12,095
اون اختصاریش هست ،
بعضی وقتا این رو فقط مینویسم اچ ایکس .

30
00:02:13,917 --> 00:02:19,330
در رگرسیون خطی ، مجموعه آموزشی ایی رو 
داریم ، احتمالا مثل اونی که چاپ کردم اینجا .

31
00:02:19,330 --> 00:02:24,880
کاری که میخوایم انجام بدیم ، 
بدست آوردن مقادیر حالت های صفر و

32
00:02:24,880 --> 00:02:29,960
تتا یک هست ، بنابراین خط مستقیمی که 
ازش بدست میاریم ، وابسته به

33
00:02:29,960 --> 00:02:33,500
خط مستقیمی هست که بعضی وقتا به خوبی
روی داده منطبق میشه ، احتمالا شبیه این خط اینجا .

34
00:02:34,590 --> 00:02:37,190
پس ، چطور مقادیر رو بدست میاریم ،

35
00:02:37,190 --> 00:02:40,650
تتا صفر ، تتا یک ، که وابسته به 
خطی منطبق بر روی داده باشه ؟

36
00:02:42,540 --> 00:02:46,460
ایده انتخاب پارامترهای تتا 0 ، 
تتا 1 هست ، پس

37
00:02:46,460 --> 00:02:51,190
اچ ایکس ، به معنی مقداری که پیش بینی
میکنیم بر روی ایکس هست .

38
00:02:51,190 --> 00:02:55,730
اون حداقل نزدیک به مقادیر ایگرگ برای

39
00:02:55,730 --> 00:02:59,908
نمونه های مجموعه آموزش مون 
توی نمونه های آموزشی مون هست .

40
00:02:59,908 --> 00:03:04,000
پس توی مجموعه آموزشی مون ، تعدادی 
نمونه دادیم که میدونیم ایکس

41
00:03:04,000 --> 00:03:07,350
تصمیم گیرنده کلی هست و 
میدونیم قیمت واقعی فروش چی بوده .

42
00:03:07,350 --> 00:03:11,100
پس ، بزارین مقادیری رو برای پارامترها
انتخاب کنیم که خب ،

43
00:03:11,100 --> 00:03:13,830
حداقل توی مجموعه آموزشی ، ایکس داده شده .

44
00:03:13,830 --> 00:03:19,040
توی مجموعه آموزشی دلیلی علتی برای 
پیش بینی های فعال  برای مقادیر ایگرگ میسازیم .

45
00:03:19,040 --> 00:03:20,980
بزارین فرمول بندیش کنیم .

46
00:03:20,980 --> 00:03:23,700
بنابراین ، پس رگرسیون ،
کاری که میخوایم انجام بدیم اینه ،

47
00:03:23,700 --> 00:03:27,430
میخوام مساله رو با کمینه سازی
حل کنم .

48
00:03:27,430 --> 00:03:34,319
پس خواهم نوشت کمینه روی
تتا 0 ، تتا 1 .

49
00:03:34,319 --> 00:03:39,620
و میخوام که کوچیک باشه ، درست ؟

50
00:03:39,620 --> 00:03:42,960
میخوام اختلاف بین اچ ایکس و ایگرگ
کوچیک باشه .

51
00:03:42,960 --> 00:03:47,770
و چیزی که میخوام انجام بدم 
سعی بر کمینه کردن اختلاف مربع

52
00:03:47,770 --> 00:03:51,226
بین خروجی فرضیه و قیمت واقعی 
خانه است .

53
00:03:51,226 --> 00:03:54,600
بسیارخب .
بزارین کمی جزییات پیداکنیم .

54
00:03:54,600 --> 00:03:59,328
یادتونه که از علامت های ایکس آی،
ایگرگ آی استفاده کردم

55
00:03:59,328 --> 00:04:02,380
به نمایندگی آی امین نمونه مجموعه آموزشی .

56
00:04:02,380 --> 00:04:07,480
خب چیزی که درواقع میخوام 
خلاصه سازی روی مجموعه آموزشیمه ،

57
00:04:07,480 --> 00:04:10,666
چیزی مثل آی مساوی 1 به ام ،

58
00:04:10,666 --> 00:04:16,040
از اختلاف مربع مابین شان ،
پیش بینی

59
00:04:16,040 --> 00:04:21,261
فرضیه ام است ، وقتیکه 
ورودی اندازه خانه شماره آی هست .

60
00:04:22,560 --> 00:04:25,530
درست ؟ منهای قیمت واقعی شماره 
خانه ایی که

61
00:04:25,530 --> 00:04:29,630
فروختم به اون قیمت ، و میخوام که
خلاصه مجموعه داده ام رو کمینه کنم .

62
00:04:29,630 --> 00:04:34,240
خلاصه آی برابره با یک از طریق ام 
اختلاف خطای مربع این .

63
00:04:34,240 --> 00:04:37,160
اختلاف مربع بین قیمت پیش بینی شده خانه و

64
00:04:37,160 --> 00:04:40,550
و قیمتی که درواقع فروخته شده .

65
00:04:40,550 --> 00:04:46,950
و فقط برای یادآوری تون از علامت ها ،
ام در اینجا اندازه مجموعه داده آموزشیم بود . درسته ؟

66
00:04:46,950 --> 00:04:50,570
So my m there is my number
of training examples.

67
00:04:50,570 --> 00:04:57,750
Right that hash sign is the abbreviation
for number of training examples, okay?

68
00:04:57,750 --> 00:05:01,270
And to make some of our,
make the math a little bit easier,

69
00:05:01,270 --> 00:05:05,950
I'm going to actually look at
we are 1 over m times that so

70
00:05:05,950 --> 00:05:09,380
let's try to minimize my
average minimize one over 2m.

71
00:05:09,380 --> 00:05:14,450
Putting the 2 at the constant one
half in front, it may just sound

72
00:05:14,450 --> 00:05:18,730
the math probably easier so minimizing
one-half of something, right, should give

73
00:05:18,730 --> 00:05:23,130
you the same values of the process, theta
0 theta 1, as minimizing that function.

74
00:05:24,300 --> 00:05:27,640
And just to be sure,
this equation is clear, right?

75
00:05:27,640 --> 00:05:31,452
This expression in here, h subscript

76
00:05:31,452 --> 00:05:36,560
theta(x), this is our usual, right?

77
00:05:37,890 --> 00:05:42,668
That is equal to this plus theta one xi.

78
00:05:42,668 --> 00:05:48,050
And this notation,
minimize over theta 0 theta 1, this means

79
00:05:48,050 --> 00:05:53,140
you'll find me the values of theta 0 and
theta 1 that causes this expression

80
00:05:53,140 --> 00:05:57,620
to be minimized and this expression
depends on theta 0 and theta 1, okay?

81
00:05:57,620 --> 00:05:58,710
So just a recap.

82
00:05:58,710 --> 00:06:03,380
We're closing this problem as, find me
the values of theta zero and theta one so

83
00:06:03,380 --> 00:06:07,210
that the average, the 1 over the 2m,

84
00:06:07,210 --> 00:06:11,240
times the sum of square errors between
my predictions on the training set

85
00:06:11,240 --> 00:06:15,250
minus the actual values of the houses
on the training set is minimized.

86
00:06:15,250 --> 00:06:20,709
So this is going to be my overall
objective function for linear regression.

87
00:06:22,080 --> 00:06:27,250
And just to rewrite this out a little bit
more cleanly, what I'm going to do is,

88
00:06:27,250 --> 00:06:29,790
by convention we usually
define a cost function,

89
00:06:31,240 --> 00:06:35,930
which is going to be exactly this,
that formula I have up here.

90
00:06:37,040 --> 00:06:45,289
And what I want to do is
minimize over theta0 and theta1.

91
00:06:45,289 --> 00:06:51,770
My function j(theta0, theta1).

92
00:06:51,770 --> 00:06:52,430
Just write this out.

93
00:06:53,730 --> 00:06:56,540
This is my cost function.

94
00:06:59,380 --> 00:07:04,960
So, this cost function is also
called the squared error function.

95
00:07:06,850 --> 00:07:11,190
When sometimes called the squared
error cause function and

96
00:07:11,190 --> 00:07:15,730
it turns out that why do we
take the squares of the areas.

97
00:07:15,730 --> 00:07:19,660
It turns out that these squared error
cause function is a reasonable choice and

98
00:07:19,660 --> 00:07:22,990
works well for problems for
most regression programs.

99
00:07:22,990 --> 00:07:25,740
There are other cost functions
that will work pretty well.

100
00:07:25,740 --> 00:07:29,860
But the square cost function is
probably the most commonly used one for

101
00:07:29,860 --> 00:07:30,935
regression problems.

102
00:07:30,935 --> 00:07:34,980
Later in this class we'll talk about
alternative cost functions as well,

103
00:07:34,980 --> 00:07:39,085
but this choice that we just had should
be a pretty reasonable thing to try for

104
00:07:39,085 --> 00:07:41,030
most linear regression problems.

105
00:07:42,340 --> 00:07:43,030
Okay.

106
00:07:43,030 --> 00:07:44,280
So that's the cost function.

107
00:07:45,340 --> 00:07:50,840
So far we've just seen a mathematical
definition of this cost function.

108
00:07:50,840 --> 00:07:54,310
In case this function j of theta zero,
theta one.

109
00:07:54,310 --> 00:07:56,260
In case this function seems
a little bit abstract,

110
00:07:56,260 --> 00:07:58,885
and you still don't have a good
sense of what it's doing,

111
00:07:58,885 --> 00:08:03,210
in the next video, in the next
couple videos, I'm actually going

112
00:08:03,210 --> 00:08:07,930
to go a little bit deeper into what
the cause function "J" is doing and try to

113
00:08:07,930 --> 00:08:11,730
give you better intuition about what
is computing and why we want to use it.