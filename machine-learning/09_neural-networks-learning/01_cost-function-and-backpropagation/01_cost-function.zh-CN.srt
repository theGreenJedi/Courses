1
00:00:00,270 --> 00:00:01,380
神经网络是当今

2
00:00:01,450 --> 00:00:03,610
最强大的学习算法之一

3
00:00:04,310 --> 00:00:05,490
在本节课视频

4
00:00:05,590 --> 00:00:06,690
和后面几次课程中

5
00:00:06,760 --> 00:00:08,030
我将开始讲述一种

6
00:00:08,380 --> 00:00:09,920
在给定训练集下

7
00:00:10,630 --> 00:00:12,530
为神经网络拟合参数的学习算法

8
00:00:13,460 --> 00:00:14,840
正如我们讨论大多数

9
00:00:15,020 --> 00:00:16,300
学习算法一样

10
00:00:16,450 --> 00:00:17,860
我们准备从拟合神经网络参数的

11
00:00:17,960 --> 00:00:20,510
代价函数开始讲起

12
00:00:21,170 --> 00:00:22,650
我准备重点讲解

13
00:00:23,270 --> 00:00:24,790
神经网络在分类问题

14
00:00:25,060 --> 00:00:26,590
中的应用

15
00:00:27,660 --> 00:00:28,860
假设我们有一个如左边所示的

16
00:00:29,120 --> 00:00:31,300
神经网络结构

17
00:00:31,530 --> 00:00:32,510
然后假设我们有一个

18
00:00:32,710 --> 00:00:33,850
像这样的训练集

19
00:00:33,980 --> 00:00:36,550
m个训练样本x(i) y(i)

20
00:00:37,920 --> 00:00:39,040
我用大写字母 L

21
00:00:39,450 --> 00:00:40,640
来表示

22
00:00:40,790 --> 00:00:42,460
这个神经网络结构的总层数

23
00:00:43,330 --> 00:00:44,550
所以 对于左边的网络结构

24
00:00:44,810 --> 00:00:45,720
我们得到

25
00:00:46,370 --> 00:00:47,920
L等于4

26
00:00:48,020 --> 00:00:48,910
然后我准备用

27
00:00:49,180 --> 00:00:50,740
sl表示

28
00:00:51,260 --> 00:00:52,540
第L层的单元的数量

29
00:00:52,730 --> 00:00:54,490
也就是神经元的数量

30
00:00:54,770 --> 00:00:57,180
这其中不包括L层的偏差单元

31
00:00:57,900 --> 00:00:59,440
比如说

32
00:00:59,580 --> 00:01:01,280
我们得到s1 也就是输入层

33
00:01:01,370 --> 00:01:03,330
是等于3的单元

34
00:01:04,140 --> 00:01:05,970
s2在这个例子里等于5个单位

35
00:01:06,900 --> 00:01:08,670
然后输出层s4

36
00:01:09,940 --> 00:01:12,820
也就是sl 因为L本身等于4

37
00:01:12,990 --> 00:01:14,290
在左边这个例子中输出层

38
00:01:14,450 --> 00:01:16,230
有4个单位

39
00:01:17,630 --> 00:01:19,880
我们将会讨论两种分类问题

40
00:01:20,430 --> 00:01:21,780
第一种是二元分类

41
00:01:22,970 --> 00:01:25,550
在这里y只能等于0或1

42
00:01:26,240 --> 00:01:28,540
在这个例子中 我们有一个输出单元

43
00:01:29,140 --> 00:01:30,260
上面这个神经网络的有四个输出单元

44
00:01:30,510 --> 00:01:32,430
但是如果我们

45
00:01:32,570 --> 00:01:33,960
用二元分类的话

46
00:01:34,120 --> 00:01:35,810
我们就只能有一个输出结果

47
00:01:36,720 --> 00:01:38,360
也就是计算出来的h(x)

48
00:01:40,310 --> 00:01:41,550
神经网络的输出结果

49
00:01:41,630 --> 00:01:42,960
h(x)就会是

50
00:01:43,140 --> 00:01:45,580
一个实数

51
00:01:46,900 --> 00:01:47,980
在这类问题里

52
00:01:48,360 --> 00:01:50,240
输出单元的个数 sl

53
00:01:50,480 --> 00:01:51,880
L同样代表最后一层的序号

54
00:01:52,300 --> 00:01:53,970
因为这就是我们

55
00:01:54,240 --> 00:01:55,630
在这个网络结构中的层数

56
00:01:56,570 --> 00:01:57,960
所以我们在输出层的单元数目

57
00:01:58,110 --> 00:02:00,060
就将是1

58
00:02:01,040 --> 00:02:02,430
在这类问题里 为了简化记法

59
00:02:02,950 --> 00:02:05,340
我会把K设为1

60
00:02:05,460 --> 00:02:06,560
这样你可以把K看作

61
00:02:06,770 --> 00:02:08,240
输出层的

62
00:02:08,700 --> 00:02:10,780
单元数目

63
00:02:11,410 --> 00:02:12,980
我们要考虑的第二类分类问题

64
00:02:13,280 --> 00:02:15,160
就是多类别的分类问题

65
00:02:15,780 --> 00:02:18,020
也就是会有K个不同的类

66
00:02:19,160 --> 00:02:20,760
比如说

67
00:02:21,070 --> 00:02:22,530
如果我们有四类的话

68
00:02:23,080 --> 00:02:24,900
我们就用这样的表达形式来代表y

69
00:02:25,160 --> 00:02:27,050
在这类问题里 我们就会有K个输出单元

70
00:02:27,340 --> 00:02:29,530
我们的假设输出

71
00:02:30,350 --> 00:02:33,720
就是一个K维向量

72
00:02:34,980 --> 00:02:36,230
输出单元的个数

73
00:02:36,760 --> 00:02:38,390
就等于K

74
00:02:39,000 --> 00:02:40,020
通常这类问题里

75
00:02:40,370 --> 00:02:41,620
我们都有K大于

76
00:02:41,820 --> 00:02:42,960
或等于3

77
00:02:43,980 --> 00:02:45,340
因为如果只有两个类别

78
00:02:45,710 --> 00:02:46,560
我们就不需要

79
00:02:46,690 --> 00:02:48,330
使用这种一对多的方法

80
00:02:48,720 --> 00:02:49,640
我们只有在K大于

81
00:02:49,970 --> 00:02:50,950
或者等于3个类的时候

82
00:02:51,110 --> 00:02:52,460
才会使用这种

83
00:02:52,740 --> 00:02:54,250
一对多的方法

84
00:02:54,470 --> 00:02:56,100
因为如果只有两个类别

85
00:02:56,180 --> 00:02:57,670
我们就只需要一个输出单元就可以了

86
00:02:58,250 --> 00:03:00,870
现在我们来为神经网络定义代价函数

87
00:03:03,880 --> 00:03:05,130
我们在神经网络里

88
00:03:05,240 --> 00:03:06,530
使用的代价函数

89
00:03:06,680 --> 00:03:08,300
应该是逻辑回归里

90
00:03:08,360 --> 00:03:09,340
使用的代价函数的一般化形式

91
00:03:09,510 --> 00:03:11,500
对于逻辑回归而言

92
00:03:12,100 --> 00:03:13,440
我们通常使代价函数 J(θ)

93
00:03:13,510 --> 00:03:14,490
最小化

94
00:03:15,270 --> 00:03:16,550
也就是-1/m

95
00:03:16,770 --> 00:03:17,760
乘以后面这个代价函数

96
00:03:18,720 --> 00:03:20,570
然后再加上这个额外正则化项

97
00:03:21,300 --> 00:03:22,660
这里是一个

98
00:03:22,850 --> 00:03:24,020
j从1到n的求和形式

99
00:03:24,700 --> 00:03:26,190
因为我们

100
00:03:26,270 --> 00:03:29,760
并没有把偏差项 0正则化

101
00:03:31,030 --> 00:03:32,590
对于一个神经网络来说

102
00:03:32,910 --> 00:03:34,490
我们的代价函数是这个式子的一般化形式

103
00:03:35,650 --> 00:03:37,060
这里不再是仅有一个

104
00:03:37,530 --> 00:03:39,360
逻辑回归输出单元

105
00:03:39,650 --> 00:03:41,650
取而代之的是K个

106
00:03:42,590 --> 00:03:43,520
所以这是我们的代价函数

107
00:03:44,770 --> 00:03:46,300
神经网络现在输出了

108
00:03:46,720 --> 00:03:47,920
在K维的向量

109
00:03:48,170 --> 00:03:48,830
这里K可以取到1 也就是

110
00:03:49,200 --> 00:03:50,350
原来的二元分类问题

111
00:03:51,380 --> 00:03:52,240
我准备用这样一个记法

112
00:03:53,300 --> 00:03:56,470
h(x)带下标i 来表示第i个输出

113
00:03:57,440 --> 00:03:59,860
也就是h(x)是一个K维向量

114
00:04:00,840 --> 00:04:02,590
下标 i 表示

115
00:04:02,960 --> 00:04:04,400
选择了神经网络输出向量的

116
00:04:05,200 --> 00:04:07,510
第i个元素

117
00:04:08,900 --> 00:04:10,050
我的代价函数

118
00:04:10,180 --> 00:04:11,580
J(θ) 将成为下面这样的形式

119
00:04:11,760 --> 00:04:13,790
-1/m乘以

120
00:04:13,940 --> 00:04:14,850
一个类似于我们在

121
00:04:15,420 --> 00:04:16,780
逻辑回归里所用的

122
00:04:16,960 --> 00:04:18,990
求和项

123
00:04:19,300 --> 00:04:20,360
除了这里我们求的是

124
00:04:21,020 --> 00:04:22,490
k从1到K的所有和

125
00:04:22,600 --> 00:04:23,650
这个求和项主要是

126
00:04:23,720 --> 00:04:25,580
K个输出单元的求和

127
00:04:26,060 --> 00:04:28,290
所以如果我有四个输出单元

128
00:04:29,400 --> 00:04:30,740
也就是我的神经网络最后一层

129
00:04:30,850 --> 00:04:32,530
有四个输出单元

130
00:04:32,860 --> 00:04:34,420
那么这个求和就是

131
00:04:34,700 --> 00:04:35,680
这个求和项就是

132
00:04:35,900 --> 00:04:37,140
求k等于从1到4的

133
00:04:38,050 --> 00:04:40,550
每一个的逻辑回归算法的代价函数

134
00:04:42,070 --> 00:04:43,640
然后按四次输出的顺序

135
00:04:43,750 --> 00:04:45,570
依次把这些代价函数

136
00:04:45,890 --> 00:04:47,120
加起来

137
00:04:47,800 --> 00:04:48,970
所以你会特别注意到

138
00:04:49,380 --> 00:04:50,700
这个求和符号应用于

139
00:04:51,400 --> 00:04:53,530
yk和hk  因为

140
00:04:53,740 --> 00:04:55,040
我们主要是讨论

141
00:04:55,500 --> 00:04:57,020
K个输出单元

142
00:04:57,780 --> 00:04:59,590
并且把它和yk的值相比

143
00:04:59,810 --> 00:05:02,020
yk的值就是

144
00:05:02,210 --> 00:05:03,260
这些向量里表示

145
00:05:03,740 --> 00:05:05,110
它应当属于哪个类别的量

146
00:05:06,280 --> 00:05:08,060
最后 这里的第二项

147
00:05:08,360 --> 00:05:09,490
这就是类似于我们在逻辑回归里所用的

148
00:05:10,440 --> 00:05:12,970
正则化项

149
00:05:14,080 --> 00:05:15,640
这个求和项看起来

150
00:05:15,850 --> 00:05:17,370
确实非常复杂

151
00:05:17,840 --> 00:05:19,460
它所做的就是把这些项全部相加

152
00:05:19,950 --> 00:05:21,670
也就是对所有i j和l

153
00:05:21,860 --> 00:05:23,340
的θji的值都相加

154
00:05:23,410 --> 00:05:24,830
正如我们在逻辑回归里一样

155
00:05:25,010 --> 00:05:26,340
这里要除去那些对应于偏差值的项

156
00:05:26,710 --> 00:05:28,210
那些项我们是不加进去的

157
00:05:28,900 --> 00:05:30,000
那些项我们是不加进去的

158
00:05:30,900 --> 00:05:32,080
具体地说 我们不把

159
00:05:32,240 --> 00:05:33,590
那些对于i等于0的项

160
00:05:34,300 --> 00:05:36,290
加入其中

161
00:05:36,780 --> 00:05:38,310
这是因为

162
00:05:38,920 --> 00:05:40,010
当我们计算神经元的激励值时

163
00:05:40,590 --> 00:05:41,930
我们会有这些项

164
00:05:42,280 --> 00:05:43,630
θi0

165
00:05:43,810 --> 00:05:47,860
加上θi1

166
00:05:48,160 --> 00:05:50,410
乘以x1 再加上 等等等等

167
00:05:50,520 --> 00:05:51,780
这里我认为

168
00:05:52,020 --> 00:05:53,310
我们可以加上2的上标

169
00:05:53,490 --> 00:05:54,420
如果这是第一个隐含层的话

170
00:05:55,250 --> 00:05:56,800
所以这些带0的项

171
00:05:57,230 --> 00:05:58,730
所以这些带0的项

172
00:05:58,730 --> 00:06:00,110
对应于乘进去了

173
00:06:00,260 --> 00:06:01,460
x0 或者是a0什么的

174
00:06:02,210 --> 00:06:02,950
这就是一个类似于

175
00:06:03,120 --> 00:06:04,810
偏差单元的项

176
00:06:04,980 --> 00:06:06,020
类比于我们在做

177
00:06:06,130 --> 00:06:07,680
逻辑回归的时候

178
00:06:07,890 --> 00:06:09,090
我们就不应该把这些项

179
00:06:09,160 --> 00:06:11,050
加入到正规化项里去

180
00:06:11,160 --> 00:06:13,470
因为我们并不想正规化这些项

181
00:06:13,670 --> 00:06:15,140
并把这些项设定为0

182
00:06:15,360 --> 00:06:16,530
但这只是一个合理的规定

183
00:06:17,670 --> 00:06:18,670
即使我们真的把他们加进去了

184
00:06:18,840 --> 00:06:20,960
也就是i从0加到sL

185
00:06:21,200 --> 00:06:22,810
这依然成立

186
00:06:23,160 --> 00:06:24,720
并且不会有大的差异

187
00:06:25,530 --> 00:06:26,760
但是这个"不把偏差项正规化"

188
00:06:27,500 --> 00:06:28,790
的规定可能只是会

189
00:06:29,070 --> 00:06:30,320
更常见一些

190
00:06:32,960 --> 00:06:34,200
好了 这就是我们准备

191
00:06:34,690 --> 00:06:36,270
应用于神经网络的代价函数

192
00:06:36,790 --> 00:06:38,130
在下一个视频中

193
00:06:38,480 --> 00:06:40,270
我会开始讲解一个算法

194
00:06:40,570 --> 00:06:42,530
来最优化这个代价函数 【教育无边界字幕组】翻译:reanghect 校对:Roy薛~ 审核:柳桦