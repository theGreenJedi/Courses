1
00:00:00,250 --> 00:00:01,530
在上一段视频中

2
00:00:01,850 --> 00:00:02,870
我们谈到了怎样使用反向传播算法

3
00:00:03,980 --> 00:00:05,810
计算代价函数的导数

4
00:00:06,780 --> 00:00:07,770
在这段视频中

5
00:00:08,030 --> 00:00:10,260
我想快速地向你介绍一个细节的实现过程

6
00:00:11,220 --> 00:00:13,110
怎样把你的参数

7
00:00:13,670 --> 00:00:15,500
从矩阵展开成向量

8
00:00:15,610 --> 00:00:17,870
以便我们在高级最优化步骤中的使用需要

9
00:00:20,230 --> 00:00:21,470
具体来讲

10
00:00:21,640 --> 00:00:23,120
你执行了代价函数costFunction

11
00:00:23,660 --> 00:00:24,870
输入参数是theta

12
00:00:25,420 --> 00:00:28,690
函数返回值是代价函数以及导数值

13
00:00:30,050 --> 00:00:31,260
然后你可以将返回值

14
00:00:31,510 --> 00:00:33,820
传递给高级最优化算法fminunc

15
00:00:34,080 --> 00:00:34,790
顺便提醒

16
00:00:34,890 --> 00:00:35,900
fminunc并不是唯一的算法

17
00:00:36,060 --> 00:00:38,660
你也可以使用别的优化算法

18
00:00:39,710 --> 00:00:40,910
但它们的功能

19
00:00:41,030 --> 00:00:41,970
都是取出这些输入值

20
00:00:42,730 --> 00:00:43,560
@costFunction

21
00:00:44,490 --> 00:00:45,730
以及theta值的一些初始值

22
00:00:47,010 --> 00:00:48,490
并且这些程序

23
00:00:48,730 --> 00:00:51,600
都假设theta

24
00:00:51,740 --> 00:00:53,360
和这些theta初始值

25
00:00:53,580 --> 00:00:55,410
都是参数向量

26
00:00:55,640 --> 00:00:57,040
也许是n或者n+1阶

27
00:00:57,870 --> 00:01:00,440
但它们都是向量

28
00:01:00,530 --> 00:01:01,880
同时假设这个代价函数

29
00:01:02,150 --> 00:01:03,770
第二个返回值

30
00:01:03,960 --> 00:01:05,640
也就是gradient值

31
00:01:05,830 --> 00:01:07,410
也是n阶或者n+1阶

32
00:01:07,640 --> 00:01:09,860
所以它也是一个向量

33
00:01:10,840 --> 00:01:11,890
这部分在我们使用逻辑回归的时候

34
00:01:12,040 --> 00:01:14,030
运行顺利

35
00:01:14,220 --> 00:01:15,120
但现在 对于神经网络

36
00:01:15,280 --> 00:01:17,160
我们的参数将不再是

37
00:01:17,220 --> 00:01:18,370
向量

38
00:01:18,980 --> 00:01:21,110
而是矩阵了

39
00:01:21,310 --> 00:01:22,670
因此对于一个完整的神经网络

40
00:01:22,830 --> 00:01:26,050
我们的参数矩阵为θ(1) θ(2) θ(3)

41
00:01:26,700 --> 00:01:28,080
在Octave中我们可以设为

42
00:01:28,680 --> 00:01:30,660
Theta1 Theta2 Theta3

43
00:01:31,450 --> 00:01:33,160
类似的 这些梯度项gradient

44
00:01:33,760 --> 00:01:35,030
也是需要得到的返回值

45
00:01:35,720 --> 00:01:36,890
那么在之前的视频中

46
00:01:36,980 --> 00:01:38,430
我们演示了如何计算

47
00:01:38,840 --> 00:01:40,520
这些梯度矩阵

48
00:01:40,980 --> 00:01:42,290
它们是D(1) D(2) D(3)

49
00:01:42,560 --> 00:01:43,950
在Octave中

50
00:01:44,080 --> 00:01:46,130
我们用矩阵D1 D2 D3来表示

51
00:01:48,080 --> 00:01:49,150
在这节视频中

52
00:01:49,480 --> 00:01:50,420
我想很快地向你介绍

53
00:01:50,510 --> 00:01:51,480
怎样取出这些矩阵

54
00:01:51,980 --> 00:01:54,060
并且将它们展开成向量

55
00:01:54,590 --> 00:01:55,750
以便它们最终

56
00:01:55,910 --> 00:01:57,790
成为恰当的格式

57
00:01:57,930 --> 00:02:00,090
能够传入这里的Theta

58
00:02:00,460 --> 00:02:01,850
并且得到正确的梯度返回值gradient

59
00:02:03,220 --> 00:02:04,540
具体来说

60
00:02:04,670 --> 00:02:06,740
假设我们有这样一个神经网络

61
00:02:06,950 --> 00:02:08,250
其输入层有10个输入单元

62
00:02:09,010 --> 00:02:10,000
隐藏层有10个单元

63
00:02:10,540 --> 00:02:11,870
最后的输出层

64
00:02:12,020 --> 00:02:13,090
只有一个输出单元

65
00:02:13,270 --> 00:02:14,030
因此s1等于第一层的单元数

66
00:02:14,440 --> 00:02:15,710
s2等于第二层的单元数

67
00:02:15,860 --> 00:02:18,220
s3等于第三层的

68
00:02:18,520 --> 00:02:20,700
单元个数

69
00:02:21,560 --> 00:02:23,200
在这种情况下

70
00:02:23,460 --> 00:02:25,240
矩阵θ的维度

71
00:02:25,350 --> 00:02:26,380
和矩阵D的维度

72
00:02:26,570 --> 00:02:28,110
将由这些表达式确定

73
00:02:28,520 --> 00:02:30,300
比如说

74
00:02:30,630 --> 00:02:33,220
θ(1)是一个10x11的矩阵 以此类推

75
00:02:34,420 --> 00:02:35,740
因此 在Octave中

76
00:02:35,950 --> 00:02:37,960
如果你想将这些矩阵

77
00:02:38,580 --> 00:02:38,580
转化为向量

78
00:02:39,330 --> 00:02:40,590
那么你要做的

79
00:02:40,830 --> 00:02:42,130
是取出你的Theta1 Theta2

80
00:02:42,350 --> 00:02:44,220
Theta3

81
00:02:44,410 --> 00:02:45,470
然后使用这段代码

82
00:02:45,610 --> 00:02:46,820
这段代码将取出

83
00:02:46,900 --> 00:02:48,540
三个θ矩阵中的所有元素

84
00:02:48,770 --> 00:02:49,400
也就是说取出Theta1

85
00:02:49,860 --> 00:02:51,150
的所有元素

86
00:02:51,260 --> 00:02:52,290
Theta2的所有元素

87
00:02:52,400 --> 00:02:53,840
Theta3的所有元素

88
00:02:54,130 --> 00:02:55,510
然后把它们全部展开

89
00:02:55,770 --> 00:02:57,420
成为一个很长的向量

90
00:02:58,540 --> 00:02:59,880
也就是thetaVec

91
00:03:00,960 --> 00:03:02,510
同样的 第二段代码

92
00:03:02,830 --> 00:03:04,350
将取出D矩阵的所有元素

93
00:03:04,490 --> 00:03:05,600
然后展开

94
00:03:05,930 --> 00:03:07,340
成为一个长向量

95
00:03:07,510 --> 00:03:08,810
被叫做DVec

96
00:03:09,370 --> 00:03:10,330
最后 如果你想从向量表达

97
00:03:10,520 --> 00:03:13,380
返回到矩阵表达式的话

98
00:03:14,620 --> 00:03:15,630
你要做的是

99
00:03:15,840 --> 00:03:17,720
比如想再得到Theta1

100
00:03:17,940 --> 00:03:19,250
那么取thetaVec

101
00:03:19,530 --> 00:03:20,980
抽出前110个元素

102
00:03:21,470 --> 00:03:22,930
因此

103
00:03:23,390 --> 00:03:24,650
Theta1就有110个元素

104
00:03:24,720 --> 00:03:26,420
因为它应该是一个10x11的矩阵

105
00:03:26,810 --> 00:03:28,200
所以 抽出前110个元素

106
00:03:28,540 --> 00:03:30,200
然后你就可以

107
00:03:30,370 --> 00:03:32,960
reshape矩阵变维命令来重新得到Theta1

108
00:03:33,010 --> 00:03:34,730
同样类似的

109
00:03:34,900 --> 00:03:35,850
要重新得到Theta2矩阵

110
00:03:36,280 --> 00:03:39,010
你需要抽出下一组110个元素并且重新组合

111
00:03:39,670 --> 00:03:41,410
然后对于Theta3

112
00:03:41,450 --> 00:03:43,320
你需要抽出最后11个元素

113
00:03:43,500 --> 00:03:45,210
然后执行reshape命令 重新得到Theta3

114
00:03:48,840 --> 00:03:50,700
以下是这一过程的Octave演示

115
00:03:51,270 --> 00:03:52,370
对于这一个例子

116
00:03:53,010 --> 00:03:54,530
让我们假设Theta1

117
00:03:55,340 --> 00:03:57,440
为一个10x11的单位矩阵

118
00:03:57,670 --> 00:03:59,580
因此它每一项都为1

119
00:04:00,360 --> 00:04:01,400
为了更易看清

120
00:04:01,750 --> 00:04:03,060
让我们把Theta2设为

121
00:04:03,280 --> 00:04:05,150
一个10行11列矩阵

122
00:04:05,310 --> 00:04:07,390
每个元素都为2

123
00:04:07,600 --> 00:04:09,570
然后设Theta3 是一个1x11的矩阵

124
00:04:10,290 --> 00:04:12,110
每个元素都为3

125
00:04:12,390 --> 00:04:13,680
因此 这样我们得到三个独立的矩阵

126
00:04:13,980 --> 00:04:17,030
Theta1 Theta2 Theta3

127
00:04:17,770 --> 00:04:19,010
现在我们想把所有这些矩阵变成一个向量

128
00:04:19,670 --> 00:04:22,740
thetaVec =

129
00:04:23,380 --> 00:04:26,660
[Theta1(:); Theta2(:); Theta3(:)];

130
00:04:28,540 --> 00:04:28,990
好的

131
00:04:29,260 --> 00:04:32,060
注意中间有冒号

132
00:04:32,540 --> 00:04:34,220
像这样

133
00:04:35,350 --> 00:04:37,420
现在thetaVec矩阵

134
00:04:37,590 --> 00:04:40,090
就变成了一个很长的向量

135
00:04:41,050 --> 00:04:41,910
含有231个元素

136
00:04:42,970 --> 00:04:46,000
如果把它打出来

137
00:04:46,290 --> 00:04:47,640
我们就能看出它是一个很长的向量

138
00:04:47,780 --> 00:04:48,610
包括第一个矩阵的所有元素

139
00:04:48,880 --> 00:04:49,630
第二个矩阵的所有元素

140
00:04:50,090 --> 00:04:52,360
以及第三个矩阵的所有元素

141
00:04:53,480 --> 00:04:54,450
如果我想重新得到

142
00:04:54,930 --> 00:04:56,420
我最初的三个矩阵

143
00:04:56,500 --> 00:05:00,040
我可以对thetaVec使用reshape命令

144
00:05:01,400 --> 00:05:02,580
抽出前110个元素

145
00:05:03,100 --> 00:05:05,640
将它们重组为一个10x11的矩阵

146
00:05:06,810 --> 00:05:08,240
这样我又再次得到了Theta1矩阵

147
00:05:08,690 --> 00:05:09,770
然后我再取出

148
00:05:10,280 --> 00:05:12,220
接下来的110个元素

149
00:05:12,720 --> 00:05:14,690
也就是111到220号元素

150
00:05:14,850 --> 00:05:16,470
我就又重组还原了第二个矩阵

151
00:05:18,030 --> 00:05:19,330
最后

152
00:05:20,850 --> 00:05:22,110
再抽出221到最后一个元素

153
00:05:22,280 --> 00:05:24,240
也就是第231个元素

154
00:05:24,440 --> 00:05:25,970
然后重组为1x11的矩阵

155
00:05:26,070 --> 00:05:28,130
我就又得到了Theta3矩阵

156
00:05:30,810 --> 00:05:32,110
为了使这个过程更形象

157
00:05:32,950 --> 00:05:34,750
下面我们来看怎样将这一方法

158
00:05:35,320 --> 00:05:36,990
应用于我们的学习算法

159
00:05:38,200 --> 00:05:39,180
假设说你有一些

160
00:05:39,490 --> 00:05:40,600
初始参数值

161
00:05:41,170 --> 00:05:42,410
θ(1) θ(2) θ(3)

162
00:05:42,950 --> 00:05:43,740
我们要做的是

163
00:05:44,020 --> 00:05:45,880
取出这些参数并且将它们

164
00:05:46,290 --> 00:05:47,610
展开为一个长向量

165
00:05:47,960 --> 00:05:50,380
我们称之为initialTheta

166
00:05:50,600 --> 00:05:52,170
然后作为theta参数的初始设置

167
00:05:52,360 --> 00:05:54,900
传入函数fminunc

168
00:05:56,160 --> 00:05:58,310
我们要做的另一件事是执行代价函数costFunction

169
00:05:59,310 --> 00:06:01,510
实现算法如下

170
00:06:02,900 --> 00:06:04,070
代价函数costFunction

171
00:06:04,160 --> 00:06:05,500
将传入参数thetaVec

172
00:06:05,980 --> 00:06:07,090
这也是包含

173
00:06:07,350 --> 00:06:08,770
我所有参数的向量

174
00:06:08,870 --> 00:06:10,680
是将所有的参数展开成一个向量的形式

175
00:06:11,960 --> 00:06:12,800
因此我要做的第一件事是

176
00:06:13,000 --> 00:06:13,890
我要使用

177
00:06:14,100 --> 00:06:16,580
thetaVec和重组函数reshape

178
00:06:17,040 --> 00:06:18,120
因此我要抽出thetaVec中的元素

179
00:06:18,320 --> 00:06:19,440
然后重组

180
00:06:19,750 --> 00:06:20,950
以得到我的初始参数矩阵

181
00:06:21,320 --> 00:06:23,560
θ(1) θ(2) θ(3)

182
00:06:24,120 --> 00:06:26,530
所以这些是我需要得到的矩阵

183
00:06:26,620 --> 00:06:28,000
因此 这样我就有了

184
00:06:28,060 --> 00:06:29,920
一个使用这些矩阵的

185
00:06:30,130 --> 00:06:31,580
更方便的形式

186
00:06:31,750 --> 00:06:33,590
这样我就能执行前向传播

187
00:06:33,880 --> 00:06:35,400
和反向传播

188
00:06:35,570 --> 00:06:38,140
来计算出导数 以求得代价函数的J(θ)

189
00:06:39,710 --> 00:06:40,900
最后

190
00:06:41,120 --> 00:06:42,620
我可以取出这些导数值 然后展开它们

191
00:06:43,030 --> 00:06:44,530
让它们保持和我展开的θ值

192
00:06:45,140 --> 00:06:47,440
同样的顺序

193
00:06:48,390 --> 00:06:49,780
我要展开D1 D2 D3

194
00:06:50,030 --> 00:06:51,330
来得到gradientVec

195
00:06:52,190 --> 00:06:55,180
这个值可由我的代价函数返回

196
00:06:55,490 --> 00:06:57,420
它可以以一个向量的形式返回这些导数值

197
00:06:59,150 --> 00:07:00,310
现在 我想

198
00:07:00,490 --> 00:07:01,650
对怎样进行参数的矩阵表达式

199
00:07:01,890 --> 00:07:03,200
和向量表达式

200
00:07:03,360 --> 00:07:04,970
之间的转换

201
00:07:05,090 --> 00:07:08,220
有了一个更清晰的认识

202
00:07:09,360 --> 00:07:10,290
使用矩阵表达式

203
00:07:10,760 --> 00:07:12,330
的好处是

204
00:07:12,470 --> 00:07:13,530
当你的参数以矩阵的形式储存时

205
00:07:13,670 --> 00:07:15,670
你在进行正向传播

206
00:07:15,830 --> 00:07:17,430
和反向传播时

207
00:07:17,530 --> 00:07:19,110
你会觉得更加方便

208
00:07:19,850 --> 00:07:21,160
当你将参数储存为矩阵时

209
00:07:21,360 --> 00:07:22,770
一大好处是

210
00:07:23,400 --> 00:07:24,780
充分利用了向量化的实现过程

211
00:07:26,230 --> 00:07:27,900
相反地

212
00:07:28,090 --> 00:07:30,250
向量表达式的优点是

213
00:07:30,320 --> 00:07:31,820
如果你有像thetaVec或者DVec这样的矩阵

214
00:07:32,500 --> 00:07:34,540
当你使用一些高级的优化算法时

215
00:07:34,770 --> 00:07:36,640
这些算法通常要求

216
00:07:36,760 --> 00:07:37,730
你所有的参数

217
00:07:38,090 --> 00:07:40,730
都要展开成一个长向量的形式

218
00:07:41,720 --> 00:07:42,930
希望通过我们刚才介绍的内容

219
00:07:43,140 --> 00:07:44,650
你能够根据需要 更加轻松地

220
00:07:45,410 --> 00:07:47,020
在两种形式之间转换【果壳教育无边界字幕组】翻译：所罗门捷列夫 校对:  Roy薛