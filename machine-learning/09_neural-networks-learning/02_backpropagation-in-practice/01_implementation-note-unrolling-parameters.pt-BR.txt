No vídeo anterior, falamos sobre como usar retropropagação para calcular as derivadas da sua função de custo. Neste vídeo eu quero rapidamente falar sobre um detalhe na implementação de transformar seus parâmetros de matrizes para vetores, que precisaremos para usar nas rotinas de otimização avançada. Digamos que você implementou uma função de custo que recebe este parâmetro ϴ e retorna a função custo e retorna as suas derivadas. Então você pode passar isto para um algoritmo de otimização avançada com fminunc, porém fminunc não é a única forma. Existem outros algoritmos de otimização avançados mas o que todos eles fazem é pegar esses parâmetros, claramente a função custo, a algum valor inicial de ϴ. E ambas rotinas assumem que ϴ e o valor inicial de ϴ sejam vetores de parâmetros, seja ℝⁿ ou ℝ⁽ⁿ⁺¹⁾. Mas estes são vetores e também assumimos que a sua função custo vai retornar um segundo valor, este gradiente que também é ℝⁿ ou  ℝ⁽ⁿ⁺¹⁾, são também vetores. Isto servia muito bem quando nós usávamos regressão logística, mas agora que estamos usando redes neurais nosso parametros não são mais vetores, mas no lugar eles são estas matrizes, que para uma rede neural completa nós teríamos matrizes ϴ⁽¹⁾, ϴ⁽²⁾, ϴ⁽³⁾ como parâmetros., que podemos representar no Octave como as matrizes ϴ⁽¹⁾, ϴ⁽²⁾ e ϴ⁽³⁾. E igualmente estes termos gradientes que eram esperados como retorno. Bem, no vídeo anterior nós mostramos como computar estas matrizes de gradiente, que foram D⁽¹⁾, D⁽²⁾ e D⁽³⁾, que nós representamos no Octave como matrizes D⁽¹⁾, D⁽²⁾, D⁽³⁾. Neste vídeo, quero falar falar brevemente sobre a a ideia de como pegar estas matrizes e estender elas em vetores. Para que elas fiquem se transformando numa forma compatível, permitindo passar como ϴ aqui, e para retornar como um gradiente aqui. Na prática, vamos dizer que temos uma rede neural com uma camada de entrada com dez unidades, uma camada intermediaria com dez unidades e uma camada de saída com apenas uma unidade, então s1 é o número de unidades na camada um e s2 é o número de unidades na camadas dois, e s3 o número de unidades na camada três. Neste caso, a dimensão de suas matrizes ϴ e D vão ser dadas por estas expressões. Por exemplo, ϴ⁽¹⁾ vai ser uma matriz 10 por 11 e assim por diante. Então no Octave, se você quer converter entre estas matrizes e vetores, o que você pode fazer é pegar seu parametro Theta1, Theta2, Theta3, e escrever este pedaço de código e isso irá pegar todos os elementos de suas três matrizes. Todos os elementos de Theta1, todos os elementos de Theta2, todos os elementos de Theta3, e estender todos eles, colocando-os em um grande e longo vetor. Que é o nosso thetaVec, e igualmente o segundo comando vai pegar todas as suas matrizes D e estender em um grande e longo vetor o qual chamamos de DVec. E finalmente se você quer voltar de seus vetores para a representação original em matrizes. O que nós fazemos para ter de volta ϴ⁽¹⁾ é pegar thetaVec e remover os primeiros 110 elementos. ϴ⁽¹⁾ tem 110 elementos porque é uma matriz 10 por 11, então nós pegamos os primeiros 110 elementos e então você pode usar o comando 'reshape' para redimensionar os elementos de volta em Theta1. E igualmente para ter de volta Theta2, você remove os próximos 110 elementos e reconfigura eles. E para Theta3, você remove os últimos 11 elementos e executar o comando 'reshape' para ter de volta ϴ⁽³⁾. Aqui está um pequeno demo do processo no Octave. Neste exemplo vamos usar Theta1 igual 'ones(10, 11)' e temos uma matriz só de 1's. E para ficar fácil de ver vamos setar este para ser 2 vezes 'ones', 10 por 11 e vamos setar theta3 igual 3 vezes 1's de tamanho 1 por 11. Temos então estas 3 matrizes separadas: theta1, theta2, theta3. Nós queremos colocar todas elas juntas em um vetor. thetaVec recebe theta1, ponto e virgula theta2, theta3 Exato, são dois pontos no meio dos parenteses e agora thetaVec vai ser um longo vetor. São 231 elementos. Se eu mostrar na tela, eu vejo que é um vetor longo com todos os elementos da primeira matriz, todos os elementos da segunda matriz, e todos os elementos da terceira matriz. E se eu quero ter de volta minhas matrizes originais, eu posso usar o comando 'reshape' em thetaVec. Vamos remover os primeiros 110 elementos e reorganizar eles em uma matriz 10 por 11. E isto me retorna Theta1. E se então eu removo os próximos 110 elementos que vai dar os indices 111 até 220, eu tenho de volta todos os meus 2's E se eu vou de 221 até o último elemento que é o elemento 231, e reorganizo para uma matriz 1x11 eu tenho novamente theta3. Para mostrar este processo na prática, nós usamos esta ideia de estender matrizes para implementar nosso algoritmos assim: Digamos você tem alguns valores iniciais para os parametros theta1, theta2, theta3. O que nós vamos fazer pegar estes e estender eles em um longo vetor que vamos chamar de ϴ inicial e passar para a função fminunc como a configuração inicial dos parâmetros ϴ. A outra coisa que precisamos fazer é implementar a função custo. Aqui está minha implementação da função custo. A função custo terá como entrada nosso thetaVec que é todos os nossos parâmetros que foram transformados em um único vetor. A primeira coisa que eu vou fazer é usar thetaVec e aplicar as funções 'reshape'. Eu pego os elementos de thetaVec e uso 'reshape' para ter de volta meus parâmetros originais, matrizes theta1, theta2, theta3. Essas são as matrizes resultantes. Isto me dá uma forma mais fácil para que possa usar estas matrizes para poder rodar propagação adiante e retropropagação para computar minhas derivadas, e para computar minha função custo J( ϴ ). E finalmente eu posso então pegar minhas derivadas e estender elas, para manter os elementos na mesma ordem que eu fiz quando estendi os meus Tetas. Mas eu vou estender D1, D2 e D3 para ter meu gradientVec que minha função custo pode retornar. Ela pode retornar um vetor dessas derivadas. Finalmente, espero que agora você tenha uma boa noção de como converter entre uma representação em matriz dos parâmetros versus a representação em vetores dos parâmetros. A vantagem da representação em matriz é que quando são guardados como matrizes é mais conveniente para executar propagação adiantes e retropropagação e é mais facil quando seus parâmetros são guardados como matrizes ter vantagem nesses tipos de implementações vetorizadas. Em contraste a vantagem de usar a representação em vetor, quando você tem thetaVec ou DVec é quando você vai usar os algoritmos de otimização avançada. Estes algoritmos tendem a assumir que você já tem todos os parâmetros em um único grande vetor. E com isso nós terminamos. Espero que agora você possa facilmente converter entre os dois como precisar.
Tradução: Danimar Ribeiro | Revisão: Eduardo Bonet