在之前的视频里，我们总结了所有内容 来帮助你应用 这是最后一个视频 关于随机初始化。 当你使用梯度下降算法， 或者其他高级的优化算法，我们需要设置初始值 恩 所以，对于高级的优化算法 假设你是有一个初始值 现在我们假设就是梯度下降 为此， 通过初值，我们之后一步步通过梯度下降走到山坡底部 当然，这里就是求最小值 所以，我们怎么设置呢 能不能就全部是零呢 这在之前的逻辑回归里可行 全部为零是可以的 恩。 假设我们现在有这么一个网络， 假设全部参数为0 恩，如果你这么做 可以看到蓝色的权值，全是0 我用红色标记的， 等于这个权值 用绿色标记的也等于它 所以，对于A1和A2隐层单元 将会用同一个函数计算 结果， A21等于A22 此外，因为 这输出的权值 你可以发现他们的误差值也一样 所以，结果delta11 delta21等于delta22 所以，如果继续下去 我们可以发现他满足下述情况 即所有的偏导数 就编程这两条蓝色的波浪线 你会发现他们都一样。 得知植物能闻到彼此时一定很惊讶恩 也就是说， 你会发现， 更新的时候 通过计算梯度，更新的结果 这两个参数是一样。 所以，会得到非零的值，但这个值会相等 相似的， 即使是通过梯度下降算法，结果也是相等 可能有一些非零的结果 就是红色的箱单 类似绿色也相等 他们都改变结果 但是结果都是一样的 所以每一次更新，参数对应的结果 都是完全一致 这和前面所说的一样， 红色、绿色、蓝色都一样， 这意味着什么呢 你会发现，两个单元仍然计算同样的结果 恩 你仍然有a1(2)=a2(2) 回到原点。 所以，你不断计算 不断计算，都是一样 红色的情况也是 绿色的也是 所以， 你的神经网络实际上进入很有意思的情况 相信，你不仅有两个隐层， 二是有很多很多层 那么 这将是同样的特性。 所有你的隐层的结果都一样 这是非常冗余的 因为，你发现是逻辑回归 本质上只有一个特征 这就使得你的神经网络性能下降 无法进行更有意义的功能。 所以我们需要随机初始化 具体来说，我们之前看到的问题 叫做对称现象 所以，初始化也被称作打破对称 所以我们进行初始化的操作目的就是 打破对称，而初始区间就是在特定范围内 这是一种我们用的标记。 所以，我的权值参数 将会在这个范围内生成。 这是我们写代码采用的方式1 恩 rand10通过11 这是你如何计算随机的10乘11矩阵 所有的值都在0到1 这是连续的0到1的值 所以， 你再乘以这两个参数 你会得到最后满足区间要求的结果 这是生成特定区间随机数常用的计算操作 这里的epsilon和梯度检验的epsilon是两码事情 不要混淆 这只是一个符号数字而已 完全没有关联。 只是喜欢用epsilon来表示而已 这里我们可以区别他们。 类似的，如果你想要初始化theta2为一个1乘11 的矩阵，你可以用这个代码 原理是一样的 不再赘述 -epsilon到+epsilon范围 然后你再使用反向传播， 使用梯度检验，1b 在()从头开始进行计算 随机初始化结果 也就是打破对称 希望， 这个梯度下降算法或者更高级的优化算法能够找到这个理想的theta值。