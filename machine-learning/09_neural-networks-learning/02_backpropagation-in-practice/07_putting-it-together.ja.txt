ニューラルネットワークのアルゴリズムを 見ていくのに たくさんのビデオを費やしてきた。 このビデオでは、 全てのピースを一つに合わせて 全体的なサマリー、 あるいはより大きな 鳥瞰図を提供したい、 全てのピースがどう組み合わさり ニューラルネットワークを全体として どう実装したらいいのかについて。 ニューラルネットワークをトレーニングする時は 最初にやらなくてはいけない事は なんらかのネットワークアーキテクチャを選ぶ事だ。 ここでアーキテクチャという言葉で ニューロン同士の接続のパターンを意味している。 だから例えば 3つの入力ユニットに 5つの隠れユニット、 そして4つの出力を 選ぶかもしれないし、対して 3つ、5つの隠れ、5つの隠れ 4つの出力を選ぶかもしれないし、 そしてこれは3で、 5, 5, 5ユニットが 三つの隠れレイヤーに あって、そして 4つの出力ユニット、これを選ぶかもしれない。 これらが幾つの隠れユニットと 幾つの隠れレイヤーを持つかの 選択肢だ。 これらがアーキテクチャの選択肢だ。 これらの選択肢をどうやって選ぶのか？ まず、入力ユニットの総数は かっちりと定義されている。 そして一旦固定されたフィーチャーの集まりxを 決定してしまえば、 入力ユニットの数は、 フィーチャーx(i)の次元によって 決定される。 そしてもしマルチクラスの 分類問題を扱っているなら、 出力の数は、 あなたの扱ってる分類問題の クラスの数によって決められる。 思い出してもらう為に触れておくと マルチクラスの分類問題の時には、 例えばyが1から10の間の数を取るとすると、 つまり10個のとりうるクラスが ある事になるが、 その時は、覚えているだろうか、 出力のyはこれらのベクトルとなるのだった。 つまりこれがクラス1で、 それをこのベクトルに再コーディングする。 二番目のクラスは こんなベクトルに再コーディングする。 だから、これらのうちの一つを 例として挙げると、 5番目のクラスの場合、yイコール5の時は、 ニューラルネットワークにおいて見る事になるのは y=5という値では無く、 その代わりにここでは 出力レイヤで、 この場合は10個の出力ユニットがあり、 そこに5番目に1があって それ以外が0で埋まっているような ベクトルを 食わせることになる。 つまり入力ユニットと出力ユニットの 総数の選択は 割とまっすぐ決まると思う。 そして隠れユニットと 隠れレイヤーの 数の選択については もっとも普通の基本形は 隠れレイヤは一層。 つまりこの左側に示したような 隠れレイヤ一つのニューラルネットワークが 恐らくもっとも一般的な物と言える。 そしてもし一つ以上の 隠れレイヤを使うなら ここでも普通の基本形は 各レイヤの隠れユニットは 同じ数にする、という物。 つまりここでは2つの 隠れレイヤがあり、 これらの隠れレイヤはそれぞれ 同じ隠れユニットの数である5つの隠れユニットを持ち、 こちらは見ての通り 3つの隠れレイヤがあり それらはそれぞれ、同じユニット数で それぞれ5つの隠れユニットがある。 だがこれらのネットワークアーキテクチャを使うまでもなく この左側のアーキテクチャでも十分に悪くないデフォルトの選択肢だ。 そして隠れユニットの 数としては、通常は 隠れユニットが多ければ多い程良いが 隠れユニットがたくさんあると 計算量的には 高価になる。 だがしばしば、隠れユニットは多ければ多い程良い。 そして通常は、各レイヤーの 隠れユニットの総数は だいたいxの次元と同じ程度、 フィーチャーの総数と 同じ程度で、あるいは 入力のフィーチャーの総数と 同じ程度から2倍とか3倍とか4倍程度の 適当な数が良い。 だから入力のフィーチャーと 同じ程度の数か、 その数倍程度の数でも やる価値のある事が 多い。 以上で、ニューラルネットワークの アーキテクチャのデフォルトの選択として リーズナブルな選択肢を幾つか示せただろう。 そしてもしあなたがこれらのガイドラインに沿えば、 たぶんうまく機能する物が得られるだろう。 だがこの後の ビデオでは、 アルゴリズムを どう適用したらいいかについて 詳細な話をしていく。 ニューラルネットワークのアーキテクチャをどうやって選んだらいいかについて、もっと多くの事を話していく。 また、実際に隠れユニットや 隠れレイヤーの総数の 良い選び方について、などは たくさん話すつもりだ。 次に、ニューラルネットワークをトレーニングする為に 実装しなくてはいけない事は これだ。 6つのステップがある。 このスライドでは4つ、 そして次のスライドに 残り二つ。 最初のステップはニューラルネットワークを セットアップして、ウェイトの値を ランダムに初期化する。 ウェイトの初期化は普通、 0付近の小さな値で初期化する。 次にフォワードプロパゲーションを実装する、 入力をニューラルネットワークに入れて 予測が計算出来るように。 つまりhのxを計算し、 この出力ベクトルのyの値を得る為に。 そして次にコスト関数である Jのシータを計算するコードを実装する。 そして次にバックプロパゲーションを バックプロパゲーションアルゴリズムを実装する、 これらの偏微分の項を 計算する為に、、、 Jのシータの、パラメータによる 偏微分を計算する為に。
具体的には、バックプロバゲーションを実装するには、 普通はトレーニング手本に渡って for文を回して あなたがたの中には アドバンスドな、凄いアドバンスドな ベクトル化した方法を聞いた事がある人もいるかもしれない。 それはm個のトレーニング手本に渡っての for文を回さない物だ。 だが最初にバックプロパゲーションを 実装する時には、ほぼ確実にforループのある実装から 始めるべきだろう。 そこでトレーニング手本に渡ってイテレーションを行い、 つまり、まずx1とy1に対して 最初の手本に対して フォワードプロパケーションとバックプロパゲーションを行い、 そして次に forループの二回目のイテレーションで 二番目の手本に対し フォワードプロパケーションとバックワードプロパゲーションを行う、などなど。 これを最後の手本に至るまで進める。 つまり、少なくとも初めて実装する時には あなたのバックプロパゲーションの実装には forループがあるはずだ。 そして簡単に言うと なんか難しいやり方で forループ無しのもある、という事。 だが私は心から 最初にバックプロパケーションを実装する時に その複雑なバージョンにチャレンジするのは、おすすめしない。 さて、では具体的には、mトレーニング手本に渡る for文があり、 そしてfor文の中では、 フォワードプロパゲートとバックワードプロパゲートを この一つの手本だけに対して行う。 つまり、 x(i)を持ってきて、 それを入力レイヤーに食わせて、 フォワードプロパゲートとバックプロパゲートを実行する。 するとニューラルネットワーク内の 全てのレイヤーの全てのユニットの アクティベーション全部と デルタの項が全部 得られるので、 次に、まだforループの 内部だが、、、よし、 中括弧を書こう、 forループのスコープを示す為に。 これはもちろんOctaveのコードなんだが、 だがC++とかJavaのコードのシーケンスみたいな意味で これはfor文をここ全体に拡張する。 我らはこれらのデルタの項を計算する訳だが その式は以前に既に得ている。 、、、足すことの デルタ l+1 掛ける aのlの転置に、さらにコードが続く。 そして最後に、これらのデルタ項が アキュームレーションの項が 計算されたあとの外側では さらに幾つかコードがあったあとで、 これらの偏微分の項の 計算が出来るようになる。 そしてこれらの偏微分の項は 正規化項のラムダも同様に 考慮に入れる必要がある。 それらの式は前半のビデオで与えてあった。 だからそれを終えているなら、 これらの偏微分項を計算するコードを 既に手中に収めているはずだ。 次はステップ5。 そこではグラディアントチェッキングを用いて 計算した偏微分の項と 比較する。 つまりバックプロパケーションを用いて 計算したバージョンと、 微分を数値計算的に推計した偏微分の数値推計を 比較する。 つまりグラディアントチェッキングを これらの値が両方とも似通っている事を確認する為に使う。 グラディアントチェッキングで 我らのバックプロパゲーションの実装が正しいと 確認した後には、 次のとても大事な事だが、グラディアントチェッキングを disableする。何故なら グラディアントチェッキングのコードは計算量的にとても遅いからだ。 そして最後に、 最急降下法なり アドバンスドな最適化アルゴリズムの L-BFGSとかConjugateグラディアントとか fminuncで使われているアルゴリズムでもそれ以外のアルゴリズムでも とにかく何かしらを用いて、、、 これらをバックプロパゲーションと共に 用いる、 つまりバックプロパゲーションが これらの偏微分を我らに提供してくれる。 そしてコスト関数を 計算する方法を知っていて、 バックプロパゲーションを用いて偏微分を計算する方法も 知っているので、 これらの最適化手法のどれかを用いて Jのシータをパラメータシータの関数として 最小化する事を、試みる事が出来る。 ところで、 ニューラルネットワークの場合、 このコスト関数Jのシータは非凸関数、 言い換えると凸関数では無いので、 理論上はローカル最小に 陥りうるし、 実際、最急降下法や アドバンスドな最適化手法は、 理論上はローカル最適に捕まってしまう事がありうる。 だが実際には、 通常はこれは そんなに大きな問題にならない。 そしてこれらのアルゴリズムが グローバル最小を探す、とは保証出来なくても、 普通は最急降下法のようなアルゴリズムは このコスト関数Jのシータを 小さくする、という仕事を とてもうまくこなす。 そしてとても良い 局所最小が得られる。 それはグローバル最適では無いかもしれないが。 最後に、ニューラルネットワークに対する 最急降下法は、まだこれでもちょっと魔法っぽく見えるかもしれない。 だからもう一つ 最急降下法がニューラルネットワークに 何をしているかを示す図をお見せしたい。 これは最急降下法を以前説明した時に用いた図に 実は似た物だ。 あるコスト関数があり、 ニューラルネットワークの パラメータがある。 ここではパラメータの値を二つ書いた。 実際には、もちろん、ニューラルネットワークの場合、 もっとたくさんのパラメータを持ちうる。 シータ1もシータ2も行列だ。 だからパラメータの次元はとても高次になりうる。 だがプロットして 表示してみる側の 制約の為に、 ニューラルネットワークに二つのパラメータしか無いかのように話をする。 実際にはもっとたくさんあるのは明らかだけど。 いま、このコスト関数Jのシータは そのニューラルネットワークが トレーニングデータにどれだけフィットしているかを測っている物だ。 つまりこんな点を 取るなら、この下側の、 そこはJのシータがとても低い 点に対応し、 つまりこれは以下のようなシチュエーションのパラメータに対応する、 つまりだいたいのトレーニング手本に対して 仮説の出力が y(i)に極めて近くなるような そういうシチュエーションの パラメータに対応する。 この条件が真の時が、 コスト関数が極めて低い、という状態に対応する。 他方、対照的に、 こっちのような値を取る時には、 この点はトレーニング手本の大多数が、 ニューラルネットワークの出力が 実際の値、 実際の観測された値のy(i)から、 大きく離れた値に 対応した点となる。 つまりこの右側の点のような点は トレーニングセットに対する 仮説の出力、 ニューラルネットワークの出力が、 y(i)から遠く離れた値となる点に対応する。 だからそれはトレーニングセットには あまり良くフィットしそうに無い。 他方このような点は、 コスト関数の値が低いというのは Jのシータが低い所に対応しているので、 つまりはニューラルネットワークが ちょうど良く私のトレーニングセットに フィットする場所に対応する。 何故ならこれこそがJのシータが小さくなる時に 真である必要のある事だから。 だから最急降下法のやる事は、 なんらかの ランダムな初期点から始めて、 たとえばこことか、そこから繰り返し丘を降りていく。 つまりバックプロパゲーションが行うのは グラディアントの方向を 計算して、 そして最急降下法がやる事は、 一歩一歩ちょっとずつ丘を降りていき、 期待するのは、そしてこの場合は実際にそうだが、 かなり良いローカル最適に至るまで進む訳だ。 つまり、バックプロパゲーションを 実装して、 最急降下法やアドバンスドな最適化法の一つを 使う時には、 この図はアルゴリズムが何をするかの、いくらかの説明となっている。 それはニューラルネットワークの 出力する値が、 トレーニングセットにおける観測値のy(i)に なるべく近いような ニューラルネットのパラメータを 探す事を試みる。 これで、あなたも ニューラルネットワークの それぞれのピースが、どう組み合わさるのか 分かったんじゃないかな。 でも、もしこのビデオが終わった後でも、 これらの様々なピースの中に いまいちどうもしっくり来ない物があったり、 あるいはそれらのうちの幾つかが 完全にはっきりと分かったという訳で無くても、 あるいはこれらのピースがどうくっつくのか全部は分からなくても、実際は問題無い。 ニューラルネットワーク学習とバックプロパゲーションは複雑なアルゴリズムだ。 そして私ですら、 バックプロパゲーションの背後にある数学を 長年見てきて、さらに自分で思うには バックプロパゲーションを とても成功裡に何年も使い続けてきた私ですら、 こんにちでも、まだ時々 バックプロパゲーションが正確に何やってるのかを いつもしっかりつかんでる、という訳では無いと感じる事がある。 そしてJのシータを最小化する最適化が どう進んでいくかもしっかりとつかんでないと感じる事がある。 これはより難しいアルゴリズムで、 これはしっかり分かるのが、、、 これが正確に何をやっているのかを しっかりと把握するのが、 線形回帰とかロジスティック回帰に比べて難しい。 線形回帰などの方が数学的にも概念的にも よりシンプルで、よりクリーンなアルゴリズムだ。 だがもしあなたが似たような物だ、と感じたとしても それはそれで完全にOKだ。 だがバックプロパゲーションを 実装してみれば、 きっとこれがもっとも強力な 学習アルゴリズムの一つだと 分かるだろう。 そしてもしこのアルゴリズム、バックプロパゲーションを これらの最適化手法の一つを実装すれば、 バックプロパゲーションが とても複雑で強力で 非線型な関数で あなたのデータにフィッティング出来て そしてこれがこんにちある中でも 最も効率的な学習アルゴリズムの一つである事が分かるだろう。