前回のビデオでは ニューラルネットワークを実装し 訓練するのに必要な ほとんど全てのピースをまとめた。 だがまだもう一つ、最後のピースを あなたにも伝えなくてはいけない。 それはランダム初期化と呼ばれるアイデアだ。 最急降下法なりアドバンスドな最適化アルゴリズムなりのような アルゴリズムを 走らせる時には、 パラメータシータのある初期値を選ぶ必要がある。 アドバンスドな最適化アルゴリズムは、 あなたがパラメータのシータの ある初期値を渡すと 想定している。 ここでは最急降下法を考えよう。 その場合でも、シータを何かしらには初期化しなくてはいけない。 そしてそこから、ゆっくりと丘を 最急降下法を使って一歩一歩降りていき、 Jのシータの最小値まで降りていく。 ではシータの初期値をどうセットしたらいいだろうか？ シータの初期値に 全部0を入れる、というのが 考えられる。 これはロジスティック回帰の時にはうまく行ったが、 パラメータの初期値を全て0にするのは ニューラルネットワークをトレーニングする時には うまく行かない。 以下のニューラルネットワークをトレーニングする事を考えてみよう。 そして全てのネットワークのパラメータを0に初期化したとしよう。 そうすると、 それの意味する事は、 この青いウェイトを初期化する時に、、、ここでこのウェイトを 青で色づけする。 これらは共に0となる。 そしてこのウェイト、赤でいろづけしておくと、 このウェイトがこっちのウェイト、 これも赤で色付けしておくが、これらが等しい。 そしてこのウェイト、 これは緑に色付けしよう、 これはこっちのウェイトの値と等しくなる。 その意味する所は、隠れユニット、a1とa2はどちらも 同じ入力の関数を 計算する事になる、という事。 かくして、 どのトレーニング手本に対しても a(2)1 = a(2)2 となってしまう。 さらに、あまり細かい話に 首をつっこむ気は無いけれど、 しかしこれらのウェイトが 等しければ、 デルタの値も等しくなる、という事を 示す事が出来る。 具体的には、 デルタ1 1、、、じゃなかった。デルタ2, 1は イコールデルタ2, 2。 さらに数学を続けていくと、 あなたのパラメータによる 偏微分は 以下を満たす事を示す事が出来る。 コスト関数の 偏微分、、、 ここに書いている、 ニューラルネットワークの これら二つの青のウェイトによる偏微分。 これら二つの偏微分は お互いに等しい事が分かる。 この意味する所は、 一回最急降下法のアップデートが行われた後も、 この青いウェイトを 学習率 掛ける これ でアップデートし、 二番目の青いウェイトも 学習率掛けるこれ、でアップデートする。 だがその意味する所は、 一回最急降下法のアップデートを かました後でも、 これら二つの青のウェイトは、これら二つの 青の色付けしたパラメータは 結局相等しいままだ。 なんらかの非0の値にはなるだろう、 だがこの値は こっちの値と等しい。 そして同様に、一回最急降下法のアップデートを行った後でも、 この値はこっちの値と等しい。 なんらかの非0の値にはなる。 この二つの赤の値もお互いに相等しい。 そして同様に、二つの緑のウェイトも それらの値も共に変化するが、 だが変化した結果はどちらも同じ値になる。 だから各アップデートの後でも、各入力から 二つの隠れユニットへと至る 二つのウェイトは、同一となる。 それは単に、 二つの緑のウェイトが等しくなり、 二つの赤のウェイトも等しくなり、 二つの青のウェイトも等しいままだ、と 言っているだけだ。 そしてその意味する所は、 例えば最急降下法などの一回のイテレーションの後でも、 あなたの二つの隠れユニットが 全く同一の入力の関数を 計算し続ける事となる。 だから、a(1)2 = a(2)2 のままとなる。 そしてこのケースに戻る。 そして最急降下法を走らせ続けても、 この二つの青のウェイトはお互いに等しいままだ。 二つの赤いウェイトはお互いに等しいままだ。 二つの緑のウェイトはお互いに等しいままだ。 この意味する所は、 このニューラルネットワークはあまり面白い関数は 計算出来ない、という事。 二つの隠れユニットだけじゃなくて もっともっとたくさんの 隠れユニットがある場合を 考えてみよう。 するとこれの意味する所は、 隠れユニットが全て 完全に同じフィーチャーを計算する、という事になる。 全ての隠れユニットが、全く同一の、入力の関数を計算する事になる。 これはとても冗長な表現になってしまう。 何故ならそれはつまりは、 最後のロジスティック回帰のユニットは、実際は一つの入力があるだけに見えるから。 何故ならこれらは全て同じで、 このことがあなたのニューラルネットワークが何かしら面白い学習をする事を妨げているから。 この問題を回避する為に、 ニューラルネットワークのパラメータを 初期化する方法は、 ランダム初期化の方法だ。 具体的には、前のスライドで 我らが見た問題は 対称ウェイトの問題と呼ばれる事もあり、 それはウェイトが全て同じという事だ。 だからこのランダム初期化で 対称性を破る訳だ。 我らのやるべき事は、 各シータの値を -エプシロン から エプシロン までの間の ランダムな値で初期化する。 これは -エプシロン と +エプシロン の間の値を 表す記法だ。 パラメータのウェイトは -エプシロンから+エプシロンまでの間の値で ランダムに初期化される。 Octaveでこれをやるコードを書く方法は、 Theta1をイコール これ、とする。 このrand(10, 11)で、 これが10x11次元の ランダムな値を持つ行列を 計算する方法で、 そしてそれらの要素の値は全て、0と1の間の数となる。 つまりこれらは、 連続的な実数の0と1の間のいかなる値でも 取る事が出来る。 すると、0と1の間の数を 取って、 2*エプシロン を掛けて、 そこからエプシロンを引く。 すると最終的には -エプシロン と エプシロンの 間の数となる。 ついでに言っておくと、このエプシロンは グラディアントチェッキングで使ってた エプシロンとは 全く関係が無い。 数値的なグラディアントチェッキングを行っていた時は、 シータにあるエプシロンという値を足していたが、 この値は、そのエプシロンとは無関係だ。 そんな訳でこのエプシロンを INIT_EPSILONと書いている。 これはグラディアントチェッキングで使ったエプシロンの値と区別する為だけの理由だ。 同様にもしシータ2を 1x11のランダムの行列で 初期化したければ、 このコード辺でそれが行える。 さて、まとめると、 ニューラルネットワークをトレーニングする為には、 ウェイトを小さな値、 0のそばの-エプシロンから+エプシロンの間のどこかの値で ランダムに初期化 しなければならない。 そしてバックプロパゲーションを実装し、 グラディアントチェッキングをする。 そして最急降下法なり アドバンスドな最適化アルゴリズムなりを 用いて、 Jのシータを パラメータシータの関数として、 ランダムに初期化したパラメータから始めて 最小化を試みる。 そして対称性の破れを行う事で、それはこのプロセスだが、 最急降下法なりアドバンスドな最適化アルゴリズムなりが、 シータの良い値を 見つける事が期待出来るようになる。