पिछले कुछ वीडियोज़ में हमने बात की फ़ॉर्वर्ड प्रॉपगेशन की और बैक प्रॉपगेशन की न्यूरल नेटवर्क में कम्प्यूट करने के लिए डेरिवेटिव्स. लेकिन बैक प्रॉपगेशन अल्गोरिद्म में हैं बहुत सी विस्तृत जानकारी और हो सकता है थोड़ा कठिन इम्प्लमेंट करने के लिए. और एक और खेदजनक गुण है कि ऐसे बहुत से तरीक़े हैं जिससे बहुत से छिपे हुए बग हो सकते हैं बैक प्रॉपगेशन में. तो यदि आप रन करते हैं इसे ग्रेडीयंट डिसेंट के साथ या किसी और ऑप्टिमायज़ेशन अल्गोरिद्म के साथ, ऐसा वास्तव में लग सकता है कि यह काम कर रहा है. और आपका कॉस्ट फ़ंक्शन, जे ऑफ़ थीटा शायद कम होगा प्रत्येक इटरेशन्स के बाद ग्रेडीयंट डिसेंट की. लेकिन यह तभी भी हो सकता है जब शायद कोई बग है आपकी इम्प्लमेंटेशन में बैक प्रॉपगेशन की. ताकि ऐसा प्रतीत होगा कि जे ऑफ़ थीटा कम हो रहा है, लेकिन आपको शायद मिले एक न्यूरल नेटवर्क जिसमें है एक बहुत अधिक एरर तुलना में एक बग-फ़्री इम्प्लमेंटेशन के. और आपको शायद पता भी न हो कि वहाँ एक छिपा हुआ बग था जो आपको दे रहा था बदतर पर्फ़ॉर्मन्स. तो, हम क्या कर सकते हैं इस बारे में? यहाँ है एक सुझाव जिसे कहते हैं ग्रेडीयंट चेकिंग जो समाप्त करता है लगभग सभी इन समस्याओं को. तो, आजकल हर बार जब मैं इम्प्लमेंट करता हूँ बैक प्रॉपगेशन या एक इसी प्रकार का ग्रेडीयंट [सुनाई नहीं दिया] एक न्यूरल नेटवर्क पर या कोई और काफ़ी जटिल मॉडल, मैं हमेशा इम्प्लमेंट करता हूँ ग्रेडीयंट चेकिंग. और यदि आप इसे करते हैं, यह आपकी सहायता करेगा सुनिश्चित करने के लिए और एक तरह से उच्च विश्वास पाने के लिए कि आपकी इम्प्लमेंटेशन फ़ॉर्वर्ड प्रॉपगेशन और बैक प्रॉपगेशन या जो भी 100% सही है. और जो भी मैंने देखा है यह काफ़ी कुछ समाप्त करता है सारी समस्याएँ जो जुड़ी हैं इस तरह की बग वाली इम्प्लमेंटेशन में बैक प्रॉपगेशन की. और पिछले वीडियो में मैंने आपसे कहा था कि आप विश्वास करें कि फ़ॉर्म्युला जो मैंने दिया हैं कम्प्यूट करने के लिए डेल्टा और डेरिवेटिव्स और जो कुछ भी, मैंने आपको कहा था कि रखें विश्वास कि वे वास्तव में कम्प्यूट करते हैं ग्रेडीयंट्स कॉस्ट फ़ंक्शन के. लेकिन एक बार जब आप इम्प्लमेंट करते हैं नूमेरिकल ग्रेडीयंट चेकिंग, जो विषय है इस वीडियो का, आप पूर्ण सक्षम होंगे खुद के लिए सत्यापित करने में कि कोड जो आप लिख रहे हैं करता है वास्तव में, वास्तव में कम्प्यूट करता है डेरिवेटिव कॉस्ट फ़ंक्शन J के. तो यहाँ है विचार, निम्न उदाहरण पर ग़ौर करें. मान लो कि मेरे पास है फ़ंक्शन J ऑफ़ थीटा और मेरे पास कुछ वैल्यू थीटा की और इस उदाहरण के लिए मैं मानूँगा कि थीटा है सिर्फ़ एक रियल नम्बर. और मान लो मैं चाहता हूँ अनुमानित करना डेरिवेटिव इस फ़ंक्शन का इस पोईँट पर और इसलिए डेरिवेटिव है बराबर स्लोप के उस टैंजेंट / स्पर्श रेखा के. यहाँ है कि कैसे मैं नूमेरिक्ली अनुमान करूँगा डेरिवेटिव का, या यहाँ है विधि करने के लिए नूमेरिक्ली अनुमानित डेरिवेटिव को. मैं कम्प्यूट करूँगा थीटा प्लस एप्सिलोन, तो अब हम इसे करते हैं दाईं तरफ़. और मैं कम्प्यूट करूँगा थीटा माइनस एप्सिलोन और मैं देखूँगा इन दो पोईँट्स को और जोड़ूँगा उन्हें एक सीधी रेखा से मैं जोड़ूँगा इन दो पोईँट्स को एक सीधी लाइन से, और मैं इस्तेमाल करूँगा स्लोप का उस छोटी लाल लाइन का बतौर मेरे अनुमान डेरिवेटिव के. जो है, असली डेरिवेटिव है स्लोप उस नीली लाइन का वहाँ पर. तो, आप जानते हैं ऐसा लगता है कि यह होगा एक काफ़ी अच्छा अनुमान. गणितीय रूप में, इस लाल लाइन की स्लोप है यह वर्टिकल ऊँचाई विभाजित द्वारा इस हॉरिज़ॉंटल चौड़ाई से. तो यह पोईँट ऊपर है J ऑफ़ (थीटा प्लस एप्सिलोन). यह पोईँट यहाँ है J ऑफ़ (थीटा माइनस एप्सिलोन), तो यह वर्टिकल अंतर है J ऑफ़ (थीटा प्लस एप्सिलोन) माइनस J ऑफ़ (थीटा माइनस एप्सिलोन) और यह हॉरिज़ॉंटल अंतर है सिर्फ़ 2 एप्सिलोन. तो मेरा अनुमान होगा उस डेरिवेटिव का विद रिस्पेक्ट टु थीटा J ऑफ़ थीटा का इस वैल्यू पर थीटा कि, वह है लगभग J (थीटा +एप्सिलोन) - J(थीटा - एप्सिलोन) / 2 एप्सिलोन. आमतौर पर, मैं इस्तेमाल करता हूँ एक बहुत छोटी वैल्यू एप्सिलोन के लिए, सोचें एप्सिलोन को 10 की पॉवर -4 के क़रीब. आम तौर एक बड़ी रेंज होती है एप्सिलॉन की विभिन्न वैल्यूज़ के लिए जो सही काम करती है. और वास्तव में, यदि आप रखते हैं एप्सिलोन को बहुत छोटा, तब गणित के अनुसार यह टर्म यहाँ, वास्तव में गणित के अनुसार, यह बन जाता है डेरिवेटिव, यह बन जाता है वास्तव में स्लोप फ़ंक्शन का इस पोईँट पर. सिर्फ़ इतना है कि हम नहीं चाहते इस्तेमाल करना एप्सिलोन की एक बहुत ही छोटी वैल्यू, क्योंकि तब आपको शायद नूमेरिक्ली कठिनाई आ सकती है. तो मैं आमतौर पर एप्सिलोन को लगभग दस की पॉवर माइनस चार रखता हूँ. और वैसे भी आप में से कुछ ने शायद देखा होगा एक विकल्प इस फ़ॉर्म्युला का कम्प्यूट करने के लिए डेरिवेटिव जो है यह फ़ॉर्म्युला. यह दाईं तरफ इसे कहते हैं एक वन-साइडेड डिफ़्रेन्स, जबकि फ़ॉर्म्युला जो बाईं तरफ है, इसे कहते हैं एक टू-साइडेड डिफ़्रेन्स. टू-साइडेड डिफ़्रेन्स हमें देता है थोड़ा बेहतर अनुमान, तो मैं आमतौर पर उसका इस्तेमाल करता हूँ बजाय इस्तेमाल करने के वन-साइडेड डिफ़्रेन्स. तो, वास्तव में, जब इम्प्लमेंट करते हैं ओकटेव में, आप इम्प्लमेंट करते हैं निम्नलिखित, आप इम्प्लमेंट करते हैं कॉल कम्प्यूट करने के लिए gradApprox, जो होगा हमारा लगभग अनुमान डेरिवेटिव के जैसे यहाँ इस फ़ॉर्म्युला से, J (थीटा +एप्सिलोन) - J(थीटा - एप्सिलोन) / 2 एप्सिलोन. और यह देगा आपको एक नूमेरिकल अनुमान ग्रेडीयंट का उस पोईँट पर. और इस उदाहरण में ऐसा लगता है कि यह एक बहुत अच्छा अनुमान है. अब पिछली स्लाइड पर, हमने लिया था केस जब थीटा था एक रियल नम्बर. चलो अब देखते है अधिक सामान्य केस जब थीटा है एक वेक्टर पेरामिटर, तो मान लो थीटा है एक Rn. और यह शायद है एक अनरोल किया हुआ वर्ज़न पेरमिटर्स का हमारे न्यूरल नेटवर्क के. तो थीटा है एक वेक्टर जिसमें n एलिमेंट्स हैं थीटा 1 से थीटा n तक. हम तब कर सकते हैं इस्तेमाल एक समान विचार अनुमानित करने के लिए सारे पर्शियल डेरिवेटिव टर्म्ज़. वास्तव में पर्शियल डेरिवेटिव एक कॉस्ट फ़ंक्शन के विद रिस्पेक्ट टु पहले पेरामिटर, थीटा एक, वह प्राप्त किया जा सकता हैं लेकर J और बढ़ाने से थीटा एक. तो आपके पास है J ऑफ़ थीटा वन प्लस एप्सिलोन और इसी तरह आगे. माइनस J ऑफ़ थीटा वन माइनस एप्सिलोन और विभाजित करें इसे दो एप्सिलोन से. पर्शियल डेरिवेटिव विद रिस्पेक्ट टु दूसरे पेरामिटर थीटा दो, है फिर से यह चीज़ सिवाय कि आप लेंगे J यहाँ और बढ़ाएँगे थीटा दो को एप्सिलोन से, और यहाँ घटाएँगे थीटा दो को एप्सिलोन से और इसी प्रकार बाक़ी के डेरिवेटिव विद रिस्पेक्ट टु थीटा n तक देगा आपको बढ़ना और घटना थीटा का एप्सिलोन से वहाँ पर. तो, ये इक्वेज़न्स देती है आपको एक तरीक़ा नूमेरिक्ली अनुमानित करने का पर्शियल डेरिवेटिव J के विद रिस्पेक्ट टु किसी भी आपके पेरामिटर थीटा j. वास्तव में जो आप इम्प्लमेंट करते हैं वह है निम्नलिखित. हम इम्प्लमेंट करते हैं निम्न ओकटेव में नूमेरिक्ली कम्प्यूट करने के लिए डेरिवेटिव्स. हम कहते हैं, फ़ॉर i=1:n, जहाँ n है डिमेन्शन हमारे पेरामिटर वेक्टर थीटा की. और प्रायः मैं करता हूँ इसे अनरोल किए हुए पेरमिटर्स के साथ. तो थीटा है सिर्फ़ एक लम्बी लिस्ट मेरे सारे पेरमिटर्स की मेरे न्यूरल नेटवर्क में, मान लो. मैं सेट करूँगा थीटाप्लस= थीटा, फिर बढ़ाऊँगा थीटाप्लस (i) एलिमेंट का एप्सिलोन से. और यह है मूलरूप में थीटाप्लस बराबर थीटा के सिवाय थीटाप्लस(i) के जो बढ़ाया है एप्सिलोन से. एप्सिलोन, तो थीटाप्लस है बराबर, ठीक है, थीटा 1, थीटा 2 और आगे. फिर थीटा i में है जोड़ा हुआ एप्सिलोन और हम जाते हैं थीटा n तक. तो यह है जो थीटाप्लस है. और इसी प्रकार ये दो लाइन सेट करती हैं थीटामाइनस को इसी समान सिवाय कि यह बजाय थीटा i प्लस एप्सिलोन, यह अब बन जाता है थीटा i माइनस एप्सिलोन. और फिर बाद में आप इम्प्लमेंट करते हो यह gradApprox(i) और यह देगा आपको आपके अनुमान पर्शियल डेरिवेटिव के विद रिस्पेक्ट टु थीटा i, J ऑफ़ थीटा के. और जिस तरह हम इस्तेमाल करेंगे इसे हमारे न्यूरल नेटवर्क इम्प्लमेंटेशन में है, हम इम्प्लमेंट करेंगे यह फ़ॉर लूप कम्प्यूट करने के लिए पर्शियल डेरिवेटिव कॉस्ट फ़ंक्शन का विद रिस्पेक्ट टु पेरमिटर्स जो सब हैं उस नेटवर्क के, और हम तब ले सकते हैं ग्रेडीयंट जो हमें मिला बैक प्रॉपगेशन से. तो Dvec था डेरिवेटिव जो हमें मिला बैक प्रॉपगेशन से. ठीक है, तो बैक प्रॉपगेशन, था एक अपेक्षाकृत कुशल ढंग कम्प्यूट करने के लिए एक डेरिवेटिव या एक पर्शियल डेरिवेटिव. एक कॉस्ट फ़ंक्शन का विद रिस्पेक्ट टु सभी हमारे पेरामिटर्स. और अक्सर मैं क्या करता हूँ तब, लेता हूँ मेरे नूमेरिक्ली कम्प्यूट किए हुए डेरिवेटिव जो है यह gradApprox जो हमें मिला था ऊपर यहाँ. और सुनिश्चित करता हूँ कि वह है बराबर या लगभग बराबर नूमेरिकल राउंड ऑफ़ की छोटी वैल्यू तक, वह हैं काफ़ी क़रीब. तो Dvec जो मुझे मिला बैक प्रॉपगेशन से. और यदि कम्प्यूट करने के ये दोनो तरीक़ों से मुझे मिलता है एक ही उत्तर, या देते हैं मुझे समान उत्तर, कुछ डेसिमल स्थानों तक, तब मुझे विश्वास हो जाता है कि मेरी इम्प्लमेंटेशन बैक प्रॉपगेशन की सही है. और फिर मैं प्लग करता हूँ ये DVec वेक्टर ग्रेडीयंट डिसेंट या किसी एडवांस्ड ऑप्टिमायज़ेशन अल्गोरिद्म में, मुझे तब है और अधिक विश्वास कि मैं कम्प्यूट कर रहा हूँ सही डेरिवेटिव्स, और इसलिए उम्मीद है मेरा कोड सही चलेगा और अच्छे से ऑप्टिमायज़ करेगा J ऑफ़ थीटा. अंत में, मैं रखना चाहता हूँ सब चीज़ एक साथ और बताना चाहता हूँ आपको कि कैसे इम्प्लमेंट करना है इस नूमेरिकल ग्रेडीयंट चेकिंग को. यहाँ है कि मैं अक्सर क्या करता हूँ. पहला काम मैं करता हूँ कि इम्प्लमेंट करता हूँ बैक प्रॉपगेशन कम्प्यूट करने के लिए DVec. तो एक विधि है जिसकी हमने बात की थी पिछले वीडियो में कम्प्यूट करने के लिए DVec जो शायद हमारा अनरोल किया हुआ वर्ज़न था इन मेट्रिसीज़ का. तो फिर मैं क्या करता हूँ, इम्प्लमेंट करता हूँ नूमेरिकल ग्रेडीयंट चेकिंग, कम्प्यूट करने के लिए gradApprox. तो यह है जो मैंने बताया था पिछली स्लाइड में इस वीडियो में. फिर सुनिश्चित करें कि DVec और gradApprox दे रहे हैं समान वैल्यूज़, आप जानते हैं मान लो कुछ डेसिमल स्थानों तक. और अंत में, और यह है महत्वपूर्ण सटेप, इससे पहले कि शुरू करें आपका कोड लर्निंग के लिए, ट्रेनिंग के लिए आपके नेटवर्क की, यह महत्वपूर्ण है कि बंद करें ग्रेडीयंट चेकिंग और अब नहीं करना है कम्प्यूट gradApprox इस्तेमाल करके नूमेरिकल डेरिवेटिव फ़ॉर्म्युला जिनकी हमने पहले बात की थी इस वीडियो में. और उसका कारण है नूमेरिक कोड ग्रेडीयंट चेकिंग कोड, जिसकी हमने बात की इस वीडियो में, वह है एक बहुत कॉम्प्यूटेशनली महँगा, वह हैं एक बहुत धीमी गति का ढंग अनुमानित करने के लिए डेरिवेटिव. जबकि इसके विपरीत, बैक प्रॉपगेशन एल्गोरिद्म जिसके बारे में हमने पहले बात की थी, वह है जिसके बारे में हमने पहले बात की थी कम्प्यूट करने के लिए आप जानते हैं D1, D2, D3 DVec के लिए. बैक प्रॉपगेशन अधिक कॉम्प्यूटेशनली कुशल ढंग हैं कम्प्यूट करने के लिए डेरिवेटिव्स. एक बार आपने देख लिया कि आपकी इम्प्लमेंटेशन बैक प्रॉपगेशन की सही है, आपको बंद कर देनी चाहिए ग्रेडीयंट चेकिंग और उसका इस्तेमाल बंद कर देना चाहिए. तो सिर्फ़ दोहराने के लिए, आपको निश्चय ही निष्क्रिय कर देना चाहिए ग्रेडीयंट चेकिंग कोड चलाने से पहले आपका अल्गोरिद्म ग्रेडीयंट डिसेंट की बहुत सी इटरेशन्स के लिए या एडवांसड ऑप्टिमायज़ेशन अल्गोरिद्म्स की बहुत सी इटरेशन्स के लिए. ट्रेन करने के लिए आपके क्लैसिफ़ायअर को. वास्तव में, यदि आपको रन करना होता नूमेरिकल ग्रेडीयंट चेकिंग प्रत्येक इटरेशन में ग्रेडीयंट डिसेंट की, या यदि आप होते अंदर के लूप में आपके कॉस्ट फ़ंक्शन के, तब आपका कोड चलेगा बहुत धीमे. क्योंकि नूमेरिकल ग्रेडीयंट चेकिंग काफ़ी धीरे है तुलना में बैक प्रॉपगेशन अल्गोरिद्म का, तुलना में बैक प्रॉपगेशन विधि के जहाँ, आपको याद होगा, हम कम्प्यूट कर रहे थे डेल्टा(4), डेल्टा(3), डेल्टा(2) और आगे. वह था बैक प्रॉपगेशन अल्गोरिद्म. वह है एक अधिक तीव्र ढंग कम्प्यूट करने के लिए डेरिवेटिव तुलना में ग्रेडीयंट चेकिंग के. तो जब आप तैयार है, एक बार आपने देख लिया कि आपकी इम्प्लमेंटेशन बैक प्रॉपगेशन सही है, आपको निश्चय ही निष्क्रिय कर देना चाहिए ग्रेडीयंट चेकिंग कोड जब आप ट्रेन करें आपका अल्गोरिद्म, वरना आपका कोड बहुत धीरे चलेगा. तो, ऐसे लेते हैं आप ग्रेडीयंट नूमेरिक्ली, और ऐसे आप चेक करते हैं आपकी इम्प्लमेंटेशन बैक प्रॉपगेशन सही है. जब मैं इम्प्लमेंट करता हूँ बैक प्रॉपगेशन या ऐसा कोई ग्रेडीयंट डिसेंट अल्गोरिद्म एक जटिल मॉडल के लिए, मैं हमेशा इस्तेमाल करता हूँ ग्रेडीयंट चेकिंग और यह वास्तव में सहायता करता है तय करने में कि मेरा कोड सही है.