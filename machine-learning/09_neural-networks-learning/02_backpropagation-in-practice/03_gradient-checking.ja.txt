前回までの一連のビデオで ニューラルネットワークにおいて微分を計算するために、 フォワードプロパゲーションとバックワードプロパゲーションを 行うやり方を見てきた。 だがバックプロパはたくさんの 細かい部分のあるアルゴリズムで 実装するのにちょっと トリッキーな所がある。 そして一つの不運な特徴として バックプロパは 色々と微妙にバグる、というのがある。 だから最急降下法なり 別の最適化アルゴリズムなりなどで 実行すると、ぱっと見うまく行ってるように見えたりする。 そしてあなたのコスト関数 Jのシータが、結局は 最急降下法の各イテレーションで 減少していく場合があるが、 それはあなたのバックプロパの実装にバグがあるのを見逃していても 起こりうる。 つまり、Jのシータは 減少しているように見えるが、 バグ無しの実装に比べて 結局はより高レベルの エラーに 見舞われる 可能性があり、 そんなパフォーマンスになってしまっている 微妙なバグに 単に気づいていないだけ、という事にもなりかねない。 ではこの事態にどう対処すれば良いか？ グラディアントチェッキングと呼ばれるアイデアがあり、 それはこれらの問題のほとんどを駆逐してくれる。 だからこんにちでは、 バックプロパゲーションなり それ以外でもそれなりに複雑なモデルの 最急降下法を実装する時には、 私は毎回、 グラディアントチェッキングを実装するようにしている。 そしてこれを行えば、 あなたのフォワードプロパゲートやバックワードプロパゲートや それ以外のなんでも、とにかく実装した物が、 100%正しい、と確認したり、深く確信したりするのを 助けてくれる。 私はこの手法が バックプロパゲートの実装に関した あらゆるバグを駆逐してくれるのを 見て来た。 そして前回のビデオでは、 私はあなたに、 デルタやDたちを 計算する式を 単に信じてくれ、と頼んだ。 私はあなたにそれらの公式が 実際にコスト関数の微分を計算している事を 単に信じてくれ、と頼んだ。 だがひとたびあなたが数値的なグラディアントチェッキングを実装すれば、 それこそがこのビデオのトピックだが、 そうすればあなた自身が あなたの書いたコードが確かに コスト関数Jの微分を計算している事を 確認する事が出来る。 そのアイデアはこうだ。 以下のような例を考えてみよう。 Jのシータがあるとして、 そしてある値シータが あるとする。 そしてこの例では、シータは単なる実数だと仮定しよう。 そしてこの関数の、例えばこの点の微分を推計したいとしよう。 すると微分は、 この接線の傾きに等しい。 これが、数値的に微分を 近似する方法、あるいはむしろ 微分を数値的に近似する 手続きはこうだ： シータ+エプシロンを計算する、 つまりちょっとだけ右の値だ。 そしてシータ-エプシロンも計算する。 そしてこれら二つの点を 見て、それらを直線で つなげる。 これら二つの点を 直線でつなげよう。 そしてこの 小さな赤い線の 傾きを、微分の近似として 用いる。 ここで、真の微分の値は ここの青い線の傾きだ。 つまり、ふむ、それはとても良い近似になりそうだ。 数学的には、この赤い直線の傾きは 垂直方向の高さ 割る事の この水平方向の幅だ。 この上の点はJのシータ+エプシロン。 この、ここの点は Jのシータ-エプシロン。 つまりこの垂直の差は Jのシータ+エプシロン 引くことの Jのシータ-エプシロン。 そしてこの水平距離は2エプシロンだ。 つまり、私の近似は 以下のようになる： Jのシータの、シータによる微分は、 このシータの場所での微分は、 だいたい近似的には Jのシータ+エプシロン 引く事の Jのシータ-エプシロン, 割ることの2エプシロンだ。 普通私は、エプシロンには とても小さい数字を用いる。 エプシロンにだいたい10の-4乗とか そういうオーダーの数をセットしてる。 だいたいにおいて、うまく行くようなエプシロンの範囲は 結構大きな範囲に渡る。 そして実際に、 エプシロンにとても小さい値を入れていくと 数学的には、この項は 実際に数学的に、 微分となる。この点における 関数の完全な傾きになる。 そんなに小さな エプシロンを使いたくない理由は、 単に小さすぎるエプシロンは数値的な問題を引き起こすからというだけ。 だから私はだいたい、 エプシロンに10の-4乗あたりの値を使う。 ところで、あなたがたの中には 微分を推計する別の式、 こんな式を見た事がある人もいるかもしれない。 この右側のは片側微分と呼ばれる物だ。 一方で、左側の式は両側微分と呼ばれる。 両側微分は通常は わずかだがより良い推計を与えるので、 私は通常はこの片側微分の代わりに 両側微分を用いている。 つまり、具体的に言えば、Octaveで あなたが実装するのは、以下のような物だ。 gradApproxを計算するコードは、こうなる、 これは微分を 近似する。 それはこんな式だ： Jのシータ+エプシロン 引くことの Jのシータ-エプシロン 割ることの2掛けるエプシロン。 この式はこの点の微分の 数値的な推計を与える。 そしてこの例では、これは極めて良い推計になっているようだ。 ここで、前のスライドでは、 シータが実数の場合を 検討した。 ここでは、より一般的なケースとなる、 シータがパラメータベクトルの場合を見てみよう。 シータがRnとしよう。 これはニューラルネットワークのパラメータを アンロールしたバージョンと 考えても良い。 つまりシータはn個の要素を持つ ベクトルで、つまりシータ1から シータnまでで、 これらそれぞれに関する偏微分の項について さっきと似たような近似のアイデアを用いる事が出来る。 具体的には、コスト関数の 最初のパラメータ、シータ1による 偏微分は、以下のように 求める事が出来る。 それはJに対しシータ1を増加させて、 つまりJのシータ1+エプシロン にして、 そこから引く事の Jのシータ1-エプシロン に、 全体を2エプシロンで割る。 二番目のパラメータシータ2に関しての 偏微分は、 だいたいこれと同じ事をするが、 唯一の違いは、エプシロンだけ増加させるのが シータ2だという所。 そしてここは、シータ2をエプシロンだけ減少させる。 などと、シータnに関する微分まで 降りていく。 そこではここにあるシータnを エプシロンだけ 増加させたり減少させたりする。 さて、これらの等式は Jの、各パラメータに対する 偏微分を数値的に 近似する方法を与える。 具体的にはあなたが実装するのは、以下のような物だ。 我らはOctaveで以下のように実装して 数値的に微分を求める。 for i=1からnまでの、、、 ここでnはパラメータベクトル、シータの 次元だ。 そして私は普通、これをアンロールしたバージョンのパラメータでやる。 つまりシータは私のニューラルネットワークのパラメータの 単なる長いリストに過ぎない。 thetaPlusにthetaをセットし、 thetaPlusのi番目の要素を EPSILONだけ増加させる。 つまりこれは、thetaPlusは 基本的にはthetaに等しい、 thetaPlus(i)以外は。 thetaPlus(i)はEPSILONだけ増加させてある。 つまり、thetaPlusは theta1, theta2, ...などと 等しくて、 そしてtheta(i)の所では、EPSILONを足した物に等しい。 そしてさらにtheta(n)まで降りていく。 これがthetaPlusだ。 同様に、これら二つの行は thetaMinusに、上と似たような物を 代入しているが、 theta(i)+EPSILONの代わりに theta(i)-EPSILONな所だけが違う。 そして最後に、このgradApprox(i)を 実装する。 これがJのシータの シータiによる 偏微分の近似を 与える。 そしてこれの使い方としては、 ニューラルネットワークの実装において、 ニューラルネットワークの各パラメータによる コスト関数の偏微分を求める為に これを実装する、 このforループを実装する。 そして次に、バックプロパから グラディアントを取得出来る。 つまりDVecはバックプロパから得た 微分だ。 つまり、バックプロパ、バックプロパゲーションは 微分を計算する 比較的効率的な方法だ、より正確に言えば コスト関数の各パラメータによる 偏微分を計算する為の。 そして私がよくやるのは、 数値的に計算した微分に対して、 それはこの ここの上で得た gradApproxだが、 これがback propで得た物と 等しいかほとんど等しい事を 確認する事だ。小さい数値的な 丸めの範囲に収まっているかを。 back propで得たDVecと極めて近いかを。 そしてこれら二つの方法で計算した 微分の値が、同じ答えか、 少なくともとても近い答えを はじきだしたなら、小数点以下数桁の範囲で近ければ、 私のバックプロパの実装が正しい、という事に よりしっかりと自信を持つ事が出来る。 そうしてから、これらのDVecベクトルを 最急降下法なり 何らかのアドバンスドな最適化アルゴリズムに食わせれば、 その時には微分をちゃんと 計算していると 自信を持っているから、 自分のコードが正しく走るとも 期待出来て、 Jのシータを最適化するのに良い仕事をしてくれると期待出来る。 最後に、全部を合わせて 数値的グラディアントチェッキングを どう実装するかをお話したい。 私はいつも、こんな風にする。 最初にやるのは、 DVecを計算する為にバックプロパゲーションを実装する。 これは以前のビデオで話した DVecを計算する手順となり、 それはこれらの行列を展開したバージョンとなる。 次に私がやるのは、 gradApproxを計算する為に数値的なグラディアントチェッキングを実装する事だ。 これが私がこのビデオで話してきた所だ、前のスライドで話した奴。 そして次に、DVecとgradApproxが似た値かどうかを確認する、 たとえば小数点第二位とか第三位までで一致するかを見る。 そして最後に、そしてこれは大切なステップなのだが、 あなたのコードを実際に 学習させ始める前に、 真面目にネットワークをトレーニングする前に、 グラディアントチェッキングを切るのが大切だ。 そしてそれ以降は このビデオでやってきた gradApproxの 数値的な微分の式での計算を しないように。 その理由は、 数値的なグラディアントチェッキングのコードは、 このビデオで議論してきた内容は、 計算量的にとても高価で、 微分を近似しようとするのには 凄い遅いやり方だ。 一方で対照的に、以前に話した バックプロパゲーションのアルゴリズムは それは前に D1とかD2とかD3とかDVecを計算するのに 議論してきた物だが、 そのバックプロパゲーションは 微分を計算するのに、もっとずっと計算量的に効率的な方法だ。 だからひとたびあなたのバックプロパゲーションの実装が 正しい、と確認した後には、 グラディアントチェッキングは切るべきだ、 単純に使うのをやめるべきだ。 もう一度繰り返そう。 あなたのアルゴリズムを 最急降下法のたくさんの繰り返しで 走らせる前には、 あるいはアドバンスドな 最適化アルゴリズムで分類器を訓練する為に たくさんの繰り返しを走らせる前には、 グラディアントチェッキングのコードを切る事を忘れないようにしよう。 具体的には、もし万が一 最急降下法の各イテレーションで毎回 数値的グラディアントチェッキングを走らせてしまったら、 またはcostFunctionの内側のループで 走らせてしまったら、 あなたのコードはとてものろくなってしまうだろう。 何故なら数値的なグラディアントチェッキングのコードは バックプロパゲーションのアルゴリズムに比べて ずっと遅いからだ。 つまりデルタ4、デルタ3、デルタ2などを 計算する時に用いた バックプロパゲーションと比較するとだ。 それがバックプロパゲーションだった。 それはグラディアントチェッキングよりもずっと早い微分の計算方法だ。 だから準備が出来たら。 一旦あなたのバックプロパゲーションの実装が正しいと確認出来たら、 グラディアントチェッキングのコードを 切るなりdisableするなりを 確実に行おう、 アルゴリズムをトレーニングする間は。さもないとコードがとてもゆっくり実行されてしまう。 以上がグラディアントを数値的に計算する方法だ。 こうやって、あなたのバックプロパゲーションの実装が正しい、と 検証する事が出来る。 私がバックプロパゲーションや、似たような複雑なモデルに対して 最急降下法を実装する時にはいつでも、 グラディアントチェッキングを使っている。 これは自分のコードが正しいと確認する為の、本当に良い助けとなってくれるんだ。