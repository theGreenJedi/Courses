तो,लगें हैं हमें बहुत से वीडियो समझने के लिए न्यूरल नेटवर्क लर्निंग अल्गोरिद्म. इस वीडियो में, मैं क्या चाहूँगा करना कि कोशिश करूँ रखने की सभी हिस्सों को एक साथ, देने के लिए एक समग्र सारांश एक पूरी तस्वीर, कि कैसे सारे टुकड़े जुड़ते हैं एक साथ और पूरी प्रक्रिया कि कैसे इम्प्लमेंट करते हैं एक न्यूरल नेटवर्क लर्निंग अल्गोरिद्म. जब ट्रेन कर रहे हैं एक न्यूरल नेटवर्क को, पहला काम जो आपको करने की ज़रूरत है चुनें कोई नेटवर्क आर्किटेक्चर और आर्किटेक्चर से मेरा सिर्फ़ मतलब है न्यूरॉन्स के बीच कनेक्टिविटी पैटर्न. तो, आप जानते हैं, हम चुन सकते हैं मान लो, एक न्यूरल नेटवर्क जिसमें तीन इनपुट यूनिट्स हैं और पाँच हिडन यूनिट्स हैं और चार आउट्पुट यूनिट्स या एक जिसमें 3 हिडन लेयर्स हैं, 5 यूनिट्स प्रत्येक हिडन लेयर में, 4 आउट्पुट और यहाँ है 3, 5, 5, 5 यूनिट्स प्रत्येक इन तीन हिडन लेयर्स की और चार आउट्पुट यूनिट्स, और इसलिए ये चुनाव कि कितनी हिडन यूनिट्स प्रत्येक लेयर में और कितनी हिडन लेयर्स, वे हैं आर्किटेक्चर के विकल्प. तो, कैसे आप करते हैं चुनाव? ठीक है, पहले, संख्या इनपुट यूनिट्स की वह है परिभाषित सही ढंग से. और एक बार आपने तय कर लिया निश्चित सेट x फ़ीचर्ज़ का इनपुट यूनिट्स की संख्या फिर होगी, आप जानते हैं, डिमेन्शन आपके फ़ीचर x(i) निर्धारित उससे. और यदि आप कर रहे हैं मल्टीक्लास क्लैसिफ़िकेशन, संख्या यूनिट्स की होगी निर्धारित संख्या से क्लास की आपकी क्लैसिफ़िकेशन प्रॉब्लम की. और बस एक चेतावनी कि यदि आपके पास है मल्टीक्लास क्लैसिफ़िकेशन जहाँ y लेता है वैल्यूज़ मान लो 1 से 10 के बीच, तो आपके पास हैं दस संभव क्लास. तब याद रखें लिखना, आपकी आउट्पुट y को वेक्टर्स की तरह. तो बजाय क्लास एक के, आप लिखते हैं इसे एक वेक्टर जैसे इस तरह, या दूसरी क्लास के लिए आप लिखते हैं उसे एक वेक्टर इस तरह. तो यदि एक इन आउट्पुट लेता है पाँचवी क्लास, आप जानते हैं y बराबर 5, तब जो आप दिखा रहे हैं आपके न्यूरल नेटवर्क को वास्तव में नहीं है एक वैल्यू y बराबर 5 की, इसेक स्थान पर यह आउट्पुट लेयर में जिसमें हैं दस आउट्पुट यूनिट्स, आप इसके स्थान पर फ़ीड करेंगे वेक्टर को जिसमें आप जानते हैं एक पाँचवे स्थान पर और बाक़ी सब ज़ीरो हैं. तो चुनाव संख्या का इनपुट यूनिट्स की और आउट्पुट यूनिट्स की संख्या है शायद कुछ हद तक काफ़ी सरल. और संख्या के लिए हिडन यूनिट्स की और संख्या हिडन लेयर्स की, एक उचित डिफ़ॉल्ट है कि लें एक ही हिडन लेयर और इस तरह का न्यूरल नेटवर्क दिखाया हुआ बाईं तरफ़ जिसमें है केवल एक हिडन लेयर है सम्भवत: काफ़ी आम है. या यदि आप लेते हैं अधिक हिडन लेयर्स, फिर से उचित डिफ़ॉल्ट होगा लें समान संख्या हिडन यूनिट्स की प्रत्येक लेयर में. तो, यहाँ हमारे पास एक है दो हिडन लेयर्स और प्रत्येक में इन हिडन लेयर्स में हैं समान संख्या पाँच हिडन यूनिट्स की और यहाँ हमारे पास है, आप जानते हैं, तीन हिडन लेयर्स और इनमें से प्रत्येक में हैं समान संख्या, जो है पाँच हिडन यूनिट्स. बजाय लेने के इस तरह का नेटवर्क, आर्किटेक्चर बाईं तरफ़ का सटीक उचित डिफ़ॉल्ट. और संख्या के लिए हिडन यूनिट्स की - अक्सर जितनी अधिक हिडन यूनिट्स उतना बेहतर; सिर्फ़ इतना कि यदि आपके पास हैं बहुत सी हिडन यूनिट्स, यह हो सकता है कॉम्प्यूटेशनली अधिक महँगा, लेकिन बहुधा, होना अधिक हिडन यूनिट्स अच्छी बात है. और अक्सर संख्या हिडन यूनिट्स की प्रत्येक लेयर में होगी शायद लगभग समान डिमेन्शन के x की, तुलनात्मक फ़ीचर्ज़ की संख्या के, या यह हो सकता है समान, संख्या के हिडन यूनिट्स से लेकर इनपुट फ़ीचर्ज़ तक से या शायद तीन या चार गुणा उसका. तो रखना हिडन यूनिट्स है तुलनात्मक. आप जानते हैं, कई गुणा, या कुछ बड़ा संख्या से इनपुट फ़ीचर्ज़ की अक्सर एक उपयोगी चीज़ है वैसा करना. तो, उम्मीद है यह देता है आपको एक उचित सेट डिफ़ॉल्ट विकल्पों का न्यूरल आर्किटेक्चर के लिए और यदि आप पालन करते हैं इन निर्देशों का, आपको शायद मिले कुछ जो करे बेहतर, लेकिन एक बाद के सेट में वीडियोज़ के जहाँ मैं बात करूँगा विशेष रूप से सलाह की कि कैसे अप्लाई करने है अल्गोरिद्म्स, मैं वास्तव में बताऊँगा और अधिक कि कैसे चुनना है एक न्यूरल नेटवर्क आर्किटेक्चर. वास्तव में है काफ़ी कुछ जो मैं चाहता हूँ बाद में बताना लेने के लिए अच्छे विकल्प हिडन यूनिट्स की संख्या के लिए, हिडन लेयर्स की संख्या के लिए, और इसी तरह और कुछ. इसके बाद, यहाँ है कि हमें क्या इम्प्लमेंट करने की ज़रूरत है करने के लिए ट्रेन एक न्यूरल नेटवर्क को, हैं वास्तव में छः स्टेप्स जो मेरे पास हैं, मेरे पास चार हैं इस स्लाइड पर और दो और स्टेप्स हैं अगली स्लाइड पर. पहला स्टेप है सेट अप करना न्यूरल नेटवर्क को और रैंडम ढंग से देना प्रारम्भिक वैल्यूज़ वेट्स को. और हम आम तौर पर प्रारंभ करते हैं वेट्स छोटी वैल्यूज़ से ज़ीरो के क़रीब की. फिर हम इम्प्लमेंट करते हैं फ़ॉर्वर्ड प्रॉपगेशन ताकि हम इनपुट कर सकें फ़ीचर्ज़ न्यूरल नेटवर्क को और कम्प्यूट करें h ऑफ़ x जो है यह आउट्पुट वेक्टर y वैल्यूज़ का. हम फिर इम्प्लमेंट करते हैं कोड भी कम्प्यूट करने के लिए यह कॉस्ट फ़ंक्शन जे ऑफ़ थीटा. और उसके बाद हम इम्प्लमेंट करते हैं बैक-प्रॉप या बैक प्रॉपगेशन अल्गोरिद्म का, कम्प्यूट करने के लिए ये पार्शियल डेरिवेटिव टर्म्ज़, पर्शियल डेरिवेटिव्स जे ऑफ़ थीटा के विद रिस्पेक्ट टु पेरमिटर्स. वास्तव में इम्प्लमेंट करने के लिए बैक प्रॉप, अक्सर हम करेंगे वह एक फ़ॉर लूप से ट्रेनिंग इग्ज़ाम्पल्ज़ पर. आप में कुछ ने शायद सुना हो उन्नत / एडवांस्ड , और स्पष्ट रूप से बहुत एडवांस्ड फ़ैक्टर करने की विधि के बारे में जहाँ आप को नहीं करना पड़ता एक फ़ॉर-लूप m ट्रेनिंग इग्ज़ाम्पल्ज़ पर, परंतु पहली बार जब आप इम्प्लमेंट कर रहे हैं बैक प्रॉप वहाँ लगभग निश्चित रूप से होना चाहिए फ़ॉर लूप आपके कोड में, जहाँ आप इटरेट कर रहे हैं इग्ज़ाम्पल्ज़ पर, आप जानते हैं, x1, y1, तब इसलिए आप करते हैं फ़ॉर्वर्ड प्रॉप और बैक-प्रॉप पहले इग्ज़ाम्पल पर, और फिर दूसरी इटरेशन में फ़ॉर लूप की, आप करते हैं फ़ॉर्वर्ड प्रॉपगेशन और बैक प्रॉपगेशन दूसरे इग्ज़ाम्पल पर, और इसी तरह आगे. जब तक पहुँच नहीं जाते अंतिम इग्ज़ाम्पल पर. तो वहाँ होना चाहिए एक फ़ॉर-लूप आपकी इम्प्लमेंटेशन में बैक प्रॉप की, कम से कम पहली बार जब आप इसे इम्प्लमेंट कर रहे हैं. और फिर वहाँ स्पष्ट रूप से हैं कुछ हद तक जटिल तरीके करने के लिए ऐसा बिना फ़ॉर-लूप के, लेकिन मैं निश्चित रूप से सलाह नहीं दूँगा करने की वह अधिक पेचीदा वर्ज़न पहली बार जब आप प्रयास कर रहे हैं इम्प्लमेंट करने का बैक प्रॉप. तो वस्तुतः, हमारे पास है एक फ़ॉर लूप मेरे m-ट्रेनिंग इग्ज़ाम्पल्ज़ पर इस फ़ॉर-लूप के अंदर हम करेंगे फ़ॉर्वर्ड प्रॉप तथा बैक-प्रॉप इस्तेमाल करके केवल एक इग्ज़ाम्पल. तो क्या मतलब है उससे कि हम लेंगे x(i), और फ़ीड करेंगे उसे मेरी इनपुट लेयर को, और करेंगे फ़ॉर्वर्ड-प्रॉप, करेंगे बैक-प्रॉप और वह देगा हमें सारे ये ऐक्टिवेशन्स और सारे डेल्टा टर्म्ज़ सारी लेयर्स का मेरी सारी यूनिट्स के न्यूरल नेटवर्क की तब अभी भी इस फ़ॉर-लूप के अंदर ही, मैं बनाता हूँ कुछ करली कोषठक सिर्फ़ दर्शाने के लिए स्कोप फ़ॉर-लूप का, यह है ओकटेव में निश्चय ही, लेकिन यह ज़्यादा एक जावा के जैसा ही कोड है, और एक फ़ॉर-लूप में यह सब शामिल है. हम करेंगे कम्प्यूट वे सब डेल्टा टर्म्ज़, जो वे फ़ॉर्म्युला हैं जो हमने पहले बताए थे. इसके अलावा, आप जानते हैं, डेल्टा i प्लस वन टाइम्ज़ a i, ट्रान्स्पोज़. और फिर अंत में, बाहर कम्प्यूट कर लेने के बाद ये डेल्टा टर्म्ज़, ये संचित टर्म्ज़, हम फिर लिखेंगे कुछ और कोड और फिर वह कम्प्यूट करने देगा ये पर्शियल डेरिवेटिव टर्म्ज़. ठीक है और ये पर्शियल डेरिवेटिव टर्म्ज़ में लेना पड़ेगा रेगुलराइज़ेशन टर्म लैम्डा भी. और इसलिए, वे फ़ॉर्म्युला बताए थे पहले वीडियो में. तो, वैसा कर लेने के बाद आपके पास उम्मीद है होगा कम्प्यूट करने के लिए ये पर्शियल डेरिवेटिव टर्म्ज़. आगे है स्टेप पाँच, मैं क्या करता हूँ तब इस्तेमाल करता हूँ ग्रेडीयंट चेकिंग तुलना करने के लिए ये पर्शियल डेरिवेटिव टर्म्ज़ जो कम्प्यूट की थी. तो, मैंने तुलना कर ली है वैल्यूज़ की जो कम्प्यूट की थी बैक प्रॉपगेशन से और पार्शियल डेरिवेटिव्स जो कम्प्यूट किए थे नूमेरिकल अनुमान से डेरिवाटिव्स जो नूमेरिकल अनुमान थे. तो, मैं ग्रेडीयंट चेकिंग करता हूँ सुनिश्चित करने के लिए कि ये दोनो मुझे समान वैल्यू देते हैं. ग्रेडीयंट चेकिंग कर लेने के बाद हम आश्वस्त है कि हमारा इम्प्लमेंटेशन बैक प्रॉपगेशन का सही है, और तब बहुत महत्वपूर्ण है कि हम निष्क्रिय करें ग्रेडीयंट चेकिंग को, क्योंकि ग्रेडीयंट चेकिंग कोड कॉम्प्यूटेशनली बहुत धीरे चलता है. और अंत में, हम तब इस्तेमाल करके एक ऑप्टिमायज़ेशन अल्गोरिद्म जैसे ग्रेडीयंट डिसेंट, या एक एडवांस्ड ऑप्टिमायज़ेशन तरीक़ा, जैसे GS का LB, कॉंट्रैक्ट ग्रेडीयंट जिसमें है fminunc या अन्य ऑप्टिमायज़ेशन के तरीके. हम उपयोग करते हैं इनका साथ में बैक प्रॉपगेशन के, इसलिए बैक प्रॉपगेशन है वह जो कम्प्यूट कर रहा है ये पर्शियल डेरिवेटिव टर्म्ज़ हमारे लिए. और इसलिए, हम जानते हैं कैसे कंपयूट करना है कॉस्ट फंक्शन, हम जानते हैं कैसे कम्प्यूट करने हैं ये पर्शियल डेरिवेटिव्स इस्तेमाल करके बैक प्रॉपगेशन, इसलिए हम कर सकते हैं इस्तेमाल इनमें से एक ऑप्टिमायज़ेशन तरीक़ों का मिनमायज़ करने की कोशिश में J ऑफ़ थीटा जो है फ़ंक्शन पेरमिटर्स थीटा का. और वैसे तो, न्यूरल नेटवर्क्स के लिए, यह कॉस्ट फ़ंक्शन जे ऑफ़ थीटा है नॉन-कान्वेक्स, या नहीं है कान्वेक्स और इसलिए यह सैद्धांतिक रूप से अतिसंवेदनशील हो सकता है लोकल मिनिमा को, और वास्तव में, अल्गोरिद्म्स जैसे कि ग्रेडीयंट डिसेंट और एडवांस्ड ऑप्टिमायज़ेशन तरीक़े, सिद्धांत रूप में, अटक सकते हैं लोकल ऑप्टिमा में, लेकिन यह होता है व्यावहारिक दृष्टि से यह नहीं है अक्सर एक बड़ी समस्या और जबकि हम गारंटी नहीं दे सकते कि ये अल्गोरिद्म ढूँढ पाएँगे एक ग्लोबल ओप्टिमम, आमतौर पर अल्गोरिद्म जैसे ग्रेडीयंट डिसेंट करेंगे एक एक अच्छा काम मिनमायज़ करने में इस कॉस्ट फ़ंक्शन जे ऑफ़ थीटा को और पाएँगे एक अच्छा लोकल मिनिमम, भले ही यदि यह नही पहुँच पाता है ग्लोबल ओप्टिमम पर. अंत में, ग्रेडीयंट डिसेंट एक न्यूरल नेटवर्क के लिए अभी भी थोड़ा मनोहर सा प्रतीत होता है. तो, चलो मैं दिखाता हूँ एक और चित्र पाने के लिए वह अनुभव कि ग्रेडीयंट डिसेंट क्या करता है न्यूरल नेटवर्क में. यह वास्तव में समान है उस चित्र के जो मैं पहले इस्तेमाल कर रहा था समझाने के लिए ग्रेडीयंट डिसेंट. तो, हमारे पास है कुछ कॉस्ट फ़ंक्शन, और हमारे पास हैं कुछ पेरमिटर्स हमारे न्यूरल नेटवर्क के. यहीं पर मैंने अभी लिखी हैं दो पेरमिटर्स की वैल्यूज़. हकीकत में, ज़ाहिर है, न्यूरल नेटवर्क में, हमारे पास इससे कही अधिक पेरमिटर्स हो सकते हैं. थीटा एक, थीटा दो - ये सारे मेट्रिसीज़ है, ठीक है? तो हमारे पास हो सकते हैं बहुत अधिक डिमेन्शन्स के पेरमिटर्स लेकिन क्योंकि सीमित हैं हम उससे जो हम चित्रित कर सकते हैं, मैं मान रहा हूँ कि हमारे पास है केवल दो पेरमिटर्स इस न्यूरल नेटवर्क में. हालांकि जाहिर है कि हमारे पास होते हैं बहुत अधिक वास्तव में. अब, कॉस्ट फ़ंक्शन जे ऑफ़ थीटा मापता है कि कितना सही न्यूरल नेटवर्क फ़िट होता है ट्रेनिंग डेटा को. तो, यदि आप लेते हैं एक पोईँट जैसे यह, नीचे यहाँ, वह है एक पोईँट जहाँ जे थीटा है काफ़ी कम, तो यह कॉरेस्पॉंड करता है पेरमिटर्स की सेटिंग्स को. ऐसी सेटिंग है पेरमिटर्स थीटा की, जहाँ, आप जानते हैं, ज़्यादातर ट्रेनिंग इग्ज़ाम्पल के लिए, आउट्पुट मेरी हायपॉथिसस की, वह हो सकती है काफ़ी क़रीब y(i) के और यदि यह है सही, तब वह है जो मेरे कॉस्ट फ़ंक्शन को काफ़ी कम करता है. जबकि इसके विपरीत, यदि आपको लेनी होती एक वैल्यू उस तरह की, एक पोईँट वैसा जो कॉरेस्पॉंड करता है जहाँ बहुत से ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए, आउट्पुट मेरे न्यूरल नेटवर्क काफ़ी दूर y(i) की असली वैल्यू से जो थी ट्रेनिंग सेट में. तो पोईँट्स जैसे यह लाइन पर कॉरेस्पॉंड करते हैं जहाँ हायपॉथिसस, जहाँ न्यूरल नेटवर्क आउट्पुट करता है वैल्यूज़ ट्रेनिंग सेट पर जो हैं y(i) से बिल्कुल भिन्न. तो, यह नहीं कर रहा फ़िट ट्रेनिंग सेट को सही ढंग से, जबकि जैसे यह जिसकी कम वैल्यू है कॉस्ट फ़ंक्शन की कॉरेस्पॉंड करता है जहाँ जे ऑफ़ थीटा है कम, और इसलिए कॉरेस्पॉंड करता है जहाँ न्यूरल नेटवर्क हो जाता है फ़िट मेरे ट्रेनिंग सेट को सही, क्योंकि इसका मतलब है यह है जिसकी आवश्यकता है कि सही हो ताकि जे ऑफ़ थीटा हो सके कम / छोटा.. तो ग्रेडीयंट डिसेंट क्या करता है हम शूरु करते हैं कुछ रैंडम प्रारम्भिक पोईँट से जैसे वह वहाँ पर, और यह जाएगा लगातार नीचे की ओर. और इसलिए बैक प्रॉपगेशन क्या करता है कि कम्प्यूट करता है दिशा ग्रेडीयंट की, और क्या ग्रेडीयंट डिसेंट करता है कि लेता है छोटे स्टेप्स नीचे की ओर जब तक उम्मीद है यह पहुँचता है, इस केस में, एक काफ़ी अच्छे लोकल ऑप्टिमम पर. तो जब आप इम्प्लमेंट करते हैं बैक प्रॉपगेशन और इस्तेमाल करते हैं ग्रेडीयंट डिसेंट या कोई एक एडवांस्ड ऑप्टिमायज़ेशन तरीक़ा, यह चित्र एक तरह से समझा देता है कि अल्गोरिद्म क्या करता है. यह कोशिश कर रहा है ढूँढने की एक वैल्यू पेरमिटर्स की जहाँ आउट्पुट वैल्यू न्यूरल नेटवर्क में क़रीबी से मैच करती है y(i) की वैल्यू को जो आपके ट्रेनिंग सेट में है. तो, उम्मीद है यह देता है आपको एक बेहतर समझ कि कैसे बहुत से भिन्न हिस्से न्यूरल नेटवर्क के जुड़ते हैं एक साथ. यदि इस वीडियो के बाद भी, यदि आपको अभी भी लगता है जैसे ऐसे है, जैसे, बहुत से भिन्न हिस्से और यह पूरी तरह स्पष्ट नहीं है कि क्या उनमें से कुछ करते हैं या कैसे सब ये हिस्से एक साथ जुड़ते हैं, वह वास्तव में ठीक है. न्यूरल नेटवर्क लर्निंग और बैक प्रॉपगेशन है एक जटिल अल्गोरिद्म. और यद्यपि मैंने देखा है बैक प्रॉपगेशन के पीछे का गणित कई वर्षों से और मैंने इस्तेमाल किया है बैक प्रॉपगेशन, मैं सोचता हूँ बहुत सफलतापूर्वक, कई वर्षों से, फिर भी आज भी मुझे लगता है मुझे नहीं है हमेशा एक बढ़िया समझ कि वास्तव में बैक प्रॉपगेशन क्या कर रहा है कभी कभी. और ऑप्टिमायज़ेशन प्रक्रिया क्या दिखाती है कि यह मिनमायज़ कर रही है जे ऑफ़ थीटा को. जबकि यह एक बहुत कठिन अल्गोरिद्म है यह लगता है कि मुझे एक कम समझ है कि वास्तव में यह क्या कर रहा है तुलना में, मान लो, लिनीअर रेग्रेशन या लॉजिस्टिक रेग्रेशन के. जो गणितीय और सैद्धांतिक रूप में अधिक सरल थे और अधिक स्पष्ट अल्गोरिद्म थे. लेकिन इसलिए, यदि आपको महसूस होता है उसी तरह, आप जानते हैं, वह वास्तव में पूरी तरह से ठीक है, लेकिन यदि आप फिर भी इम्प्लमेंट करते हैं बैक प्रॉपगेशन, उम्मीद है आपको मिलेगा कि यह है एक ज़बरदस्त लर्निंग अल्गोरिद्म, और यदि आप इम्प्लमेंट करते हैं इस अल्गोरिद्म को, इम्प्लमेंट करते हैं बैक प्रॉपगेशन, इम्प्लमेंट करते हैं इनमें से एक ऑप्टिमायज़ेशन तरीक़ा, आपको मिलेगा कि बैक प्रॉपगेशन हो पाएगा फ़िट एक बहुत जटिल, प्रभावशाली, नॉन-लिनीअर फ़ंक्शन आपके डेटा को, और यह है एक सबसे प्रभावी लर्निंग अल्गोरिद्म आज हमारे पास.