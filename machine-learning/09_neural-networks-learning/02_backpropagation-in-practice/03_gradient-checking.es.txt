En los últimos videos, hemos hablado acerca de cómo realizar una propagación hacia delante y una retropropagación en una red neuronal con el fin de procesar los derivados. Sin embargo la retropropagación como un algoritmo tiene muchos detalles y, como saben, puede ser un poco difícil de implementar. Y una variable desafortunada es que existen muchas formas de tener errores imperceptibles en la retropropagación de forma que si los ejecutan con el gradiente de descenso o algún otro algoritmo de optimización, podría parecer que en realidad está funcionando. Y, como saben,  la función de costos J de «theta» puede terminar disminuyendo en cada iteración de la gradiente de descenso, pero esto podría llevarse a cabo a pesar de que puede haber algún error en la implementación de la retropropagación. Así que parece que J de «theta» está disminuyendo, sin embargo podrían terminar con una red neuronal que tiene un nivel más alto de errores de lo que tendrían con una implementación libre de errores y simplemente no sabrían que este error imperceptible estuvo ocasionándoles este desempeño. Entonces, ¿qué podemos hacer al respecto? Existe un concepto llamado comprobación de gradiente la cual elimina casi todos estos problemas. Entonces hoy, cada vez que implemente una retropropagación o un algoritmo de gradiente de descenso similar en la red neuronal o en cualquier otro modelo de una complejidad razonable, siempre implementaré la comprobación de gradiente. Y si ustedes lo llevan a cabo les ayudará a comprobar y en cierto modo a tener mayor confianza de que la implementación de la propagación hacia delante y la retropropagación o lo que sea, es 100% correcta. Y de acuerdo a lo que he visto esto elimina prácticamente todos los problemas relacionados con el tipo de implementación errónea de la retropropagación. Y en los videos anteriores, en cierta forma les pido que tengan confianza en que las fórmulas que les he dado para procesar los «delta»s, y las "D" y así sucesivamente, les pido que tengan confianza en que aquellas realmente  procesan las gradientes de la función de costos, pero una vez que ustedes implementan la comprobación de la gradiente numérica, que es el tema de este video, podrán ser capaces de verificar por sí mismos que el código que escriban en efecto está procesando la derivada de la función de costos de "J". 
Entonces, este es el concepto. Consideren el siguiente ejemplo. Supongan que tengo la función J de «theta», y tengo algún valor, «theta», y para este ejemplo, voy a asumir que «theta» es sólo un número real. Y digamos que deseo calcular la derivada de esta función en este punto. Y así la derivada es, como saben, igual a la pendiente de ese tipo de línea de la tangente. Así es como voy a aproximar en forma numérica la derivada, o más bien este es un procedimiento para aproximar en forma numérica la derivada: Voy a procesar «theta» más «épsilon», de forma que el valor está un poco a la derecha. Y vamos a procesar «theta» menos «épsilon». Y voy a,  observen estos dos puntos y conéctenlos por medio de una línea recta. Y voy a conectar estos dos puntos por medio de una línea recta y voy a utilizar la pendiente de esta pequeña línea roja como mi aproximación a la derivada que es, la verdadera derivada es la pendiente de esa línea azul ahí. De forma que, como saben, parece que sería una muy buena aproximación. Matemáticamente, la pendiente de esta línea roja es esta altura vertical, dividida por esta anchura horizontal, así que este punto en la parte superior es J de «theta» más «épsilon» Este punto aquí es J de «theta» menos «épsilon». De forma que esta diferencia vertical es J de «theta» más «épsilon», menos J de «theta», menos «épsilon» y esta distancia horizontal es sólo 2 «épsilon». Así que, mi aproximación va a ser la derivada, con respecto a «theta» de J de «theta»-- añadan este valor de «theta»-- que eso es aproximadamente J de «theta» más «épsilon», menos J de «theta», menos «épsilon», sobre 2 «épsilon». Por lo general, utilizo un valor muy pequeño para  «épsilon» y establecemos que «épsilon» tal vez se encuentre en el orden de 10 a menos 4. Por lo general existe un rango amplio de distintos valores para «épsilon» que funcionan muy bien. De hecho, si ustedes dejan que «épsilon» sea demasiado pequeño entonces, matemáticamente, este término de aquí de hecho, matemáticamente, como saben, se convierte en la derivada, se vuelve con exactitud en la pendiente de la función en este punto. Es sólo que no queremos utilizar «épsilon», que es demasiado, demasiado pequeño porque entonces podríamos encontrarnos con problemas numéricos. Entonces, como saben, por lo general utilizo «épsilon» alrededor de 10 a menos 4, digamos. Y por cierto que algunos de ustedes pueden haber visto una fórmula alternativa para estimar la derivada que es esta fórmula. La que se encuentra en el lado derecho se llama de diferencia unilateral. Mientras que la fórmula de la izquierda se llama de diferencia bilateral. La diferencia bilateral nos proporciona una estimación ligeramente más precisa, de forma que por lo general utilizo esa en lugar de sólo esta estimación de diferencia unilateral. Así, en concreto,  lo que implementan en Octava es la implementación de lo siguiente. Implementan el proceso de aproximación de la gradiente que va a ser tan sólo una aproximación a la derivada, como saben, en esta fórmula: J de «theta» más «épsilon», menos J de «theta», menos «épsilon», dividido por dos veces «épsilon». Y esto nos proporcionará una estimación numérica de la gradiente en ese punto. Y en este ejemplo parece ser una estimación bastante buena. Ahora, en la diapositiva anterior consideramos el caso de cuando «theta» es un número real. Ahora veamos un caso más general donde «theta» es un parámetro  vector. Así que digamos que «theta» es una Rn y pudiera ser una versión irreal de los parámetros de nuestra red neuronal. De modo que «theta» es un vector que tiene "n" elementos, «theta» 1 hasta «theta» n. Entonces podemos utilizar un concepto similar para aproximar todos los términos de la derivada parcial. En concreto,  la derivada parcial de la función de costos con respecto al primer parámetro de «theta» 1, esa puede obtenerse al tomar J e incrementar «theta» 1. Entonces tienen J de «theta» 1 más «épsilon» y así sucesivamente menos J de esta «theta» 1 menos «épsilon» y dividida por 2 «épsilon». La derivada parcial respecto al segundo parámetro «theta» 2, es de nuevo esto, excepto que ustedes toman J de-- aquí están incrementando «theta» 2 por «épsilon». Y aquí ustedes están disminuyendo «theta» 2 por «épsilon». Y así hasta llegar a la derivada parcial con respecto a «theta» n. Lo que sería si ustedes incrementan y disminuyen «theta» n por «épsilon» allí. Así, estas ecuaciones les proporcionan una forma para aproximar en forma numérica la derivada parcial de "J" con respecto a cualquiera de los parámetros que se derivan. En concreto, lo que implementarán es por lo tanto, lo siguiente. Implementamos lo siguiente en Octave para ejecutar en forma numérica las derivadas. Decimos que i es igual a 1 hasta n donde "n" es la dimensión de nuestro parámetro vector «theta». Y por lo general hago esto con la versión desarrollada de los parámetros. De forma que saben que «theta» es sólo una lista larga de todos mis parámetros en mis redes neuronales. Voy a establecer que «theta» más es igual a «theta», a continuación incrementen «theta» más el elemento «i-nésimo» por ««épsilon»». Y esto es básicamente, «theta» más que es igual a «theta» excepto para «theta» más i, que ahora está incrementada por «épsilon». De forma que si «theta» más es igual a,  correcto, «theta» 1, «theta» 2 y así sucesivamente y luego  «theta» i tiene un «épsilon» añadido, y luego sigue hasta «theta» n. Así que en esto consiste «theta» más. Y del mismo modo estas dos líneas establecen a «theta» menos en algo similar excepto que esto, en lugar de «theta» i más «épsilon», esto ahora se convierte en «theta» i menos «épsilon». Y entonces finalmente, implementamos esta aproximación de la gradiente i, y esto les dará la aproximación a la derivada derivada parcial con respecto a «theta» i de J de «theta». Y la forma en que utilizamos esto en nuestra complementación de la red neuronal es que queremos poner en práctica esto, implementar estos cuatro bucles para procesar, como saben, la derivada parcial de la función de costos con respecto a cada parámetro en nuestra red. Y podemos tomar el gradiente que obtuvimos de la retropropagación. Así, los vectores DVec fueron las derivadas que obtuvimos de la retropropagación. Correcto, así la retropropagación, la retropropagación fue una forma relativamente eficiente para procesar las derivadas o las derivadas parciales de una función de costos con respecto a todos nuestros parámetros. Y lo que por lo general hago es que tomo mi derivada procesada en forma numérica, que es esta aproximación de la gradiente que acabamos de obtener de aquí arriba y nos aseguramos de que esa sea igual o aproximadamente igual a, como saben, a los valores pequeños del redondeo numérico que están bastante cercanos a los vectores DVec que obtuve de la retropropagación. Y si estas dos formas de procesar la derivada me dan la misma respuesta o al menos me dan respuestas muy parecidas, como saben, hasta unas plazas decimales, Entonces tengo confianza de que es correcta mi implementación de retropropagación. Y cuando conecto estos vectores DVec a la gradiente de descenso o a algún algoritmo de optimización avanzada, puedo entonces estar mucho más seguro de que estoy calculando las derivadas correctamente y por lo tanto, con suerte mis códigos funcionarán correctamente y harán un buen trabajo optimizando J de «theta». Finalmente,  deseo resumir todo y decirles cómo implementar esta comprobación de la gradiente numérica. Esto es lo que suelo hacer. La primer cosa que hago es implementar la retropropagación para procesar los defectos. También, este es un procedimiento del cual hablamos en un video anterior para procesar el DVec que puede ser nuestra versión desarrollada de estas matrices. Entonces lo que hago es implementar una comprobación de la gradiente numérica para calcular la aproximación de la gradiente. Así que esto es lo que he descrito antes en este video, en la diapositiva anterior. Después deberán asegurarse de que el DVec y la aproximación de la gradiente den valores similares, como ya saben, digamos que hasta unos cuantas plazas decimales. Y finalmente, y este es el paso importante, entre más empiecen a utilizar su código para el aprendizaje, para entrenar en serio su red, es importante desactivar la comprobación de gradiente. Y para no procesar ya esta aproximación de la gradiente utilizando las fórmulas de las derivadas numéricas de las que hablamos anteriormente en este video. Y la razón de esto es que-- el código numérico, el código de la comprobación de gradiente, las cosas de las que hablamos en este video--  es muy caro computacionalmente, esa es una forma muy lenta de tratar de aproximar la derivada. En contraste con el algoritmo de retropropagación del que hablamos antes, que es de lo que hemos hablado antes para procesar, como saben, D1, D2, D3 o para DVec. La retropropagación es una forma mucho más eficiente computacionalmente de procesar las derivadas. De forma que una vez que hayan verificado que la implementación de su retropropagación es correcta, deberán desconectar la comprobación de la gradiente y sólo dejar de utilizarla. Sólo para recapitular, deberán estar seguros de deshabilitar el código de comprobación de la gradiente antes de  ejecutar su algoritmo para muchas iteraciones de la gradiente de descenso, o para muchas iteraciones de los algoritmos de optimización avanzada con el fin de entrenar su clasificador. Específicamente, si fueran a ejecutar la comprobación de la gradiente numérica en cada iteración de la gradiente de descenso, o si estuvieran en el bucle interno de la función de costos, entonces el código sería muy lento. Porque el código de la comprobación de la gradiente numérica es mucho más lento que el algoritmo de retropropagación,  que el método de retropropagación donde si recuerdan estuvimos procesando «delta» 4, «delta» 2, «delta» 2 y así sucesivamente. Ese fue el algoritmo de retropropagación. Esa es una forma mucho más rápida de procesar las derivadas de la comprobación de la gradiente. Entonces cuando estén listos, una vez que han verificado que la implementación de la retropropagación es correcta, asegúrense de desactivar o deshabilitar el código de comprobación de la gradiente mientras entrenan su algoritmo o de lo contrario el código funcionará de manera muy lenta. Entonces así es como obtienen las gradientes en forma numérica Y así es cómo ustedes pueden verificar que la implementación de la retropropagación es correcta. Cuando implemento la retropropagación o un algoritmo de gradiente de descenso similar para un modelo complicado, siempre utilizo la comprobación de gradiente. Esto realmente me ayuda a estar seguro de que mi código es correcto.