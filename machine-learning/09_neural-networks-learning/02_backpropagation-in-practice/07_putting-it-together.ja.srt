1
00:00:00,240 --> 00:00:01,560
ニューラルネットワークのアルゴリズムを

2
00:00:01,700 --> 00:00:02,690
見ていくのに

3
00:00:03,120 --> 00:00:04,480
たくさんのビデオを費やしてきた。

4
00:00:05,620 --> 00:00:06,640
このビデオでは、

5
00:00:06,800 --> 00:00:08,090
全てのピースを一つに合わせて

6
00:00:08,350 --> 00:00:10,040
全体的なサマリー、

7
00:00:10,370 --> 00:00:12,120
あるいはより大きな

8
00:00:12,360 --> 00:00:13,410
鳥瞰図を提供したい、

9
00:00:13,650 --> 00:00:15,290
全てのピースがどう組み合わさり

10
00:00:15,530 --> 00:00:16,990
ニューラルネットワークを全体として

11
00:00:17,260 --> 00:00:18,830
どう実装したらいいのかについて。

12
00:00:21,870 --> 00:00:23,210
ニューラルネットワークをトレーニングする時は

13
00:00:23,280 --> 00:00:24,290
最初にやらなくてはいけない事は

14
00:00:24,400 --> 00:00:25,920
なんらかのネットワークアーキテクチャを選ぶ事だ。

15
00:00:26,680 --> 00:00:27,950
ここでアーキテクチャという言葉で

16
00:00:28,200 --> 00:00:30,510
ニューロン同士の接続のパターンを意味している。

17
00:00:31,080 --> 00:00:31,840
だから例えば

18
00:00:32,700 --> 00:00:33,770
3つの入力ユニットに

19
00:00:34,230 --> 00:00:35,440
5つの隠れユニット、

20
00:00:35,960 --> 00:00:37,400
そして4つの出力を

21
00:00:37,500 --> 00:00:39,560
選ぶかもしれないし、対して

22
00:00:39,800 --> 00:00:41,460
3つ、5つの隠れ、5つの隠れ

23
00:00:41,700 --> 00:00:43,430
4つの出力を選ぶかもしれないし、

24
00:00:43,910 --> 00:00:45,220
そしてこれは3で、 5, 5, 5ユニットが

25
00:00:45,550 --> 00:00:47,060
三つの隠れレイヤーに

26
00:00:47,320 --> 00:00:48,870
あって、そして

27
00:00:49,120 --> 00:00:50,250
4つの出力ユニット、これを選ぶかもしれない。

28
00:00:50,430 --> 00:00:52,000
これらが幾つの隠れユニットと

29
00:00:52,270 --> 00:00:53,410
幾つの隠れレイヤーを持つかの

30
00:00:53,810 --> 00:00:55,560
選択肢だ。

31
00:00:55,780 --> 00:00:57,580
これらがアーキテクチャの選択肢だ。

32
00:00:57,910 --> 00:00:58,680
これらの選択肢をどうやって選ぶのか？

33
00:00:59,710 --> 00:01:01,270
まず、入力ユニットの総数は

34
00:01:01,530 --> 00:01:03,840
かっちりと定義されている。

35
00:01:04,680 --> 00:01:05,960
そして一旦固定されたフィーチャーの集まりxを

36
00:01:06,580 --> 00:01:07,870
決定してしまえば、

37
00:01:08,080 --> 00:01:09,420
入力ユニットの数は、

38
00:01:10,140 --> 00:01:12,180
フィーチャーx(i)の次元によって

39
00:01:12,330 --> 00:01:14,470
決定される。

40
00:01:14,760 --> 00:01:15,970
そしてもしマルチクラスの

41
00:01:16,210 --> 00:01:17,370
分類問題を扱っているなら、

42
00:01:17,520 --> 00:01:18,320
出力の数は、

43
00:01:18,420 --> 00:01:19,720
あなたの扱ってる分類問題の

44
00:01:20,060 --> 00:01:22,860
クラスの数によって決められる。

45
00:01:23,260 --> 00:01:24,890
思い出してもらう為に触れておくと

46
00:01:25,160 --> 00:01:27,290
マルチクラスの分類問題の時には、

47
00:01:27,570 --> 00:01:28,970
例えばyが1から10の間の数を取るとすると、

48
00:01:30,040 --> 00:01:31,350
つまり10個のとりうるクラスが

49
00:01:31,470 --> 00:01:33,560
ある事になるが、

50
00:01:34,690 --> 00:01:37,200
その時は、覚えているだろうか、

51
00:01:37,820 --> 00:01:39,340
出力のyはこれらのベクトルとなるのだった。

52
00:01:40,130 --> 00:01:41,560
つまりこれがクラス1で、

53
00:01:41,730 --> 00:01:42,840
それをこのベクトルに再コーディングする。

54
00:01:43,150 --> 00:01:44,600
二番目のクラスは

55
00:01:44,670 --> 00:01:47,280
こんなベクトルに再コーディングする。

56
00:01:48,130 --> 00:01:49,080
だから、これらのうちの一つを

57
00:01:49,210 --> 00:01:51,000
例として挙げると、

58
00:01:51,140 --> 00:01:53,910
5番目のクラスの場合、yイコール5の時は、

59
00:01:54,120 --> 00:01:55,130
ニューラルネットワークにおいて見る事になるのは

60
00:01:55,380 --> 00:01:56,840
y=5という値では無く、

61
00:01:57,250 --> 00:01:59,520
その代わりにここでは

62
00:02:00,030 --> 00:02:00,950
出力レイヤで、

63
00:02:01,280 --> 00:02:02,650
この場合は10個の出力ユニットがあり、

64
00:02:02,740 --> 00:02:03,920
そこに5番目に1があって

65
00:02:04,070 --> 00:02:05,710
それ以外が0で埋まっているような

66
00:02:07,470 --> 00:02:08,430
ベクトルを

67
00:02:08,770 --> 00:02:11,050
食わせることになる。

68
00:02:11,420 --> 00:02:12,470
つまり入力ユニットと出力ユニットの

69
00:02:12,890 --> 00:02:14,330
総数の選択は

70
00:02:14,970 --> 00:02:16,600
割とまっすぐ決まると思う。

71
00:02:18,000 --> 00:02:18,950
そして隠れユニットと

72
00:02:19,410 --> 00:02:21,040
隠れレイヤーの

73
00:02:21,140 --> 00:02:23,110
数の選択については

74
00:02:23,210 --> 00:02:24,350
もっとも普通の基本形は

75
00:02:24,540 --> 00:02:26,010
隠れレイヤは一層。

76
00:02:26,660 --> 00:02:28,040
つまりこの左側に示したような

77
00:02:28,880 --> 00:02:30,400
隠れレイヤ一つのニューラルネットワークが

78
00:02:30,580 --> 00:02:33,270
恐らくもっとも一般的な物と言える。

79
00:02:34,490 --> 00:02:35,870
そしてもし一つ以上の

80
00:02:36,140 --> 00:02:38,410
隠れレイヤを使うなら

81
00:02:38,670 --> 00:02:39,600
ここでも普通の基本形は

82
00:02:39,760 --> 00:02:40,950
各レイヤの隠れユニットは

83
00:02:41,130 --> 00:02:42,560
同じ数にする、という物。

84
00:02:42,810 --> 00:02:44,600
つまりここでは2つの

85
00:02:45,020 --> 00:02:46,370
隠れレイヤがあり、

86
00:02:46,610 --> 00:02:47,650
これらの隠れレイヤはそれぞれ

87
00:02:47,860 --> 00:02:49,500
同じ隠れユニットの数である5つの隠れユニットを持ち、

88
00:02:49,790 --> 00:02:50,740
こちらは見ての通り

89
00:02:51,600 --> 00:02:53,020
3つの隠れレイヤがあり

90
00:02:53,170 --> 00:02:54,790
それらはそれぞれ、同じユニット数で

91
00:02:54,980 --> 00:02:56,400
それぞれ5つの隠れユニットがある。

92
00:02:57,440 --> 00:02:59,440
だがこれらのネットワークアーキテクチャを使うまでもなく

93
00:02:59,740 --> 00:03:02,850
この左側のアーキテクチャでも十分に悪くないデフォルトの選択肢だ。

94
00:03:04,020 --> 00:03:04,780
そして隠れユニットの

95
00:03:05,120 --> 00:03:07,040
数としては、通常は

96
00:03:07,120 --> 00:03:08,100
隠れユニットが多ければ多い程良いが

97
00:03:08,560 --> 00:03:09,640
隠れユニットがたくさんあると

98
00:03:09,900 --> 00:03:11,110
計算量的には

99
00:03:11,330 --> 00:03:13,150
高価になる。

100
00:03:13,300 --> 00:03:15,850
だがしばしば、隠れユニットは多ければ多い程良い。

101
00:03:17,250 --> 00:03:18,560
そして通常は、各レイヤーの

102
00:03:18,720 --> 00:03:20,820
隠れユニットの総数は

103
00:03:21,080 --> 00:03:22,130
だいたいxの次元と同じ程度、

104
00:03:22,490 --> 00:03:23,670
フィーチャーの総数と

105
00:03:23,810 --> 00:03:24,950
同じ程度で、あるいは

106
00:03:25,140 --> 00:03:26,880
入力のフィーチャーの総数と

107
00:03:27,180 --> 00:03:29,590
同じ程度から2倍とか3倍とか4倍程度の

108
00:03:29,770 --> 00:03:32,430
適当な数が良い。

109
00:03:32,680 --> 00:03:34,770
だから入力のフィーチャーと

110
00:03:35,140 --> 00:03:36,350
同じ程度の数か、

111
00:03:36,410 --> 00:03:37,380
その数倍程度の数でも

112
00:03:37,430 --> 00:03:38,750
やる価値のある事が

113
00:03:39,280 --> 00:03:41,320
多い。

114
00:03:42,150 --> 00:03:43,490
以上で、ニューラルネットワークの

115
00:03:43,810 --> 00:03:45,140
アーキテクチャのデフォルトの選択として

116
00:03:45,650 --> 00:03:47,770
リーズナブルな選択肢を幾つか示せただろう。

117
00:03:48,200 --> 00:03:49,460
そしてもしあなたがこれらのガイドラインに沿えば、

118
00:03:49,540 --> 00:03:50,580
たぶんうまく機能する物が得られるだろう。

119
00:03:50,930 --> 00:03:52,180
だがこの後の

120
00:03:52,360 --> 00:03:53,770
ビデオでは、

121
00:03:54,050 --> 00:03:55,270
アルゴリズムを

122
00:03:55,580 --> 00:03:56,900
どう適用したらいいかについて

123
00:03:57,410 --> 00:03:58,770
詳細な話をしていく。

124
00:03:58,840 --> 00:04:01,880
ニューラルネットワークのアーキテクチャをどうやって選んだらいいかについて、もっと多くの事を話していく。

125
00:04:02,540 --> 00:04:03,920
また、実際に隠れユニットや

126
00:04:03,970 --> 00:04:04,960
隠れレイヤーの総数の

127
00:04:04,960 --> 00:04:06,180
良い選び方について、などは

128
00:04:06,710 --> 00:04:08,780
たくさん話すつもりだ。

129
00:04:10,620 --> 00:04:12,310
次に、ニューラルネットワークをトレーニングする為に

130
00:04:12,420 --> 00:04:13,740
実装しなくてはいけない事は

131
00:04:13,860 --> 00:04:15,360
これだ。

132
00:04:15,510 --> 00:04:16,820
6つのステップがある。

133
00:04:17,080 --> 00:04:18,030
このスライドでは4つ、

134
00:04:18,160 --> 00:04:19,100
そして次のスライドに

135
00:04:19,380 --> 00:04:21,480
残り二つ。

136
00:04:21,620 --> 00:04:22,220
最初のステップはニューラルネットワークを

137
00:04:22,430 --> 00:04:23,510
セットアップして、ウェイトの値を

138
00:04:24,080 --> 00:04:25,570
ランダムに初期化する。

139
00:04:25,790 --> 00:04:27,000
ウェイトの初期化は普通、

140
00:04:27,080 --> 00:04:29,710
0付近の小さな値で初期化する。

141
00:04:31,100 --> 00:04:33,120
次にフォワードプロパゲーションを実装する、

142
00:04:34,080 --> 00:04:35,060
入力をニューラルネットワークに入れて

143
00:04:35,480 --> 00:04:37,150
予測が計算出来るように。

144
00:04:37,490 --> 00:04:38,860
つまりhのxを計算し、

145
00:04:39,070 --> 00:04:40,820
この出力ベクトルのyの値を得る為に。

146
00:04:44,260 --> 00:04:45,910
そして次にコスト関数である

147
00:04:46,010 --> 00:04:47,500
Jのシータを計算するコードを実装する。

148
00:04:49,770 --> 00:04:51,160
そして次にバックプロパゲーションを

149
00:04:52,120 --> 00:04:53,330
バックプロパゲーションアルゴリズムを実装する、

150
00:04:54,400 --> 00:04:55,680
これらの偏微分の項を

151
00:04:55,910 --> 00:04:58,000
計算する為に、、、

152
00:04:58,440 --> 00:04:59,830
Jのシータの、パラメータによる

153
00:05:00,340 --> 00:05:04,240
偏微分を計算する為に。
具体的には、バックプロバゲーションを実装するには、

154
00:05:04,960 --> 00:05:05,880
普通はトレーニング手本に渡って

155
00:05:06,250 --> 00:05:08,460
for文を回して

156
00:05:09,700 --> 00:05:10,650
あなたがたの中には

157
00:05:10,830 --> 00:05:12,640
アドバンスドな、凄いアドバンスドな

158
00:05:12,940 --> 00:05:14,500
ベクトル化した方法を聞いた事がある人もいるかもしれない。

159
00:05:14,670 --> 00:05:15,720
それはm個のトレーニング手本に渡っての

160
00:05:16,570 --> 00:05:18,580
for文を回さない物だ。

161
00:05:18,660 --> 00:05:19,900
だが最初にバックプロパゲーションを

162
00:05:20,250 --> 00:05:21,420
実装する時には、ほぼ確実にforループのある実装から

163
00:05:21,420 --> 00:05:22,980
始めるべきだろう。

164
00:05:23,800 --> 00:05:25,010
そこでトレーニング手本に渡ってイテレーションを行い、

165
00:05:25,810 --> 00:05:27,760
つまり、まずx1とy1に対して

166
00:05:28,030 --> 00:05:29,510
最初の手本に対して

167
00:05:29,640 --> 00:05:30,400
フォワードプロパケーションとバックプロパゲーションを行い、

168
00:05:30,850 --> 00:05:32,510
そして次に

169
00:05:32,710 --> 00:05:33,730
forループの二回目のイテレーションで

170
00:05:33,780 --> 00:05:35,360
二番目の手本に対し

171
00:05:35,980 --> 00:05:38,050
フォワードプロパケーションとバックワードプロパゲーションを行う、などなど。

172
00:05:38,170 --> 00:05:40,900
これを最後の手本に至るまで進める。

173
00:05:41,680 --> 00:05:43,110
つまり、少なくとも初めて実装する時には

174
00:05:43,230 --> 00:05:44,250
あなたのバックプロパゲーションの実装には

175
00:05:45,050 --> 00:05:47,180
forループがあるはずだ。

176
00:05:48,120 --> 00:05:49,160
そして簡単に言うと

177
00:05:49,390 --> 00:05:50,520
なんか難しいやり方で

178
00:05:50,890 --> 00:05:52,660
forループ無しのもある、という事。

179
00:05:52,810 --> 00:05:53,950
だが私は心から

180
00:05:54,360 --> 00:05:55,340
最初にバックプロパケーションを実装する時に

181
00:05:55,660 --> 00:05:58,420
その複雑なバージョンにチャレンジするのは、おすすめしない。

182
00:05:59,850 --> 00:06:00,920
さて、では具体的には、mトレーニング手本に渡る

183
00:06:01,010 --> 00:06:02,200
for文があり、

184
00:06:03,240 --> 00:06:04,630
そしてfor文の中では、

185
00:06:04,770 --> 00:06:06,300
フォワードプロパゲートとバックワードプロパゲートを

186
00:06:06,580 --> 00:06:08,090
この一つの手本だけに対して行う。

187
00:06:09,310 --> 00:06:10,320
つまり、

188
00:06:10,560 --> 00:06:12,470
x(i)を持ってきて、

189
00:06:12,690 --> 00:06:14,010
それを入力レイヤーに食わせて、

190
00:06:14,770 --> 00:06:16,370
フォワードプロパゲートとバックプロパゲートを実行する。

191
00:06:17,370 --> 00:06:18,360
するとニューラルネットワーク内の

192
00:06:18,430 --> 00:06:19,840
全てのレイヤーの全てのユニットの

193
00:06:19,930 --> 00:06:22,090
アクティベーション全部と

194
00:06:22,300 --> 00:06:23,440
デルタの項が全部

195
00:06:23,770 --> 00:06:24,720
得られるので、

196
00:06:24,950 --> 00:06:27,170
次に、まだforループの

197
00:06:27,610 --> 00:06:28,760
内部だが、、、よし、

198
00:06:29,180 --> 00:06:30,450
中括弧を書こう、

199
00:06:30,940 --> 00:06:31,950
forループのスコープを示す為に。

200
00:06:32,030 --> 00:06:32,930
これはもちろんOctaveのコードなんだが、

201
00:06:34,160 --> 00:06:35,480
だがC++とかJavaのコードのシーケンスみたいな意味で

202
00:06:36,190 --> 00:06:38,350
これはfor文をここ全体に拡張する。

203
00:06:39,060 --> 00:06:40,060
我らはこれらのデルタの項を計算する訳だが

204
00:06:40,480 --> 00:06:43,690
その式は以前に既に得ている。

205
00:06:45,540 --> 00:06:47,370
、、、足すことの デルタ l+1

206
00:06:48,630 --> 00:06:51,150
掛ける aのlの転置に、さらにコードが続く。

207
00:06:51,490 --> 00:06:53,540
そして最後に、これらのデルタ項が

208
00:06:54,180 --> 00:06:55,630
アキュームレーションの項が

209
00:06:55,970 --> 00:06:57,550
計算されたあとの外側では

210
00:06:57,870 --> 00:06:59,050
さらに幾つかコードがあったあとで、

211
00:06:59,170 --> 00:07:00,430
これらの偏微分の項の

212
00:07:00,720 --> 00:07:03,240
計算が出来るようになる。

213
00:07:03,860 --> 00:07:05,450
そしてこれらの偏微分の項は

214
00:07:05,970 --> 00:07:07,020
正規化項のラムダも同様に

215
00:07:07,210 --> 00:07:10,270
考慮に入れる必要がある。

216
00:07:11,050 --> 00:07:13,240
それらの式は前半のビデオで与えてあった。

217
00:07:14,830 --> 00:07:15,720
だからそれを終えているなら、

218
00:07:16,680 --> 00:07:18,080
これらの偏微分項を計算するコードを

219
00:07:18,180 --> 00:07:20,050
既に手中に収めているはずだ。

220
00:07:21,190 --> 00:07:23,030
次はステップ5。

221
00:07:23,240 --> 00:07:24,420
そこではグラディアントチェッキングを用いて

222
00:07:24,730 --> 00:07:26,700
計算した偏微分の項と

223
00:07:27,120 --> 00:07:28,530
比較する。

224
00:07:29,420 --> 00:07:30,980
つまりバックプロパケーションを用いて

225
00:07:31,270 --> 00:07:33,990
計算したバージョンと、

226
00:07:34,430 --> 00:07:36,470
微分を数値計算的に推計した偏微分の数値推計を

227
00:07:37,710 --> 00:07:39,850
比較する。

228
00:07:40,350 --> 00:07:41,810
つまりグラディアントチェッキングを

229
00:07:41,970 --> 00:07:44,340
これらの値が両方とも似通っている事を確認する為に使う。

230
00:07:45,830 --> 00:07:47,410
グラディアントチェッキングで

231
00:07:47,910 --> 00:07:49,280
我らのバックプロパゲーションの実装が正しいと

232
00:07:49,590 --> 00:07:51,470
確認した後には、

233
00:07:51,610 --> 00:07:52,850
次のとても大事な事だが、グラディアントチェッキングを

234
00:07:53,530 --> 00:07:54,710
disableする。何故なら

235
00:07:55,080 --> 00:07:57,150
グラディアントチェッキングのコードは計算量的にとても遅いからだ。

236
00:07:59,020 --> 00:08:00,880
そして最後に、

237
00:08:01,120 --> 00:08:03,280
最急降下法なり

238
00:08:03,510 --> 00:08:04,940
アドバンスドな最適化アルゴリズムの

239
00:08:04,960 --> 00:08:07,520
L-BFGSとかConjugateグラディアントとか

240
00:08:07,740 --> 00:08:10,020
fminuncで使われているアルゴリズムでもそれ以外のアルゴリズムでも

241
00:08:10,250 --> 00:08:13,120
とにかく何かしらを用いて、、、

242
00:08:13,940 --> 00:08:15,500
これらをバックプロパゲーションと共に

243
00:08:15,730 --> 00:08:17,380
用いる、

244
00:08:17,620 --> 00:08:18,670
つまりバックプロパゲーションが

245
00:08:18,770 --> 00:08:20,640
これらの偏微分を我らに提供してくれる。

246
00:08:21,730 --> 00:08:22,680
そしてコスト関数を

247
00:08:22,860 --> 00:08:24,020
計算する方法を知っていて、

248
00:08:24,100 --> 00:08:25,550
バックプロパゲーションを用いて偏微分を計算する方法も

249
00:08:25,830 --> 00:08:27,410
知っているので、

250
00:08:27,480 --> 00:08:28,830
これらの最適化手法のどれかを用いて

251
00:08:29,580 --> 00:08:30,850
Jのシータをパラメータシータの関数として

252
00:08:31,130 --> 00:08:33,500
最小化する事を、試みる事が出来る。

253
00:08:34,330 --> 00:08:35,410
ところで、

254
00:08:35,660 --> 00:08:37,330
ニューラルネットワークの場合、

255
00:08:38,300 --> 00:08:39,630
このコスト関数Jのシータは非凸関数、

256
00:08:40,530 --> 00:08:42,490
言い換えると凸関数では無いので、

257
00:08:43,260 --> 00:08:45,600
理論上はローカル最小に

258
00:08:46,250 --> 00:08:47,480
陥りうるし、

259
00:08:47,650 --> 00:08:49,580
実際、最急降下法や

260
00:08:49,840 --> 00:08:51,950
アドバンスドな最適化手法は、

261
00:08:52,400 --> 00:08:53,660
理論上はローカル最適に捕まってしまう事がありうる。

262
00:08:55,190 --> 00:08:56,300
だが実際には、

263
00:08:56,480 --> 00:08:57,680
通常はこれは

264
00:08:57,870 --> 00:08:59,230
そんなに大きな問題にならない。

265
00:08:59,560 --> 00:09:00,800
そしてこれらのアルゴリズムが

266
00:09:01,210 --> 00:09:02,320
グローバル最小を探す、とは保証出来なくても、

267
00:09:02,510 --> 00:09:04,260
普通は最急降下法のようなアルゴリズムは

268
00:09:04,390 --> 00:09:05,870
このコスト関数Jのシータを

269
00:09:05,930 --> 00:09:07,700
小さくする、という仕事を

270
00:09:07,850 --> 00:09:09,230
とてもうまくこなす。

271
00:09:09,280 --> 00:09:10,350
そしてとても良い

272
00:09:10,420 --> 00:09:11,820
局所最小が得られる。

273
00:09:12,060 --> 00:09:13,690
それはグローバル最適では無いかもしれないが。

274
00:09:14,500 --> 00:09:16,950
最後に、ニューラルネットワークに対する

275
00:09:17,230 --> 00:09:19,500
最急降下法は、まだこれでもちょっと魔法っぽく見えるかもしれない。

276
00:09:20,170 --> 00:09:21,680
だからもう一つ

277
00:09:21,890 --> 00:09:22,990
最急降下法がニューラルネットワークに

278
00:09:23,170 --> 00:09:25,660
何をしているかを示す図をお見せしたい。

279
00:09:27,020 --> 00:09:28,460
これは最急降下法を以前説明した時に用いた図に

280
00:09:28,590 --> 00:09:31,190
実は似た物だ。

281
00:09:31,730 --> 00:09:32,750
あるコスト関数があり、

282
00:09:33,090 --> 00:09:34,480
ニューラルネットワークの

283
00:09:34,710 --> 00:09:36,590
パラメータがある。

284
00:09:36,810 --> 00:09:39,190
ここではパラメータの値を二つ書いた。

285
00:09:40,080 --> 00:09:41,250
実際には、もちろん、ニューラルネットワークの場合、

286
00:09:41,520 --> 00:09:43,570
もっとたくさんのパラメータを持ちうる。

287
00:09:44,190 --> 00:09:46,980
シータ1もシータ2も行列だ。

288
00:09:47,030 --> 00:09:48,130
だからパラメータの次元はとても高次になりうる。

289
00:09:48,580 --> 00:09:49,870
だがプロットして

290
00:09:49,960 --> 00:09:51,620
表示してみる側の

291
00:09:51,790 --> 00:09:52,970
制約の為に、

292
00:09:53,410 --> 00:09:55,840
ニューラルネットワークに二つのパラメータしか無いかのように話をする。

293
00:09:56,270 --> 00:09:56,890
実際にはもっとたくさんあるのは明らかだけど。

294
00:09:59,280 --> 00:10:00,700
いま、このコスト関数Jのシータは

295
00:10:00,800 --> 00:10:02,470
そのニューラルネットワークが

296
00:10:02,880 --> 00:10:04,730
トレーニングデータにどれだけフィットしているかを測っている物だ。

297
00:10:06,000 --> 00:10:06,920
つまりこんな点を

298
00:10:07,120 --> 00:10:08,590
取るなら、この下側の、

299
00:10:10,270 --> 00:10:11,180
そこはJのシータがとても低い

300
00:10:11,460 --> 00:10:12,580
点に対応し、

301
00:10:12,870 --> 00:10:16,170
つまりこれは以下のようなシチュエーションのパラメータに対応する、

302
00:10:17,020 --> 00:10:17,840
つまりだいたいのトレーニング手本に対して

303
00:10:18,350 --> 00:10:19,920
仮説の出力が

304
00:10:20,140 --> 00:10:22,450
y(i)に極めて近くなるような

305
00:10:24,120 --> 00:10:26,270
そういうシチュエーションの

306
00:10:26,410 --> 00:10:27,420
パラメータに対応する。

307
00:10:27,650 --> 00:10:28,720
この条件が真の時が、

308
00:10:28,840 --> 00:10:31,560
コスト関数が極めて低い、という状態に対応する。

309
00:10:32,690 --> 00:10:33,770
他方、対照的に、

310
00:10:33,820 --> 00:10:35,140
こっちのような値を取る時には、

311
00:10:35,510 --> 00:10:37,260
この点はトレーニング手本の大多数が、

312
00:10:38,080 --> 00:10:39,260
ニューラルネットワークの出力が

313
00:10:39,890 --> 00:10:40,780
実際の値、

314
00:10:41,040 --> 00:10:42,860
実際の観測された値のy(i)から、

315
00:10:43,110 --> 00:10:44,340
大きく離れた値に

316
00:10:44,540 --> 00:10:45,850
対応した点となる。

317
00:10:46,610 --> 00:10:47,480
つまりこの右側の点のような点は

318
00:10:47,590 --> 00:10:50,100
トレーニングセットに対する

319
00:10:50,450 --> 00:10:51,450
仮説の出力、

320
00:10:51,740 --> 00:10:53,330
ニューラルネットワークの出力が、

321
00:10:53,770 --> 00:10:54,810
y(i)から遠く離れた値となる点に対応する。

322
00:10:55,020 --> 00:10:56,260
だからそれはトレーニングセットには

323
00:10:56,470 --> 00:10:57,970
あまり良くフィットしそうに無い。

324
00:10:58,170 --> 00:10:59,640
他方このような点は、

325
00:10:59,970 --> 00:11:01,300
コスト関数の値が低いというのは

326
00:11:02,130 --> 00:11:03,380
Jのシータが低い所に対応しているので、

327
00:11:04,130 --> 00:11:05,270
つまりはニューラルネットワークが

328
00:11:05,950 --> 00:11:07,590
ちょうど良く私のトレーニングセットに

329
00:11:07,850 --> 00:11:09,290
フィットする場所に対応する。

330
00:11:09,510 --> 00:11:11,340
何故ならこれこそがJのシータが小さくなる時に

331
00:11:11,550 --> 00:11:14,070
真である必要のある事だから。

332
00:11:15,480 --> 00:11:16,810
だから最急降下法のやる事は、

333
00:11:16,870 --> 00:11:18,330
なんらかの

334
00:11:18,730 --> 00:11:20,300
ランダムな初期点から始めて、

335
00:11:20,430 --> 00:11:22,990
たとえばこことか、そこから繰り返し丘を降りていく。

336
00:11:24,040 --> 00:11:25,400
つまりバックプロパゲーションが行うのは

337
00:11:25,570 --> 00:11:27,220
グラディアントの方向を

338
00:11:27,940 --> 00:11:29,370
計算して、

339
00:11:29,520 --> 00:11:30,740
そして最急降下法がやる事は、

340
00:11:31,040 --> 00:11:32,060
一歩一歩ちょっとずつ丘を降りていき、

341
00:11:32,880 --> 00:11:34,220
期待するのは、そしてこの場合は実際にそうだが、

342
00:11:34,610 --> 00:11:36,410
かなり良いローカル最適に至るまで進む訳だ。

343
00:11:37,880 --> 00:11:39,250
つまり、バックプロパゲーションを

344
00:11:39,410 --> 00:11:40,840
実装して、

345
00:11:41,200 --> 00:11:42,420
最急降下法やアドバンスドな最適化法の一つを

346
00:11:42,840 --> 00:11:44,750
使う時には、

347
00:11:45,330 --> 00:11:47,290
この図はアルゴリズムが何をするかの、いくらかの説明となっている。

348
00:11:47,450 --> 00:11:48,820
それはニューラルネットワークの

349
00:11:49,260 --> 00:11:50,920
出力する値が、

350
00:11:51,260 --> 00:11:52,180
トレーニングセットにおける観測値のy(i)に

351
00:11:52,450 --> 00:11:54,300
なるべく近いような

352
00:11:54,410 --> 00:11:55,520
ニューラルネットのパラメータを

353
00:11:55,660 --> 00:11:58,800
探す事を試みる。

354
00:11:58,910 --> 00:12:00,250
これで、あなたも

355
00:12:00,400 --> 00:12:01,610
ニューラルネットワークの

356
00:12:01,920 --> 00:12:03,930
それぞれのピースが、どう組み合わさるのか

357
00:12:04,120 --> 00:12:05,760
分かったんじゃないかな。

358
00:12:07,120 --> 00:12:09,010
でも、もしこのビデオが終わった後でも、

359
00:12:09,120 --> 00:12:10,130
これらの様々なピースの中に

360
00:12:10,360 --> 00:12:11,420
いまいちどうもしっくり来ない物があったり、

361
00:12:12,070 --> 00:12:13,450
あるいはそれらのうちの幾つかが

362
00:12:13,690 --> 00:12:14,670
完全にはっきりと分かったという訳で無くても、

363
00:12:14,860 --> 00:12:17,760
あるいはこれらのピースがどうくっつくのか全部は分からなくても、実際は問題無い。

364
00:12:18,790 --> 00:12:21,780
ニューラルネットワーク学習とバックプロパゲーションは複雑なアルゴリズムだ。

365
00:12:23,000 --> 00:12:23,960
そして私ですら、

366
00:12:24,290 --> 00:12:25,340
バックプロパゲーションの背後にある数学を

367
00:12:25,860 --> 00:12:26,710
長年見てきて、さらに自分で思うには

368
00:12:27,030 --> 00:12:28,470
バックプロパゲーションを

369
00:12:28,680 --> 00:12:30,210
とても成功裡に何年も使い続けてきた私ですら、

370
00:12:30,380 --> 00:12:31,510
こんにちでも、まだ時々

371
00:12:31,570 --> 00:12:32,670
バックプロパゲーションが正確に何やってるのかを

372
00:12:33,400 --> 00:12:35,610
いつもしっかりつかんでる、という訳では無いと感じる事がある。

373
00:12:36,200 --> 00:12:37,850
そしてJのシータを最小化する最適化が

374
00:12:38,520 --> 00:12:41,480
どう進んでいくかもしっかりとつかんでないと感じる事がある。

375
00:12:41,920 --> 00:12:42,830
これはより難しいアルゴリズムで、

376
00:12:43,450 --> 00:12:44,680
これはしっかり分かるのが、、、

377
00:12:44,830 --> 00:12:46,590
これが正確に何をやっているのかを

378
00:12:46,690 --> 00:12:47,690
しっかりと把握するのが、

379
00:12:48,240 --> 00:12:49,360
線形回帰とかロジスティック回帰に比べて難しい。

380
00:12:51,390 --> 00:12:53,180
線形回帰などの方が数学的にも概念的にも

381
00:12:53,510 --> 00:12:55,090
よりシンプルで、よりクリーンなアルゴリズムだ。

382
00:12:56,200 --> 00:12:57,030
だがもしあなたが似たような物だ、と感じたとしても

383
00:12:57,070 --> 00:12:58,560
それはそれで完全にOKだ。

384
00:12:58,970 --> 00:13:01,010
だがバックプロパゲーションを

385
00:13:01,170 --> 00:13:02,790
実装してみれば、

386
00:13:03,160 --> 00:13:04,260
きっとこれがもっとも強力な

387
00:13:04,460 --> 00:13:05,410
学習アルゴリズムの一つだと

388
00:13:05,790 --> 00:13:08,030
分かるだろう。

389
00:13:08,130 --> 00:13:09,510
そしてもしこのアルゴリズム、バックプロパゲーションを

390
00:13:10,250 --> 00:13:11,230
これらの最適化手法の一つを実装すれば、

391
00:13:11,340 --> 00:13:13,260
バックプロパゲーションが

392
00:13:13,610 --> 00:13:14,940
とても複雑で強力で

393
00:13:15,390 --> 00:13:17,330
非線型な関数で

394
00:13:17,830 --> 00:13:19,370
あなたのデータにフィッティング出来て

395
00:13:20,080 --> 00:13:21,060
そしてこれがこんにちある中でも

396
00:13:21,190 --> 00:13:22,790
最も効率的な学習アルゴリズムの一つである事が分かるだろう。