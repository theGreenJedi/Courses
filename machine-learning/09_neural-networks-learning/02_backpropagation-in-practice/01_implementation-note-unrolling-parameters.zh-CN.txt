在上一段视频中 我们谈到了怎样使用反向传播算法 计算代价函数的导数 在这段视频中 我想快速地向你介绍一个细节的实现过程 怎样把你的参数 从矩阵展开成向量 以便我们在高级最优化步骤中的使用需要 具体来讲 你执行了代价函数costFunction 输入参数是theta 函数返回值是代价函数以及导数值 然后你可以将返回值 传递给高级最优化算法fminunc 顺便提醒 fminunc并不是唯一的算法 你也可以使用别的优化算法 但它们的功能 都是取出这些输入值 @costFunction 以及theta值的一些初始值 并且这些程序 都假设theta 和这些theta初始值 都是参数向量 也许是n或者n+1阶 但它们都是向量 同时假设这个代价函数 第二个返回值 也就是gradient值 也是n阶或者n+1阶 所以它也是一个向量 这部分在我们使用逻辑回归的时候 运行顺利 但现在 对于神经网络 我们的参数将不再是 向量 而是矩阵了 因此对于一个完整的神经网络 我们的参数矩阵为θ(1) θ(2) θ(3) 在Octave中我们可以设为 Theta1 Theta2 Theta3 类似的 这些梯度项gradient 也是需要得到的返回值 那么在之前的视频中 我们演示了如何计算 这些梯度矩阵 它们是D(1) D(2) D(3) 在Octave中 我们用矩阵D1 D2 D3来表示 在这节视频中 我想很快地向你介绍 怎样取出这些矩阵 并且将它们展开成向量 以便它们最终 成为恰当的格式 能够传入这里的Theta 并且得到正确的梯度返回值gradient 具体来说 假设我们有这样一个神经网络 其输入层有10个输入单元 隐藏层有10个单元 最后的输出层 只有一个输出单元 因此s1等于第一层的单元数 s2等于第二层的单元数 s3等于第三层的 单元个数 在这种情况下 矩阵θ的维度 和矩阵D的维度 将由这些表达式确定 比如说 θ(1)是一个10x11的矩阵 以此类推 因此 在Octave中 如果你想将这些矩阵 转化为向量 那么你要做的 是取出你的Theta1 Theta2 Theta3 然后使用这段代码 这段代码将取出 三个θ矩阵中的所有元素 也就是说取出Theta1 的所有元素 Theta2的所有元素 Theta3的所有元素 然后把它们全部展开 成为一个很长的向量 也就是thetaVec 同样的 第二段代码 将取出D矩阵的所有元素 然后展开 成为一个长向量 被叫做DVec 最后 如果你想从向量表达 返回到矩阵表达式的话 你要做的是 比如想再得到Theta1 那么取thetaVec 抽出前110个元素 因此 Theta1就有110个元素 因为它应该是一个10x11的矩阵 所以 抽出前110个元素 然后你就可以 reshape矩阵变维命令来重新得到Theta1 同样类似的 要重新得到Theta2矩阵 你需要抽出下一组110个元素并且重新组合 然后对于Theta3 你需要抽出最后11个元素 然后执行reshape命令 重新得到Theta3 以下是这一过程的Octave演示 对于这一个例子 让我们假设Theta1 为一个10x11的单位矩阵 因此它每一项都为1 为了更易看清 让我们把Theta2设为 一个10行11列矩阵 每个元素都为2 然后设Theta3 是一个1x11的矩阵 每个元素都为3 因此 这样我们得到三个独立的矩阵 Theta1 Theta2 Theta3 现在我们想把所有这些矩阵变成一个向量 thetaVec = [Theta1(:); Theta2(:); Theta3(:)]; 好的 注意中间有冒号 像这样 现在thetaVec矩阵 就变成了一个很长的向量 含有231个元素 如果把它打出来 我们就能看出它是一个很长的向量 包括第一个矩阵的所有元素 第二个矩阵的所有元素 以及第三个矩阵的所有元素 如果我想重新得到 我最初的三个矩阵 我可以对thetaVec使用reshape命令 抽出前110个元素 将它们重组为一个10x11的矩阵 这样我又再次得到了Theta1矩阵 然后我再取出 接下来的110个元素 也就是111到220号元素 我就又重组还原了第二个矩阵 最后 再抽出221到最后一个元素 也就是第231个元素 然后重组为1x11的矩阵 我就又得到了Theta3矩阵 为了使这个过程更形象 下面我们来看怎样将这一方法 应用于我们的学习算法 假设说你有一些 初始参数值 θ(1) θ(2) θ(3) 我们要做的是 取出这些参数并且将它们 展开为一个长向量 我们称之为initialTheta 然后作为theta参数的初始设置 传入函数fminunc 我们要做的另一件事是执行代价函数costFunction 实现算法如下 代价函数costFunction 将传入参数thetaVec 这也是包含 我所有参数的向量 是将所有的参数展开成一个向量的形式 因此我要做的第一件事是 我要使用 thetaVec和重组函数reshape 因此我要抽出thetaVec中的元素 然后重组 以得到我的初始参数矩阵 θ(1) θ(2) θ(3) 所以这些是我需要得到的矩阵 因此 这样我就有了 一个使用这些矩阵的 更方便的形式 这样我就能执行前向传播 和反向传播 来计算出导数 以求得代价函数的J(θ) 最后 我可以取出这些导数值 然后展开它们 让它们保持和我展开的θ值 同样的顺序 我要展开D1 D2 D3 来得到gradientVec 这个值可由我的代价函数返回 它可以以一个向量的形式返回这些导数值 现在 我想 对怎样进行参数的矩阵表达式 和向量表达式 之间的转换 有了一个更清晰的认识 使用矩阵表达式 的好处是 当你的参数以矩阵的形式储存时 你在进行正向传播 和反向传播时 你会觉得更加方便 当你将参数储存为矩阵时 一大好处是 充分利用了向量化的实现过程 相反地 向量表达式的优点是 如果你有像thetaVec或者DVec这样的矩阵 当你使用一些高级的优化算法时 这些算法通常要求 你所有的参数 都要展开成一个长向量的形式 希望通过我们刚才介绍的内容 你能够根据需要 更加轻松地 在两种形式之间转换【果壳教育无边界字幕组】翻译：所罗门捷列夫 校对:  Roy薛