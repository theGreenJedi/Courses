1
00:00:00,240 --> 00:00:01,560
我们已经用了

2
00:00:01,700 --> 00:00:02,690
几节视频的内容

3
00:00:03,120 --> 00:00:04,480
来介绍神经网络算法

4
00:00:05,620 --> 00:00:06,640
在这段视频中

5
00:00:06,800 --> 00:00:08,090
我想结合我们所讲的

6
00:00:08,350 --> 00:00:10,040
所有这些内容

7
00:00:10,370 --> 00:00:12,120
来做一个总体的回顾

8
00:00:12,360 --> 00:00:13,410
看看这些零散的内容

9
00:00:13,650 --> 00:00:15,290
相互之间有怎样的联系

10
00:00:15,530 --> 00:00:16,990
以及神经网络学习算法的

11
00:00:17,260 --> 00:00:18,830
总体实现过程

12
00:00:21,870 --> 00:00:23,210
当我们在训练一个神经网络时

13
00:00:23,280 --> 00:00:24,290
我们要做的第一件事

14
00:00:24,400 --> 00:00:25,920
就是搭建网络的大体框架

15
00:00:26,680 --> 00:00:27,950
这里我说的框架 意思是

16
00:00:28,200 --> 00:00:30,510
神经元之间的连接模式

17
00:00:31,080 --> 00:00:31,840
我们可能会从以下几种结构中选择

18
00:00:32,700 --> 00:00:33,770
第一种神经网络的结构是

19
00:00:34,230 --> 00:00:35,440
包含三个输入单元

20
00:00:35,960 --> 00:00:37,400
五个隐藏单元

21
00:00:37,500 --> 00:00:39,560
和四个输出单元

22
00:00:39,800 --> 00:00:41,460
第二种结构是 三个输入单元作为输入层

23
00:00:41,700 --> 00:00:43,430
两组五个隐藏单元作为隐藏层 四个输出单元的输出层

24
00:00:43,910 --> 00:00:45,220
然后第三种是3 5 5 5

25
00:00:45,550 --> 00:00:47,060
其中每个隐藏层包含五个单元

26
00:00:47,320 --> 00:00:48,870
然后是四个输出单元

27
00:00:49,120 --> 00:00:50,250
这些就是可能选择的结构

28
00:00:50,430 --> 00:00:52,000
每一层可以选择

29
00:00:52,270 --> 00:00:53,410
多少个隐藏单元

30
00:00:53,810 --> 00:00:55,560
以及可以选择多少个隐藏层

31
00:00:55,780 --> 00:00:57,580
这些都是你构建时的选择

32
00:00:57,910 --> 00:00:58,680
那么我们该如何做出选择呢？

33
00:00:59,710 --> 00:01:01,270
首先 我们知道

34
00:01:01,530 --> 00:01:03,840
我们已经定义了输入单元的数量

35
00:01:04,680 --> 00:01:05,960
一旦你确定了特征集x

36
00:01:06,580 --> 00:01:07,870
对应的输入单元数目

37
00:01:08,080 --> 00:01:09,420
也就确定了

38
00:01:10,140 --> 00:01:12,180
也就是等于特征x{i}的维度

39
00:01:12,330 --> 00:01:14,470
输入单元数目将会由此确定

40
00:01:14,760 --> 00:01:15,970
如果你正在进行

41
00:01:16,210 --> 00:01:17,370
多类别分类

42
00:01:17,520 --> 00:01:18,320
那么输出层的单元数目

43
00:01:18,420 --> 00:01:19,720
将会由你分类问题中

44
00:01:20,060 --> 00:01:22,860
所要区分的类别个数确定

45
00:01:23,260 --> 00:01:24,890
值得提醒的是

46
00:01:25,160 --> 00:01:27,290
如果你的多元分类问题

47
00:01:27,570 --> 00:01:28,970
y的取值范围

48
00:01:30,040 --> 00:01:31,350
是在1到10之间

49
00:01:31,470 --> 00:01:33,560
那么你就有10个可能的分类

50
00:01:34,690 --> 00:01:37,200
别忘了把你的y

51
00:01:37,820 --> 00:01:39,340
重新写成向量的形式

52
00:01:40,130 --> 00:01:41,560
所以现在我们的y不是一个数了

53
00:01:41,730 --> 00:01:42,840
我们重新把y写成

54
00:01:43,150 --> 00:01:44,600
这种形式的向量

55
00:01:44,670 --> 00:01:47,280
第二个分类我们可以写成这样的向量

56
00:01:48,130 --> 00:01:49,080
所以 比如说

57
00:01:49,210 --> 00:01:51,000
如果要表达

58
00:01:51,140 --> 00:01:53,910
第五个分类 也就是说y等于5

59
00:01:54,120 --> 00:01:55,130
那么在你的神经网络中

60
00:01:55,380 --> 00:01:56,840
就不能直接用

61
00:01:57,250 --> 00:01:59,520
数值5来表达

62
00:02:00,030 --> 00:02:00,950
因为这里的输出层

63
00:02:01,280 --> 00:02:02,650
有十个输出单元

64
00:02:02,740 --> 00:02:03,920
你应该用一个向量

65
00:02:04,070 --> 00:02:05,710
来表示

66
00:02:07,470 --> 00:02:08,430
这个向量的第五个位置值是1

67
00:02:08,770 --> 00:02:11,050
其它的都是0

68
00:02:11,420 --> 00:02:12,470
所以对于输入单元

69
00:02:12,890 --> 00:02:14,330
和输出单元数目的选择

70
00:02:14,970 --> 00:02:16,600
还是比较容易理解的

71
00:02:18,000 --> 00:02:18,950
而对于隐藏单元的个数

72
00:02:19,410 --> 00:02:21,040
单元的个数

73
00:02:21,140 --> 00:02:23,110
以及隐藏层的数目

74
00:02:23,210 --> 00:02:24,350
我们有一个默认的规则

75
00:02:24,540 --> 00:02:26,010
那就是只使用单个隐藏层

76
00:02:26,660 --> 00:02:28,040
所以最左边所示的

77
00:02:28,880 --> 00:02:30,400
这种只有一个隐藏层的神经网络

78
00:02:30,580 --> 00:02:33,270
一般来说是最普遍的

79
00:02:34,490 --> 00:02:35,870
或者如果你使用

80
00:02:36,140 --> 00:02:38,410
不止一个隐藏层的话

81
00:02:38,670 --> 00:02:39,600
同样我们也有一个默认规则

82
00:02:39,760 --> 00:02:40,950
那就是每一个隐藏层

83
00:02:41,130 --> 00:02:42,560
通常都应有相同的单元数

84
00:02:42,810 --> 00:02:44,600
所以对于这个结构

85
00:02:45,020 --> 00:02:46,370
我们有两个隐藏层

86
00:02:46,610 --> 00:02:47,650
每个隐藏层都有相同的单元数

87
00:02:47,860 --> 00:02:49,500
都是5个隐藏单元

88
00:02:49,790 --> 00:02:50,740
这里也是一样

89
00:02:51,600 --> 00:02:53,020
我们有三个隐藏层

90
00:02:53,170 --> 00:02:54,790
每个隐藏层有相同的单元数

91
00:02:54,980 --> 00:02:56,400
都是5个隐藏单元

92
00:02:57,440 --> 00:02:59,440
但实际上通常来说

93
00:02:59,740 --> 00:03:02,850
左边这个结构是较为合理的默认结构

94
00:03:04,020 --> 00:03:04,780
而对于隐藏单元的个数

95
00:03:05,120 --> 00:03:07,040
通常情况下

96
00:03:07,120 --> 00:03:08,100
隐藏单元越多越好

97
00:03:08,560 --> 00:03:09,640
不过 我们需要注意的是

98
00:03:09,900 --> 00:03:11,110
如果有大量隐藏单元

99
00:03:11,330 --> 00:03:13,150
计算量一般会比较大

100
00:03:13,300 --> 00:03:15,850
当然 一般来说隐藏单元还是越多越好

101
00:03:17,250 --> 00:03:18,560
并且一般来说 每个隐藏层

102
00:03:18,720 --> 00:03:20,820
所包含的单元数量

103
00:03:21,080 --> 00:03:22,130
还应该和输入x

104
00:03:22,490 --> 00:03:23,670
的维度相匹配

105
00:03:23,810 --> 00:03:24,950
也要和特征的数目匹配

106
00:03:25,140 --> 00:03:26,880
可能隐藏单元的数目

107
00:03:27,180 --> 00:03:29,590
和输入特征的数量相同

108
00:03:29,770 --> 00:03:32,430
或者是它的二倍 或者三倍 四倍

109
00:03:32,680 --> 00:03:34,770
因此 隐藏单元的数目需要和其他参数相匹配

110
00:03:35,140 --> 00:03:36,350
一般来说

111
00:03:36,410 --> 00:03:37,380
隐藏单元的数目取为稍大于

112
00:03:37,430 --> 00:03:38,750
输入特征数目

113
00:03:39,280 --> 00:03:41,320
都是可以接受的

114
00:03:42,150 --> 00:03:43,490
希望这些能够给你

115
00:03:43,810 --> 00:03:45,140
在选择神经网络结构时

116
00:03:45,650 --> 00:03:47,770
提供一些有用的建议和选择的参考

117
00:03:48,200 --> 00:03:49,460
如果你遵循了这些建议

118
00:03:49,540 --> 00:03:50,580
你一般会得到比较好的模型结构

119
00:03:50,930 --> 00:03:52,180
但是

120
00:03:52,360 --> 00:03:53,770
在以后的一系列视频中

121
00:03:54,050 --> 00:03:55,270
特别是在我谈到

122
00:03:55,580 --> 00:03:56,900
学习算法的应用时

123
00:03:57,410 --> 00:03:58,770
我还会更详细地介绍

124
00:03:58,840 --> 00:04:01,880
如何选择神经网络的结构

125
00:04:02,540 --> 00:04:03,920
后面的视频中

126
00:04:03,970 --> 00:04:04,960
我还会着重介绍

127
00:04:04,960 --> 00:04:06,180
怎样正确地选择隐藏层的个数

128
00:04:06,710 --> 00:04:08,780
以及隐藏单元的数目 等等

129
00:04:10,620 --> 00:04:12,310
下面我们就来具体介绍

130
00:04:12,420 --> 00:04:13,740
如何实现神经网络的

131
00:04:13,860 --> 00:04:15,360
训练过程

132
00:04:15,510 --> 00:04:16,820
这里一共有六个步骤

133
00:04:17,080 --> 00:04:18,030
这页幻灯片中罗列了前四步

134
00:04:18,160 --> 00:04:19,100
剩下的两步

135
00:04:19,380 --> 00:04:21,480
放在下一张幻灯片中

136
00:04:21,620 --> 00:04:22,220
首先 第一步是构建一个

137
00:04:22,430 --> 00:04:23,510
神经网络

138
00:04:24,080 --> 00:04:25,570
然后随机初始化权值

139
00:04:25,790 --> 00:04:27,000
通常我们把权值

140
00:04:27,080 --> 00:04:29,710
初始化为很小的值 接近于零

141
00:04:31,100 --> 00:04:33,120
然后我们执行前向传播算法

142
00:04:34,080 --> 00:04:35,060
也就是 对于该神经网络的

143
00:04:35,480 --> 00:04:37,150
任意一个输入x(i)

144
00:04:37,490 --> 00:04:38,860
计算出对应的h(x)值

145
00:04:39,070 --> 00:04:40,820
也就是一个输出值y的向量

146
00:04:44,260 --> 00:04:45,910
接下来我们通过代码

147
00:04:46,010 --> 00:04:47,500
计算出代价函数J(θ)

148
00:04:49,770 --> 00:04:51,160
然后我们执行

149
00:04:52,120 --> 00:04:53,330
反向传播算法

150
00:04:54,400 --> 00:04:55,680
来算出这些偏导数 或偏微分项

151
00:04:55,910 --> 00:04:58,000
也就是

152
00:04:58,440 --> 00:04:59,830
J(θ)关于参数θ的偏微分

153
00:05:00,340 --> 00:05:04,240
具体来说

154
00:05:04,960 --> 00:05:05,880
我们要对所有训练集数据

155
00:05:06,250 --> 00:05:08,460
使用一个for循环进行遍历

156
00:05:09,700 --> 00:05:10,650
可能有部分同学之前听说过

157
00:05:10,830 --> 00:05:12,640
一些比较先进的分解方法

158
00:05:12,940 --> 00:05:14,500
可能不需要像这里一样使用

159
00:05:14,670 --> 00:05:15,720
for循环来对所有

160
00:05:16,570 --> 00:05:18,580
m个训练样本进行遍历

161
00:05:18,660 --> 00:05:19,900
但是 这是你第一次进行反向传播算法

162
00:05:20,250 --> 00:05:21,420
所以我建议你最好还是

163
00:05:21,420 --> 00:05:22,980
使用一个for循环来完成程序

164
00:05:23,800 --> 00:05:25,010
对每一个训练样本进行迭代

165
00:05:25,810 --> 00:05:27,760
从x(1) y(1)开始

166
00:05:28,030 --> 00:05:29,510
我们对第一个样本进行

167
00:05:29,640 --> 00:05:30,400
前向传播运算和反向传播运算

168
00:05:30,850 --> 00:05:32,510
然后在第二次循环中

169
00:05:32,710 --> 00:05:33,730
同样地对第二个样本

170
00:05:33,780 --> 00:05:35,360
执行前向传播和反向传播算法

171
00:05:35,980 --> 00:05:38,050
以此类推

172
00:05:38,170 --> 00:05:40,900
直到最后一个样本

173
00:05:41,680 --> 00:05:43,110
因此 在你第一次做反向传播的时候

174
00:05:43,230 --> 00:05:44,250
你还是应该用这样的for循环

175
00:05:45,050 --> 00:05:47,180
来实现这个过程

176
00:05:48,120 --> 00:05:49,160
其实实际上

177
00:05:49,390 --> 00:05:50,520
有复杂的方法可以实现

178
00:05:50,890 --> 00:05:52,660
并不一定要使用for循环

179
00:05:52,810 --> 00:05:53,950
但我非常不推荐

180
00:05:54,360 --> 00:05:55,340
在第一次实现反向传播算法的时候

181
00:05:55,660 --> 00:05:58,420
使用更复杂更高级的方法

182
00:05:59,850 --> 00:06:00,920
所以具体来讲 我们对所有的

183
00:06:01,010 --> 00:06:02,200
m个训练样本上使用了for循环遍历

184
00:06:03,240 --> 00:06:04,630
在这个for循环里

185
00:06:04,770 --> 00:06:06,300
我们对每个样本执行

186
00:06:06,580 --> 00:06:08,090
前向和反向算法

187
00:06:09,310 --> 00:06:10,320
具体来说就是

188
00:06:10,560 --> 00:06:12,470
我们把x(i)

189
00:06:12,690 --> 00:06:14,010
传到输入层

190
00:06:14,770 --> 00:06:16,370
然后执行前向传播和反向传播

191
00:06:17,370 --> 00:06:18,360
这样我们就能得到

192
00:06:18,430 --> 00:06:19,840
该神经网络中

193
00:06:19,930 --> 00:06:22,090
每一层中每一个单元对应的

194
00:06:22,300 --> 00:06:23,440
所有这些激励值a(l)

195
00:06:23,770 --> 00:06:24,720
和delta项

196
00:06:24,950 --> 00:06:27,170
接下来

197
00:06:27,610 --> 00:06:28,760
还是在for循环中

198
00:06:29,180 --> 00:06:30,450
让我画一个大括号

199
00:06:30,940 --> 00:06:31,950
来标明这个

200
00:06:32,030 --> 00:06:32,930
for循环的范围

201
00:06:34,160 --> 00:06:35,480
当然这些是octave的代码

202
00:06:36,190 --> 00:06:38,350
括号里是for循环的循环体

203
00:06:39,060 --> 00:06:40,060
我们要计算出这些delta值

204
00:06:40,480 --> 00:06:43,690
也就是用我们之前给出的公式

205
00:06:45,540 --> 00:06:47,370
加上 delta(l+1)

206
00:06:48,630 --> 00:06:51,150
a(l)的转置矩阵

207
00:06:51,490 --> 00:06:53,540
最后 外面的部分

208
00:06:54,180 --> 00:06:55,630
计算出的这些delta值

209
00:06:55,970 --> 00:06:57,550
这些累加项

210
00:06:57,870 --> 00:06:59,050
我们将用别的程序

211
00:06:59,170 --> 00:07:00,430
来计算出

212
00:07:00,720 --> 00:07:03,240
这些偏导数项

213
00:07:03,860 --> 00:07:05,450
那么这些偏导数项

214
00:07:05,970 --> 00:07:07,020
也应该考虑使用

215
00:07:07,210 --> 00:07:10,270
正则化项lambda值

216
00:07:11,050 --> 00:07:13,240
这些公式在前面的视频中已经给出

217
00:07:14,830 --> 00:07:15,720
那么 搞定所有这些内容

218
00:07:16,680 --> 00:07:18,080
现在你就应该已经得到了

219
00:07:18,180 --> 00:07:20,050
计算这些偏导数项的程序了

220
00:07:21,190 --> 00:07:23,030
下面就是第五步了

221
00:07:23,240 --> 00:07:24,420
我要做的就是使用梯度检查

222
00:07:24,730 --> 00:07:26,700
来比较这些

223
00:07:27,120 --> 00:07:28,530
已经计算得到的偏导数项

224
00:07:29,420 --> 00:07:30,980
把用反向传播算法

225
00:07:31,270 --> 00:07:33,990
得到的偏导数值

226
00:07:34,430 --> 00:07:36,470
与用数值方法得到的

227
00:07:37,710 --> 00:07:39,850
估计值进行比较

228
00:07:40,350 --> 00:07:41,810
因此 通过进行梯度检查来

229
00:07:41,970 --> 00:07:44,340
确保两种方法得到基本接近的两个值

230
00:07:45,830 --> 00:07:47,410
通过梯度检查我们能确保

231
00:07:47,910 --> 00:07:49,280
我们的反向传播算法

232
00:07:49,590 --> 00:07:51,470
得到的结果是正确的

233
00:07:51,610 --> 00:07:52,850
但必须要说明的一点是

234
00:07:53,530 --> 00:07:54,710
我们需要去掉梯度检查的代码

235
00:07:55,080 --> 00:07:57,150
因为梯度检查的计算非常慢

236
00:07:59,020 --> 00:08:00,880
最后 我们就可以

237
00:08:01,120 --> 00:08:03,280
使用一个最优化算法

238
00:08:03,510 --> 00:08:04,940
比如说梯度下降算法

239
00:08:04,960 --> 00:08:07,520
或者说是更加高级的优化方法

240
00:08:07,740 --> 00:08:10,020
比如说BFGS算法 共轭梯度法

241
00:08:10,250 --> 00:08:13,120
或者其他一些已经内置到fminunc函数中的方法

242
00:08:13,940 --> 00:08:15,500
将所有这些优化方法

243
00:08:15,730 --> 00:08:17,380
和反向传播算法相结合

244
00:08:17,620 --> 00:08:18,670
这样我们就能计算出

245
00:08:18,770 --> 00:08:20,640
这些偏导数项的值

246
00:08:21,730 --> 00:08:22,680
到现在 我们已经知道了

247
00:08:22,860 --> 00:08:24,020
如何去计算代价函数

248
00:08:24,100 --> 00:08:25,550
我们知道了如何使用

249
00:08:25,830 --> 00:08:27,410
反向传播算法来计算偏导数

250
00:08:27,480 --> 00:08:28,830
那么 我们就能使用某个最优化方法

251
00:08:29,580 --> 00:08:30,850
来最小化关于theta的函数值

252
00:08:31,130 --> 00:08:33,500
代价函数J(θ)

253
00:08:34,330 --> 00:08:35,410
另外顺便提一下

254
00:08:35,660 --> 00:08:37,330
对于神经网络 代价函数

255
00:08:38,300 --> 00:08:39,630
J(θ)是一个非凸函数

256
00:08:40,530 --> 00:08:42,490
就是说不是凸函数

257
00:08:43,260 --> 00:08:45,600
因此理论上是能够停留在

258
00:08:46,250 --> 00:08:47,480
局部最小值的位置

259
00:08:47,650 --> 00:08:49,580
实际上 梯度下降算法

260
00:08:49,840 --> 00:08:51,950
和其他一些高级优化方法

261
00:08:52,400 --> 00:08:53,660
理论上都能收敛于局部最小值

262
00:08:55,190 --> 00:08:56,300
但一般来讲

263
00:08:56,480 --> 00:08:57,680
这个问题其实

264
00:08:57,870 --> 00:08:59,230
并不是什么要紧的事

265
00:08:59,560 --> 00:09:00,800
尽管我们不能保证

266
00:09:01,210 --> 00:09:02,320
这些优化算法一定会得到

267
00:09:02,510 --> 00:09:04,260
全局最优值 但通常来讲

268
00:09:04,390 --> 00:09:05,870
像梯度下降这类的算法

269
00:09:05,930 --> 00:09:07,700
在最小化代价函数

270
00:09:07,850 --> 00:09:09,230
J(θ)的过程中

271
00:09:09,280 --> 00:09:10,350
还是表现得很不错的

272
00:09:10,420 --> 00:09:11,820
通常能够得到一个很小的局部最小值

273
00:09:12,060 --> 00:09:13,690
尽管这可能不一定是全局最优值

274
00:09:14,500 --> 00:09:16,950
最后 梯度下降算法

275
00:09:17,230 --> 00:09:19,500
似乎对于神经网络来说还是比较神秘

276
00:09:20,170 --> 00:09:21,680
希望下面这幅图

277
00:09:21,890 --> 00:09:22,990
能让你对梯度下降法在神经网络中的应用

278
00:09:23,170 --> 00:09:25,660
产生一个更直观的理解

279
00:09:27,020 --> 00:09:28,460
这实际上有点类似

280
00:09:28,590 --> 00:09:31,190
我们早先时候解释梯度下降时的思路

281
00:09:31,730 --> 00:09:32,750
我们有某个代价函数

282
00:09:33,090 --> 00:09:34,480
并且在我们的神经网络中

283
00:09:34,710 --> 00:09:36,590
有一系列参数值

284
00:09:36,810 --> 00:09:39,190
这里我只写下了两个参数值

285
00:09:40,080 --> 00:09:41,250
当然实际上

286
00:09:41,520 --> 00:09:43,570
在神经网络里 我们可以有很多的参数值

287
00:09:44,190 --> 00:09:46,980
theta1 theta2 等等 所有的这些都是矩阵 是吧

288
00:09:47,030 --> 00:09:48,130
因此我们参数的维度就会很高了

289
00:09:48,580 --> 00:09:49,870
由于绘图所限 我们不能绘出

290
00:09:49,960 --> 00:09:51,620
更高维度情况的图像

291
00:09:51,790 --> 00:09:52,970
所以这里我们假设

292
00:09:53,410 --> 00:09:55,840
这个神经网络中只有两个参数值

293
00:09:56,270 --> 00:09:56,890
实际上应该有更多参数

294
00:09:59,280 --> 00:10:00,700
那么 代价函数J(θ)

295
00:10:00,800 --> 00:10:02,470
度量的就是这个神经网络

296
00:10:02,880 --> 00:10:04,730
对训练数据的拟合情况

297
00:10:06,000 --> 00:10:06,920
所以 如果你取某个参数

298
00:10:07,120 --> 00:10:08,590
比如说这个 下面这点

299
00:10:10,270 --> 00:10:11,180
在这个点上 J(θ)

300
00:10:11,460 --> 00:10:12,580
的值是非常小的

301
00:10:12,870 --> 00:10:16,170
这一点的位置所对应的

302
00:10:17,020 --> 00:10:17,840
参数theta的情况是

303
00:10:18,350 --> 00:10:19,920
对于大部分

304
00:10:20,140 --> 00:10:22,450
的训练集数据

305
00:10:24,120 --> 00:10:26,270
我的假设函数的输出

306
00:10:26,410 --> 00:10:27,420
会非常接近于y(i)

307
00:10:27,650 --> 00:10:28,720
那么如果是这样的话

308
00:10:28,840 --> 00:10:31,560
那么我们的代价函数值就会很小

309
00:10:32,690 --> 00:10:33,770
而反过来 如果我们

310
00:10:33,820 --> 00:10:35,140
取这个值

311
00:10:35,510 --> 00:10:37,260
也就是这个点对应的值

312
00:10:38,080 --> 00:10:39,260
那么对于大部分的训练集样本

313
00:10:39,890 --> 00:10:40,780
该神经网络的输出

314
00:10:41,040 --> 00:10:42,860
应该是远离

315
00:10:43,110 --> 00:10:44,340
y(i)的实际值的

316
00:10:44,540 --> 00:10:45,850
也就是我们在训练集观测到的输出值

317
00:10:46,610 --> 00:10:47,480
因此 像这样的点

318
00:10:47,590 --> 00:10:50,100
右边的这个点

319
00:10:50,450 --> 00:10:51,450
对应的假设就是

320
00:10:51,740 --> 00:10:53,330
神经网络的输出值

321
00:10:53,770 --> 00:10:54,810
在这个训练集上的测试值

322
00:10:55,020 --> 00:10:56,260
应该是远离y(i)的

323
00:10:56,470 --> 00:10:57,970
因此这一点对应着对训练集拟合得不好的情况

324
00:10:58,170 --> 00:10:59,640
而像这些点

325
00:10:59,970 --> 00:11:01,300
代价函数值很小的点

326
00:11:02,130 --> 00:11:03,380
对应的J(θ)值

327
00:11:04,130 --> 00:11:05,270
是很小的 因此对应的是

328
00:11:05,950 --> 00:11:07,590
神经网络对训练集数据

329
00:11:07,850 --> 00:11:09,290
拟合得比较好的情况

330
00:11:09,510 --> 00:11:11,340
我想表达的是 如果是这种情况的话

331
00:11:11,550 --> 00:11:14,070
那么J(θ)的值应该是比较小的

332
00:11:15,480 --> 00:11:16,810
因此梯度下降算法的原理是

333
00:11:16,870 --> 00:11:18,330
我们从某个随机的

334
00:11:18,730 --> 00:11:20,300
初始点开始 比如这一点

335
00:11:20,430 --> 00:11:22,990
它将会不停的往下下降

336
00:11:24,040 --> 00:11:25,400
那么反向传播算法

337
00:11:25,570 --> 00:11:27,220
的目的就是算出

338
00:11:27,940 --> 00:11:29,370
梯度下降的方向

339
00:11:29,520 --> 00:11:30,740
而梯度下降的过程

340
00:11:31,040 --> 00:11:32,060
就是沿着这个方向

341
00:11:32,880 --> 00:11:34,220
一点点的下降 一直到我们希望得到的点

342
00:11:34,610 --> 00:11:36,410
在这里我们希望找到的就是局部最优点

343
00:11:37,880 --> 00:11:39,250
所以 当你在执行反向传播算法

344
00:11:39,410 --> 00:11:40,840
并且使用梯度下降

345
00:11:41,200 --> 00:11:42,420
或者

346
00:11:42,840 --> 00:11:44,750
更高级的优化方法时

347
00:11:45,330 --> 00:11:47,290
这幅图片很好地帮你解释了基本的原理

348
00:11:47,450 --> 00:11:48,820
也就是 试图找到某个最优的参数值

349
00:11:49,260 --> 00:11:50,920
这个值使得

350
00:11:51,260 --> 00:11:52,180
我们神经网络的输出值

351
00:11:52,450 --> 00:11:54,300
与y(i)的实际值

352
00:11:54,410 --> 00:11:55,520
也就是训练集的输出观测值

353
00:11:55,660 --> 00:11:58,800
尽可能的接近

354
00:11:58,910 --> 00:12:00,250
希望这节课的内容能让你对

355
00:12:00,400 --> 00:12:01,610
这些零散的神经网络知识

356
00:12:01,920 --> 00:12:03,930
如何有机地结合起来

357
00:12:04,120 --> 00:12:05,760
能有一个更直观的认识

358
00:12:07,120 --> 00:12:09,010
但可能你即使看了这段视频

359
00:12:09,120 --> 00:12:10,130
你可能还是觉得

360
00:12:10,360 --> 00:12:11,420
有许多的细节

361
00:12:12,070 --> 00:12:13,450
不能完全明白

362
00:12:13,690 --> 00:12:14,670
为什么这么做 或者说是这些是如何

363
00:12:14,860 --> 00:12:17,760
联系在一起的 没关系

364
00:12:18,790 --> 00:12:21,780
神经网络和反向传播算法本身就是非常复杂的算法

365
00:12:23,000 --> 00:12:23,960
尽管我已经完全理解了

366
00:12:24,290 --> 00:12:25,340
反向传播算法背后的数学原理

367
00:12:25,860 --> 00:12:26,710
尽管我使用反向传播已经很多年了

368
00:12:27,030 --> 00:12:28,470
我认为 这么多年的使用还算是成功的

369
00:12:28,680 --> 00:12:30,210
但尽管如此

370
00:12:30,380 --> 00:12:31,510
到现在我还是觉得

371
00:12:31,570 --> 00:12:32,670
我自己也并不是总能

372
00:12:33,400 --> 00:12:35,610
很好地理解反向传播到底在做什么

373
00:12:36,200 --> 00:12:37,850
以及最优化过程是如何

374
00:12:38,520 --> 00:12:41,480
使J(θ)值达到最小值的

375
00:12:41,920 --> 00:12:42,830
因为这本身的确是一个很难的算法

376
00:12:43,450 --> 00:12:44,680
很难让你感觉到

377
00:12:44,830 --> 00:12:46,590
自己已经完全理解

378
00:12:46,690 --> 00:12:47,690
它不像线性回归

379
00:12:48,240 --> 00:12:49,360
或者逻辑回归那样

380
00:12:51,390 --> 00:12:53,180
数学上和概念上都很简单

381
00:12:53,510 --> 00:12:55,090
反向传播算法不是那样的直观

382
00:12:56,200 --> 00:12:57,030
如果你也有同感

383
00:12:57,070 --> 00:12:58,560
那么完全不必担心

384
00:12:58,970 --> 00:13:01,010
但如果你自己动手

385
00:13:01,170 --> 00:13:02,790
完成一次反向传播算法

386
00:13:03,160 --> 00:13:04,260
你一定会发现

387
00:13:04,460 --> 00:13:05,410
这的确是一个很强大的

388
00:13:05,790 --> 00:13:08,030
学习算法 如果你

389
00:13:08,130 --> 00:13:09,510
执行一下这个算法 执行反向传播

390
00:13:10,250 --> 00:13:11,230
执行其中的优化方法

391
00:13:11,340 --> 00:13:13,260
你一定会发现

392
00:13:13,610 --> 00:13:14,940
反向传播算法能够很好的

393
00:13:15,390 --> 00:13:17,330
让更复杂 维度更大的 非线性的

394
00:13:17,830 --> 00:13:19,370
函数模型跟你的数据很好地拟合

395
00:13:20,080 --> 00:13:21,060
因此它的确是一种

396
00:13:21,190 --> 00:13:22,790
最为高效的学习算法 【教育无边界字幕组】翻译: Naplessss 校对/审核: 所罗门捷列夫