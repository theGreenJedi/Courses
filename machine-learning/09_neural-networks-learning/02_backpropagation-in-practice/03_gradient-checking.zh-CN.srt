1
00:00:00,310 --> 00:00:04,060
在之前几个视频里，我们讨论了如何进行前向传播

2
00:00:04,060 --> 00:00:08,800
以及后向传播，从而计算导数

3
00:00:08,800 --> 00:00:12,560
但，后向传播有很多细节，

4
00:00:12,560 --> 00:00:14,840
这些细节有点复杂

5
00:00:14,840 --> 00:00:18,900
有一个不幸的消息是，

6
00:00:18,900 --> 00:00:21,070
它们有很多细节会导致一些BUG

7
00:00:21,070 --> 00:00:23,600
如果你用梯度下降来计算，

8
00:00:23,600 --> 00:00:27,230
你会发现表面上它可以工作

9
00:00:27,230 --> 00:00:28,500
实际上，

10
00:00:28,500 --> 00:00:32,920
J 虽然每次迭代都在下降

11
00:00:32,920 --> 00:00:36,240
但是可能，

12
00:00:36,240 --> 00:00:38,380
仍然你的代码有很多BUG

13
00:00:38,380 --> 00:00:40,820
所以，表面上关于theta的函数J在减小

14
00:00:40,820 --> 00:00:45,080
但是你可能最后得到的结果

15
00:00:45,080 --> 00:00:47,570
实际上有很大的误差

16
00:00:47,570 --> 00:00:50,890
你这时候可能知道，有一些小的BUG导致

17
00:00:50,890 --> 00:00:52,330
这种不好的算法性能表现

18
00:00:52,330 --> 00:00:54,170
所以，怎么办呢

19
00:00:54,170 --> 00:00:56,810
有一个想法叫梯度检验 Gradient Checking

20
00:00:56,810 --> 00:00:59,180
它能减少这种错误的概率

21
00:00:59,180 --> 00:01:02,160
就我个人而言，每次我使用后向传播

22
00:01:02,160 --> 00:01:04,780
我都会[]用这种方法

23
00:01:04,780 --> 00:01:09,680
即使是其他比较复杂的模型，我都会做这种检查

24
00:01:09,680 --> 00:01:13,570
如果你这么做，你会对你的模型更有自信

25
00:01:13,570 --> 00:01:17,360
这样，你会更加确信的模型是100%正确的

26
00:01:17,360 --> 00:01:22,320
从我看到的情况，这种方法，

27
00:01:22,320 --> 00:01:25,860
很大程度可以减少错误的可能性

28
00:01:25,860 --> 00:01:31,570
在之前的视频里，我让你们相信

29
00:01:31,570 --> 00:01:34,910
我给你们的公式是正确的，

30
00:01:34,910 --> 00:01:39,480
我还让你们相信，这就是支付函数的梯度值

31
00:01:39,480 --> 00:01:43,120
但，一旦你使用梯度检验，

32
00:01:43,120 --> 00:01:47,070
也就是我们这个视频的主题，你会证明你的代码

33
00:01:47,070 --> 00:01:51,270
实际上就是梯度函数

34
00:01:52,440 --> 00:01:55,480
所以，这就是我们的想法，来看一个例子

35
00:01:55,480 --> 00:02:01,320
假设我们有一个关于theta的函数H

36
00:02:01,320 --> 00:02:05,450
我现在有它的一个值，假设是实数

37
00:02:05,450 --> 00:02:08,560
我们说，我想要预测它的倒数，

38
00:02:08,560 --> 00:02:13,130
所以倒数是等于这里的斜度

39
00:02:14,300 --> 00:02:17,850
现在我用这种方法来接近，

40
00:02:17,850 --> 00:02:21,770
我们不采用数值的计算倒数，

41
00:02:21,770 --> 00:02:26,320
这里我用epsilon，

42
00:02:26,320 --> 00:02:31,260
同样也有一个减去epsilon的值

43
00:02:31,260 --> 00:02:36,183
然后把他们链接起来

44
00:02:43,426 --> 00:02:47,586
我将得到一条直线

45
00:02:47,586 --> 00:02:51,620
我用这个红色线来近似我的导数

46
00:02:51,620 --> 00:02:54,860
恩，真正的斜率是蓝色的线。

47
00:02:54,860 --> 00:02:56,719
所以，你可以看到这是一个很好的近似。

48
00:02:58,260 --> 00:03:03,060
数学上，这里的红线垂直高度

49
00:03:03,060 --> 00:03:05,470
除以这个水平宽度，就是我们的斜率

50
00:03:05,470 --> 00:03:10,650
所以，这个点，就是J()

51
00:03:10,650 --> 00:03:13,990
这个点，(theta减掉epsilon)

52
00:03:13,990 --> 00:03:17,870
我们有一个垂直的差值()减去

53
00:03:17,870 --> 00:03:21,920
这两个点的差值，以及水平宽度2epsilon

54
00:03:23,620 --> 00:03:28,730
所以我们可以近似来表示

55
00:03:28,730 --> 00:03:34,970
这是近似的值，

56
00:03:34,970 --> 00:03:41,319
它等于J加上epsilon减去J减去epsilon对应的函数值，除以2倍的epsilon

57
00:03:42,350 --> 00:03:44,430
通常，

58
00:03:44,430 --> 00:03:48,800
这个epsilon非常小，可能就是10的-4次方

59
00:03:48,800 --> 00:03:53,060
而误差值往往很大，

60
00:03:53,060 --> 00:03:58,010
所以近似效果很好。实际上

61
00:03:58,010 --> 00:04:02,000
如果让epsilon无穷小，这就是导数的定义

62
00:04:02,000 --> 00:04:05,090
恩，它就是导数。

63
00:04:05,090 --> 00:04:07,020
所以，

64
00:04:07,020 --> 00:04:10,110
但我们不希望epsilon太小，否则会有计算上的问题

65
00:04:10,110 --> 00:04:13,900
一般来说在10的-4次方比较合适

66
00:04:13,900 --> 00:04:17,860
通常，你可能见到这个类似的公式

67
00:04:17,860 --> 00:04:20,180
恩。

68
00:04:21,610 --> 00:04:24,000
所以，右边的叫做单边导数

69
00:04:24,000 --> 00:04:27,630
左边的叫做双边导数

70
00:04:27,630 --> 00:04:30,360
后者的精确度更高，

71
00:04:30,360 --> 00:04:33,920
当然，一般我们用后者

72
00:04:35,960 --> 00:04:39,560
所以，具体来说，当你用octave，

73
00:04:39,560 --> 00:04:42,040
你计算近似梯度时候，

74
00:04:42,040 --> 00:04:46,840
我们用的是这个公式。

75
00:04:46,840 --> 00:04:52,080
恩。。就是红色标注的双边导数近似公式，除以的是2倍

76
00:04:52,080 --> 00:04:56,650
所以，我们有一个近似的值

77
00:04:56,650 --> 00:04:59,290
在这个例子，它看起来非常好地近似我们的结果

78
00:05:01,913 --> 00:05:03,618
在之前的PPT里

79
00:05:03,618 --> 00:05:08,060
我们考虑了theta是一个向量

80
00:05:08,060 --> 00:05:12,450
现在，我们来看一般的情况

81
00:05:12,450 --> 00:05:14,000
我们说theta是一个矩阵

82
00:05:14,000 --> 00:05:18,030
我们有一个舒展的参数版本，

83
00:05:18,030 --> 00:05:21,270
所以，这里theta是从1到n的向量

84
00:05:21,270 --> 00:05:25,720
我们，

85
00:05:25,720 --> 00:05:30,260
可以用近似的方式来进行计算

86
00:05:30,260 --> 00:05:35,140
可以看到，我们列出各个近似的求导公式

87
00:05:35,140 --> 00:05:40,980
theta 1套前面的公式是这样，

88
00:05:40,980 --> 00:05:43,530
如此种种。

89
00:05:43,530 --> 00:05:48,130
恩。。

90
00:05:48,130 --> 00:05:52,320
这里都是偏导数。

91
00:05:52,320 --> 00:05:56,620
只改变theta 1的值，其他的值固定

92
00:05:56,620 --> 00:06:00,820
分母一样，还是2 epsilon

93
00:06:00,820 --> 00:06:04,958
我们现在可以得到我们想要的近似结果

94
00:06:04,958 --> 00:06:06,393
恩，

95
00:06:09,691 --> 00:06:15,380
所以这个公式组给出了近似的方法

96
00:06:15,380 --> 00:06:20,450
对于任意一个theta我们都有

97
00:06:23,590 --> 00:06:26,450
完整地，你可以这样应用。

98
00:06:27,930 --> 00:06:32,230
我们用octave来数值计算，

99
00:06:32,230 --> 00:06:33,000
比如，

100
00:06:33,000 --> 00:06:37,780
对于i=1:n，其中n是我们参数的个数

101
00:06:37,780 --> 00:06:41,260
一般我们习惯是舒展的向量而不是矩阵

102
00:06:41,260 --> 00:06:46,250
所以theta是长长的参数清单 

103
00:06:46,250 --> 00:06:48,050
这里设置thetaPlus = theta

104
00:06:48,050 --> 00:06:51,700
之后增加(i)项epsilon

105
00:06:51,700 --> 00:06:55,785
恩，这就等于我们

106
00:06:55,785 --> 00:06:58,260
thetaPlus(i)，

107
00:06:58,260 --> 00:07:03,390
theta1, theta2如此种种

108
00:07:03,390 --> 00:07:07,250
thetal ，一直到N

109
00:07:07,250 --> 00:07:09,670
所以，这是thetaPlus的含义

110
00:07:09,670 --> 00:07:15,070
类似的

111
00:07:15,070 --> 00:07:19,329
我们现在也有l 减去epsilon

112
00:07:20,670 --> 00:07:25,690
最后你会使用这个gradApprox(i)

113
00:07:25,690 --> 00:07:29,840
并且能够给你一个偏微分单数

114
00:07:29,840 --> 00:07:32,770
对于i

115
00:07:35,310 --> 00:07:38,900
这就是我们使用的方法

116
00:07:38,900 --> 00:07:45,250
我们可以用一个循环来写

117
00:07:45,250 --> 00:07:49,650
来检验这个近似计算的结果是不是等于我们的计算结果

118
00:07:49,650 --> 00:07:53,720
也就是反向传播算法计算的梯度

119
00:07:53,720 --> 00:07:58,382
Dvec就是我们得到的导数

120
00:07:58,382 --> 00:08:00,640
好的，

121
00:08:00,640 --> 00:08:04,810
反向传播是一个非常高效的算法，

122
00:08:04,810 --> 00:08:07,850
针对所有的参数

123
00:08:07,850 --> 00:08:12,670
我们通常做的是数值计算的结果

124
00:08:12,670 --> 00:08:15,820
也就是刚才所做的

125
00:08:15,820 --> 00:08:20,780
确信这是相等的，

126
00:08:20,780 --> 00:08:24,110
应该说非常接近

127
00:08:24,110 --> 00:08:26,540
所以DVec，我们从反向传播得到，

128
00:08:26,540 --> 00:08:30,850
如果得到同一个结果

129
00:08:30,850 --> 00:08:34,750
或者相近的结果，只相差一些小数位

130
00:08:34,750 --> 00:08:39,850
我们很确信这个反向传播的算法是正确的

131
00:08:39,850 --> 00:08:43,920
如果我代入梯度计算

132
00:08:43,920 --> 00:08:47,280
一些高级的算法

133
00:08:47,280 --> 00:08:51,640
我们会更加确信我们的导数计算是正确的

134
00:08:51,640 --> 00:08:55,779
因此，我们的代码不仅正确，而且在优化上性能很好

135
00:08:57,680 --> 00:08:59,870
最后，我想总结一下

136
00:08:59,870 --> 00:09:03,670
告诉你梯度检验的相关内容

137
00:09:03,670 --> 00:09:05,070
这是我通常做的事情

138
00:09:05,070 --> 00:09:08,460
首先，使用反向传播来计算，它是很好的算法

139
00:09:08,460 --> 00:09:11,290
这里就是前面介绍的流程

140
00:09:11,290 --> 00:09:14,230
这里的参数我们把矩阵展开成向量

141
00:09:14,230 --> 00:09:16,280
然后

142
00:09:16,280 --> 00:09:20,220
我们使用数值的梯度来检验

143
00:09:20,220 --> 00:09:23,830
这是刚刚介绍的内容

144
00:09:24,930 --> 00:09:29,410
我们要确信这两个方法算出来结果一致

145
00:09:29,410 --> 00:09:31,079
你知道，就差一点

146
00:09:32,270 --> 00:09:36,740
最后，也是最重要的步骤

147
00:09:36,740 --> 00:09:40,390
就是在你开始学习之前，

148
00:09:40,390 --> 00:09:45,100
一定要关掉我们的梯度检验，

149
00:09:45,100 --> 00:09:49,119
也就是我们讨论的数值计算方法

150
00:09:50,530 --> 00:09:54,160
原因是这个计算过程，

151
00:09:54,160 --> 00:09:58,400
实际上代价更高，复杂度也很高

152
00:09:58,400 --> 00:10:02,030
这不是一个很好的计算导数的方法

153
00:10:02,030 --> 00:10:05,730
相反，我们前面讨论的反向传播算法

154
00:10:05,730 --> 00:10:08,190
很早以前介绍的内容

155
00:10:08,190 --> 00:10:11,030
你知道D1 D2 D3对于DVEC

156
00:10:11,030 --> 00:10:14,970
相对来说非常高效。

157
00:10:14,970 --> 00:10:15,660
恩

158
00:10:17,050 --> 00:10:20,770
所以，一旦你检验证明你的算法没有错误

159
00:10:20,770 --> 00:10:25,090
就要把梯度检验关掉

160
00:10:25,090 --> 00:10:28,690
所以，你一定要关掉

161
00:10:28,690 --> 00:10:32,910
在你开始迭代训练之前

162
00:10:32,910 --> 00:10:35,880
对于其他很多优化算法也一样

163
00:10:35,880 --> 00:10:38,020
为了训练你的分类器

164
00:10:38,020 --> 00:10:41,600
具体来说，如果你一定要用数值方法

165
00:10:41,600 --> 00:10:43,800
来计算梯度，

166
00:10:43,800 --> 00:10:46,690
那么你的算法会非常慢。

167
00:10:46,690 --> 00:10:48,370
在你的支付函数的循环过程当中

168
00:10:48,370 --> 00:10:52,000
因为，正如前面所说

169
00:10:52,000 --> 00:10:56,270
我们再重复一下...它很慢

170
00:10:56,270 --> 00:11:00,440
记得，我们这里计算(4)(3)(2)等等

171
00:11:00,440 --> 00:11:02,460
这是我们的反向传播算法

172
00:11:02,460 --> 00:11:06,610
它快得多

173
00:11:06,610 --> 00:11:10,880
所以，再说一遍...检验完了后向传播没有问题

174
00:11:10,880 --> 00:11:15,130
关掉梯度检验，重要的事情说三遍

175
00:11:15,130 --> 00:11:18,220
当你在训练你的算法的时候，

176
00:11:20,430 --> 00:11:24,390
所以数值的计算，

177
00:11:24,390 --> 00:11:27,260
这是你的检验方法而已。

178
00:11:27,260 --> 00:11:31,210
对我而言，每当我要使用梯度算法，比如后向传播

179
00:11:31,210 --> 00:11:33,970
我都会用梯度检验一下这个算法是否正确

180
00:11:33,970 --> 00:11:36,750
这会让我更加自信我的算法是正确的。