1
00:00:00,540 --> 00:00:01,820
前回のビデオでは

2
00:00:01,950 --> 00:00:03,220
ニューラルネットワークを実装し

3
00:00:03,270 --> 00:00:04,620
訓練するのに必要な

4
00:00:04,820 --> 00:00:07,170
ほとんど全てのピースをまとめた。

5
00:00:07,940 --> 00:00:09,060
だがまだもう一つ、最後のピースを

6
00:00:09,120 --> 00:00:09,980
あなたにも伝えなくてはいけない。

7
00:00:10,200 --> 00:00:11,570
それはランダム初期化と呼ばれるアイデアだ。

8
00:00:13,220 --> 00:00:14,360
最急降下法なりアドバンスドな最適化アルゴリズムなりのような

9
00:00:14,510 --> 00:00:15,990
アルゴリズムを

10
00:00:16,280 --> 00:00:17,810
走らせる時には、

11
00:00:17,940 --> 00:00:20,770
パラメータシータのある初期値を選ぶ必要がある。

12
00:00:21,610 --> 00:00:22,990
アドバンスドな最適化アルゴリズムは、

13
00:00:23,570 --> 00:00:24,620
あなたがパラメータのシータの

14
00:00:24,780 --> 00:00:26,090
ある初期値を渡すと

15
00:00:26,700 --> 00:00:27,640
想定している。

16
00:00:29,010 --> 00:00:30,680
ここでは最急降下法を考えよう。

17
00:00:31,320 --> 00:00:34,090
その場合でも、シータを何かしらには初期化しなくてはいけない。

18
00:00:34,580 --> 00:00:36,030
そしてそこから、ゆっくりと丘を

19
00:00:36,680 --> 00:00:38,830
最急降下法を使って一歩一歩降りていき、

20
00:00:38,910 --> 00:00:40,920
Jのシータの最小値まで降りていく。

21
00:00:41,990 --> 00:00:43,960
ではシータの初期値をどうセットしたらいいだろうか？

22
00:00:44,240 --> 00:00:47,000
シータの初期値に

23
00:00:47,520 --> 00:00:48,930
全部0を入れる、というのが

24
00:00:49,250 --> 00:00:50,450
考えられる。

25
00:00:51,870 --> 00:00:54,800
これはロジスティック回帰の時にはうまく行ったが、

26
00:00:55,630 --> 00:00:56,690
パラメータの初期値を全て0にするのは

27
00:00:56,760 --> 00:00:57,970
ニューラルネットワークをトレーニングする時には

28
00:00:58,310 --> 00:01:00,290
うまく行かない。

29
00:01:01,410 --> 00:01:03,150
以下のニューラルネットワークをトレーニングする事を考えてみよう。

30
00:01:03,650 --> 00:01:06,430
そして全てのネットワークのパラメータを0に初期化したとしよう。

31
00:01:07,970 --> 00:01:09,210
そうすると、

32
00:01:09,780 --> 00:01:10,920
それの意味する事は、

33
00:01:11,160 --> 00:01:13,870
この青いウェイトを初期化する時に、、、ここでこのウェイトを

34
00:01:15,390 --> 00:01:16,540
青で色づけする。

35
00:01:17,510 --> 00:01:17,510
これらは共に0となる。

36
00:01:18,580 --> 00:01:19,880
そしてこのウェイト、赤でいろづけしておくと、

37
00:01:20,330 --> 00:01:21,940
このウェイトがこっちのウェイト、

38
00:01:22,550 --> 00:01:23,040
これも赤で色付けしておくが、これらが等しい。

39
00:01:23,790 --> 00:01:25,280
そしてこのウェイト、

40
00:01:25,620 --> 00:01:26,500
これは緑に色付けしよう、

41
00:01:26,680 --> 00:01:28,940
これはこっちのウェイトの値と等しくなる。

42
00:01:30,030 --> 00:01:32,820
その意味する所は、隠れユニット、a1とa2はどちらも

43
00:01:32,950 --> 00:01:35,940
同じ入力の関数を

44
00:01:36,660 --> 00:01:36,810
計算する事になる、という事。

45
00:01:37,810 --> 00:01:38,900
かくして、

46
00:01:39,500 --> 00:01:40,870
どのトレーニング手本に対しても

47
00:01:41,430 --> 00:01:43,640
a(2)1 = a(2)2 となってしまう。

48
00:01:46,950 --> 00:01:48,700
さらに、あまり細かい話に

49
00:01:48,960 --> 00:01:50,050
首をつっこむ気は無いけれど、

50
00:01:50,310 --> 00:01:51,420
しかしこれらのウェイトが

51
00:01:51,580 --> 00:01:52,990
等しければ、

52
00:01:53,080 --> 00:01:54,630
デルタの値も等しくなる、という事を

53
00:01:54,710 --> 00:01:56,560
示す事が出来る。

54
00:01:56,790 --> 00:01:57,790
具体的には、

55
00:01:57,970 --> 00:02:00,070
デルタ1 1、、、じゃなかった。デルタ2, 1は

56
00:02:00,760 --> 00:02:02,900
イコールデルタ2, 2。

57
00:02:06,120 --> 00:02:07,150
さらに数学を続けていくと、

58
00:02:07,230 --> 00:02:08,480
あなたのパラメータによる

59
00:02:08,760 --> 00:02:09,990
偏微分は

60
00:02:11,560 --> 00:02:14,080
以下を満たす事を示す事が出来る。

61
00:02:15,120 --> 00:02:16,710
コスト関数の

62
00:02:17,550 --> 00:02:19,260
偏微分、、、

63
00:02:19,580 --> 00:02:21,020
ここに書いている、

64
00:02:21,800 --> 00:02:23,680
ニューラルネットワークの

65
00:02:23,900 --> 00:02:25,320
これら二つの青のウェイトによる偏微分。

66
00:02:26,190 --> 00:02:27,290
これら二つの偏微分は

67
00:02:27,680 --> 00:02:30,340
お互いに等しい事が分かる。

68
00:02:31,970 --> 00:02:33,180
この意味する所は、

69
00:02:33,320 --> 00:02:35,820
一回最急降下法のアップデートが行われた後も、

70
00:02:36,690 --> 00:02:38,200
この青いウェイトを

71
00:02:38,470 --> 00:02:40,800
学習率 掛ける これ でアップデートし、

72
00:02:41,580 --> 00:02:42,500
二番目の青いウェイトも

73
00:02:42,920 --> 00:02:44,620
学習率掛けるこれ、でアップデートする。

74
00:02:44,820 --> 00:02:45,870
だがその意味する所は、

75
00:02:45,980 --> 00:02:47,090
一回最急降下法のアップデートを

76
00:02:47,420 --> 00:02:49,330
かました後でも、

77
00:02:49,680 --> 00:02:50,710
これら二つの青のウェイトは、これら二つの

78
00:02:51,430 --> 00:02:53,050
青の色付けしたパラメータは

79
00:02:53,240 --> 00:02:54,960
結局相等しいままだ。

80
00:02:55,190 --> 00:02:56,210
なんらかの非0の値にはなるだろう、

81
00:02:56,750 --> 00:02:57,720
だがこの値は

82
00:02:58,550 --> 00:02:59,520
こっちの値と等しい。

83
00:03:00,360 --> 00:03:02,790
そして同様に、一回最急降下法のアップデートを行った後でも、

84
00:03:03,690 --> 00:03:05,740
この値はこっちの値と等しい。

85
00:03:06,170 --> 00:03:07,200
なんらかの非0の値にはなる。

86
00:03:07,640 --> 00:03:09,450
この二つの赤の値もお互いに相等しい。

87
00:03:10,240 --> 00:03:11,760
そして同様に、二つの緑のウェイトも

88
00:03:12,060 --> 00:03:13,720
それらの値も共に変化するが、

89
00:03:13,860 --> 00:03:16,350
だが変化した結果はどちらも同じ値になる。

90
00:03:17,590 --> 00:03:19,020
だから各アップデートの後でも、各入力から

91
00:03:19,740 --> 00:03:20,890
二つの隠れユニットへと至る

92
00:03:21,060 --> 00:03:22,870
二つのウェイトは、同一となる。

93
00:03:23,700 --> 00:03:24,490
それは単に、

94
00:03:24,710 --> 00:03:25,590
二つの緑のウェイトが等しくなり、

95
00:03:25,640 --> 00:03:26,310
二つの赤のウェイトも等しくなり、

96
00:03:26,550 --> 00:03:27,750
二つの青のウェイトも等しいままだ、と

97
00:03:28,010 --> 00:03:30,000
言っているだけだ。

98
00:03:30,160 --> 00:03:31,590
そしてその意味する所は、

99
00:03:31,770 --> 00:03:33,070
例えば最急降下法などの一回のイテレーションの後でも、

100
00:03:33,460 --> 00:03:34,860
あなたの二つの隠れユニットが

101
00:03:35,600 --> 00:03:37,250
全く同一の入力の関数を

102
00:03:37,800 --> 00:03:40,380
計算し続ける事となる。

103
00:03:40,830 --> 00:03:43,040
だから、a(1)2 = a(2)2 のままとなる。

104
00:03:43,510 --> 00:03:45,200
そしてこのケースに戻る。

105
00:03:45,930 --> 00:03:47,380
そして最急降下法を走らせ続けても、

106
00:03:48,390 --> 00:03:50,940
この二つの青のウェイトはお互いに等しいままだ。

107
00:03:51,190 --> 00:03:52,920
二つの赤いウェイトはお互いに等しいままだ。

108
00:03:53,060 --> 00:03:54,990
二つの緑のウェイトはお互いに等しいままだ。

109
00:03:55,160 --> 00:03:56,860
この意味する所は、

110
00:03:57,130 --> 00:03:58,260
このニューラルネットワークはあまり面白い関数は

111
00:03:58,470 --> 00:03:59,980
計算出来ない、という事。

112
00:04:00,700 --> 00:04:01,910
二つの隠れユニットだけじゃなくて

113
00:04:02,240 --> 00:04:03,670
もっともっとたくさんの

114
00:04:04,010 --> 00:04:05,470
隠れユニットがある場合を

115
00:04:05,640 --> 00:04:07,100
考えてみよう。

116
00:04:08,080 --> 00:04:09,160
するとこれの意味する所は、

117
00:04:09,430 --> 00:04:10,680
隠れユニットが全て

118
00:04:10,740 --> 00:04:12,320
完全に同じフィーチャーを計算する、という事になる。

119
00:04:12,540 --> 00:04:16,300
全ての隠れユニットが、全く同一の、入力の関数を計算する事になる。

120
00:04:17,030 --> 00:04:18,980
これはとても冗長な表現になってしまう。

121
00:04:20,140 --> 00:04:21,010
何故ならそれはつまりは、

122
00:04:21,110 --> 00:04:24,160
最後のロジスティック回帰のユニットは、実際は一つの入力があるだけに見えるから。

123
00:04:24,730 --> 00:04:25,460
何故ならこれらは全て同じで、

124
00:04:26,330 --> 00:04:28,690
このことがあなたのニューラルネットワークが何かしら面白い学習をする事を妨げているから。

125
00:04:31,600 --> 00:04:32,830
この問題を回避する為に、

126
00:04:32,960 --> 00:04:34,050
ニューラルネットワークのパラメータを

127
00:04:34,590 --> 00:04:35,680
初期化する方法は、

128
00:04:36,050 --> 00:04:37,660
ランダム初期化の方法だ。

129
00:04:41,820 --> 00:04:43,130
具体的には、前のスライドで

130
00:04:43,250 --> 00:04:44,470
我らが見た問題は

131
00:04:44,760 --> 00:04:46,240
対称ウェイトの問題と呼ばれる事もあり、

132
00:04:46,640 --> 00:04:49,040
それはウェイトが全て同じという事だ。

133
00:04:49,810 --> 00:04:51,470
だからこのランダム初期化で

134
00:04:52,590 --> 00:04:54,240
対称性を破る訳だ。

135
00:04:55,520 --> 00:04:56,480
我らのやるべき事は、

136
00:04:56,680 --> 00:04:58,200
各シータの値を

137
00:04:58,310 --> 00:04:59,460
-エプシロン から エプシロン までの間の

138
00:04:59,830 --> 00:05:01,300
ランダムな値で初期化する。

139
00:05:02,080 --> 00:05:03,200
これは -エプシロン と +エプシロン の間の値を

140
00:05:03,310 --> 00:05:05,350
表す記法だ。

141
00:05:06,330 --> 00:05:07,430
パラメータのウェイトは

142
00:05:07,540 --> 00:05:08,660
-エプシロンから+エプシロンまでの間の値で

143
00:05:08,710 --> 00:05:11,470
ランダムに初期化される。

144
00:05:12,300 --> 00:05:13,330
Octaveでこれをやるコードを書く方法は、

145
00:05:13,420 --> 00:05:16,770
Theta1をイコール これ、とする。

146
00:05:17,550 --> 00:05:19,620
このrand(10, 11)で、

147
00:05:19,910 --> 00:05:21,060
これが10x11次元の

148
00:05:21,640 --> 00:05:23,620
ランダムな値を持つ行列を

149
00:05:24,670 --> 00:05:26,640
計算する方法で、

150
00:05:27,070 --> 00:05:30,380
そしてそれらの要素の値は全て、0と1の間の数となる。

151
00:05:30,580 --> 00:05:31,350
つまりこれらは、

152
00:05:31,520 --> 00:05:32,700
連続的な実数の0と1の間のいかなる値でも

153
00:05:32,870 --> 00:05:34,860
取る事が出来る。

154
00:05:35,450 --> 00:05:36,290
すると、0と1の間の数を

155
00:05:36,320 --> 00:05:37,440
取って、

156
00:05:37,550 --> 00:05:38,310
2*エプシロン

157
00:05:38,590 --> 00:05:39,550
を掛けて、

158
00:05:39,600 --> 00:05:41,050
そこからエプシロンを引く。

159
00:05:41,160 --> 00:05:42,270
すると最終的には -エプシロン と エプシロンの

160
00:05:42,690 --> 00:05:44,160
間の数となる。

161
00:05:45,640 --> 00:05:46,970
ついでに言っておくと、このエプシロンは

162
00:05:47,230 --> 00:05:48,410
グラディアントチェッキングで使ってた

163
00:05:48,730 --> 00:05:49,860
エプシロンとは

164
00:05:50,070 --> 00:05:51,710
全く関係が無い。

165
00:05:52,590 --> 00:05:54,070
数値的なグラディアントチェッキングを行っていた時は、

166
00:05:54,850 --> 00:05:57,060
シータにあるエプシロンという値を足していたが、

167
00:05:57,430 --> 00:05:59,560
この値は、そのエプシロンとは無関係だ。

168
00:05:59,780 --> 00:06:00,590
そんな訳でこのエプシロンを

169
00:06:00,990 --> 00:06:02,200
INIT_EPSILONと書いている。

170
00:06:02,480 --> 00:06:04,970
これはグラディアントチェッキングで使ったエプシロンの値と区別する為だけの理由だ。

171
00:06:06,490 --> 00:06:07,590
同様にもしシータ2を

172
00:06:07,690 --> 00:06:09,620
1x11のランダムの行列で

173
00:06:09,640 --> 00:06:10,820
初期化したければ、

174
00:06:10,920 --> 00:06:13,430
このコード辺でそれが行える。

175
00:06:15,910 --> 00:06:17,460
さて、まとめると、

176
00:06:17,660 --> 00:06:18,910
ニューラルネットワークをトレーニングする為には、

177
00:06:19,060 --> 00:06:20,850
ウェイトを小さな値、

178
00:06:20,930 --> 00:06:21,810
0のそばの-エプシロンから+エプシロンの間のどこかの値で

179
00:06:22,120 --> 00:06:23,370
ランダムに初期化

180
00:06:23,740 --> 00:06:24,740
しなければならない。

181
00:06:25,160 --> 00:06:27,150
そしてバックプロパゲーションを実装し、

182
00:06:27,620 --> 00:06:29,330
グラディアントチェッキングをする。

183
00:06:30,220 --> 00:06:31,300
そして最急降下法なり

184
00:06:31,660 --> 00:06:32,620
アドバンスドな最適化アルゴリズムなりを

185
00:06:32,880 --> 00:06:34,860
用いて、

186
00:06:35,100 --> 00:06:36,250
Jのシータを

187
00:06:36,790 --> 00:06:37,860
パラメータシータの関数として、

188
00:06:38,050 --> 00:06:39,610
ランダムに初期化したパラメータから始めて

189
00:06:39,890 --> 00:06:41,900
最小化を試みる。

190
00:06:42,970 --> 00:06:45,440
そして対称性の破れを行う事で、それはこのプロセスだが、

191
00:06:46,000 --> 00:06:47,110
最急降下法なりアドバンスドな最適化アルゴリズムなりが、

192
00:06:47,580 --> 00:06:48,820
シータの良い値を

193
00:06:48,980 --> 00:06:50,710
見つける事が期待出来るようになる。