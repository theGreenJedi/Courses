1
00:00:00,290 --> 00:00:01,510
En los últimos videos, hemos hablado

2
00:00:01,840 --> 00:00:02,770
acerca de cómo realizar una propagación hacia delante

3
00:00:03,570 --> 00:00:05,200
y una retropropagación en una

4
00:00:05,250 --> 00:00:07,560
red neuronal con el fin de procesar los derivados.

5
00:00:08,800 --> 00:00:10,070
Sin embargo la retropropagación como un algoritmo

6
00:00:10,580 --> 00:00:11,910
tiene muchos detalles y,

7
00:00:12,170 --> 00:00:12,920
como saben, puede ser un poco

8
00:00:13,050 --> 00:00:14,930
difícil de implementar.

9
00:00:15,700 --> 00:00:17,480
Y una variable desafortunada es

10
00:00:17,750 --> 00:00:18,690
que existen muchas

11
00:00:18,780 --> 00:00:20,080
formas de tener errores imperceptibles en la

12
00:00:20,320 --> 00:00:22,000
retropropagación de forma que si

13
00:00:22,140 --> 00:00:23,130
los ejecutan con el gradiente de descenso

14
00:00:23,480 --> 00:00:26,590
o algún otro algoritmo de optimización, podría parecer que en realidad está funcionando.

15
00:00:27,240 --> 00:00:28,480
Y, como saben,  la función de costos J

16
00:00:28,700 --> 00:00:29,930
de «theta» puede terminar

17
00:00:30,090 --> 00:00:31,240
disminuyendo en cada iteración

18
00:00:31,830 --> 00:00:33,660
de la gradiente de descenso, pero esto

19
00:00:33,830 --> 00:00:35,180
podría llevarse a cabo a pesar de que

20
00:00:35,440 --> 00:00:37,690
puede haber algún error en la implementación de la retropropagación.

21
00:00:38,400 --> 00:00:39,280
Así que parece que J de

22
00:00:39,360 --> 00:00:40,830
«theta» está disminuyendo, sin embargo

23
00:00:40,920 --> 00:00:42,230
podrían terminar con

24
00:00:42,410 --> 00:00:43,760
una red neuronal que

25
00:00:43,880 --> 00:00:44,970
tiene un nivel más alto de errores de lo que

26
00:00:45,490 --> 00:00:46,540
tendrían con una implementación

27
00:00:46,780 --> 00:00:48,130
libre de errores y simplemente

28
00:00:48,330 --> 00:00:49,330
no sabrían que este

29
00:00:49,460 --> 00:00:50,470
error imperceptible estuvo ocasionándoles

30
00:00:50,530 --> 00:00:52,260
este desempeño.

31
00:00:52,950 --> 00:00:53,320
Entonces, ¿qué podemos hacer al respecto?

32
00:00:54,160 --> 00:00:55,940
Existe un concepto llamado comprobación de gradiente

33
00:00:56,790 --> 00:00:58,720
la cual elimina casi todos estos problemas.

34
00:00:59,250 --> 00:01:00,550
Entonces hoy, cada vez que

35
00:01:00,770 --> 00:01:02,150
implemente una retropropagación o un

36
00:01:02,370 --> 00:01:03,320
algoritmo de gradiente de descenso similar en

37
00:01:03,450 --> 00:01:04,950
la red neuronal o en cualquier otro

38
00:01:05,640 --> 00:01:07,310
modelo de una complejidad razonable,

39
00:01:07,540 --> 00:01:08,840
siempre implementaré la comprobación de gradiente.

40
00:01:09,650 --> 00:01:10,610
Y si ustedes lo llevan a cabo les

41
00:01:10,730 --> 00:01:12,010
ayudará a comprobar y

42
00:01:12,140 --> 00:01:13,410
en cierto modo a tener mayor confianza de que

43
00:01:13,540 --> 00:01:14,940
la implementación de la propagación hacia delante

44
00:01:15,370 --> 00:01:17,430
y la retropropagación o lo que sea, es 100% correcta.

45
00:01:18,240 --> 00:01:19,090
Y de acuerdo a lo que he visto

46
00:01:19,330 --> 00:01:20,880
esto elimina prácticamente todos los

47
00:01:21,160 --> 00:01:23,090
problemas relacionados con el tipo de

48
00:01:23,420 --> 00:01:25,790
implementación errónea de la retropropagación.

49
00:01:26,330 --> 00:01:27,470
Y en los videos anteriores,

50
00:01:28,170 --> 00:01:29,120
en cierta forma les pido que tengan

51
00:01:29,390 --> 00:01:30,950
confianza en que las fórmulas que les

52
00:01:31,170 --> 00:01:33,000
he dado para procesar los «delta»s, y las

53
00:01:33,110 --> 00:01:34,220
"D" y así sucesivamente, les pido

54
00:01:34,260 --> 00:01:35,480
que tengan confianza en que aquellas

55
00:01:36,330 --> 00:01:37,600
realmente  procesan las gradientes

56
00:01:38,180 --> 00:01:39,790
de la función de costos, pero una vez

57
00:01:40,150 --> 00:01:41,740
que ustedes implementan la comprobación de la gradiente numérica,

58
00:01:42,130 --> 00:01:43,210
que es el tema de este video,

59
00:01:43,800 --> 00:01:45,250
podrán ser capaces de verificar por sí

60
00:01:45,350 --> 00:01:46,490
mismos que el código que escriban

61
00:01:46,610 --> 00:01:48,530
en efecto está procesando

62
00:01:49,600 --> 00:01:50,520
la derivada de la función

63
00:01:50,820 --> 00:01:53,060
de costos de "J". 
Entonces, este es el concepto.

64
00:01:53,550 --> 00:01:54,520
Consideren el siguiente ejemplo.

65
00:01:55,450 --> 00:01:56,230
Supongan que tengo la función

66
00:01:56,710 --> 00:01:58,140
J de «theta», y tengo

67
00:01:58,250 --> 00:02:01,320
algún valor, «theta», y

68
00:02:01,610 --> 00:02:04,380
para este ejemplo, voy a asumir que «theta» es sólo un número real.

69
00:02:05,470 --> 00:02:08,210
Y digamos que deseo calcular la derivada de esta función en este punto.

70
00:02:08,710 --> 00:02:10,220
Y así la derivada es, como saben, igual

71
00:02:10,750 --> 00:02:13,190
a la pendiente de ese tipo de línea de la tangente.

72
00:02:14,270 --> 00:02:15,420
Así es como voy a aproximar

73
00:02:16,180 --> 00:02:17,840
en forma numérica la derivada, o

74
00:02:17,970 --> 00:02:19,190
más bien este es un procedimiento para

75
00:02:19,780 --> 00:02:21,480
aproximar en forma numérica la derivada: Voy a

76
00:02:21,800 --> 00:02:23,520
procesar «theta» más «épsilon»,

77
00:02:24,000 --> 00:02:25,550
de forma que el valor está un poco a la derecha.

78
00:02:26,340 --> 00:02:27,900
Y vamos a procesar «theta» menos «épsilon».

79
00:02:28,410 --> 00:02:30,800
Y voy a,  observen

80
00:02:30,950 --> 00:02:34,360
estos dos puntos y conéctenlos

81
00:02:34,840 --> 00:02:35,860
por medio de una línea recta.

82
00:02:43,160 --> 00:02:44,280
Y voy a conectar

83
00:02:44,480 --> 00:02:45,490
estos dos puntos por medio de una línea

84
00:02:45,680 --> 00:02:46,430
recta y voy a

85
00:02:46,480 --> 00:02:47,740
utilizar la pendiente de esta

86
00:02:48,000 --> 00:02:49,200
pequeña línea roja como mi

87
00:02:49,390 --> 00:02:50,940
aproximación a la derivada

88
00:02:51,460 --> 00:02:53,110
que es, la verdadera derivada es

89
00:02:53,280 --> 00:02:54,740
la pendiente de esa línea azul ahí.

90
00:02:55,260 --> 00:02:56,660
De forma que, como saben, parece que sería una muy buena aproximación.

91
00:02:58,220 --> 00:02:59,450
Matemáticamente, la pendiente de esta

92
00:02:59,670 --> 00:03:01,340
línea roja es esta altura

93
00:03:01,890 --> 00:03:03,680
vertical, dividida por esta

94
00:03:03,890 --> 00:03:05,580
anchura horizontal, así que este

95
00:03:05,840 --> 00:03:07,500
punto en la parte superior es J de

96
00:03:08,920 --> 00:03:10,840
«theta» más «épsilon» Este punto

97
00:03:11,140 --> 00:03:13,020
aquí es J de «theta» menos «épsilon».

98
00:03:13,830 --> 00:03:15,450
De forma que esta diferencia vertical es J

99
00:03:15,670 --> 00:03:17,530
de «theta» más «épsilon», menos J

100
00:03:17,810 --> 00:03:18,810
de «theta», menos «épsilon» y

101
00:03:19,700 --> 00:03:21,730
esta distancia horizontal es sólo 2 «épsilon».

102
00:03:23,620 --> 00:03:25,340
Así que, mi aproximación va

103
00:03:25,410 --> 00:03:27,280
a ser la derivada,

104
00:03:29,110 --> 00:03:30,160
con respecto a «theta» de J de

105
00:03:30,490 --> 00:03:32,170
«theta»-- añadan este valor de

106
00:03:32,320 --> 00:03:34,950
«theta»-- que eso es aproximadamente J

107
00:03:35,150 --> 00:03:36,860
de «theta» más «épsilon», menos

108
00:03:37,460 --> 00:03:40,600
J de «theta», menos «épsilon», sobre 2 «épsilon».

109
00:03:42,280 --> 00:03:43,330
Por lo general, utilizo un valor

110
00:03:43,600 --> 00:03:44,790
muy pequeño para  «épsilon» y

111
00:03:45,040 --> 00:03:46,270
establecemos que «épsilon» tal vez se encuentre

112
00:03:46,530 --> 00:03:48,220
en el orden de 10 a menos 4.

113
00:03:48,740 --> 00:03:49,890
Por lo general existe un rango amplio

114
00:03:50,190 --> 00:03:52,280
de distintos valores para «épsilon» que funcionan muy bien.

115
00:03:53,050 --> 00:03:54,470
De hecho, si ustedes

116
00:03:55,280 --> 00:03:56,540
dejan que «épsilon» sea demasiado pequeño

117
00:03:57,010 --> 00:03:58,580
entonces, matemáticamente, este término de aquí

118
00:03:59,210 --> 00:04:00,790
de hecho, matemáticamente, como saben,

119
00:04:01,000 --> 00:04:02,340
se convierte en la derivada, se vuelve con exactitud

120
00:04:02,860 --> 00:04:04,310
en la pendiente de la función en este punto.

121
00:04:05,050 --> 00:04:05,730
Es sólo que no queremos

122
00:04:05,910 --> 00:04:06,980
utilizar «épsilon», que es demasiado, demasiado

123
00:04:07,170 --> 00:04:09,630
pequeño porque entonces podríamos encontrarnos con problemas numéricos.

124
00:04:10,130 --> 00:04:11,070
Entonces, como saben, por lo general utilizo

125
00:04:11,380 --> 00:04:14,200
«épsilon» alrededor de 10 a menos 4, digamos.

126
00:04:14,470 --> 00:04:15,220
Y por cierto que algunos de ustedes pueden

127
00:04:15,330 --> 00:04:17,590
haber visto una fórmula alternativa para

128
00:04:17,750 --> 00:04:19,710
estimar la derivada que es esta fórmula.

129
00:04:21,590 --> 00:04:23,500
La que se encuentra en el lado derecho se llama de diferencia unilateral.

130
00:04:24,040 --> 00:04:26,580
Mientras que la fórmula de la izquierda se llama de diferencia bilateral.

131
00:04:27,120 --> 00:04:28,670
La diferencia bilateral nos

132
00:04:28,890 --> 00:04:29,750
proporciona una estimación ligeramente más precisa,

133
00:04:30,170 --> 00:04:31,410
de forma que por lo general utilizo esa en lugar

134
00:04:31,670 --> 00:04:33,540
de sólo esta estimación de diferencia unilateral.

135
00:04:35,900 --> 00:04:37,280
Así, en concreto,  lo que implementan

136
00:04:37,750 --> 00:04:39,280
en Octava es la implementación de lo siguiente.

137
00:04:40,270 --> 00:04:41,490
Implementan el proceso de aproximación de la gradiente

138
00:04:41,600 --> 00:04:43,160
que va a ser

139
00:04:43,270 --> 00:04:44,590
tan sólo una aproximación a la derivada,

140
00:04:45,380 --> 00:04:46,820
como saben, en esta fórmula: J de

141
00:04:47,200 --> 00:04:48,550
«theta» más «épsilon», menos J de «theta»,

142
00:04:48,730 --> 00:04:50,800
menos «épsilon», dividido por dos veces «épsilon».

143
00:04:52,060 --> 00:04:52,980
Y esto nos proporcionará una

144
00:04:53,100 --> 00:04:56,110
estimación numérica de la gradiente en ese punto.

145
00:04:56,590 --> 00:04:58,910
Y en este ejemplo parece ser una estimación bastante buena.

146
00:05:01,970 --> 00:05:03,460
Ahora, en la diapositiva anterior

147
00:05:03,710 --> 00:05:05,040
consideramos el caso de

148
00:05:05,290 --> 00:05:07,010
cuando «theta» es un número real.

149
00:05:08,000 --> 00:05:08,670
Ahora veamos un caso más

150
00:05:08,900 --> 00:05:11,650
general donde «theta» es un parámetro  vector.

151
00:05:12,220 --> 00:05:13,270
Así que digamos que «theta» es una

152
00:05:13,520 --> 00:05:14,610
Rn y pudiera ser una versión

153
00:05:15,000 --> 00:05:16,510
irreal de los parámetros de

154
00:05:16,610 --> 00:05:18,010
nuestra red neuronal. De modo que

155
00:05:18,250 --> 00:05:19,580
«theta» es un vector que

156
00:05:19,800 --> 00:05:21,230
tiene "n" elementos, «theta» 1

157
00:05:21,350 --> 00:05:25,100
hasta «theta» n. Entonces podemos

158
00:05:25,240 --> 00:05:26,530
utilizar un concepto similar

159
00:05:27,080 --> 00:05:29,300
para aproximar todos los términos de la derivada parcial.

160
00:05:30,250 --> 00:05:31,730
En concreto,  la derivada parcial

161
00:05:32,420 --> 00:05:33,840
de la función de costos con respecto

162
00:05:34,110 --> 00:05:35,710
al primer parámetro de «theta» 1,

163
00:05:36,110 --> 00:05:37,270
esa puede

164
00:05:37,410 --> 00:05:40,270
obtenerse al tomar J e incrementar «theta» 1.

165
00:05:40,380 --> 00:05:43,030
Entonces tienen J de «theta» 1 más «épsilon» y así sucesivamente

166
00:05:43,520 --> 00:05:44,780
menos J de esta «theta» 1

167
00:05:45,520 --> 00:05:46,820
menos «épsilon» y dividida por 2 «épsilon».

168
00:05:48,130 --> 00:05:49,660
La derivada parcial respecto al

169
00:05:49,740 --> 00:05:51,090
segundo parámetro «theta» 2, es

170
00:05:51,620 --> 00:05:53,130
de nuevo esto, excepto que ustedes

171
00:05:53,270 --> 00:05:54,370
toman J de-- aquí están

172
00:05:54,740 --> 00:05:56,240
incrementando «theta» 2 por «épsilon».

173
00:05:56,570 --> 00:05:58,290
Y aquí ustedes están disminuyendo «theta» 2 por «épsilon».

174
00:05:59,100 --> 00:06:00,170
Y así hasta llegar a la

175
00:06:00,260 --> 00:06:01,680
derivada parcial con respecto a

176
00:06:01,780 --> 00:06:02,780
«theta» n. Lo que sería si ustedes

177
00:06:03,030 --> 00:06:04,550
incrementan y disminuyen «theta» n

178
00:06:05,060 --> 00:06:06,140
por «épsilon» allí.

179
00:06:09,790 --> 00:06:11,550
Así, estas ecuaciones les proporcionan

180
00:06:11,720 --> 00:06:13,580
una forma para aproximar en forma numérica

181
00:06:14,690 --> 00:06:16,500
la derivada parcial de "J"

182
00:06:17,250 --> 00:06:20,100
con respecto a cualquiera de los parámetros que se derivan.

183
00:06:23,640 --> 00:06:26,030
En concreto, lo que implementarán es por lo tanto, lo siguiente.

184
00:06:27,900 --> 00:06:29,260
Implementamos lo siguiente en Octave

185
00:06:29,820 --> 00:06:31,000
para ejecutar en forma numérica las derivadas.

186
00:06:32,220 --> 00:06:33,670
Decimos que i es igual a 1

187
00:06:33,790 --> 00:06:35,110
hasta n donde "n" es

188
00:06:35,310 --> 00:06:37,140
la dimensión de nuestro parámetro vector «theta».

189
00:06:37,730 --> 00:06:40,680
Y por lo general hago esto con la versión desarrollada de los parámetros.

190
00:06:41,250 --> 00:06:42,210
De forma que saben que «theta» es sólo

191
00:06:42,530 --> 00:06:44,770
una lista larga de todos mis parámetros en mis redes neuronales.

192
00:06:46,230 --> 00:06:47,550
Voy a establecer que «theta» más es igual

193
00:06:47,830 --> 00:06:49,270
a «theta», a continuación incrementen «theta» más

194
00:06:49,630 --> 00:06:51,170
el elemento «i-nésimo» por ««épsilon»».

195
00:06:51,660 --> 00:06:53,010
Y esto es básicamente,

196
00:06:53,720 --> 00:06:54,830
«theta» más que es igual a «theta»

197
00:06:55,340 --> 00:06:56,280
excepto para «theta» más i,

198
00:06:56,580 --> 00:06:57,820
que ahora está incrementada por «épsilon».

199
00:06:58,310 --> 00:06:59,400
De forma que si «theta» más

200
00:07:00,810 --> 00:07:01,880
es igual a,  correcto, «theta»

201
00:07:01,970 --> 00:07:03,370
1, «theta» 2 y así sucesivamente y luego  «theta»

202
00:07:04,020 --> 00:07:05,160
i tiene un «épsilon» añadido,

203
00:07:05,350 --> 00:07:06,590
y luego sigue hasta

204
00:07:06,780 --> 00:07:08,440
«theta» n. Así que en esto consiste «theta» más.

205
00:07:08,690 --> 00:07:11,340
Y del mismo modo estas dos

206
00:07:11,530 --> 00:07:13,380
líneas establecen a «theta» menos en

207
00:07:13,480 --> 00:07:15,090
algo similar excepto que

208
00:07:15,560 --> 00:07:16,720
esto, en lugar de «theta» i

209
00:07:16,930 --> 00:07:19,150
más «épsilon», esto ahora se convierte en «theta» i menos «épsilon».

210
00:07:20,670 --> 00:07:22,320
Y entonces finalmente, implementamos

211
00:07:22,830 --> 00:07:24,370
esta aproximación de la gradiente i,

212
00:07:25,190 --> 00:07:26,430
y esto les dará

213
00:07:27,210 --> 00:07:28,420
la aproximación a la derivada

214
00:07:28,800 --> 00:07:30,250
derivada parcial con respecto a

215
00:07:30,430 --> 00:07:32,430
«theta» i de J de «theta».

216
00:07:35,330 --> 00:07:36,420
Y la forma en que utilizamos esto

217
00:07:36,760 --> 00:07:38,530
en nuestra complementación de la red neuronal es que

218
00:07:38,850 --> 00:07:41,530
queremos poner en práctica esto, implementar estos

219
00:07:41,770 --> 00:07:43,310
cuatro bucles para procesar, como saben, la derivada

220
00:07:44,080 --> 00:07:45,570
parcial de la función

221
00:07:45,860 --> 00:07:48,570
de costos con respecto a cada parámetro en nuestra red.

222
00:07:49,450 --> 00:07:51,120
Y podemos tomar el

223
00:07:51,350 --> 00:07:53,070
gradiente que obtuvimos de la retropropagación.

224
00:07:53,740 --> 00:07:55,110
Así, los vectores DVec fueron las derivadas

225
00:07:55,770 --> 00:07:57,150
que obtuvimos de la retropropagación.

226
00:07:58,380 --> 00:08:00,610
Correcto, así la retropropagación, la retropropagación fue

227
00:08:00,890 --> 00:08:02,030
una forma relativamente eficiente para

228
00:08:02,090 --> 00:08:03,350
procesar las derivadas o las

229
00:08:03,430 --> 00:08:04,970
derivadas parciales de una

230
00:08:05,110 --> 00:08:06,850
función de costos con respecto a todos nuestros parámetros.

231
00:08:07,820 --> 00:08:08,960
Y lo que por lo general hago

232
00:08:09,350 --> 00:08:10,820
es que tomo mi derivada procesada

233
00:08:11,440 --> 00:08:12,830
en forma numérica, que es

234
00:08:12,960 --> 00:08:14,080
esta aproximación de la gradiente que

235
00:08:14,250 --> 00:08:15,830
acabamos de obtener de aquí arriba y

236
00:08:15,920 --> 00:08:17,030
nos aseguramos de que esa sea

237
00:08:17,290 --> 00:08:19,420
igual o aproximadamente igual

238
00:08:19,980 --> 00:08:21,080
a, como saben, a los valores pequeños

239
00:08:21,810 --> 00:08:22,770
del redondeo numérico que están

240
00:08:22,970 --> 00:08:25,640
bastante cercanos a los vectores DVec que obtuve de la retropropagación.

241
00:08:26,510 --> 00:08:27,460
Y si estas dos formas

242
00:08:27,930 --> 00:08:29,550
de procesar la derivada me dan

243
00:08:29,650 --> 00:08:31,040
la misma respuesta o al menos me dan

244
00:08:31,300 --> 00:08:33,670
respuestas muy parecidas, como saben, hasta unas plazas decimales,

245
00:08:34,720 --> 00:08:36,560
Entonces tengo confianza de que

246
00:08:36,710 --> 00:08:38,720
es correcta mi implementación de retropropagación.

247
00:08:40,000 --> 00:08:41,230
Y cuando conecto estos vectores DVec

248
00:08:41,660 --> 00:08:43,320
a la gradiente de descenso

249
00:08:43,760 --> 00:08:45,610
o a algún algoritmo de optimización avanzada,

250
00:08:45,760 --> 00:08:46,850
puedo entonces estar mucho

251
00:08:47,100 --> 00:08:48,870
más seguro de que estoy calculando

252
00:08:49,360 --> 00:08:51,010
las derivadas correctamente y por lo tanto,

253
00:08:51,450 --> 00:08:52,670
con suerte mis códigos funcionarán

254
00:08:52,790 --> 00:08:53,890
correctamente y harán un

255
00:08:53,980 --> 00:08:55,570
buen trabajo optimizando J de «theta».

256
00:08:57,700 --> 00:08:58,680
Finalmente,  deseo resumir

257
00:08:58,860 --> 00:09:00,050
todo y decirles

258
00:09:00,310 --> 00:09:02,950
cómo implementar esta comprobación de la gradiente numérica.

259
00:09:03,630 --> 00:09:04,370
Esto es lo que suelo hacer.

260
00:09:04,970 --> 00:09:06,020
La primer cosa que hago es implementar

261
00:09:06,500 --> 00:09:08,180
la retropropagación para procesar los defectos.

262
00:09:08,490 --> 00:09:09,560
También, este es un procedimiento del cual hablamos

263
00:09:09,830 --> 00:09:11,250
en un video anterior para

264
00:09:11,490 --> 00:09:13,530
procesar el DVec que puede ser nuestra versión desarrollada de estas matrices.

265
00:09:15,410 --> 00:09:16,550
Entonces lo que hago es implementar

266
00:09:17,010 --> 00:09:20,130
una comprobación de la gradiente numérica para calcular la aproximación de la gradiente.

267
00:09:20,590 --> 00:09:23,550
Así que esto es lo que he descrito antes en este video, en la diapositiva anterior.

268
00:09:24,900 --> 00:09:27,680
Después deberán asegurarse de que el DVec y la aproximación de la gradiente

269
00:09:28,170 --> 00:09:30,860
den valores similares, como ya saben, digamos que hasta unos cuantas plazas decimales.

270
00:09:32,270 --> 00:09:33,160
Y finalmente, y este es

271
00:09:33,240 --> 00:09:35,230
el paso importante, entre más

272
00:09:35,480 --> 00:09:36,690
empiecen a utilizar su código

273
00:09:37,000 --> 00:09:38,220
para el aprendizaje, para entrenar en serio

274
00:09:38,570 --> 00:09:40,960
su red, es importante desactivar la comprobación de gradiente.

275
00:09:41,490 --> 00:09:42,800
Y para no procesar ya

276
00:09:43,630 --> 00:09:44,940
esta aproximación de la gradiente utilizando

277
00:09:45,250 --> 00:09:47,660
las fórmulas de las derivadas numéricas de las

278
00:09:47,980 --> 00:09:48,950
que hablamos anteriormente en este

279
00:09:50,560 --> 00:09:50,560
video.

280
00:09:50,960 --> 00:09:52,180
Y la razón de esto es que--

281
00:09:52,330 --> 00:09:53,800
el código numérico, el código de la comprobación de gradiente,

282
00:09:54,120 --> 00:09:54,930
las cosas de las que hablamos en

283
00:09:55,010 --> 00:09:56,220
este video--  es muy

284
00:09:56,650 --> 00:09:58,570
caro computacionalmente, esa es una

285
00:09:58,600 --> 00:10:00,960
forma muy lenta de tratar de aproximar la derivada.

286
00:10:02,080 --> 00:10:03,490
En contraste con el algoritmo de

287
00:10:03,900 --> 00:10:04,710
retropropagación del que hablamos

288
00:10:04,940 --> 00:10:06,120
antes, que es de

289
00:10:06,370 --> 00:10:07,270
lo que hemos hablado antes

290
00:10:07,460 --> 00:10:08,900
para procesar, como saben, D1, D2,

291
00:10:09,320 --> 00:10:11,620
D3 o para DVec. La retropropagación es

292
00:10:11,790 --> 00:10:14,930
una forma mucho más eficiente computacionalmente de procesar las derivadas.

293
00:10:17,070 --> 00:10:18,650
De forma que una vez que hayan verificado que

294
00:10:18,770 --> 00:10:20,270
la implementación de su retropropagación es

295
00:10:20,620 --> 00:10:21,840
correcta, deberán desconectar

296
00:10:22,160 --> 00:10:24,140
la comprobación de la gradiente y sólo dejar de utilizarla.

297
00:10:25,090 --> 00:10:26,380
Sólo para recapitular, deberán

298
00:10:26,540 --> 00:10:27,720
estar seguros de deshabilitar el

299
00:10:27,840 --> 00:10:29,380
código de comprobación de la gradiente antes de  ejecutar

300
00:10:29,690 --> 00:10:30,840
su algoritmo para muchas

301
00:10:31,140 --> 00:10:32,560
iteraciones de la gradiente de descenso, o

302
00:10:32,670 --> 00:10:33,690
para muchas iteraciones de los

303
00:10:33,890 --> 00:10:34,990
algoritmos de optimización avanzada con

304
00:10:35,820 --> 00:10:37,140
el fin de entrenar su clasificador.

305
00:10:37,980 --> 00:10:39,120
Específicamente, si fueran

306
00:10:39,290 --> 00:10:40,830
a ejecutar la comprobación de la gradiente numérica

307
00:10:41,340 --> 00:10:43,710
en cada iteración de la gradiente de

308
00:10:44,040 --> 00:10:44,650
descenso, o si estuvieran en el

309
00:10:44,850 --> 00:10:45,780
bucle interno de la función de costos,

310
00:10:46,670 --> 00:10:47,910
entonces el código sería muy lento.

311
00:10:48,240 --> 00:10:49,860
Porque el código de la comprobación de la

312
00:10:50,180 --> 00:10:51,690
gradiente numérica es mucho más lento que el

313
00:10:51,900 --> 00:10:53,960
algoritmo de retropropagación,  que el

314
00:10:54,160 --> 00:10:56,160
método de retropropagación donde si

315
00:10:56,340 --> 00:10:57,650
recuerdan estuvimos procesando «delta»

316
00:10:58,000 --> 00:10:59,820
4, «delta» 2, «delta» 2 y así sucesivamente.

317
00:10:59,900 --> 00:11:02,470
Ese fue el algoritmo de retropropagación.

318
00:11:02,990 --> 00:11:05,770
Esa es una forma mucho más rápida de procesar las derivadas de la comprobación de la gradiente.

319
00:11:06,620 --> 00:11:08,400
Entonces cuando estén listos, una vez

320
00:11:08,620 --> 00:11:10,190
que han verificado que la implementación de la retropropagación

321
00:11:10,480 --> 00:11:12,140
es correcta, asegúrense

322
00:11:12,220 --> 00:11:13,050
de desactivar o deshabilitar

323
00:11:13,640 --> 00:11:15,070
el código de comprobación de la gradiente mientras

324
00:11:15,270 --> 00:11:17,880
entrenan su algoritmo o de lo contrario el código funcionará de manera muy lenta.

325
00:11:20,420 --> 00:11:22,470
Entonces así es como obtienen las gradientes en forma numérica

326
00:11:23,110 --> 00:11:24,300
Y así es cómo ustedes pueden verificar que

327
00:11:24,420 --> 00:11:26,300
la implementación de la retropropagación es correcta.

328
00:11:27,230 --> 00:11:29,290
Cuando implemento la retropropagación o

329
00:11:29,450 --> 00:11:31,130
un algoritmo de gradiente de descenso similar para

330
00:11:31,250 --> 00:11:33,410
un modelo complicado, siempre utilizo la comprobación de gradiente.

331
00:11:33,730 --> 00:11:36,230
Esto realmente me ayuda a estar seguro de que mi código es correcto.