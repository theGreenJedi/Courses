1
00:00:00,240 --> 00:00:01,560
तो,लगें हैं हमें

2
00:00:01,700 --> 00:00:02,690
बहुत से वीडियो समझने के लिए

3
00:00:03,120 --> 00:00:04,480
न्यूरल नेटवर्क लर्निंग अल्गोरिद्म.

4
00:00:05,620 --> 00:00:06,640
इस वीडियो में, मैं क्या चाहूँगा

5
00:00:06,800 --> 00:00:08,090
करना कि कोशिश करूँ

6
00:00:08,350 --> 00:00:10,040
रखने की सभी हिस्सों को एक साथ,

7
00:00:10,370 --> 00:00:12,120
देने के लिए एक समग्र सारांश

8
00:00:12,360 --> 00:00:13,410
एक पूरी तस्वीर, कि कैसे

9
00:00:13,650 --> 00:00:15,290
सारे टुकड़े जुड़ते हैं एक साथ और

10
00:00:15,530 --> 00:00:16,990
पूरी प्रक्रिया कि कैसे

11
00:00:17,260 --> 00:00:18,830
इम्प्लमेंट करते हैं एक न्यूरल नेटवर्क लर्निंग अल्गोरिद्म.

12
00:00:21,870 --> 00:00:23,210
जब ट्रेन कर रहे हैं एक न्यूरल नेटवर्क को,

13
00:00:23,280 --> 00:00:24,290
पहला काम जो आपको करने की ज़रूरत है

14
00:00:24,400 --> 00:00:25,920
चुनें कोई नेटवर्क आर्किटेक्चर

15
00:00:26,680 --> 00:00:27,950
और आर्किटेक्चर से मेरा सिर्फ़

16
00:00:28,200 --> 00:00:30,510
मतलब है न्यूरॉन्स के बीच कनेक्टिविटी पैटर्न.

17
00:00:31,080 --> 00:00:31,840
तो, आप जानते हैं, हम चुन सकते हैं

18
00:00:32,700 --> 00:00:33,770
मान लो, एक न्यूरल नेटवर्क

19
00:00:34,230 --> 00:00:35,440
जिसमें तीन इनपुट यूनिट्स हैं

20
00:00:35,960 --> 00:00:37,400
और पाँच हिडन यूनिट्स हैं और

21
00:00:37,500 --> 00:00:39,560
चार आउट्पुट यूनिट्स या एक

22
00:00:39,800 --> 00:00:41,460
जिसमें 3 हिडन लेयर्स हैं, 5

23
00:00:41,700 --> 00:00:43,430
यूनिट्स प्रत्येक हिडन लेयर में, 4 आउट्पुट और

24
00:00:43,910 --> 00:00:45,220
यहाँ है 3, 5,

25
00:00:45,550 --> 00:00:47,060
5, 5 यूनिट्स प्रत्येक

26
00:00:47,320 --> 00:00:48,870
इन तीन हिडन लेयर्स की और चार

27
00:00:49,120 --> 00:00:50,250
आउट्पुट यूनिट्स, और इसलिए ये

28
00:00:50,430 --> 00:00:52,000
चुनाव कि कितनी हिडन

29
00:00:52,270 --> 00:00:53,410
यूनिट्स प्रत्येक लेयर में

30
00:00:53,810 --> 00:00:55,560
और कितनी हिडन लेयर्स, वे

31
00:00:55,780 --> 00:00:57,580
हैं आर्किटेक्चर के विकल्प.

32
00:00:57,910 --> 00:00:58,680
तो, कैसे आप करते हैं चुनाव?

33
00:00:59,710 --> 00:01:01,270
ठीक है, पहले, संख्या

34
00:01:01,530 --> 00:01:03,840
इनपुट यूनिट्स की वह है परिभाषित सही ढंग से.

35
00:01:04,680 --> 00:01:05,960
और एक बार आपने तय कर लिया निश्चित

36
00:01:06,580 --> 00:01:07,870
सेट x फ़ीचर्ज़ का

37
00:01:08,080 --> 00:01:09,420
इनपुट यूनिट्स की संख्या फिर होगी, आप जानते हैं,

38
00:01:10,140 --> 00:01:12,180
डिमेन्शन आपके फ़ीचर x(i)

39
00:01:12,330 --> 00:01:14,470
निर्धारित उससे.

40
00:01:14,760 --> 00:01:15,970
और यदि आप कर रहे हैं मल्टीक्लास

41
00:01:16,210 --> 00:01:17,370
क्लैसिफ़िकेशन, संख्या

42
00:01:17,520 --> 00:01:18,320
यूनिट्स की होगी

43
00:01:18,420 --> 00:01:19,720
निर्धारित संख्या से

44
00:01:20,060 --> 00:01:22,860
क्लास की आपकी क्लैसिफ़िकेशन प्रॉब्लम की.

45
00:01:23,260 --> 00:01:24,890
और बस एक चेतावनी कि यदि आपके पास है

46
00:01:25,160 --> 00:01:27,290
मल्टीक्लास क्लैसिफ़िकेशन जहाँ y

47
00:01:27,570 --> 00:01:28,970
लेता है वैल्यूज़ मान लो

48
00:01:30,040 --> 00:01:31,350
1 से 10 के बीच, तो

49
00:01:31,470 --> 00:01:33,560
आपके पास हैं दस संभव क्लास.

50
00:01:34,690 --> 00:01:37,200
तब याद रखें लिखना, आपकी

51
00:01:37,820 --> 00:01:39,340
आउट्पुट y को वेक्टर्स की तरह.

52
00:01:40,130 --> 00:01:41,560
तो बजाय क्लास एक के, आप

53
00:01:41,730 --> 00:01:42,840
लिखते हैं इसे एक वेक्टर जैसे

54
00:01:43,150 --> 00:01:44,600
इस तरह, या

55
00:01:44,670 --> 00:01:47,280
दूसरी क्लास के लिए आप लिखते हैं उसे एक वेक्टर इस तरह.

56
00:01:48,130 --> 00:01:49,080
तो यदि एक इन

57
00:01:49,210 --> 00:01:51,000
आउट्पुट लेता है

58
00:01:51,140 --> 00:01:53,910
पाँचवी क्लास, आप जानते हैं y बराबर 5, तब

59
00:01:54,120 --> 00:01:55,130
जो आप दिखा रहे हैं आपके न्यूरल

60
00:01:55,380 --> 00:01:56,840
नेटवर्क को वास्तव में नहीं है एक वैल्यू

61
00:01:57,250 --> 00:01:59,520
y बराबर 5 की, इसेक स्थान पर यह

62
00:02:00,030 --> 00:02:00,950
आउट्पुट लेयर में जिसमें हैं

63
00:02:01,280 --> 00:02:02,650
दस आउट्पुट यूनिट्स, आप

64
00:02:02,740 --> 00:02:03,920
इसके स्थान पर फ़ीड करेंगे

65
00:02:04,070 --> 00:02:05,710
वेक्टर को जिसमें आप जानते हैं

66
00:02:07,470 --> 00:02:08,430
एक पाँचवे

67
00:02:08,770 --> 00:02:11,050
स्थान पर और बाक़ी सब ज़ीरो हैं.

68
00:02:11,420 --> 00:02:12,470
तो चुनाव संख्या का

69
00:02:12,890 --> 00:02:14,330
इनपुट यूनिट्स की और आउट्पुट यूनिट्स की संख्या

70
00:02:14,970 --> 00:02:16,600
है शायद कुछ हद तक काफ़ी सरल.

71
00:02:18,000 --> 00:02:18,950
और संख्या के लिए

72
00:02:19,410 --> 00:02:21,040
हिडन यूनिट्स की और

73
00:02:21,140 --> 00:02:23,110
संख्या हिडन लेयर्स की, एक

74
00:02:23,210 --> 00:02:24,350
उचित डिफ़ॉल्ट है

75
00:02:24,540 --> 00:02:26,010
कि लें एक ही हिडन लेयर

76
00:02:26,660 --> 00:02:28,040
और इस तरह का

77
00:02:28,880 --> 00:02:30,400
न्यूरल नेटवर्क दिखाया हुआ बाईं तरफ़ जिसमें है

78
00:02:30,580 --> 00:02:33,270
केवल एक हिडन लेयर है सम्भवत: काफ़ी आम है.

79
00:02:34,490 --> 00:02:35,870
या यदि आप लेते हैं अधिक

80
00:02:36,140 --> 00:02:38,410
हिडन लेयर्स, फिर से

81
00:02:38,670 --> 00:02:39,600
उचित डिफ़ॉल्ट होगा

82
00:02:39,760 --> 00:02:40,950
लें समान संख्या

83
00:02:41,130 --> 00:02:42,560
हिडन यूनिट्स की प्रत्येक लेयर में.

84
00:02:42,810 --> 00:02:44,600
तो, यहाँ हमारे पास एक है दो

85
00:02:45,020 --> 00:02:46,370
हिडन लेयर्स और प्रत्येक में

86
00:02:46,610 --> 00:02:47,650
इन हिडन लेयर्स में हैं

87
00:02:47,860 --> 00:02:49,500
समान संख्या पाँच हिडन

88
00:02:49,790 --> 00:02:50,740
यूनिट्स की और यहाँ हमारे पास है, आप जानते हैं,

89
00:02:51,600 --> 00:02:53,020
तीन हिडन लेयर्स और

90
00:02:53,170 --> 00:02:54,790
इनमें से प्रत्येक में हैं समान

91
00:02:54,980 --> 00:02:56,400
संख्या, जो है पाँच हिडन यूनिट्स.

92
00:02:57,440 --> 00:02:59,440
बजाय लेने के इस तरह का

93
00:02:59,740 --> 00:03:02,850
नेटवर्क, आर्किटेक्चर बाईं तरफ़ का सटीक उचित डिफ़ॉल्ट.

94
00:03:04,020 --> 00:03:04,780
और संख्या के लिए

95
00:03:05,120 --> 00:03:07,040
हिडन यूनिट्स की - अक्सर

96
00:03:07,120 --> 00:03:08,100
जितनी अधिक हिडन यूनिट्स उतना बेहतर;

97
00:03:08,560 --> 00:03:09,640
सिर्फ़ इतना कि यदि आपके पास हैं

98
00:03:09,900 --> 00:03:11,110
बहुत सी हिडन यूनिट्स, यह

99
00:03:11,330 --> 00:03:13,150
हो सकता है कॉम्प्यूटेशनली अधिक महँगा, लेकिन

100
00:03:13,300 --> 00:03:15,850
बहुधा, होना अधिक हिडन यूनिट्स अच्छी बात है.

101
00:03:17,250 --> 00:03:18,560
और अक्सर संख्या हिडन

102
00:03:18,720 --> 00:03:20,820
यूनिट्स की प्रत्येक लेयर में होगी शायद

103
00:03:21,080 --> 00:03:22,130
लगभग समान डिमेन्शन के

104
00:03:22,490 --> 00:03:23,670
x की, तुलनात्मक

105
00:03:23,810 --> 00:03:24,950
फ़ीचर्ज़ की संख्या के, या यह हो सकता है

106
00:03:25,140 --> 00:03:26,880
समान, संख्या के

107
00:03:27,180 --> 00:03:29,590
हिडन यूनिट्स से लेकर इनपुट फ़ीचर्ज़ तक से

108
00:03:29,770 --> 00:03:32,430
या शायद तीन या चार गुणा उसका.

109
00:03:32,680 --> 00:03:34,770
तो रखना हिडन यूनिट्स है तुलनात्मक.

110
00:03:35,140 --> 00:03:36,350
आप जानते हैं, कई गुणा, या

111
00:03:36,410 --> 00:03:37,380
कुछ बड़ा संख्या से

112
00:03:37,430 --> 00:03:38,750
इनपुट फ़ीचर्ज़ की अक्सर

113
00:03:39,280 --> 00:03:41,320
एक उपयोगी चीज़ है वैसा करना. तो,

114
00:03:42,150 --> 00:03:43,490
उम्मीद है यह देता है आपको एक

115
00:03:43,810 --> 00:03:45,140
उचित सेट डिफ़ॉल्ट विकल्पों का

116
00:03:45,650 --> 00:03:47,770
न्यूरल आर्किटेक्चर के लिए और

117
00:03:48,200 --> 00:03:49,460
यदि आप पालन करते हैं इन निर्देशों का, आपको

118
00:03:49,540 --> 00:03:50,580
शायद मिले कुछ जो करे

119
00:03:50,930 --> 00:03:52,180
बेहतर, लेकिन एक

120
00:03:52,360 --> 00:03:53,770
बाद के सेट में वीडियोज़ के जहाँ

121
00:03:54,050 --> 00:03:55,270
मैं बात करूँगा विशेष रूप से

122
00:03:55,580 --> 00:03:56,900
सलाह की कि कैसे अप्लाई करने है

123
00:03:57,410 --> 00:03:58,770
अल्गोरिद्म्स, मैं वास्तव में

124
00:03:58,840 --> 00:04:01,880
बताऊँगा और अधिक कि कैसे चुनना है एक न्यूरल नेटवर्क आर्किटेक्चर.

125
00:04:02,540 --> 00:04:03,920
वास्तव में है काफ़ी

126
00:04:03,970 --> 00:04:04,960
कुछ जो मैं चाहता हूँ

127
00:04:04,960 --> 00:04:06,180
बाद में बताना लेने के लिए अच्छे विकल्प

128
00:04:06,710 --> 00:04:08,780
हिडन यूनिट्स की संख्या के लिए, हिडन लेयर्स की संख्या के लिए, और इसी तरह और कुछ.

129
00:04:10,620 --> 00:04:12,310
इसके बाद, यहाँ है कि हमें क्या

130
00:04:12,420 --> 00:04:13,740
इम्प्लमेंट करने की ज़रूरत है करने के लिए

131
00:04:13,860 --> 00:04:15,360
ट्रेन एक न्यूरल नेटवर्क को, हैं

132
00:04:15,510 --> 00:04:16,820
वास्तव में छः स्टेप्स जो मेरे पास

133
00:04:17,080 --> 00:04:18,030
हैं, मेरे पास चार हैं इस

134
00:04:18,160 --> 00:04:19,100
स्लाइड पर और दो और स्टेप्स हैं

135
00:04:19,380 --> 00:04:21,480
अगली स्लाइड पर.

136
00:04:21,620 --> 00:04:22,220
पहला स्टेप है सेट अप करना न्यूरल

137
00:04:22,430 --> 00:04:23,510
नेटवर्क को और रैंडम ढंग से

138
00:04:24,080 --> 00:04:25,570
देना प्रारम्भिक वैल्यूज़ वेट्स को.

139
00:04:25,790 --> 00:04:27,000
और हम आम तौर पर प्रारंभ करते हैं

140
00:04:27,080 --> 00:04:29,710
वेट्स छोटी वैल्यूज़ से ज़ीरो के क़रीब की.

141
00:04:31,100 --> 00:04:33,120
फिर हम इम्प्लमेंट करते हैं फ़ॉर्वर्ड प्रॉपगेशन

142
00:04:34,080 --> 00:04:35,060
ताकि हम इनपुट कर सकें

143
00:04:35,480 --> 00:04:37,150
फ़ीचर्ज़ न्यूरल नेटवर्क को और

144
00:04:37,490 --> 00:04:38,860
कम्प्यूट करें h ऑफ़ x जो है यह

145
00:04:39,070 --> 00:04:40,820
आउट्पुट वेक्टर y वैल्यूज़ का.

146
00:04:44,260 --> 00:04:45,910
हम फिर इम्प्लमेंट करते हैं कोड भी

147
00:04:46,010 --> 00:04:47,500
कम्प्यूट करने के लिए यह कॉस्ट फ़ंक्शन जे ऑफ़ थीटा.

148
00:04:49,770 --> 00:04:51,160
और उसके बाद हम इम्प्लमेंट करते हैं

149
00:04:52,120 --> 00:04:53,330
बैक-प्रॉप या बैक प्रॉपगेशन

150
00:04:54,400 --> 00:04:55,680
अल्गोरिद्म का, कम्प्यूट करने के लिए ये

151
00:04:55,910 --> 00:04:58,000
पार्शियल डेरिवेटिव टर्म्ज़, पर्शियल

152
00:04:58,440 --> 00:04:59,830
डेरिवेटिव्स जे ऑफ़ थीटा के

153
00:05:00,340 --> 00:05:04,240
विद रिस्पेक्ट टु पेरमिटर्स. वास्तव में इम्प्लमेंट करने के लिए बैक प्रॉप,

154
00:05:04,960 --> 00:05:05,880
अक्सर हम करेंगे वह

155
00:05:06,250 --> 00:05:08,460
एक फ़ॉर लूप से ट्रेनिंग इग्ज़ाम्पल्ज़ पर.

156
00:05:09,700 --> 00:05:10,650
आप में कुछ ने शायद सुना हो

157
00:05:10,830 --> 00:05:12,640
उन्नत / एडवांस्ड , और स्पष्ट रूप से बहुत

158
00:05:12,940 --> 00:05:14,500
एडवांस्ड फ़ैक्टर करने की विधि के बारे में जहाँ आप को

159
00:05:14,670 --> 00:05:15,720
नहीं करना पड़ता एक फ़ॉर-लूप

160
00:05:16,570 --> 00:05:18,580
m ट्रेनिंग इग्ज़ाम्पल्ज़ पर, परंतु

161
00:05:18,660 --> 00:05:19,900
पहली बार जब आप इम्प्लमेंट कर रहे हैं बैक प्रॉप

162
00:05:20,250 --> 00:05:21,420
वहाँ लगभग निश्चित रूप से होना चाहिए फ़ॉर

163
00:05:21,420 --> 00:05:22,980
लूप आपके कोड में,

164
00:05:23,800 --> 00:05:25,010
जहाँ आप इटरेट कर रहे हैं इग्ज़ाम्पल्ज़ पर,

165
00:05:25,810 --> 00:05:27,760
आप जानते हैं, x1, y1, तब इसलिए

166
00:05:28,030 --> 00:05:29,510
आप करते हैं फ़ॉर्वर्ड प्रॉप और

167
00:05:29,640 --> 00:05:30,400
बैक-प्रॉप पहले

168
00:05:30,850 --> 00:05:32,510
इग्ज़ाम्पल पर, और फिर

169
00:05:32,710 --> 00:05:33,730
दूसरी इटरेशन में

170
00:05:33,780 --> 00:05:35,360
फ़ॉर लूप की, आप करते हैं फ़ॉर्वर्ड प्रॉपगेशन

171
00:05:35,980 --> 00:05:38,050
और बैक प्रॉपगेशन दूसरे इग्ज़ाम्पल पर, और इसी तरह आगे.

172
00:05:38,170 --> 00:05:40,900
जब तक पहुँच नहीं जाते अंतिम इग्ज़ाम्पल पर.

173
00:05:41,680 --> 00:05:43,110
तो वहाँ होना चाहिए

174
00:05:43,230 --> 00:05:44,250
एक फ़ॉर-लूप आपकी इम्प्लमेंटेशन में

175
00:05:45,050 --> 00:05:47,180
बैक प्रॉप की, कम से कम पहली बार जब आप इसे इम्प्लमेंट कर रहे हैं.

176
00:05:48,120 --> 00:05:49,160
और फिर वहाँ स्पष्ट रूप से हैं

177
00:05:49,390 --> 00:05:50,520
कुछ हद तक जटिल तरीके करने के लिए

178
00:05:50,890 --> 00:05:52,660
ऐसा बिना फ़ॉर-लूप के, लेकिन

179
00:05:52,810 --> 00:05:53,950
मैं निश्चित रूप से सलाह नहीं दूँगा

180
00:05:54,360 --> 00:05:55,340
करने की वह अधिक

181
00:05:55,660 --> 00:05:58,420
पेचीदा वर्ज़न पहली बार जब आप प्रयास कर रहे हैं इम्प्लमेंट करने का बैक प्रॉप.

182
00:05:59,850 --> 00:06:00,920
तो वस्तुतः, हमारे पास है एक

183
00:06:01,010 --> 00:06:02,200
फ़ॉर लूप मेरे m-ट्रेनिंग इग्ज़ाम्पल्ज़ पर

184
00:06:03,240 --> 00:06:04,630
इस फ़ॉर-लूप के अंदर हम

185
00:06:04,770 --> 00:06:06,300
करेंगे फ़ॉर्वर्ड प्रॉप

186
00:06:06,580 --> 00:06:08,090
तथा बैक-प्रॉप इस्तेमाल करके केवल एक इग्ज़ाम्पल.

187
00:06:09,310 --> 00:06:10,320
तो क्या मतलब है उससे कि

188
00:06:10,560 --> 00:06:12,470
हम लेंगे x(i), और

189
00:06:12,690 --> 00:06:14,010
फ़ीड करेंगे उसे मेरी इनपुट लेयर को,

190
00:06:14,770 --> 00:06:16,370
और करेंगे फ़ॉर्वर्ड-प्रॉप, करेंगे बैक-प्रॉप

191
00:06:17,370 --> 00:06:18,360
और वह देगा हमें सारे

192
00:06:18,430 --> 00:06:19,840
ये ऐक्टिवेशन्स और सारे

193
00:06:19,930 --> 00:06:22,090
डेल्टा टर्म्ज़ सारी

194
00:06:22,300 --> 00:06:23,440
लेयर्स का मेरी सारी

195
00:06:23,770 --> 00:06:24,720
यूनिट्स के न्यूरल

196
00:06:24,950 --> 00:06:27,170
नेटवर्क की तब अभी भी

197
00:06:27,610 --> 00:06:28,760
इस फ़ॉर-लूप के अंदर ही,

198
00:06:29,180 --> 00:06:30,450
मैं बनाता हूँ कुछ करली कोषठक

199
00:06:30,940 --> 00:06:31,950
सिर्फ़ दर्शाने के लिए स्कोप

200
00:06:32,030 --> 00:06:32,930
फ़ॉर-लूप का, यह है

201
00:06:34,160 --> 00:06:35,480
ओकटेव में निश्चय ही, लेकिन यह ज़्यादा एक जावा के जैसा ही

202
00:06:36,190 --> 00:06:38,350
कोड है, और एक फ़ॉर-लूप में यह सब शामिल है.

203
00:06:39,060 --> 00:06:40,060
हम करेंगे कम्प्यूट वे सब डेल्टा

204
00:06:40,480 --> 00:06:43,690
टर्म्ज़, जो वे फ़ॉर्म्युला हैं जो हमने पहले बताए थे.

205
00:06:45,540 --> 00:06:47,370
इसके अलावा, आप जानते हैं, डेल्टा i प्लस वन टाइम्ज़

206
00:06:48,630 --> 00:06:51,150
a i, ट्रान्स्पोज़.

207
00:06:51,490 --> 00:06:53,540
और फिर अंत में, बाहर

208
00:06:54,180 --> 00:06:55,630
कम्प्यूट कर लेने के बाद ये डेल्टा

209
00:06:55,970 --> 00:06:57,550
टर्म्ज़, ये संचित टर्म्ज़, हम

210
00:06:57,870 --> 00:06:59,050
फिर लिखेंगे कुछ और

211
00:06:59,170 --> 00:07:00,430
कोड और फिर वह

212
00:07:00,720 --> 00:07:03,240
कम्प्यूट करने देगा ये पर्शियल डेरिवेटिव टर्म्ज़.

213
00:07:03,860 --> 00:07:05,450
ठीक है और ये पर्शियल डेरिवेटिव

214
00:07:05,970 --> 00:07:07,020
टर्म्ज़ में लेना पड़ेगा

215
00:07:07,210 --> 00:07:10,270
रेगुलराइज़ेशन टर्म लैम्डा भी.

216
00:07:11,050 --> 00:07:13,240
और इसलिए, वे फ़ॉर्म्युला बताए थे पहले वीडियो में.

217
00:07:14,830 --> 00:07:15,720
तो, वैसा कर लेने के बाद

218
00:07:16,680 --> 00:07:18,080
आपके पास उम्मीद है होगा

219
00:07:18,180 --> 00:07:20,050
कम्प्यूट करने के लिए ये पर्शियल डेरिवेटिव टर्म्ज़.

220
00:07:21,190 --> 00:07:23,030
आगे है स्टेप पाँच, मैं क्या

221
00:07:23,240 --> 00:07:24,420
करता हूँ तब इस्तेमाल करता हूँ ग्रेडीयंट

222
00:07:24,730 --> 00:07:26,700
चेकिंग तुलना करने के लिए ये पर्शियल

223
00:07:27,120 --> 00:07:28,530
डेरिवेटिव टर्म्ज़ जो कम्प्यूट की थी. तो, मैंने

224
00:07:29,420 --> 00:07:30,980
तुलना कर ली है वैल्यूज़ की जो कम्प्यूट की थी

225
00:07:31,270 --> 00:07:33,990
बैक प्रॉपगेशन से और

226
00:07:34,430 --> 00:07:36,470
पार्शियल डेरिवेटिव्स जो कम्प्यूट किए थे नूमेरिकल

227
00:07:37,710 --> 00:07:39,850
अनुमान से डेरिवाटिव्स जो नूमेरिकल अनुमान थे.

228
00:07:40,350 --> 00:07:41,810
तो, मैं ग्रेडीयंट चेकिंग करता हूँ सुनिश्चित

229
00:07:41,970 --> 00:07:44,340
करने के लिए कि ये दोनो मुझे समान वैल्यू देते हैं.

230
00:07:45,830 --> 00:07:47,410
ग्रेडीयंट चेकिंग कर लेने के बाद हम आश्वस्त है

231
00:07:47,910 --> 00:07:49,280
कि हमारा इम्प्लमेंटेशन बैक

232
00:07:49,590 --> 00:07:51,470
प्रॉपगेशन का सही है, और

233
00:07:51,610 --> 00:07:52,850
तब बहुत महत्वपूर्ण है कि हम निष्क्रिय करें

234
00:07:53,530 --> 00:07:54,710
ग्रेडीयंट चेकिंग को, क्योंकि ग्रेडीयंट

235
00:07:55,080 --> 00:07:57,150
चेकिंग कोड कॉम्प्यूटेशनली बहुत धीरे चलता है.

236
00:07:59,020 --> 00:08:00,880
और अंत में, हम तब

237
00:08:01,120 --> 00:08:03,280
इस्तेमाल करके एक ऑप्टिमायज़ेशन अल्गोरिद्म जैसे

238
00:08:03,510 --> 00:08:04,940
ग्रेडीयंट डिसेंट, या एक

239
00:08:04,960 --> 00:08:07,520
एडवांस्ड ऑप्टिमायज़ेशन तरीक़ा, जैसे

240
00:08:07,740 --> 00:08:10,020
GS का LB, कॉंट्रैक्ट ग्रेडीयंट जिसमें है

241
00:08:10,250 --> 00:08:13,120
fminunc या अन्य ऑप्टिमायज़ेशन के तरीके.

242
00:08:13,940 --> 00:08:15,500
हम उपयोग करते हैं इनका साथ में

243
00:08:15,730 --> 00:08:17,380
बैक प्रॉपगेशन के, इसलिए बैक

244
00:08:17,620 --> 00:08:18,670
प्रॉपगेशन है वह

245
00:08:18,770 --> 00:08:20,640
जो कम्प्यूट कर रहा है ये पर्शियल डेरिवेटिव टर्म्ज़ हमारे लिए.

246
00:08:21,730 --> 00:08:22,680
और इसलिए, हम जानते हैं कैसे

247
00:08:22,860 --> 00:08:24,020
कंपयूट करना है कॉस्ट फंक्शन, हम जानते हैं

248
00:08:24,100 --> 00:08:25,550
कैसे कम्प्यूट करने हैं ये पर्शियल डेरिवेटिव्स इस्तेमाल करके

249
00:08:25,830 --> 00:08:27,410
बैक प्रॉपगेशन, इसलिए हम

250
00:08:27,480 --> 00:08:28,830
कर सकते हैं इस्तेमाल इनमें से एक ऑप्टिमायज़ेशन तरीक़ों का

251
00:08:29,580 --> 00:08:30,850
मिनमायज़ करने की कोशिश में J ऑफ़

252
00:08:31,130 --> 00:08:33,500
थीटा जो है फ़ंक्शन पेरमिटर्स थीटा का.

253
00:08:34,330 --> 00:08:35,410
और वैसे तो,

254
00:08:35,660 --> 00:08:37,330
न्यूरल नेटवर्क्स के लिए, यह कॉस्ट फ़ंक्शन

255
00:08:38,300 --> 00:08:39,630
जे ऑफ़ थीटा है नॉन-कान्वेक्स,

256
00:08:40,530 --> 00:08:42,490
या नहीं है कान्वेक्स और इसलिए

257
00:08:43,260 --> 00:08:45,600
यह सैद्धांतिक रूप से अतिसंवेदनशील हो सकता है

258
00:08:46,250 --> 00:08:47,480
लोकल मिनिमा को, और

259
00:08:47,650 --> 00:08:49,580
वास्तव में, अल्गोरिद्म्स जैसे कि ग्रेडीयंट डिसेंट और

260
00:08:49,840 --> 00:08:51,950
एडवांस्ड ऑप्टिमायज़ेशन तरीक़े,

261
00:08:52,400 --> 00:08:53,660
सिद्धांत रूप में, अटक सकते हैं लोकल

262
00:08:55,190 --> 00:08:56,300
ऑप्टिमा में, लेकिन यह होता है

263
00:08:56,480 --> 00:08:57,680
व्यावहारिक दृष्टि से यह

264
00:08:57,870 --> 00:08:59,230
नहीं है अक्सर एक बड़ी समस्या

265
00:08:59,560 --> 00:09:00,800
और जबकि हम गारंटी नहीं दे सकते

266
00:09:01,210 --> 00:09:02,320
कि ये अल्गोरिद्म ढूँढ पाएँगे एक

267
00:09:02,510 --> 00:09:04,260
ग्लोबल ओप्टिमम, आमतौर पर अल्गोरिद्म जैसे

268
00:09:04,390 --> 00:09:05,870
ग्रेडीयंट डिसेंट करेंगे एक

269
00:09:05,930 --> 00:09:07,700
एक अच्छा काम मिनमायज़ करने में इस

270
00:09:07,850 --> 00:09:09,230
कॉस्ट फ़ंक्शन जे ऑफ़

271
00:09:09,280 --> 00:09:10,350
थीटा को और पाएँगे एक

272
00:09:10,420 --> 00:09:11,820
अच्छा लोकल मिनिमम, भले ही

273
00:09:12,060 --> 00:09:13,690
यदि यह नही पहुँच पाता है ग्लोबल ओप्टिमम पर.

274
00:09:14,500 --> 00:09:16,950
अंत में, ग्रेडीयंट डिसेंट

275
00:09:17,230 --> 00:09:19,500
एक न्यूरल नेटवर्क के लिए अभी भी थोड़ा मनोहर सा प्रतीत होता है.

276
00:09:20,170 --> 00:09:21,680
तो, चलो मैं दिखाता हूँ एक

277
00:09:21,890 --> 00:09:22,990
और चित्र पाने के लिए

278
00:09:23,170 --> 00:09:25,660
वह अनुभव कि ग्रेडीयंट डिसेंट क्या करता है न्यूरल नेटवर्क में.

279
00:09:27,020 --> 00:09:28,460
यह वास्तव में समान है

280
00:09:28,590 --> 00:09:31,190
उस चित्र के जो मैं पहले इस्तेमाल कर रहा था समझाने के लिए ग्रेडीयंट डिसेंट.

281
00:09:31,730 --> 00:09:32,750
तो, हमारे पास है कुछ कॉस्ट

282
00:09:33,090 --> 00:09:34,480
फ़ंक्शन, और हमारे पास हैं

283
00:09:34,710 --> 00:09:36,590
कुछ पेरमिटर्स हमारे न्यूरल नेटवर्क के. यहीं

284
00:09:36,810 --> 00:09:39,190
पर मैंने अभी लिखी हैं दो पेरमिटर्स की वैल्यूज़.

285
00:09:40,080 --> 00:09:41,250
हकीकत में, ज़ाहिर है,

286
00:09:41,520 --> 00:09:43,570
न्यूरल नेटवर्क में, हमारे पास इससे कही अधिक पेरमिटर्स हो सकते हैं.

287
00:09:44,190 --> 00:09:46,980
थीटा एक, थीटा दो - ये सारे मेट्रिसीज़ है, ठीक है?

288
00:09:47,030 --> 00:09:48,130
तो हमारे पास हो सकते हैं बहुत अधिक डिमेन्शन्स के

289
00:09:48,580 --> 00:09:49,870
पेरमिटर्स लेकिन क्योंकि

290
00:09:49,960 --> 00:09:51,620
सीमित हैं हम उससे

291
00:09:51,790 --> 00:09:52,970
जो हम चित्रित कर सकते हैं, मैं मान रहा हूँ

292
00:09:53,410 --> 00:09:55,840
कि हमारे पास है केवल दो पेरमिटर्स इस न्यूरल नेटवर्क में.

293
00:09:56,270 --> 00:09:56,890
हालांकि जाहिर है कि हमारे पास होते हैं बहुत अधिक वास्तव में.

294
00:09:59,280 --> 00:10:00,700
अब, कॉस्ट फ़ंक्शन जे ऑफ़

295
00:10:00,800 --> 00:10:02,470
थीटा मापता है कि कितना सही

296
00:10:02,880 --> 00:10:04,730
न्यूरल नेटवर्क फ़िट होता है ट्रेनिंग डेटा को.

297
00:10:06,000 --> 00:10:06,920
तो, यदि आप लेते हैं एक पोईँट

298
00:10:07,120 --> 00:10:08,590
जैसे यह, नीचे यहाँ,

299
00:10:10,270 --> 00:10:11,180
वह है एक पोईँट जहाँ जे

300
00:10:11,460 --> 00:10:12,580
थीटा है काफ़ी कम,

301
00:10:12,870 --> 00:10:16,170
तो यह कॉरेस्पॉंड करता है पेरमिटर्स की सेटिंग्स को.

302
00:10:17,020 --> 00:10:17,840
ऐसी सेटिंग है पेरमिटर्स

303
00:10:18,350 --> 00:10:19,920
थीटा की, जहाँ, आप जानते हैं, ज़्यादातर

304
00:10:20,140 --> 00:10:22,450
ट्रेनिंग इग्ज़ाम्पल के लिए, आउट्पुट

305
00:10:24,120 --> 00:10:26,270
मेरी हायपॉथिसस की, वह हो सकती है

306
00:10:26,410 --> 00:10:27,420
काफ़ी क़रीब y(i) के

307
00:10:27,650 --> 00:10:28,720
और यदि यह है

308
00:10:28,840 --> 00:10:31,560
सही, तब वह है जो मेरे कॉस्ट फ़ंक्शन को काफ़ी कम करता है.

309
00:10:32,690 --> 00:10:33,770
जबकि इसके विपरीत, यदि आपको

310
00:10:33,820 --> 00:10:35,140
लेनी होती एक वैल्यू उस तरह की, एक

311
00:10:35,510 --> 00:10:37,260
पोईँट वैसा जो कॉरेस्पॉंड करता है

312
00:10:38,080 --> 00:10:39,260
जहाँ बहुत से ट्रेनिंग इग्ज़ाम्पल्ज़ के लिए,

313
00:10:39,890 --> 00:10:40,780
आउट्पुट मेरे न्यूरल

314
00:10:41,040 --> 00:10:42,860
नेटवर्क काफ़ी दूर

315
00:10:43,110 --> 00:10:44,340
y(i) की असली वैल्यू से

316
00:10:44,540 --> 00:10:45,850
जो थी ट्रेनिंग सेट में.

317
00:10:46,610 --> 00:10:47,480
तो पोईँट्स जैसे यह

318
00:10:47,590 --> 00:10:50,100
लाइन पर कॉरेस्पॉंड करते हैं जहाँ

319
00:10:50,450 --> 00:10:51,450
हायपॉथिसस, जहाँ न्यूरल

320
00:10:51,740 --> 00:10:53,330
नेटवर्क आउट्पुट करता है वैल्यूज़

321
00:10:53,770 --> 00:10:54,810
ट्रेनिंग सेट पर जो हैं

322
00:10:55,020 --> 00:10:56,260
y(i) से बिल्कुल भिन्न. तो, यह नहीं कर रहा

323
00:10:56,470 --> 00:10:57,970
फ़िट ट्रेनिंग सेट को सही ढंग से, जबकि

324
00:10:58,170 --> 00:10:59,640
जैसे यह जिसकी कम

325
00:10:59,970 --> 00:11:01,300
वैल्यू है कॉस्ट फ़ंक्शन की कॉरेस्पॉंड करता है

326
00:11:02,130 --> 00:11:03,380
जहाँ जे ऑफ़ थीटा

327
00:11:04,130 --> 00:11:05,270
है कम, और इसलिए कॉरेस्पॉंड करता है

328
00:11:05,950 --> 00:11:07,590
जहाँ न्यूरल नेटवर्क हो जाता है

329
00:11:07,850 --> 00:11:09,290
फ़िट मेरे ट्रेनिंग सेट को

330
00:11:09,510 --> 00:11:11,340
सही, क्योंकि इसका मतलब है यह है जिसकी

331
00:11:11,550 --> 00:11:14,070
आवश्यकता है कि सही हो ताकि जे ऑफ़ थीटा हो सके कम / छोटा..

332
00:11:15,480 --> 00:11:16,810
तो ग्रेडीयंट डिसेंट क्या करता है

333
00:11:16,870 --> 00:11:18,330
हम शूरु करते हैं कुछ रैंडम

334
00:11:18,730 --> 00:11:20,300
प्रारम्भिक पोईँट से जैसे वह

335
00:11:20,430 --> 00:11:22,990
वहाँ पर, और यह जाएगा लगातार नीचे की ओर.

336
00:11:24,040 --> 00:11:25,400
और इसलिए बैक प्रॉपगेशन क्या

337
00:11:25,570 --> 00:11:27,220
करता है कि कम्प्यूट करता है दिशा

338
00:11:27,940 --> 00:11:29,370
ग्रेडीयंट की, और क्या

339
00:11:29,520 --> 00:11:30,740
ग्रेडीयंट डिसेंट करता है कि

340
00:11:31,040 --> 00:11:32,060
लेता है छोटे स्टेप्स नीचे की ओर

341
00:11:32,880 --> 00:11:34,220
जब तक उम्मीद है यह पहुँचता है,

342
00:11:34,610 --> 00:11:36,410
इस केस में, एक काफ़ी अच्छे लोकल ऑप्टिमम पर.

343
00:11:37,880 --> 00:11:39,250
तो जब आप इम्प्लमेंट करते हैं बैक

344
00:11:39,410 --> 00:11:40,840
प्रॉपगेशन और इस्तेमाल करते हैं ग्रेडीयंट

345
00:11:41,200 --> 00:11:42,420
डिसेंट या कोई एक

346
00:11:42,840 --> 00:11:44,750
एडवांस्ड ऑप्टिमायज़ेशन तरीक़ा, यह चित्र

347
00:11:45,330 --> 00:11:47,290
एक तरह से समझा देता है कि अल्गोरिद्म क्या करता है.

348
00:11:47,450 --> 00:11:48,820
यह कोशिश कर रहा है ढूँढने की एक वैल्यू

349
00:11:49,260 --> 00:11:50,920
पेरमिटर्स की जहाँ

350
00:11:51,260 --> 00:11:52,180
आउट्पुट वैल्यू न्यूरल

351
00:11:52,450 --> 00:11:54,300
नेटवर्क में क़रीबी से मैच करती है

352
00:11:54,410 --> 00:11:55,520
y(i) की वैल्यू को

353
00:11:55,660 --> 00:11:58,800
जो आपके ट्रेनिंग सेट में है.

354
00:11:58,910 --> 00:12:00,250
तो, उम्मीद है यह देता है आपको

355
00:12:00,400 --> 00:12:01,610
एक बेहतर समझ कि कैसे

356
00:12:01,920 --> 00:12:03,930
बहुत से भिन्न हिस्से

357
00:12:04,120 --> 00:12:05,760
न्यूरल नेटवर्क के जुड़ते हैं एक साथ.

358
00:12:07,120 --> 00:12:09,010
यदि इस वीडियो के बाद भी,

359
00:12:09,120 --> 00:12:10,130
यदि आपको अभी भी लगता है जैसे ऐसे

360
00:12:10,360 --> 00:12:11,420
है, जैसे, बहुत से भिन्न हिस्से

361
00:12:12,070 --> 00:12:13,450
और यह पूरी तरह स्पष्ट नहीं है कि क्या

362
00:12:13,690 --> 00:12:14,670
उनमें से कुछ करते हैं या कैसे सब

363
00:12:14,860 --> 00:12:17,760
ये हिस्से एक साथ जुड़ते हैं, वह वास्तव में ठीक है.

364
00:12:18,790 --> 00:12:21,780
न्यूरल नेटवर्क लर्निंग और बैक प्रॉपगेशन है एक जटिल अल्गोरिद्म.

365
00:12:23,000 --> 00:12:23,960
और यद्यपि मैंने देखा है

366
00:12:24,290 --> 00:12:25,340
बैक प्रॉपगेशन के पीछे का गणित

367
00:12:25,860 --> 00:12:26,710
कई वर्षों से और मैंने इस्तेमाल किया है

368
00:12:27,030 --> 00:12:28,470
बैक प्रॉपगेशन, मैं सोचता हूँ बहुत

369
00:12:28,680 --> 00:12:30,210
सफलतापूर्वक, कई वर्षों से, फिर भी

370
00:12:30,380 --> 00:12:31,510
आज भी मुझे लगता है मुझे

371
00:12:31,570 --> 00:12:32,670
नहीं है हमेशा एक बढ़िया

372
00:12:33,400 --> 00:12:35,610
समझ कि वास्तव में बैक प्रॉपगेशन क्या कर रहा है कभी कभी.

373
00:12:36,200 --> 00:12:37,850
और ऑप्टिमायज़ेशन प्रक्रिया क्या

374
00:12:38,520 --> 00:12:41,480
दिखाती है कि यह मिनमायज़ कर रही है जे ऑफ़ थीटा को.

375
00:12:41,920 --> 00:12:42,830
जबकि यह एक बहुत कठिन अल्गोरिद्म है

376
00:12:43,450 --> 00:12:44,680
यह लगता है कि मुझे एक

377
00:12:44,830 --> 00:12:46,590
कम समझ है कि

378
00:12:46,690 --> 00:12:47,690
वास्तव में यह क्या कर रहा है

379
00:12:48,240 --> 00:12:49,360
तुलना में, मान लो, लिनीअर रेग्रेशन या लॉजिस्टिक रेग्रेशन के.

380
00:12:51,390 --> 00:12:53,180
जो गणितीय और सैद्धांतिक रूप में

381
00:12:53,510 --> 00:12:55,090
अधिक सरल थे और अधिक स्पष्ट अल्गोरिद्म थे.

382
00:12:56,200 --> 00:12:57,030
लेकिन इसलिए, यदि आपको महसूस होता है

383
00:12:57,070 --> 00:12:58,560
उसी तरह, आप जानते हैं, वह वास्तव में पूरी तरह से

384
00:12:58,970 --> 00:13:01,010
ठीक है, लेकिन यदि आप

385
00:13:01,170 --> 00:13:02,790
फिर भी इम्प्लमेंट करते हैं बैक प्रॉपगेशन, उम्मीद है

386
00:13:03,160 --> 00:13:04,260
आपको मिलेगा कि यह

387
00:13:04,460 --> 00:13:05,410
है एक ज़बरदस्त

388
00:13:05,790 --> 00:13:08,030
लर्निंग अल्गोरिद्म, और यदि आप

389
00:13:08,130 --> 00:13:09,510
इम्प्लमेंट करते हैं इस अल्गोरिद्म को, इम्प्लमेंट करते हैं बैक प्रॉपगेशन,

390
00:13:10,250 --> 00:13:11,230
इम्प्लमेंट करते हैं इनमें से एक ऑप्टिमायज़ेशन

391
00:13:11,340 --> 00:13:13,260
तरीक़ा, आपको मिलेगा कि

392
00:13:13,610 --> 00:13:14,940
बैक प्रॉपगेशन हो पाएगा

393
00:13:15,390 --> 00:13:17,330
फ़िट एक बहुत जटिल, प्रभावशाली, नॉन-लिनीअर

394
00:13:17,830 --> 00:13:19,370
फ़ंक्शन आपके डेटा को,

395
00:13:20,080 --> 00:13:21,060
और यह है एक

396
00:13:21,190 --> 00:13:22,790
सबसे प्रभावी लर्निंग अल्गोरिद्म आज हमारे पास.