En el video anterior, hablamos acerca de cómo utilizar la retropropagación para calcular las derivadas de tu función de costo. En este video quiero decirte rápidamente un detalle para implementar el desarrollo de tus parámetros a partir de las matrices hacia vectores, las cuales necesitamos para poder utilizar las rutinas de optimización avanzada. Específicamente, digamos que has implementado una función de costo que toma esta entrada; es decir, los parámetros de «theta» y retorna la función de costo y retorna las derivadas. Después puedes pasar esto hacia un algoritmo de optimización avanzada mediante fminunc y fminunc no es el único, por cierto. También existen otros algoritmos de optimización avanzada. Pero todo lo que ellos hacen es tomar esta entrada, una función explícita para costos y algún valor inicial de «theta». Y ambos, bien, estas rutinas suponen que «theta», y el valor inicial de «theta», que estos son vectores del parámetro, tal vez Rn o Rn + 1. Aunque estos son vectores y también se asume que, bueno ya sabes, tu función de costo retornará como un segundo valor de retorno. Este gradiente, que también es Rn y Rn + 1. Bien, también es un vector. Esto funcionó bien cuando estábamos usando una progresión logística pero ahora que estamos usando una red neuronal nuestros parámetros ya no son más vectores, pero en vez de ello son estas matrices las cuales para una red neuronal completa tendríamos que son matrices de parámetro para «theta» 1, «theta» 2, «theta» 3 que podríamos representar en Octave como estas matrices «theta»1, «theta»2, «theta» 3. Y del mismo modo estos términos de gradiente que se espera que retornen. Bien, en el video anterior, mostramos cómo calcular estas matrices de gradiente, que eran D mayúscula 1, D mayúscula 2, D mayúscula 3, que podríamos representar en Octave como las matrices D1, D2, D3. En este video quiero contarte rápidamente algo acerca de la idea de cómo tomar estas matrices y desarrollarlas en vectores. Entonces, terminan por estar en un formato adecuado para pasarlas a «theta», aquí, o para obtener un gradiente acá. Digamos, en concreto, que tenemos una red neuronal con una capa de entrada con diez unidades, una capa oculta con diez unidades y una capa de salida con sólo una unidad, entonces s1 es el número de unidades en la capa uno y s2 es el número de unidades en la capa dos y s3 es un número de unidades en la capa tres. En este caso, la dimensión de tus matrices «theta» y D va a ser dada mediante estas expresiones. Por ejemplo, «theta» uno va hacia una matriz de 10 por 11 y así sucesivamente. Entonces, en Octave si quieres para convertirlas entre estas matrices. y en vectores. Lo que puedes hacer es tomar tus «theta» 1, «theta» 2, «theta» 3 y escribir esta parte del código y éste tomará todos los elementos de tus tres matrices «theta» y tomar todos los elementos de «theta» uno, todos los elementos de «theta» 2, todos los elementos de «theta» 3, desarrollarlos y poner todos los elementos en un enorme y largo vector. Que es thetaVec y asimismo el segundo comando tomaría todas tus matrices D y las desarrolla en una enorme y largo vector y lo llamamos DVec.
Para finalizar, si quieres regresar de las representaciones vectoriales a las representaciones de la matriz. ¿Qué haces para regresar a «theta» uno?, digamos que, pues tomas thetaVec y lo extraes de los primeros 110 elementos. Y así, «theta» 1 tiene 110 elementos porque es es una matriz de 10 por 11, de modo que extraes los primeros 110 elementos después usted puede, utilizar el comando remodelar para remodelar esos regresos hacia «theta» 1. Y del mismo modo, para conseguir que regrese «theta» 2 extraes los siguientes 110 elementos y los remodelas. En cuanto a «theta» 3, extraes lo once elementos finales y ejecutas la remodelación para recuperar «theta» 3. Aquí está una demostración rápida en Octave de ese proceso. Entonces, para este ejemplo pongamos «theta» 1 igual para que sean "unos" de 10 por 11, así que es una matriz de puros unos. Y para hacer esto más fácil, veamos, pongamos que sean 2 veces unos,10 por 11 y entonces digamos que «theta» 3 es igual a 3 veces los unos de 1 por 11. Así que esto es 3 matrices por separado: «theta» 1, «theta» 2, «theta» 3. Queremos poner todo esto como un vector. thetaVec es igual a «theta» 1; «theta» 2. «theta» 3. ¿Correcto? Eso es, punto y coma, y dos puntos en medio, así y ahora thetavec va a ser un vector muy largo. De 231 elementos. Si lo despliego, descubro que es un vector muy largo con todos los elementos de la primera matriz, todos los elementos de la segunda matriz, y luego todos los elementos de la tercera matriz. Y si quiero regresar a mis matrices originales, puedo remodelar thetaVec. Vamos a extraer los primeros 110 elementos y a remodelarlos en una matriz de 10 por 11. Esto me da de vuelta «theta» 1. Y si luego extraigo los siguientes 110 elementos. Entonces, eso se indexa de 111 a 220. Recupero todos mis 2. Y si voy desde 221 hasta el último elemento, que es el elemento 231 y lo remodelo 1 por 11, recupero «theta» 3. Para hacer este proceso muy concreto, he aquí cómo usamos el desarrollo de la idea para implementar nuestro algoritmo de aprendizaje. Digamos que tienes algún valor inicial de los parámetros «theta»1, «theta» 2, «theta» 3. Lo que haremos es tomar estos y desarrollarlos en un vector largo vamos a recuperar el «theta» inicial pasarlo a fminunc como esta configuración inicial de los parámetros de «theta». El otro asunto que necesitamos poner en marcha es implementar la función de costo. Aquí está mi implementación de la función de costo. La función de costo nos va a dar la entrada, thetaVec, que va a ser todos los vectores de mi parámetro que, están en, la forma que ha sido desarrollada en un vector. Entonces, lo primero que voy a hacer es utilizar thetaVec y voy a usar las funciones de remodelación. Así podré extraer los elementos de thetaVec y utilizar el remodelado para recuperar mis matrices originales de parámetro, «theta» 1, «theta» 2, «theta» 3. Así que, estas van a ser las matrices que voy a obtener. Bien, esto me da una forma más conveniente en la cual utilizar estas matrices, de modo que, puedo ejecutar la propagación hacia adelante y la retropropagación para calcular mis derivadas, y para calcular mi función de costo j de «theta». Y finalmente, puedo entonces tomar mis derivadas y desarrollarlas para mantener los elementos en el mismo orden como lo hice cuando desarrollé mis «theta»s. Pero, voy a desarrollar D1, D2, D3, para obtener gradientVec que es ahora a donde puede retornar mi función de costo. Puede retornar un vector de estas derivadas. Pues bien, espero que ahora tengan un sentido claro de cómo convertir de ida y de vuelta entre la representación matricial de los parámetros en comparación con la representación del vector de los parámetros. La ventaja de la representación de la matriz es que cuando tus parámetros se almacenan como matrices es más conveniente cuando estás haciendo una propagación hacia adelante y una retropropagación y es más fácil cuando tus parámetros se almacenan como matrices que puedes aprovechar, en cierto modo, de las implementaciones vectorizadas. Mientras que en contraste, la ventaja de la representación vectorial, cuando tienes, por ejemplo, thetaVec o DVec es que cuando estás utilizando los algoritmos de optimización avanzada, esos algoritmos tienden a suponer que tienes todos tus parámetros desarrollados en un enorme y largo vector. Así que, sólo con lo que acabamos de ver esperamos que ahora tú puedas rápidamente hacer conversiones entre estas dos formas según lo necesites.