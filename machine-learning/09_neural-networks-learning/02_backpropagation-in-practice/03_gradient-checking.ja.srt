1
00:00:00,290 --> 00:00:01,510
前回までの一連のビデオで

2
00:00:01,840 --> 00:00:02,770
ニューラルネットワークにおいて微分を計算するために、

3
00:00:03,570 --> 00:00:05,200
フォワードプロパゲーションとバックワードプロパゲーションを

4
00:00:05,250 --> 00:00:07,560
行うやり方を見てきた。

5
00:00:08,800 --> 00:00:10,070
だがバックプロパはたくさんの

6
00:00:10,580 --> 00:00:11,910
細かい部分のあるアルゴリズムで

7
00:00:12,170 --> 00:00:12,920
実装するのにちょっと

8
00:00:13,050 --> 00:00:14,930
トリッキーな所がある。

9
00:00:15,700 --> 00:00:17,480
そして一つの不運な特徴として

10
00:00:17,750 --> 00:00:18,690
バックプロパは

11
00:00:18,780 --> 00:00:20,080
色々と微妙にバグる、というのがある。

12
00:00:20,320 --> 00:00:22,000
だから最急降下法なり

13
00:00:22,140 --> 00:00:23,130
別の最適化アルゴリズムなりなどで

14
00:00:23,480 --> 00:00:26,590
実行すると、ぱっと見うまく行ってるように見えたりする。

15
00:00:27,240 --> 00:00:28,480
そしてあなたのコスト関数

16
00:00:28,700 --> 00:00:29,930
Jのシータが、結局は

17
00:00:30,090 --> 00:00:31,240
最急降下法の各イテレーションで

18
00:00:31,830 --> 00:00:33,660
減少していく場合があるが、

19
00:00:33,830 --> 00:00:35,180
それはあなたのバックプロパの実装にバグがあるのを見逃していても

20
00:00:35,440 --> 00:00:37,690
起こりうる。

21
00:00:38,400 --> 00:00:39,280
つまり、Jのシータは

22
00:00:39,360 --> 00:00:40,830
減少しているように見えるが、

23
00:00:40,920 --> 00:00:42,230
バグ無しの実装に比べて

24
00:00:42,410 --> 00:00:43,760
結局はより高レベルの

25
00:00:43,880 --> 00:00:44,970
エラーに

26
00:00:45,490 --> 00:00:46,540
見舞われる

27
00:00:46,780 --> 00:00:48,130
可能性があり、

28
00:00:48,330 --> 00:00:49,330
そんなパフォーマンスになってしまっている

29
00:00:49,460 --> 00:00:50,470
微妙なバグに

30
00:00:50,530 --> 00:00:52,260
単に気づいていないだけ、という事にもなりかねない。

31
00:00:52,950 --> 00:00:53,320
ではこの事態にどう対処すれば良いか？

32
00:00:54,160 --> 00:00:55,940
グラディアントチェッキングと呼ばれるアイデアがあり、

33
00:00:56,790 --> 00:00:58,720
それはこれらの問題のほとんどを駆逐してくれる。

34
00:00:59,250 --> 00:01:00,550
だからこんにちでは、

35
00:01:00,770 --> 00:01:02,150
バックプロパゲーションなり

36
00:01:02,370 --> 00:01:03,320
それ以外でもそれなりに複雑なモデルの

37
00:01:03,450 --> 00:01:04,950
最急降下法を実装する時には、

38
00:01:05,640 --> 00:01:07,310
私は毎回、

39
00:01:07,540 --> 00:01:08,840
グラディアントチェッキングを実装するようにしている。

40
00:01:09,650 --> 00:01:10,610
そしてこれを行えば、

41
00:01:10,730 --> 00:01:12,010
あなたのフォワードプロパゲートやバックワードプロパゲートや

42
00:01:12,140 --> 00:01:13,410
それ以外のなんでも、とにかく実装した物が、

43
00:01:13,540 --> 00:01:14,940
100%正しい、と確認したり、深く確信したりするのを

44
00:01:15,370 --> 00:01:17,430
助けてくれる。

45
00:01:18,240 --> 00:01:19,090
私はこの手法が

46
00:01:19,330 --> 00:01:20,880
バックプロパゲートの実装に関した

47
00:01:21,160 --> 00:01:23,090
あらゆるバグを駆逐してくれるのを

48
00:01:23,420 --> 00:01:25,790
見て来た。

49
00:01:26,330 --> 00:01:27,470
そして前回のビデオでは、

50
00:01:28,170 --> 00:01:29,120
私はあなたに、

51
00:01:29,390 --> 00:01:30,950
デルタやDたちを

52
00:01:31,170 --> 00:01:33,000
計算する式を

53
00:01:33,110 --> 00:01:34,220
単に信じてくれ、と頼んだ。

54
00:01:34,260 --> 00:01:35,480
私はあなたにそれらの公式が

55
00:01:36,330 --> 00:01:37,600
実際にコスト関数の微分を計算している事を

56
00:01:38,180 --> 00:01:39,790
単に信じてくれ、と頼んだ。

57
00:01:40,150 --> 00:01:41,740
だがひとたびあなたが数値的なグラディアントチェッキングを実装すれば、

58
00:01:42,130 --> 00:01:43,210
それこそがこのビデオのトピックだが、

59
00:01:43,800 --> 00:01:45,250
そうすればあなた自身が

60
00:01:45,350 --> 00:01:46,490
あなたの書いたコードが確かに

61
00:01:46,610 --> 00:01:48,530
コスト関数Jの微分を計算している事を

62
00:01:49,600 --> 00:01:50,520
確認する事が出来る。

63
00:01:50,820 --> 00:01:53,060
そのアイデアはこうだ。

64
00:01:53,550 --> 00:01:54,520
以下のような例を考えてみよう。

65
00:01:55,450 --> 00:01:56,230
Jのシータがあるとして、

66
00:01:56,710 --> 00:01:58,140
そしてある値シータが

67
00:01:58,250 --> 00:02:01,320
あるとする。

68
00:02:01,610 --> 00:02:04,380
そしてこの例では、シータは単なる実数だと仮定しよう。

69
00:02:05,470 --> 00:02:08,210
そしてこの関数の、例えばこの点の微分を推計したいとしよう。

70
00:02:08,710 --> 00:02:10,220
すると微分は、

71
00:02:10,750 --> 00:02:13,190
この接線の傾きに等しい。

72
00:02:14,270 --> 00:02:15,420
これが、数値的に微分を

73
00:02:16,180 --> 00:02:17,840
近似する方法、あるいはむしろ

74
00:02:17,970 --> 00:02:19,190
微分を数値的に近似する

75
00:02:19,780 --> 00:02:21,480
手続きはこうだ：

76
00:02:21,800 --> 00:02:23,520
シータ+エプシロンを計算する、

77
00:02:24,000 --> 00:02:25,550
つまりちょっとだけ右の値だ。

78
00:02:26,340 --> 00:02:27,900
そしてシータ-エプシロンも計算する。

79
00:02:28,410 --> 00:02:30,800
そしてこれら二つの点を

80
00:02:30,950 --> 00:02:34,360
見て、それらを直線で

81
00:02:34,840 --> 00:02:35,860
つなげる。

82
00:02:43,160 --> 00:02:44,280
これら二つの点を

83
00:02:44,480 --> 00:02:45,490
直線でつなげよう。

84
00:02:45,680 --> 00:02:46,430
そしてこの

85
00:02:46,480 --> 00:02:47,740
小さな赤い線の

86
00:02:48,000 --> 00:02:49,200
傾きを、微分の近似として

87
00:02:49,390 --> 00:02:50,940
用いる。

88
00:02:51,460 --> 00:02:53,110
ここで、真の微分の値は

89
00:02:53,280 --> 00:02:54,740
ここの青い線の傾きだ。

90
00:02:55,260 --> 00:02:56,660
つまり、ふむ、それはとても良い近似になりそうだ。

91
00:02:58,220 --> 00:02:59,450
数学的には、この赤い直線の傾きは

92
00:02:59,670 --> 00:03:01,340
垂直方向の高さ

93
00:03:01,890 --> 00:03:03,680
割る事の

94
00:03:03,890 --> 00:03:05,580
この水平方向の幅だ。

95
00:03:05,840 --> 00:03:07,500
この上の点はJのシータ+エプシロン。

96
00:03:08,920 --> 00:03:10,840
この、ここの点は

97
00:03:11,140 --> 00:03:13,020
Jのシータ-エプシロン。

98
00:03:13,830 --> 00:03:15,450
つまりこの垂直の差は

99
00:03:15,670 --> 00:03:17,530
Jのシータ+エプシロン

100
00:03:17,810 --> 00:03:18,810
引くことの Jのシータ-エプシロン。

101
00:03:19,700 --> 00:03:21,730
そしてこの水平距離は2エプシロンだ。

102
00:03:23,620 --> 00:03:25,340
つまり、私の近似は

103
00:03:25,410 --> 00:03:27,280
以下のようになる：

104
00:03:29,110 --> 00:03:30,160
Jのシータの、シータによる微分は、

105
00:03:30,490 --> 00:03:32,170
このシータの場所での微分は、

106
00:03:32,320 --> 00:03:34,950
だいたい近似的には

107
00:03:35,150 --> 00:03:36,860
Jのシータ+エプシロン

108
00:03:37,460 --> 00:03:40,600
引く事の Jのシータ-エプシロン, 割ることの2エプシロンだ。

109
00:03:42,280 --> 00:03:43,330
普通私は、エプシロンには

110
00:03:43,600 --> 00:03:44,790
とても小さい数字を用いる。

111
00:03:45,040 --> 00:03:46,270
エプシロンにだいたい10の-4乗とか

112
00:03:46,530 --> 00:03:48,220
そういうオーダーの数をセットしてる。

113
00:03:48,740 --> 00:03:49,890
だいたいにおいて、うまく行くようなエプシロンの範囲は

114
00:03:50,190 --> 00:03:52,280
結構大きな範囲に渡る。

115
00:03:53,050 --> 00:03:54,470
そして実際に、

116
00:03:55,280 --> 00:03:56,540
エプシロンにとても小さい値を入れていくと

117
00:03:57,010 --> 00:03:58,580
数学的には、この項は

118
00:03:59,210 --> 00:04:00,790
実際に数学的に、

119
00:04:01,000 --> 00:04:02,340
微分となる。この点における

120
00:04:02,860 --> 00:04:04,310
関数の完全な傾きになる。

121
00:04:05,050 --> 00:04:05,730
そんなに小さな

122
00:04:05,910 --> 00:04:06,980
エプシロンを使いたくない理由は、

123
00:04:07,170 --> 00:04:09,630
単に小さすぎるエプシロンは数値的な問題を引き起こすからというだけ。

124
00:04:10,130 --> 00:04:11,070
だから私はだいたい、

125
00:04:11,380 --> 00:04:14,200
エプシロンに10の-4乗あたりの値を使う。

126
00:04:14,470 --> 00:04:15,220
ところで、あなたがたの中には

127
00:04:15,330 --> 00:04:17,590
微分を推計する別の式、

128
00:04:17,750 --> 00:04:19,710
こんな式を見た事がある人もいるかもしれない。

129
00:04:21,590 --> 00:04:23,500
この右側のは片側微分と呼ばれる物だ。

130
00:04:24,040 --> 00:04:26,580
一方で、左側の式は両側微分と呼ばれる。

131
00:04:27,120 --> 00:04:28,670
両側微分は通常は

132
00:04:28,890 --> 00:04:29,750
わずかだがより良い推計を与えるので、

133
00:04:30,170 --> 00:04:31,410
私は通常はこの片側微分の代わりに

134
00:04:31,670 --> 00:04:33,540
両側微分を用いている。

135
00:04:35,900 --> 00:04:37,280
つまり、具体的に言えば、Octaveで

136
00:04:37,750 --> 00:04:39,280
あなたが実装するのは、以下のような物だ。

137
00:04:40,270 --> 00:04:41,490
gradApproxを計算するコードは、こうなる、

138
00:04:41,600 --> 00:04:43,160
これは微分を

139
00:04:43,270 --> 00:04:44,590
近似する。

140
00:04:45,380 --> 00:04:46,820
それはこんな式だ：

141
00:04:47,200 --> 00:04:48,550
Jのシータ+エプシロン 引くことの Jのシータ-エプシロン

142
00:04:48,730 --> 00:04:50,800
割ることの2掛けるエプシロン。

143
00:04:52,060 --> 00:04:52,980
この式はこの点の微分の

144
00:04:53,100 --> 00:04:56,110
数値的な推計を与える。

145
00:04:56,590 --> 00:04:58,910
そしてこの例では、これは極めて良い推計になっているようだ。

146
00:05:01,970 --> 00:05:03,460
ここで、前のスライドでは、

147
00:05:03,710 --> 00:05:05,040
シータが実数の場合を

148
00:05:05,290 --> 00:05:07,010
検討した。

149
00:05:08,000 --> 00:05:08,670
ここでは、より一般的なケースとなる、

150
00:05:08,900 --> 00:05:11,650
シータがパラメータベクトルの場合を見てみよう。

151
00:05:12,220 --> 00:05:13,270
シータがRnとしよう。

152
00:05:13,520 --> 00:05:14,610
これはニューラルネットワークのパラメータを

153
00:05:15,000 --> 00:05:16,510
アンロールしたバージョンと

154
00:05:16,610 --> 00:05:18,010
考えても良い。

155
00:05:18,250 --> 00:05:19,580
つまりシータはn個の要素を持つ

156
00:05:19,800 --> 00:05:21,230
ベクトルで、つまりシータ1から

157
00:05:21,350 --> 00:05:25,100
シータnまでで、

158
00:05:25,240 --> 00:05:26,530
これらそれぞれに関する偏微分の項について

159
00:05:27,080 --> 00:05:29,300
さっきと似たような近似のアイデアを用いる事が出来る。

160
00:05:30,250 --> 00:05:31,730
具体的には、コスト関数の

161
00:05:32,420 --> 00:05:33,840
最初のパラメータ、シータ1による

162
00:05:34,110 --> 00:05:35,710
偏微分は、以下のように

163
00:05:36,110 --> 00:05:37,270
求める事が出来る。

164
00:05:37,410 --> 00:05:40,270
それはJに対しシータ1を増加させて、

165
00:05:40,380 --> 00:05:43,030
つまりJのシータ1+エプシロン にして、

166
00:05:43,520 --> 00:05:44,780
そこから引く事の Jのシータ1-エプシロン に、

167
00:05:45,520 --> 00:05:46,820
全体を2エプシロンで割る。

168
00:05:48,130 --> 00:05:49,660
二番目のパラメータシータ2に関しての

169
00:05:49,740 --> 00:05:51,090
偏微分は、

170
00:05:51,620 --> 00:05:53,130
だいたいこれと同じ事をするが、

171
00:05:53,270 --> 00:05:54,370
唯一の違いは、エプシロンだけ増加させるのが

172
00:05:54,740 --> 00:05:56,240
シータ2だという所。

173
00:05:56,570 --> 00:05:58,290
そしてここは、シータ2をエプシロンだけ減少させる。

174
00:05:59,100 --> 00:06:00,170
などと、シータnに関する微分まで

175
00:06:00,260 --> 00:06:01,680
降りていく。

176
00:06:01,780 --> 00:06:02,780
そこではここにあるシータnを

177
00:06:03,030 --> 00:06:04,550
エプシロンだけ

178
00:06:05,060 --> 00:06:06,140
増加させたり減少させたりする。

179
00:06:09,790 --> 00:06:11,550
さて、これらの等式は

180
00:06:11,720 --> 00:06:13,580
Jの、各パラメータに対する

181
00:06:14,690 --> 00:06:16,500
偏微分を数値的に

182
00:06:17,250 --> 00:06:20,100
近似する方法を与える。

183
00:06:23,640 --> 00:06:26,030
具体的にはあなたが実装するのは、以下のような物だ。

184
00:06:27,900 --> 00:06:29,260
我らはOctaveで以下のように実装して

185
00:06:29,820 --> 00:06:31,000
数値的に微分を求める。

186
00:06:32,220 --> 00:06:33,670
for i=1からnまでの、、、

187
00:06:33,790 --> 00:06:35,110
ここでnはパラメータベクトル、シータの

188
00:06:35,310 --> 00:06:37,140
次元だ。

189
00:06:37,730 --> 00:06:40,680
そして私は普通、これをアンロールしたバージョンのパラメータでやる。

190
00:06:41,250 --> 00:06:42,210
つまりシータは私のニューラルネットワークのパラメータの

191
00:06:42,530 --> 00:06:44,770
単なる長いリストに過ぎない。

192
00:06:46,230 --> 00:06:47,550
thetaPlusにthetaをセットし、

193
00:06:47,830 --> 00:06:49,270
thetaPlusのi番目の要素を

194
00:06:49,630 --> 00:06:51,170
EPSILONだけ増加させる。

195
00:06:51,660 --> 00:06:53,010
つまりこれは、thetaPlusは

196
00:06:53,720 --> 00:06:54,830
基本的にはthetaに等しい、

197
00:06:55,340 --> 00:06:56,280
thetaPlus(i)以外は。

198
00:06:56,580 --> 00:06:57,820
thetaPlus(i)はEPSILONだけ増加させてある。

199
00:06:58,310 --> 00:06:59,400
つまり、thetaPlusは

200
00:07:00,810 --> 00:07:01,880
theta1, theta2, ...などと

201
00:07:01,970 --> 00:07:03,370
等しくて、

202
00:07:04,020 --> 00:07:05,160
そしてtheta(i)の所では、EPSILONを足した物に等しい。

203
00:07:05,350 --> 00:07:06,590
そしてさらにtheta(n)まで降りていく。

204
00:07:06,780 --> 00:07:08,440
これがthetaPlusだ。

205
00:07:08,690 --> 00:07:11,340
同様に、これら二つの行は

206
00:07:11,530 --> 00:07:13,380
thetaMinusに、上と似たような物を

207
00:07:13,480 --> 00:07:15,090
代入しているが、

208
00:07:15,560 --> 00:07:16,720
theta(i)+EPSILONの代わりに

209
00:07:16,930 --> 00:07:19,150
theta(i)-EPSILONな所だけが違う。

210
00:07:20,670 --> 00:07:22,320
そして最後に、このgradApprox(i)を

211
00:07:22,830 --> 00:07:24,370
実装する。

212
00:07:25,190 --> 00:07:26,430
これがJのシータの

213
00:07:27,210 --> 00:07:28,420
シータiによる

214
00:07:28,800 --> 00:07:30,250
偏微分の近似を

215
00:07:30,430 --> 00:07:32,430
与える。

216
00:07:35,330 --> 00:07:36,420
そしてこれの使い方としては、

217
00:07:36,760 --> 00:07:38,530
ニューラルネットワークの実装において、

218
00:07:38,850 --> 00:07:41,530
ニューラルネットワークの各パラメータによる

219
00:07:41,770 --> 00:07:43,310
コスト関数の偏微分を求める為に

220
00:07:44,080 --> 00:07:45,570
これを実装する、

221
00:07:45,860 --> 00:07:48,570
このforループを実装する。

222
00:07:49,450 --> 00:07:51,120
そして次に、バックプロパから

223
00:07:51,350 --> 00:07:53,070
グラディアントを取得出来る。

224
00:07:53,740 --> 00:07:55,110
つまりDVecはバックプロパから得た

225
00:07:55,770 --> 00:07:57,150
微分だ。

226
00:07:58,380 --> 00:08:00,610
つまり、バックプロパ、バックプロパゲーションは

227
00:08:00,890 --> 00:08:02,030
微分を計算する

228
00:08:02,090 --> 00:08:03,350
比較的効率的な方法だ、より正確に言えば

229
00:08:03,430 --> 00:08:04,970
コスト関数の各パラメータによる

230
00:08:05,110 --> 00:08:06,850
偏微分を計算する為の。

231
00:08:07,820 --> 00:08:08,960
そして私がよくやるのは、

232
00:08:09,350 --> 00:08:10,820
数値的に計算した微分に対して、

233
00:08:11,440 --> 00:08:12,830
それはこの

234
00:08:12,960 --> 00:08:14,080
ここの上で得た

235
00:08:14,250 --> 00:08:15,830
gradApproxだが、

236
00:08:15,920 --> 00:08:17,030
これがback propで得た物と

237
00:08:17,290 --> 00:08:19,420
等しいかほとんど等しい事を

238
00:08:19,980 --> 00:08:21,080
確認する事だ。小さい数値的な

239
00:08:21,810 --> 00:08:22,770
丸めの範囲に収まっているかを。

240
00:08:22,970 --> 00:08:25,640
back propで得たDVecと極めて近いかを。

241
00:08:26,510 --> 00:08:27,460
そしてこれら二つの方法で計算した

242
00:08:27,930 --> 00:08:29,550
微分の値が、同じ答えか、

243
00:08:29,650 --> 00:08:31,040
少なくともとても近い答えを

244
00:08:31,300 --> 00:08:33,670
はじきだしたなら、小数点以下数桁の範囲で近ければ、

245
00:08:34,720 --> 00:08:36,560
私のバックプロパの実装が正しい、という事に

246
00:08:36,710 --> 00:08:38,720
よりしっかりと自信を持つ事が出来る。

247
00:08:40,000 --> 00:08:41,230
そうしてから、これらのDVecベクトルを

248
00:08:41,660 --> 00:08:43,320
最急降下法なり

249
00:08:43,760 --> 00:08:45,610
何らかのアドバンスドな最適化アルゴリズムに食わせれば、

250
00:08:45,760 --> 00:08:46,850
その時には微分をちゃんと

251
00:08:47,100 --> 00:08:48,870
計算していると

252
00:08:49,360 --> 00:08:51,010
自信を持っているから、

253
00:08:51,450 --> 00:08:52,670
自分のコードが正しく走るとも

254
00:08:52,790 --> 00:08:53,890
期待出来て、

255
00:08:53,980 --> 00:08:55,570
Jのシータを最適化するのに良い仕事をしてくれると期待出来る。

256
00:08:57,700 --> 00:08:58,680
最後に、全部を合わせて

257
00:08:58,860 --> 00:09:00,050
数値的グラディアントチェッキングを

258
00:09:00,310 --> 00:09:02,950
どう実装するかをお話したい。

259
00:09:03,630 --> 00:09:04,370
私はいつも、こんな風にする。

260
00:09:04,970 --> 00:09:06,020
最初にやるのは、

261
00:09:06,500 --> 00:09:08,180
DVecを計算する為にバックプロパゲーションを実装する。

262
00:09:08,490 --> 00:09:09,560
これは以前のビデオで話した

263
00:09:09,830 --> 00:09:11,250
DVecを計算する手順となり、

264
00:09:11,490 --> 00:09:13,530
それはこれらの行列を展開したバージョンとなる。

265
00:09:15,410 --> 00:09:16,550
次に私がやるのは、

266
00:09:17,010 --> 00:09:20,130
gradApproxを計算する為に数値的なグラディアントチェッキングを実装する事だ。

267
00:09:20,590 --> 00:09:23,550
これが私がこのビデオで話してきた所だ、前のスライドで話した奴。

268
00:09:24,900 --> 00:09:27,680
そして次に、DVecとgradApproxが似た値かどうかを確認する、

269
00:09:28,170 --> 00:09:30,860
たとえば小数点第二位とか第三位までで一致するかを見る。

270
00:09:32,270 --> 00:09:33,160
そして最後に、そしてこれは大切なステップなのだが、

271
00:09:33,240 --> 00:09:35,230
あなたのコードを実際に

272
00:09:35,480 --> 00:09:36,690
学習させ始める前に、

273
00:09:37,000 --> 00:09:38,220
真面目にネットワークをトレーニングする前に、

274
00:09:38,570 --> 00:09:40,960
グラディアントチェッキングを切るのが大切だ。

275
00:09:41,490 --> 00:09:42,800
そしてそれ以降は

276
00:09:43,630 --> 00:09:44,940
このビデオでやってきた

277
00:09:45,250 --> 00:09:47,660
gradApproxの

278
00:09:47,980 --> 00:09:48,950
数値的な微分の式での計算を

279
00:09:50,560 --> 00:09:50,560
しないように。

280
00:09:50,960 --> 00:09:52,180
その理由は、

281
00:09:52,330 --> 00:09:53,800
数値的なグラディアントチェッキングのコードは、

282
00:09:54,120 --> 00:09:54,930
このビデオで議論してきた内容は、

283
00:09:55,010 --> 00:09:56,220
計算量的にとても高価で、

284
00:09:56,650 --> 00:09:58,570
微分を近似しようとするのには

285
00:09:58,600 --> 00:10:00,960
凄い遅いやり方だ。

286
00:10:02,080 --> 00:10:03,490
一方で対照的に、以前に話した

287
00:10:03,900 --> 00:10:04,710
バックプロパゲーションのアルゴリズムは

288
00:10:04,940 --> 00:10:06,120
それは前に

289
00:10:06,370 --> 00:10:07,270
D1とかD2とかD3とかDVecを計算するのに

290
00:10:07,460 --> 00:10:08,900
議論してきた物だが、

291
00:10:09,320 --> 00:10:11,620
そのバックプロパゲーションは

292
00:10:11,790 --> 00:10:14,930
微分を計算するのに、もっとずっと計算量的に効率的な方法だ。

293
00:10:17,070 --> 00:10:18,650
だからひとたびあなたのバックプロパゲーションの実装が

294
00:10:18,770 --> 00:10:20,270
正しい、と確認した後には、

295
00:10:20,620 --> 00:10:21,840
グラディアントチェッキングは切るべきだ、

296
00:10:22,160 --> 00:10:24,140
単純に使うのをやめるべきだ。

297
00:10:25,090 --> 00:10:26,380
もう一度繰り返そう。

298
00:10:26,540 --> 00:10:27,720
あなたのアルゴリズムを

299
00:10:27,840 --> 00:10:29,380
最急降下法のたくさんの繰り返しで

300
00:10:29,690 --> 00:10:30,840
走らせる前には、

301
00:10:31,140 --> 00:10:32,560
あるいはアドバンスドな

302
00:10:32,670 --> 00:10:33,690
最適化アルゴリズムで分類器を訓練する為に

303
00:10:33,890 --> 00:10:34,990
たくさんの繰り返しを走らせる前には、

304
00:10:35,820 --> 00:10:37,140
グラディアントチェッキングのコードを切る事を忘れないようにしよう。

305
00:10:37,980 --> 00:10:39,120
具体的には、もし万が一

306
00:10:39,290 --> 00:10:40,830
最急降下法の各イテレーションで毎回

307
00:10:41,340 --> 00:10:43,710
数値的グラディアントチェッキングを走らせてしまったら、

308
00:10:44,040 --> 00:10:44,650
またはcostFunctionの内側のループで

309
00:10:44,850 --> 00:10:45,780
走らせてしまったら、

310
00:10:46,670 --> 00:10:47,910
あなたのコードはとてものろくなってしまうだろう。

311
00:10:48,240 --> 00:10:49,860
何故なら数値的なグラディアントチェッキングのコードは

312
00:10:50,180 --> 00:10:51,690
バックプロパゲーションのアルゴリズムに比べて

313
00:10:51,900 --> 00:10:53,960
ずっと遅いからだ。

314
00:10:54,160 --> 00:10:56,160
つまりデルタ4、デルタ3、デルタ2などを

315
00:10:56,340 --> 00:10:57,650
計算する時に用いた

316
00:10:58,000 --> 00:10:59,820
バックプロパゲーションと比較するとだ。

317
00:10:59,900 --> 00:11:02,470
それがバックプロパゲーションだった。

318
00:11:02,990 --> 00:11:05,770
それはグラディアントチェッキングよりもずっと早い微分の計算方法だ。

319
00:11:06,620 --> 00:11:08,400
だから準備が出来たら。

320
00:11:08,620 --> 00:11:10,190
一旦あなたのバックプロパゲーションの実装が正しいと確認出来たら、

321
00:11:10,480 --> 00:11:12,140
グラディアントチェッキングのコードを

322
00:11:12,220 --> 00:11:13,050
切るなりdisableするなりを

323
00:11:13,640 --> 00:11:15,070
確実に行おう、

324
00:11:15,270 --> 00:11:17,880
アルゴリズムをトレーニングする間は。さもないとコードがとてもゆっくり実行されてしまう。

325
00:11:20,420 --> 00:11:22,470
以上がグラディアントを数値的に計算する方法だ。

326
00:11:23,110 --> 00:11:24,300
こうやって、あなたのバックプロパゲーションの実装が正しい、と

327
00:11:24,420 --> 00:11:26,300
検証する事が出来る。

328
00:11:27,230 --> 00:11:29,290
私がバックプロパゲーションや、似たような複雑なモデルに対して

329
00:11:29,450 --> 00:11:31,130
最急降下法を実装する時にはいつでも、

330
00:11:31,250 --> 00:11:33,410
グラディアントチェッキングを使っている。

331
00:11:33,730 --> 00:11:36,230
これは自分のコードが正しいと確認する為の、本当に良い助けとなってくれるんだ。