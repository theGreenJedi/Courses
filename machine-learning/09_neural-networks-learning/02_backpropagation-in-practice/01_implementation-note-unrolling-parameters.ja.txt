前回のビデオでは コスト関数の微分を計算する為に バックプロパゲーションをどう使うかを議論した。 このビデオでは、 行列からベクトルへの パラメータのアンロールという、 細かい実装の話を簡単に行う、 アドバンスドな最適化ルーチンを使うのに必要となるから。 具体的にいこう、 パラメータシータを受け取って コスト関数とその微分を返す 関数を実装したとしよう。 すると、これをfminuncのような アドバンスドな最適化アルゴリズムに渡す事が出来る、 ところで、このfminuncは 唯一の選択肢という訳では無い。 別のアドバンスドな最適化アルゴリズムも存在する。 でもそれらは全て コスト関数のポインタと シータの初期値を 受け取る。 そしてこれらのルーチンは シータとシータの初期値を どちらも パラメータベクトルと想定する、 RのnとかRのn+1とか。 これらはベクトルだが、 コスト関数の実装が 二番目の返値として 返す微分項も RのnなりRのn+1なりを 仮定する。つまりこれもベクトルだ。 ロジスティック回帰で使ってる時は これで問題無かったのだが、 今やニューラルネットワークなので、 パラメータはもう ベクトルでは無くなってしまった。 今やパラメータはこれらの行列で 四段のニューラルネットワークだとすると パラメータ行列 シータ1、シータ2，シータ3を持ち Octaveではこれらは 行列Theta1、Theta2、Theta3と表されるだろう。 同様にこれらのgradientの項として 返されると期待しているのは、 前回のビデオで これらのgradient項をどう計算するかを 扱ったが、 それらは大文字のD1、D2、 D3で、それはOctaveでは 行列D1、D2、D3として表される。 このビデオでは、 これらの行列をとって どうベクトルに アンロールするかをお話する。 ここのシータとして 渡すのに適切なフォーマットにしたり ここのgradientから 取り出す為に。 具体的に、入力レイヤとして 10個のユニットがあり、 隠れレイヤとして 10ユニット、 そして出力レイヤとして ユニット一つとしよう。 そしてs1はレイヤ1のユニット数、 s2はレイヤ2 の ユニット数、そしてs3は レイヤ3のユニット数とする。 この場合、行列シータの次元と Dの次元は これらの式で 与えられる。 例えば、シータ1は 10x11行列、などとなる。 だからこれらの行列をベクトルと 変換したければ、 変換したければ、 可能な手としては Theta1、Theta2、Theta3に対して こんなコードを 書くと、 3つのシータ行列から 全ての要素を取り出して、つまり シータ1の全要素、 シータ2の 全要素、 シータ3の全要素を取り出して、 それらをアンロール(展開)して、 それら全要素を一つの長いベクトルに突っ込む。 それがthetaVecとなる。 同様に二番目のコマンドは Dの行列全てを 大きな長いベクトル、 DVecにアンロールする。 そして最後に もしベクトルの表現から行列の表現に 戻したくなったら、 例えばシータ1を取り戻したいと 思ったとすると、 やるべき事はまずthetaVecから 最初の110個の要素を取り出す。 つまりシータ1は110個の要素があるという事、 何故ならそれは 10x11の行列だから。 だから最初の110個の要素を取り出し そしてそこで、単に 変形してtheta1に戻す事が出来る。 同様にシータ2を 取り戻すには、次の110要素を取り出し reshapeすれば良い。 そしてシータ3は、最後の 11要素を取り出し、reshapeを実行して シータ3が取り戻せる。 これはOctaveによる簡単なデモだ。 この例の為に シータ1を、 onesの10x11にセットしよう、 つまり全ての要素が1の行列になる。 見やすいように、 シータ2は 2掛けるonesの10x11と、 さらにシータ3は 3掛けるonesの1x11と しよう。 つまり以上で3つの 異なる行列シータ1、シータ2、シータ3が出来た。 これら全てをベクトルに突っ込みたい。 thetaVecは、イコール theta1; theta2; theta3だ。 真ん中にあるのは コロンだ。 これでthetaVecは とても長いベクトルとなる。 231要素のベクトルだ。 これを表示すると、 このとても長いベクトルは 最初の行列の要素全てと、 二番目の行列の要素全てと 三番目の行列の要素全てだ。 そして最初の行列を 取り出したいとしたら、 thetaVecをreshapeすれば良い。 最初の110の要素を取り出し それを10x11行列にreshapeしよう。 これでtheta1が得られる。 そして次に 続く110要素を取り出す。 つまりインデックスで111から220まで。 これで2を全て取り出し、 そして残りの 221から最後までの 要素で、 それは231番目の要素となるが、それを1x11に reshapeする。これでtheta3に戻せる。 このプロセスをもっともっと具体的にすべく 学習アルゴリズムを実装する時に アンロールのアイデアをどう使うかをここに示す。 シータ1、シータ2、シータ3の 何らかの初期値が あるとする。 我らがやりたいのは これら全部を取り出して 一つの長いベクトルにアンロールしたい。 これをinitialThetaと呼ぼう。 これをパラメータ、シータの 初期値としてfminuncに渡す。 他にやらなきゃいけない事としては、コスト関数を実装する、という事。 これが私のコスト関数の実装だ。 コスト関数は thetaVecという入力を受け取る、 これはパラメータの ベクトルで、それは ベクトルにアンロールされた形式で入っている。 だから最初に やるべき事は thetaVecを使って、reshape関数を使い、 thetaVecから要素を取り出し reshapeを使って 元のパラメータ行列、 シータ1、シータ2、シータ3を復元する。 これらが得られるであろう行列だ。 こうすることで、 微分やコスト関数、Jのシータを 計算する為に フォワードプロパゲーションや バックプロパゲーションを実行する為に これらの行列を使う事が出来る。 最後に、微分をとって、 シータをアンロールした時と 要素が同じ順番になるように アンロール出来る。 だが、今度はD1、D2、D3を アンロールする、 コスト関数が返す事ができるgradientVecを得る為に。 これでこれらの微分のベクトルを返す事が出来る。 以上で、ガウス分布が 行列表現にしたり ベクトル表現にしたり、の変換を どうやったらいいのか だいぶはっきり分かったんじゃないかな。 行列表現の利点は パラメータを 行列に保存しておけば フォワードプロパゲーションや バックワードプロパゲーションを行う時に より便利で、しかも 実装をいわゆるベクトル化する時にも 行列の方がやりやすい、という 利点がある。 一方対照的に、ベクトル表現の 利点は、つまり thetaVecとかDVecにしておく利点は アドバンスドな最適化アルゴリズムを使う時だ。 それらのアルゴリズムは パラメータを一つの大きなベクトルに アンロールしてある事を仮定している事が多い。 さて、以上見てきた事で、 2つの間を手早く 変換出来るようになった事でしょう。