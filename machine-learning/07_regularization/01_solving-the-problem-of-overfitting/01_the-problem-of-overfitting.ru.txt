Вы уже познакомились с двумя разными алгоритмами: линейной регрессии и логистической регрессии. Они работают хорошо для большинства задач, но в применении к  определенным приложениям машинного обучения эти алгоритмы могут столкнуться с проблемой, называемой переобучением, что может значительно ухудшить их производительность. В этой лекции я объясню вам, что такое переобучение. А в следующих лекциях мы обсудим технику, которая называется регуляризацией. Она поможет нам снизить влияние переобучения, что возможно приведет к улучшению работы алгоритма обучения. Итак, что такое переобучение? Давайте продолжим использовать наш пример предсказания стоимости дома с помощью линейной регрессии. В этом примере, мы пытаемся выразить цену дома как функцию от его размера. Один из возможных подходов состоит в подборе линейной функции для наших данных. Если мы сделаем это, мы получим какую-то прямую, подходящую под наши данные. Но это не очень хорошая модель. Глядя на график, кажется очевидным, что при увеличении размера дома, рост цены замедляется, при движении по графику вправо. Таким образом, этот алгоритм не подходит для обучения. Мы это называем недообучением, или, другими словами, этот алгоритм имеет большую систематическую ошибку. Грубо говоря, такой алгоритм плохо работает даже на данных для обучения. Термин сложился исторически или технически, но идея состоит в том, что при выборе прямой линии для графика, наш алгоритм будет предвзято считать изменение цены дома линейно от его размера, несмотря на то, что график показывает обратное. Несмотря на очевидное несоответствие, алгоритм настойчиво продолжает подгонять прямую линию, что ведет к плохой аппроксимации данных. Теперь, на среднем графике, мы можем попробовать подобрать квадратичную функцию. Подбирая для этих данных квадратичную функцию, мы можем получить такую кривую, которая неплохо работает. В последнем случае, рассмотрим что будет, если мы попробуем подобрать полином, скажем, четвертого порядка. Итак, у нас есть пять неизвестных от тета ноль до тета четыре. Таким образом, мы можем нарисовать кривую, которая проходит через все пять точек на графике. Вы можете получить кривую, похожую на эту. С одной стороны, кажется, что алгоритм хорошо справляется со своей работой на учебном наборе данных, то есть по крайней мере, проходит через все точки на графике. Но это очень волнистая кривая, верно? Итак, линия идет вверх и вниз на всем графике, и мы вряд ли считаем, что это хорошая модель для предсказания цен на жильё. Мы называем такую проблему переобучением. Или по-другому, этот алгоритм имеет большую дисперсию. Термин высокая дисперсия – еще одни исторический или технический термин. Интуитивно мы понимаем, что если мы подбираем полином большой степени, если наша гипотеза полностью соответствует данным, тогда наша гипотеза слишком сложная, слишком дисперсионна. И у нас не достаточно данных для ограничения и получения хорошей гипотезы. Это мы называем переобучением. И в центре я напишу "в самый раз", хотя это и не настоящее наименование. Где полином второй степени, квадратичная функция, представляется наиболее подходящим для аппроксимации этих данных. Итак, мы сталкиваемся с проблемой переобучения, если у нас много величин и наша гипотеза слишком точно соответствует обучающей выборке. Ваша стоимостная функция может приближаться к нулю или даже быть равна нулю, но кривая в таком случае выглядит как эта. Функция чрезмерно подходит обучающей выборке, но, в итоге, эта функция не подходит для обобщения и предсказания цен новых примеров. Здесь термин обобщение относится к тому, насколько хорошо гипотеза применима для новых данных. То есть тех данных, которые не представлены в обучающей выборке. На этом слайде мы видим переобучение для линейной регрессии. То же самое применимо и к логистической регрессии. Здесь пример логистической регрессии двух величин x1 и x2. Первое что мы можем попробовать, это представить логистическую регрессию очень простой гипотезой, подобной этой, где, обычно, g(x) - это сигмоид. И если вы сделаете это, вы получите гипотезу, которая использует просто прямую линию, разделяющую положительные и отрицательные примеры. И это выглядит не очень подходящей гипотезой. Это вновь пример недообучения, или гипотеза имеет большое смещение (bias). Напротив, если вы добавите в вашу функцию эти квадратичные одночлены, тогда вы можете получить решение, границы которого выглядят вот так. И это достаточно хорошее представление данных. Возможно это самое лучшее решение, которое мы можем получить на данной обучающей выборке. Наконец, другая крайность, если вы возьмете многочлен большой степени со всевозможными вариантами одночленов высокого порядка, логистическая регрессия может быть искажена. Поиск границ решения в таком случае может быть очень сложным. Вы получите кривую, которая соответствует каждому элементу обучающей выборки. Если признаки x1 и x2 используются для предсказания рака, предсказания того, является ли опухоль груди злокачественной или доброкачественной Это не очень подходящая гипотеза для предсказаний. Это снова пример переобучения, гипотеза имеет высокую дисперсию и маловероятно, что она хорошо обобщается на новые примеры. Позже, когда мы будем говорить об отладке и диагностике вероятных проблем с алгоритмами обучения, мы покажем вам специальные инструменты, позволяющие определить возникновение ситуации как переобучения, так и недообучения алгоритмов. Но сейчас давайте поговорим о проблеме, если мы считаем, что возникает переобучение, то что мы можем с этим поделать? В предыдущих примерах у нас были одномерные или двумерные данные, так что мы могли взять и построить гипотезы, увидеть, что происходило, и выбрать соответствующую степень многочлена. Ранее, в примере с ценами домов, мы могли просто построить гипотезу и увидеть, что она аппроксимируется очень извилистой функцией, проходящей через все ценовые точки. Также мы могли бы использовать графики, как эти, чтобы выбрать соответствующую степень многочлена. Построение гипотез могло бы быть одним из способов попытаться определить, какую степень многочлена использовать. Но это не всегда работает. На практике у нас зачастую задачи обучения, где у нас много признаков. И дело не только в выборе того, какую степень многочлена использовать. Фактически, когда у нас много признаков, становится намного труднее визуализировать данные, чтобы решить какие признаки учитывать, а какие - нет. Конкретно, если мы пытаемся предсказывать цены домов, у нас может быть множество различных признаков. И, вроде бы, все эти факторы кажутся полезными. Но, если у нас много признаков и мало обучающих данных, то переобучение может стать проблемой. Существует два основных варианта того, что мы можем сделать, чтобы решить проблему переобучения. Первым вариант - попробовать сократить число признаков. А именно, мы могли бы сделать следующее - просмотреть список факторов вручную и использовать это, чтобы попытаться решить, какие из них более важные, и, следовательно, какие из них учитывать и какие - отбросить. Далее в этом курсе, где мы также поговорим об алгоритмах выбора модели. Это алгоритмы для автоматического определения того, какие из признаков оставить, а какие отбросить. Идея сокращения числа признаков может хорошо сработать и уменьшить переобучение. И, когда мы заговорим о выборе модели, будем рассматривать еще глубже. Но недостатком является то, что отбрасывание некоторых признаков - это также и отбрасывание некоторой информации, которая у вас есть о задаче. Например, возможно, что все эти признаки полезны при предсказании стоимости дома, так что мы скорее всего не захотим отбросить часть нашей информации или несколько признаков. Второй вариант, о котором мы будем говорить в нескольких следующих видео, - это регуляризация. Здесь, мы сохраним все признаки, но ограничим величину изменения значений параметров тета J. И этот метод хорошо работает, мы увидим, в случае когда у нас много признаков, каждый из которых дает небольшой вклад в предсказание значения Y, наподобие того, что мы видели в примере предсказания стоимости жилья. Где у нас могло быть много признаков, каждый из которых в некоторой степени полезен, так что нам бы не хотелось их отбрасывать. Таким образом, это в общих чертах описывает идею регуляризации. И я понимаю, что все эти детали, возможно, пока не имеют для вас никакого значения. Но в следующем видео мы начнем строго формулировать, как применять регуляризацию и какое, собственно, она имеет значение. А затем мы начнем выяснять, как это использовать, чтобы заставить обучающие алгоритмы хорошо работать и избежать переобучения.