En este video, me gustaría trasmitirte las intuiciones principales detrás de cómo funciona la regularización. Y, también escribiremos la función de costos que usaremos, cuando utilicemos la regularización. Con los ejemplos dibujados a mano que tenemos en estas diapositivas, creo que seré capaz de transmitirte parte de la intuición. Pero, incluso una mejor forma de ver tú mismo cómo funciona la regularización, es si la implementas, y ves cómo funciona. Y si haces los ejercicios apropiados después de esto, tendrás la oportunidad de ver tú mismo la regularización en acción. Entonces, aquí está la intuición. En el vídeo anterior, vimos que, ajustar una función cuadrática a estos datos, nos da un buen ajuste a los datos. Considerando que, si fuéramos a ajustar un polinomio de muy alto grado, terminaríamos con una curva que podría ajustarse al conjunto de entrenamiento bastante bien, pero que en realidad no es un, pero que sobreajustará los datos mal, y no generalizará bien. Considera lo siguiente, supongamos que fuéramos a penalizar, y volver los parámetros «theta»3 y «theta»4 muy pequeños. Esto es lo que quiero decir, aquí esta nuestro objetivo de optimización, o aquí está nuestro problema de optimización, en donde minimizamos nuestro error al cuadrado  usual que causa la función. Digamos que tomo este objetivo y lo modifico y le añado, + 1000 «theta»3 al cuadrado, + 1000 «theta»4 al cuadrado. Solamente estoy escribiendo 1000 como ejemplo de un número muy grande. Ahora, si fuéramos a minimizar esta función, la única manera de hacer esta nueva función de costos más pequeña es si «theta»3 y «theta»4 son pequeñas, ¿correcto? Porque de otra forma, si tienes mil veces «theta»3, esta nueva función de costos va a ser grande. Así que cuando minimizamos esta nueva función vamos a terminar con el valor de «theta»3 cercano a 0 y de «theta»4 cercano a 0, y como si nos deshiciéramos de estos dos términos de ahí. Y si hacemos eso, entonces, si «theta»3 y «theta»4 están cerca de 0 nos estamos quedando con una función cuadrática, y entonces terminamos con un ajuste a los datos, que tiene, como sabes la función cuadrática más tal vez, pequeñas contribuciones de términos pequeños, «theta»3, «theta»4, que pueden estar muy cerca de 0. Y, así, terminamos con en esencia, una función cuadrática, lo que es bueno, debido a que esta es una mucho mejor hipótesis. En este ejemplo particular, analizamos el efecto de penalizar dos de los valores de parámetro más grandes. Más en general, esta es la idea detrás de la regularización. La idea es que, si tenemos valores pequeños para los parámetros, entonces, teniendo valores pequeños para los parámetros, de alguna manera, corresponderá generalmente con tener una hipótesis más simple. Así que, para nuestro último ejemplo, penalizaremos únicamente «theta»3 y «theta»4 y cuando ambos estén cerca de cero, terminaremos con una hipótesis mucho más simple que es esencialmente una función cuadrática. Pero en términos más generales, si penalizamos a todos los parámetros normalmente, nosotros pensaríamos que está tratando de darnos una hipótesis más simple también porque cuando, como sabes, estos parámetros están cerca de 0 en este ejemplo, nos da una función cuadrática. Pero más generalmente, es posible demostrar que al tener valores más pequeños de los parámetros generalmente corresponde a funciones más suaves así como más simples. Y por lo tanto, también, que son menos propensas al sobreajuste. Me doy cuenta de que el razonamiento de por qué todos los parámetros deben ser pequeños corresponde a una hipótesis más simple; me doy cuenta de que ese razonamiento puede no ser totalmente claro para ti ahora. Y es difícil explicarlo a menos de que lo implementes y lo compruebes tú mismo. Pero espero que el ejemplo de reducir «theta»3 y «theta»4 y de cómo eso nos da una hipótesis más simple, espero que ayude a explicar por qué, o por lo menos a darte una intuición en cuanto a por qué esto podría ser cierto. Veamos el ejemplo específico. Para la predicción del precio de la vivienda podríamos tener nuestras cien variables de las que hemos hablado antes, en donde x1 es el tamaño, x2 es el número de habitaciones, x3 es el número de pisos y así sucesivamente. Podríamos tener cien variables. Y a diferencia del ejemplo del polinomio, no sabemos, ¿correcto?, no sabemos que «theta»3, «theta»4, son términos polinomiales de alto grado. Así que, si tenemos sólo una bolsa, si tenemos solo un conjunto de cien variables, es difícil elegir de antemano cuales son las que tienen menos probabilidad de ser relevantes. Así es que tenemos cien o ciento un parámetros. Y no sabemos cuales elegir, no sabemos cuales parámetros elegir, para tratar de reducirlos. Así que, en la regularización, lo que vamos a hacer, es tomar nuestra función de costos, aquí está mi función de costos para la regresión lineal. Después, es, modificar esta función de costos para reducir todos mis parámetros, porque, como sabes, no sé cuáles uno o dos, tratar de reducir. Así que voy a modificar mi función de costos para agregar un término al final. Así, aquí también tenemos corchetes . Cuando agrego un término de regularización extra al final para reducir cada uno de los parámetros y así,
este término que tiende a reducir todos mis parámetros «theta»1, «theta»2, «theta»3, hasta «theta»100. Por cierto, la conversión de la suma comienza aquí desde uno, así es que no voy a penalizar «theta»0 por ser grande. En este tipo de convención, la suma de "i" es igual a 1 hasta "n", en lugar de que "i sea igual a 0 hasta "n". Pero en la práctica, hace muy poca diferencia y, ya sea que incluyas, como sabes, «theta»0 o no, en la práctica, hace muy poca diferencia en los resultados. Pero por la convención, por lo general, regularizamos únicamente de «theta»1 a «theta»100. Escribiendo nuestro objetivo de optimización regularizado, nuestra función de costos regularizada otra vez. Aquí está. Esta es "J" de «theta» donde, este término a la derecha es un término de regularización y «lambda» aquí es llamada parámetro de regularización y lo que «lambda» hace es controlar el intercambio entre dos diferentes metas. La primer meta, capturada por el primero objetivo de la meta, es que nos gustaría entrenar, es que nos gustaría que los datos del entrenamiento se ajusten bien. Nos gustaría ajustar bien el conjunto de entrenamiento. Y la segunda meta es, que queremos mantener los parámetros pequeños, y eso está capturado por el segundo término, por el objetivo de regularización, y por el término de regularización. Y lo que «lambda», el parámetro de regularización, hace es controlar el intercambio entre estas dos metas, entre la meta de ajustar bien el conjunto de entrenamiento y la meta de mantener el parámetro pequeño y por lo tanto mantener la hipótesis relativamente simple para evitar el sobreajuste. Para nuestro ejemplo de predicción de precios de vivienda, mientras que, anteriormente, si habíamos ajustado un polinomio de alto grado, podríamos haber terminado con una muy, una especie de función curva muy ondulada similar a esta. Si todavía ajustas un polinomio de alto grado con todas las variables polinomiales en él, pero en su lugar, te aseguras de usar este objetivo de regularización, entonces lo que obtienes es, de hecho, una curva que no es una función cuadrática, pero es mucho más suave y mucho más simple y tal vez una curva como la línea magenta, como sabes, te da una hipótesis mucho mejor para estos datos. Una vez más, me doy cuenta que puede ser un poco difícil por qué el fortalecer los parámetros puede tener este efecto, pero si implementas tú mismo la regularización serás capaz de ver este efecto de primera mano. En la regresión lineal regularizada, si el parámetro de regularización «lambda» está establecido para ser muy grande, entonces lo que sucederá es que terminaremos penalizando a los parámetros «theta»1, «theta»2, «theta»3, «theta»4 muy bien. Es decir, si nuestra hipótesis es esta de aquí abajo, en la parte inferior. Y si llegamos a penalizar «theta»1, «theta»2, «theta»3, «theta»4 muy fuertemente, entonces terminaremos con todos estos parámetros cercanos a cero ¿cierto? «tetha» 1 será cercana a cero, «theta»2 será cercana a cero. «tehta» 3, «theta»4, terminarán por ser cercanas a cero. Y si hacemos eso, es como si nos deshiciéramos de estos términos en la hipótesis así que nos quedamos solo con la hipótesis que dice esto. Dice que, bueno, los precios de vivienda son iguales a «theta»0, y eso es similar a ajustar una línea recta horizontal plana a los datos. Y este es un ejemplo de subajuste, y en particular esta hipótesis, esta línea recta no puede ajustarse al conjunto de entrenamiento correctamente. Es sólo una línea recta ancha, no va, como sabes, no se acerca. No va a ninguna parte cerca de la mayoría de los ejemplos de entrenamiento. Y otra manera de decir esto es que esta hipótesis tiene una preconcepción demasiado fuerte u oscilaciones demasiado altos de que los precios de la vivienda son iguales a «theta»0, y a pesar de los claros datos sobre lo contrario, como sabes, elige ajustar una especie de, línea plana, solamente una línea horizontal plana. No dibujé muy bien eso. Esto es solamente una línea plana horizontal para los datos. Así que para que la regularización funcione bien, debes tener cierto cuidado, para elegir también una buena opción para el parámetro de regularización «lambda». Y cuando hablemos de selección múltiple más adelante en este curso, vamos a hablar acerca de una manera, una variedad de maneras para también elegir automáticamente el parámetro de regularización «lambda». Así que esa es la idea de la regularización alta y de la función de costos que utilizaremos para poder aplicar la regularización en los dos videos siguientes, vamos a tomar estas ideas y aplicarlas a la regresión lineal y a la regresión logística, entonces podremos sacarlas para evitar problemas de sobreajuste.