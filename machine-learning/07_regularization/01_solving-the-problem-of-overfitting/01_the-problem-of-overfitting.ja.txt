ここまでで、幾つかの 異なる学習アルゴリズムを見てきた。 線形回帰とロジスティック回帰。 それらは様々な問題でうまく機能する。 だがそれをある種の機械学習の問題に 適用しようとする時に、 オーバーフィッティングと呼ばれる問題に遭遇して とてもしょぼいパフォーマンスしか発揮出来ないハメになる事がある。 このビデオで私がやりたい事は、 あなたに、オーバーフィッティングの問題とは 何かを説明し、 そして続く 一連のビデオで、 正規化（regularization）と呼ばれるテクニックを議論していきたい。 正規化は我らに オーバーフィッティングの問題を 緩和したり減らす事を可能にしてくれて、 これらの学習アルゴリズムをもっと良く機能するようにしてくれる事がある物だ。 さて、オーバーフィッティングとはなんだろう？ 説明の為の例として、 線形回帰で住居の価格を 住居のサイズの関数として 予測したい、という 例を引き続き用いる事にしよう。 一つ出来る事としては、 このデータに線形関数を フィッティングする、というのがある。 それをすると、こんな感じの データにフィットした直線が得られる。 だがこれはとても良いモデルとは言えない。 データを見てみると、 住居のサイズが上昇するにつれて、 住居の価格はだんだんと台地になっていく、 あるいはある種、平坦になっていく、我らが右に移動していくに連れて、 というのが、明らかに見て取れる。 だからこのアルゴリズムはトレーニングセットに あまりフィット出来ない。 この問題をアンダーフィッティングと呼ぶ。 また別の用語としては、 このアルゴリズムは高バイアスだ、とも言う。 これらはどちらもだいたいは トレーニングデータにすら、あまり良くはフィッティング出来ていない、という事を意味している。 用語はある種 歴史的、技術的な物だが、 そのアイデアは、 データに直線を フィッティングさせると、 アルゴリズムは、住居の価格が サイズに応じて線形に変わる、という 強い前提あるいはバイアスを 置く、という事を意味し、 しかもデータはそれに反している、という事だ。 反対の証拠があるにも関わらず、 前提がバイアスされたままで 直線にフィットさせる、という事に 固執したままだと データにあまりよくフィットしない、というハメになる。 ここで、真ん中は、 二次関数でフィットさせるとする。 このデータセットに、二次関数で フィッティングすると、 こんな種類のカーブを得る。 これはかなり良さそう。 そして反対側の極端として、例えば四次の多項式でデータにフィッティングする、というのが考えられる。 この場合5つのパラメータがある。 シータ0からシータ4まで。 それでもって、我らは実際に曲線を 5つの全ての手本を通るようにフィッティング出来てしまう。 たとえばこんな感じの曲線が得られる。 これは一方では トレーニングセットに フィッティングするという点では とてもよい仕事をしているように見える、 少なくともデータの上を全て通るのだから。 だがまたこれは、とてもうねうねした曲線でもあるね？ つまり上に行ったり下に行ったり あちこち通って、住居の価格を予測するのに そんなに良さそうには思えない。 だから、この問題を オーバーフィッティングと呼んでいて、 また別の用語としては、 このアルゴリズムは高バリアンスだ、とも言う。 この高バリアンスという用語はまた別の 歴史的、技術的な物だ。 だが感覚的には、 そんなに高次の多項式に フィッティングすると、 その場合仮説は まるでどんな関数にも フィッティング出来てしまい、 可能な仮説の数が単純に 多くなりすぎる、あまりにも変わりすぎるという問題に直面する事になる。 そして我らが、仮説を 良い物だけに制約出来るほどには データを持っていない。だからこれはオーバーフィッティングと呼ばれる。 そして真ん中。これには名前は無いが、 これはちょうど良い、という状態。 二次の多項式、二次関数は このデータにフィッティングするのに、ただ単にちょうど良さそう。 ちょっと復習しておこう。 オーバーフィッティングの問題は フィーチャーが多すぎる時に 起こり、その場合、仮説はトレーニングセットには とても良くフィットするように学習する。 だから、あなたのコスト関数は 実際にとても0に近い所に行くだろう、 時には完全に0に 一致する事もある。 だがそれは、こんなカーブに なってしまっているかもしれない。 つまりトレーニングセットにフィットさせようと あまりにも頑張りすぎてて、 それは新しい手本に対して一般化する事に 失敗していて、だから新しい手本に対して 価格を予測する事にも失敗してしまう。 ここで一般化という 用語は、新規の手本に対して 仮説がどれだけ良く適用出来るか、という事を意味する。 それは、トレーニングセットに無い 住居のデータについて、という事。 このスライドでは線形回帰の場合での オーバーフィッティングを見てきた。 似たような事はロジスティック回帰でもありうる。 これは2つのフィーチャー、x1とx2による ロジスティック回帰の例だ。 我らが出来る事としては、 このような単純な仮説に ロジスティック回帰をフィッティングする、というのが考えられる。 ここでgはいつも通りsigmoid関数だ。 そしてこれをやると、結局 単なる直線で 陽性と陰性の手本を分割しようと試みる 仮説が得られる事になる。 これはとても良く仮説にフィットしてるようには、とても見えない。 つまり再びこれも、 アンダーフィッティングしている例、あるいは 仮説が高バイアスになってる例だ。 対照的に、もしあなたのフィーチャーに これらの二次の項を 追加したら、 その場合はこんな感じの 決定境界が得られるだろう。 そしてこれは、かなり良くデータにフィットしてるように見える。 たぶん、このトレーニングセットに対しては もっとも良さそうに見える。 そして最後に、反対側の極端として、 とても高次の多項式に フィッティングすると、 もし仮にたくさんの高次の項のフィーチャーを 生成したとすると、 その場合、ロジスティック回帰は 自身をゆがめて、 凄い懸命に あなたのトレーニングデータにフィットするように 決定境界を見つけようとする、あるいは 凄く長くなるように自分自身をゆがめて トレーニング手本一つ一つにとても良くフィットするように進んでいく。 そしてもしフィーチャーの x1とx2が 例えば癌の予測を 提供するとすると、 癌が悪性か、乳腺腫瘍になりそうかの予想を提供するなら、 これも、とても良い仮説、という風には 思えない、予測の為に使うには。 だからここでも、 これはオーバーフィッティングの例で、 そして仮説は高バリアンスに なっていて、たぶん新規の手本に対して うまく一般化されそうには無い。 このコースの後の方で、 学習アルゴリズムが おかしな事になった時の デバッグや診断の話をする時に、 どういう時にオーバーフィッティングが起きて どういう時にアンダーフィッティングが起きているかを識別する 具体的なツールを紹介する。 だが現時点では、まずオーバーフィッティングの問題が 既に起こっているとして、 それについてどう対処したらいいのか？を 議論しよう。 以前の例では、 1次元とか2次元のデータだった。 だから仮説をプロットして何が起きているのかを見る事が出来て、 適切な次数の仮説を選ぶ事が出来た。 だから以前の住居の価格の例では、 仮説を単にプロットしてみて、 そしてそれを見て それがある種の、 とてもうねうねと波打った関数で 全ての住居の価格を予測する点を通るようにフィッティングしてしまっている事を見る事が出来るかもしれない。 そしてこのような図を用いて 多項式の適切な次数を選ぶ事が出来るかもしれない。 つまり仮説をプロットするのは、 どの次数を使うべきかを決める 一つの方法たりえる。 だがそれがいつも使える訳でも無い。 そして実のところ、多くの場合において、我らは たくさんのフィーチャーを含む学習問題を扱う事になる。 そしてまた、これは単に 多項式の次数を選ぶだけの問題では無い。 そして実際、 そんなにたくさんのフィーチャーがある時には データをプロットする事ももっと難しくなり だからそれを可視化するのも もっと困難となる、 どのフィーチャーを維持すべきか、そうでないかを決める為に使うプロットを。 具体的には、住居の価格を予測するのに 様々なフィーチャーを使って予測する、という事がありうる。 そしてこれら全てのフィーチャーが、役に立ちそうに思える事がある。 だが、たくさんのフィーチャーが ある場合には、そしてトレーニングデータが 少ない時には、 その場合はオーバーフィッティングが問題になりうる。 オーバーフィッティングの問題をなんとかする為に 取れる手段としては大きく 2つの選択肢がある。 最初の選択肢は フィーチャーの数を減らすこと。 具体的には、一つ出来る事としては 人力でフィーチャーのリストを 見ていって、 そしてどれがもっとも重要なフィーチャー群で ゆえにどれが維持すべき物か そしてどれが捨て去るべき物かを 決定する、というのが考えられる。 このコースの後半では、 モデル選択のアルゴリズムについて議論する。 それは自動的に、どのフィーチャーを 維持しつづけて、どのフィーチャーを 捨て去るかを決定するアルゴリズムだ。 このフィーチャーの数を減らす、 というアイデアは うまく行くこともあり、オーバーフィッティングを低減しうる。 そしてモデル選択の話をする時に この話ももっとつっこんで行う。 だがここでは、その欠点の話をしていこう。 フィーチャーを幾つか捨て去る事は、 それは同時に問題について自分の持っている 情報を捨て去る事でもある。 例えば、それらのフィーチャーが全て 実際に住居の価格を予測するのに 有用だったとしよう。 その場合我らは本当は 我らの持つ情報の一部を捨て去りたくは、 我らのフィーチャーを捨て去りたくは無いだろう。 二番目の選択肢は、 これは続く一連のビデオで扱う事になるが、 それは正規化(regularization)を行う、という事。 ここでは、全てのフィーチャーを 維持しつづけて、 だがパラメータシータjの 倍率を下げる。 そしてこの手法は あとで見るように、以下のようなケースではうまく行く： それは我らがたくさんのフィーチャーを持っていて、 その各々がちょっとずつ Yの値を予測するのに 貢献している、という場合だ。 ちょうど住居の価格の予測の例で見たように。 そこではたくさんのフィーチャーがありえて、 それらはおのおの、 いくらか有用で、それらを捨て去りたくは無い。 以上は正規化のアイデアを とても高いレベルで記述した物だ。 だからこれらの詳細の全てが あなたにピンと来る訳では無いという事は分かっている。 だが次のビデオから、 厳密にどう正規化を適用するのか、 正規化とは何を意味するのかを定式化していく。 その時には、我らはこれを どのように用いる事で、 学習アルゴリズムをうまく機能させ オーバーフィッティングを避ける事が出来るか、知る事となろう。