1
00:00:00,260 --> 00:00:01,490
Для линейной регрессии мы ранее разработали два

2
00:00:01,680 --> 00:00:03,130
алгоритма  обучения, базирующихся на

3
00:00:03,490 --> 00:00:05,010
градиентном спуске и на нормальном

4
00:00:05,180 --> 00:00:07,650
уравнении

5
00:00:08,750 --> 00:00:09,740
В данном видео мы возьмем эти

6
00:00:09,890 --> 00:00:11,640
два алгоритма и обобщим их

7
00:00:12,290 --> 00:00:13,380
к случаю регуляризованной линейной

8
00:00:14,330 --> 00:00:17,640
регрессии.

9
00:00:18,100 --> 00:00:19,540
Вот задача оптимизации, которую мы придумали в прошлый раз

10
00:00:20,200 --> 00:00:22,380
для регуляризованной линейной регрессии.

11
00:00:23,360 --> 00:00:24,580
Вот это первая часть - наша обычная задача для случая

12
00:00:24,980 --> 00:00:27,240
линейной регрессии, а теперь у нас появилось вот это дополнительное

13
00:00:28,170 --> 00:00:29,300
слагаемое,  где ламбда - это наш

14
00:00:30,200 --> 00:00:31,750
параметр регуляризации, и мы хотим

15
00:00:32,450 --> 00:00:34,960
найти параметры тета, которые

16
00:00:35,220 --> 00:00:36,690
минимизируют функцию

17
00:00:37,160 --> 00:00:38,550
затрат, вот

18
00:00:39,030 --> 00:00:41,280
эту регуляризованную функцию затрат, J от тета.

19
00:00:41,840 --> 00:00:43,030
Раньше мы

20
00:00:43,440 --> 00:00:45,180
использовали

21
00:00:46,620 --> 00:00:48,060
градиентный спуск для исходной функции затрат, без компонента

22
00:00:48,770 --> 00:00:49,820
регуляризации  и у нас был следующий

23
00:00:50,060 --> 00:00:51,990
алгоритм для обычной линейной регрессии

24
00:00:52,370 --> 00:00:53,620
без регуляризации.

25
00:00:54,660 --> 00:00:56,260
Мы будем последовательно вычислять параметры

26
00:00:56,330 --> 00:00:57,670
тета-j по этой формуле, где j=1,2 и

27
00:00:58,270 --> 00:01:00,030
так далее

28
00:01:00,400 --> 00:01:02,110
до n.

29
00:01:02,530 --> 00:01:03,960
Я сейчас запишу уравнение для

30
00:01:04,240 --> 00:01:06,580
тета-0 отдельно.

31
00:01:07,210 --> 00:01:08,400
Я собираюсь просто

32
00:01:08,720 --> 00:01:09,900
отделить

33
00:01:10,160 --> 00:01:12,500
формулу для

34
00:01:12,680 --> 00:01:14,380
тета-0 отдельно от формул для

35
00:01:14,780 --> 00:01:17,090
параметров

36
00:01:17,370 --> 00:01:19,760
тета-1, 2, 3 ... n.Ничего ведь не изменилось, верно?

37
00:01:19,970 --> 00:01:21,070
Мы просто записываем формулу

38
00:01:21,300 --> 00:01:23,300
для тета-0 отдельно от

39
00:01:23,550 --> 00:01:25,240
формул для тета-1, тета-2, тета-3 и так

40
00:01:25,510 --> 00:01:26,980
далее до тета-n.

41
00:01:27,040 --> 00:01:27,900
Вот почему я хочу это сделать.

42
00:01:28,230 --> 00:01:29,320
Вы возможно помните, что

43
00:01:29,880 --> 00:01:31,260
для нашей регуляризованной линейной

44
00:01:32,620 --> 00:01:33,970
регрессии мы накладываем

45
00:01:34,440 --> 00:01:35,540
штраф на

46
00:01:35,860 --> 00:01:38,360
параметры тета-1. тета-2, и так далее до тета-n, но мы не штрафуем тета-0.

47
00:01:38,820 --> 00:01:40,250
Таким образом, когда мы

48
00:01:40,410 --> 00:01:42,400
изменяем алгоритм для случая

49
00:01:42,750 --> 00:01:44,050
регуляризованной линейной регрессии,

50
00:01:44,710 --> 00:01:46,870
мы в конце концов будем  вычислять с тета-0 немного по-другому.

51
00:01:48,560 --> 00:01:50,360
А именно, чтобы изменить  этот алгоритм с использованием регуляризационной

52
00:01:50,500 --> 00:01:52,170
задачи,  все

53
00:01:52,300 --> 00:01:53,780
что нам нужно, это взять

54
00:01:53,870 --> 00:01:55,630
вот эту нижнюю формулу  и

55
00:01:55,740 --> 00:01:57,170
изменить ее

56
00:01:57,350 --> 00:02:00,010
следующим образом.

57
00:02:00,460 --> 00:02:01,860
Мы возьмем этот

58
00:02:02,670 --> 00:02:05,310
элемент и прибавим  минус

59
00:02:06,330 --> 00:02:08,920
ламбда деленная на M, умноженная

60
00:02:09,100 --> 00:02:10,850
на тета-j. Если мы так сделаем, то

61
00:02:11,000 --> 00:02:13,230
получим

62
00:02:13,960 --> 00:02:15,920
градиентный спуск для

63
00:02:16,160 --> 00:02:18,200
минимизации регуляризированной функции затрат J от тета  в частности

64
00:02:19,520 --> 00:02:20,570
(я не собираюсь делать вычисления, чтобы

65
00:02:20,680 --> 00:02:22,260
доказать это), но если вы

66
00:02:22,390 --> 00:02:23,480
посмотрите на выражение в

67
00:02:23,690 --> 00:02:26,580
квадратных скобках.

68
00:02:27,730 --> 00:02:28,930
Если произвести вычисления, можно  доказать, что

69
00:02:29,380 --> 00:02:31,150
это выражение является частной производной по

70
00:02:31,370 --> 00:02:33,150
J от тета,

71
00:02:33,980 --> 00:02:35,400
используя новое

72
00:02:35,660 --> 00:02:37,520
определение  J от тета  с регуляризационным

73
00:02:38,140 --> 00:02:39,330
компонентом.

74
00:02:39,510 --> 00:02:42,490
И, кстати, по поводу этого

75
00:02:42,760 --> 00:02:43,960
компонента в верхней

76
00:02:44,750 --> 00:02:45,570
формуле  который я обвожу другим цветом,

77
00:02:45,680 --> 00:02:47,240
это тоже частная производная

78
00:02:48,000 --> 00:02:49,270
от J от тета к

79
00:02:49,940 --> 00:02:52,700
тета-0.

80
00:02:53,680 --> 00:02:54,900
Если вы посмотрите на формулу расчета

81
00:02:55,600 --> 00:02:56,710
тета-j, то можете

82
00:02:56,910 --> 00:02:59,190
увидеть нечто примечательное.

83
00:02:59,860 --> 00:03:01,100
А именно, тета-j изменяется

84
00:03:01,280 --> 00:03:03,400
как  тета-j минус альфа умноженное

85
00:03:04,090 --> 00:03:05,010
на .. здесь еще один компонент

86
00:03:05,380 --> 00:03:06,730
зависимый

87
00:03:06,910 --> 00:03:08,310
от тета-j.

88
00:03:08,420 --> 00:03:09,410
Таким образом,  если сгруппировать все элементы

89
00:03:10,030 --> 00:03:11,690
зависящие от тета-j, то мы можем

90
00:03:11,780 --> 00:03:13,190
показать, что  эту

91
00:03:13,670 --> 00:03:15,100
формулу можно заменить эквивалентно

92
00:03:15,200 --> 00:03:16,160
следующим образом.

93
00:03:16,470 --> 00:03:17,620
Все что я сделал - это вынес тета-j,

94
00:03:18,310 --> 00:03:20,100
здесь тета-j умноженное на 1 и в этом

95
00:03:20,450 --> 00:03:21,950
компоненте ламбда деленная на M.

96
00:03:22,910 --> 00:03:24,830
Здесь еще и альфа, итак наконец

97
00:03:25,140 --> 00:03:25,990
получаем  альфа на ламбда

98
00:03:26,180 --> 00:03:27,650
деленная на m,  это

99
00:03:27,970 --> 00:03:31,450
все умножить на

100
00:03:31,820 --> 00:03:33,660
тета-j и это

101
00:03:34,230 --> 00:03:36,300
выражение следующее:

102
00:03:36,600 --> 00:03:39,470
единица минус альфа на ламбда делить на M - довольно интересный член дает интересный эффект.

103
00:03:42,310 --> 00:03:43,710
А именно, этот член:  единица

104
00:03:43,890 --> 00:03:45,320
минус альфа на ламбду делить

105
00:03:45,730 --> 00:03:46,780
на M  - это число, это ограничение,

106
00:03:46,870 --> 00:03:48,740
которое дает всегда число меньше

107
00:03:48,800 --> 00:03:50,390
единицы,

108
00:03:50,610 --> 00:03:51,670
верно?

109
00:03:51,920 --> 00:03:53,580
Так как альфа на ламбду делить

110
00:03:54,070 --> 00:03:55,920
на M  будет положительным и обычно при обучении коэффициент мал, а M большое

111
00:03:58,650 --> 00:03:58,860
Он обычно довольно мал.

112
00:03:59,650 --> 00:04:00,680
Итак. этот член будет числом,

113
00:04:00,740 --> 00:04:03,060
которое совсем немного меньше единицы.

114
00:04:03,340 --> 00:04:04,150
Мы будем его представлять числом

115
00:04:04,330 --> 00:04:05,860
типа 0,99 или что-то такое.

116
00:04:07,380 --> 00:04:08,800
Результат наших изменений тета-j

117
00:04:09,120 --> 00:04:10,550
таков, что мы  говорим, что

118
00:04:10,690 --> 00:04:11,950
тета-j  заменяется тета-j

119
00:04:12,410 --> 00:04:15,420
умноженная на 0,99.

120
00:04:15,770 --> 00:04:17,500
Хорошо, тета-j умноженная

121
00:04:18,490 --> 00:04:20,940
на 0,99  уменьшает тета-j

122
00:04:21,280 --> 00:04:23,560
приближает ее немного в сторону нуля.

123
00:04:23,670 --> 00:04:25,690
Таким образом делая тета-j немного меньше.

124
00:04:26,220 --> 00:04:28,080
Более формально эта квадратная норма тета-j

125
00:04:28,420 --> 00:04:29,750
меньше. А за ней,

126
00:04:29,870 --> 00:04:31,580
второй член многочлена

127
00:04:31,720 --> 00:04:33,430
представляет

128
00:04:33,910 --> 00:04:35,400
собой ровно такой же,  как и в

129
00:04:35,980 --> 00:04:37,930
исходном градиентном спуске,

130
00:04:38,050 --> 00:04:40,270
который мы вывели.

131
00:04:40,750 --> 00:04:42,840
До того, как мы добавили сюда регуляризацию.

132
00:04:44,270 --> 00:04:46,920
Итак, я надеюсь, что этот

133
00:04:47,380 --> 00:04:48,630
градиентный спуск,эти преобразования вам понятны,

134
00:04:48,880 --> 00:04:51,350
когда мы используем регуляризованную

135
00:04:51,550 --> 00:04:52,920
линейную регрессию, то мы

136
00:04:53,320 --> 00:04:55,210
для каждого J мы умножали

137
00:04:55,420 --> 00:04:56,310
данные на число,немного

138
00:04:56,400 --> 00:04:57,300
меньше единицы,  так мы уменьшали

139
00:04:57,400 --> 00:04:58,900
параметр совсем немного.

140
00:04:59,230 --> 00:05:00,340
А затем мы

141
00:05:00,500 --> 00:05:03,000
производим преобразование, такое же как и раньше.

142
00:05:04,170 --> 00:05:05,460
Конечно же, это упрощенное объяснение того,

143
00:05:05,610 --> 00:05:08,310
что это преобразование делает.

144
00:05:08,910 --> 00:05:10,130
Математически, это именно градиентный спуск

145
00:05:10,580 --> 00:05:12,950
функции затрат J от тета,

146
00:05:13,130 --> 00:05:14,330
который мы определили

147
00:05:15,150 --> 00:05:16,020
на прошлом слайде, с

148
00:05:16,480 --> 00:05:18,820
использованием регуляризационной составляющей.

149
00:05:19,780 --> 00:05:21,210
Градиентный спуск - это только

150
00:05:21,470 --> 00:05:23,050
один из двух алгоритмов

151
00:05:24,470 --> 00:05:25,530
подходящих для модели линейной регрессии.

152
00:05:26,630 --> 00:05:28,090
Второй алгоритм основан на нормальном

153
00:05:28,160 --> 00:05:29,130
уравнении, где

154
00:05:29,680 --> 00:05:31,650
мы записали матрицу

155
00:05:31,740 --> 00:05:32,980
плана "X", в

156
00:05:33,060 --> 00:05:34,770
которой каждой

157
00:05:35,080 --> 00:05:37,830
строке соответствует один обучающий пример.

158
00:05:38,520 --> 00:05:39,790
И мы записали

159
00:05:40,170 --> 00:05:41,780
вектор "Y",  размерности

160
00:05:41,940 --> 00:05:43,320
M, который

161
00:05:43,590 --> 00:05:45,520
состоит из меток обучающей

162
00:05:46,010 --> 00:05:47,750
выборки.

163
00:05:48,470 --> 00:05:49,600
А "x" - это матрица размерностью

164
00:05:49,830 --> 00:05:52,660
M на N+1.

165
00:05:53,590 --> 00:05:55,220
Y - это вектор

166
00:05:55,780 --> 00:05:57,550
размерности M Для

167
00:05:58,030 --> 00:05:59,200
того, чтобы минимизировать

168
00:05:59,470 --> 00:06:00,940
функцию затрат J, мы установили,

169
00:06:01,470 --> 00:06:03,000
что один из способов  - это

170
00:06:03,230 --> 00:06:04,440
приравнять ее к следующему

171
00:06:04,670 --> 00:06:06,790
выражению.

172
00:06:07,540 --> 00:06:09,040
X транспонированное на X ...

173
00:06:10,860 --> 00:06:12,770
обратная матрица на X транспонированная

174
00:06:13,020 --> 00:06:13,920
на Y.

175
00:06:14,120 --> 00:06:17,160
Я оставляю тут пустое место, чтобы потом его заполнить.

176
00:06:17,650 --> 00:06:18,820
Это значение тета

177
00:06:19,180 --> 00:06:20,980
минимизирует функцию затрат J от тета в том

178
00:06:21,250 --> 00:06:22,710
случае, когда мы

179
00:06:22,840 --> 00:06:26,280
не используем регуляризацию.

180
00:06:26,460 --> 00:06:28,580
Теперь

181
00:06:28,780 --> 00:06:30,290
будем использовать регуляризацию.

182
00:06:30,520 --> 00:06:31,820
Чтобы вычислить минимум надо...

183
00:06:31,910 --> 00:06:32,760
я только напомню, как

184
00:06:32,980 --> 00:06:34,110
вычислять минимум.

185
00:06:34,220 --> 00:06:35,220
Надо взять частные производные

186
00:06:35,930 --> 00:06:37,910
относительного каждого из параметров,

187
00:06:38,340 --> 00:06:40,600
приравнять это к

188
00:06:40,830 --> 00:06:41,910
нулю, а затем

189
00:06:42,060 --> 00:06:42,920
немного математики, и можно показать, что

190
00:06:43,100 --> 00:06:45,060
эта вот формула минимизирует

191
00:06:45,550 --> 00:06:47,640
функцию затрат.

192
00:06:48,590 --> 00:06:52,130
А именно. если вы используете регуляризацию,

193
00:06:52,240 --> 00:06:54,080
то эта формула меняется следующим

194
00:06:54,250 --> 00:06:56,320
образом

195
00:06:56,480 --> 00:06:59,120
В этих скобках мы получаем вот такую матрицу

196
00:06:59,460 --> 00:07:00,940
Ноль, один, один, один и так

197
00:07:01,800 --> 00:07:03,520
далее до конца.

198
00:07:04,510 --> 00:07:05,510
Итак, получилась матрица, у

199
00:07:05,630 --> 00:07:07,810
которой левый верхний элемент - это ноль.

200
00:07:08,560 --> 00:07:10,080
Единицы на главной диагонали, а все остальные

201
00:07:10,190 --> 00:07:11,960
нули.

202
00:07:13,050 --> 00:07:14,020
Я рисую немного небрежно

203
00:07:15,180 --> 00:07:16,790
Рассмотрим конкретный пример,

204
00:07:17,060 --> 00:07:18,210
при N=2. Тогда эта матрица

205
00:07:19,090 --> 00:07:21,110
будет матрицей

206
00:07:21,840 --> 00:07:23,500
три на три.

207
00:07:24,300 --> 00:07:26,210
Обобщенно, это матрица

208
00:07:26,360 --> 00:07:27,660
размерности

209
00:07:28,270 --> 00:07:30,290
N+1 на N+1.

210
00:07:31,620 --> 00:07:33,150
Соответственно когда N равно

211
00:07:33,370 --> 00:07:35,410
двум, то матрица будет выглядеть так:

212
00:07:35,980 --> 00:07:37,360
Ноль, затем единицы на главной диагонали, и

213
00:07:37,640 --> 00:07:39,020
нули на всех

214
00:07:39,160 --> 00:07:41,100
остальных диагоналях.

215
00:07:42,390 --> 00:07:43,990
И опять-таки я не буду вычислять эти производные

216
00:07:44,620 --> 00:07:46,280
Это откровенно длинно и сложно

217
00:07:46,620 --> 00:07:47,530
Однако можно

218
00:07:47,970 --> 00:07:49,550
доказать, что с использованием новой

219
00:07:49,940 --> 00:07:50,770
формулы для J от тета, с

220
00:07:51,250 --> 00:07:53,730
регуляризационной составляющей.

221
00:07:54,780 --> 00:07:56,070
То эта новая формула для тета - именно та,

222
00:07:56,220 --> 00:07:57,180
которая даст нам глобальный

223
00:07:57,390 --> 00:08:00,080
минимум J от тета.

224
00:08:01,420 --> 00:08:02,460
В конце, я хочу вкратце

225
00:08:02,610 --> 00:08:05,460
описать  аспект необратимости.

226
00:08:06,800 --> 00:08:08,110
Это довольно продвинутый материал

227
00:08:08,600 --> 00:08:09,530
Можно считать, что этот материал необязателен,

228
00:08:09,770 --> 00:08:11,600
так что можете его пропустить. А

229
00:08:11,750 --> 00:08:12,520
если вы его прослушаете, но

230
00:08:12,660 --> 00:08:14,180
ничего не поймете, не переживайте

231
00:08:14,320 --> 00:08:15,680
об этом.

232
00:08:16,400 --> 00:08:18,950
Ранее когда я говорил про метод нормального уравнения

233
00:08:19,700 --> 00:08:20,920
У нас уже было одно необязательное видео про

234
00:08:21,800 --> 00:08:22,960
необратимость матрицы.

235
00:08:23,700 --> 00:08:25,740
Так вот это еще одна необязательная часть, в дополнение к

236
00:08:26,170 --> 00:08:27,070
предыдущему необязательному видео

237
00:08:27,700 --> 00:08:30,100
про необратимость.

238
00:08:31,610 --> 00:08:33,350
Допустим, что M (количество примеров)

239
00:08:33,850 --> 00:08:35,340
меньше N

240
00:08:35,690 --> 00:08:37,530
(количества признаков).

241
00:08:38,650 --> 00:08:40,080
Если у нас примеров меньше, чем признаков,

242
00:08:40,200 --> 00:08:41,480
тогда матрица X транспонированная

243
00:08:42,170 --> 00:08:43,870
умноженное на X будет

244
00:08:44,070 --> 00:08:47,770
необратимой или исключительной,

245
00:08:48,060 --> 00:08:50,120
или другое название такой

246
00:08:50,360 --> 00:08:51,470
матрицы - вырожденная. Если Вы

247
00:08:51,530 --> 00:08:53,390
реализуете это на языке Octave и

248
00:08:53,860 --> 00:08:54,780
используете функцию pinv для

249
00:08:55,300 --> 00:08:56,380
того, чтобы найти псевдо-обратную

250
00:08:56,620 --> 00:08:58,570
матрицу

251
00:08:58,850 --> 00:08:59,800
Он выдаст

252
00:09:00,080 --> 00:09:01,900
нечто похожее на правду, это не

253
00:09:02,240 --> 00:09:03,450
совсем понятно, но

254
00:09:03,560 --> 00:09:04,570
он выдаст вам очень хорошую

255
00:09:05,410 --> 00:09:07,720
гипотезу пусть даже численно

256
00:09:08,370 --> 00:09:09,670
реализованная функция языка Octave pinv

257
00:09:10,020 --> 00:09:11,050
выдаст вам разумный

258
00:09:11,340 --> 00:09:13,210
результат

259
00:09:13,440 --> 00:09:15,460
Но если реализовать это на другом языке

260
00:09:16,270 --> 00:09:17,590
И если воспользоваться

261
00:09:17,710 --> 00:09:19,030
обычным обращением матрицы,

262
00:09:20,470 --> 00:09:22,070
которая в Octave представлена при помощи функции Inv

263
00:09:23,240 --> 00:09:24,010
Мы пытаемся обратить обычным образом матрицу

264
00:09:24,330 --> 00:09:25,620
X транспонированное на X, затем на

265
00:09:26,300 --> 00:09:28,030
этом шаге мы находим,

266
00:09:28,150 --> 00:09:30,340
что X транспонированное на X - это

267
00:09:30,450 --> 00:09:32,750
особенная, необратимая

268
00:09:32,790 --> 00:09:33,740
матрица и если мы попытаемся

269
00:09:33,990 --> 00:09:35,830
сделать это на другом языке

270
00:09:36,230 --> 00:09:39,160
программирования и с использованием какой-нибудь библиотеки линейной алгебры, попытаемся обратить эту матрицу.

271
00:09:39,840 --> 00:09:41,080
Оно вполне возможно и не

272
00:09:41,220 --> 00:09:43,060
заработает, так как матрица необратимая или особенная.

273
00:09:44,650 --> 00:09:47,110
К счастью, регуляризация заботится

274
00:09:47,110 --> 00:09:49,850
и об этом, а именно, до

275
00:09:50,010 --> 00:09:53,370
тех пор, пока параметр регуляризации строго больше нуля.

276
00:09:53,870 --> 00:09:55,220
Можно доказать, что эта

277
00:09:55,300 --> 00:09:56,840
матрица X транспонированная на X

278
00:09:57,080 --> 00:09:58,690
плюс ламбда умноженное

279
00:09:59,080 --> 00:10:00,400
на вот эту забавную

280
00:10:00,970 --> 00:10:02,250
матрицу, можно

281
00:10:02,470 --> 00:10:03,650
доказать, что эта матрица не

282
00:10:03,760 --> 00:10:05,710
будет особенной и то что она будет обратимой.

283
00:10:07,450 --> 00:10:09,430
Итак при использовании регуляризации

284
00:10:09,700 --> 00:10:11,910
также можно не думать о проблеме

285
00:10:12,580 --> 00:10:14,470
необратимости матрицы X транспонированная на X.

286
00:10:15,260 --> 00:10:18,000
Теперь вы знаете, как реализовать регуляризацию линейной регрессии.

287
00:10:18,870 --> 00:10:19,910
Используя ее, вы сможете

288
00:10:20,300 --> 00:10:21,970
избежать избыточной точности,

289
00:10:22,210 --> 00:10:24,720
даже когда у вас много признаков и сравнительно небольшая обучающая выборка.

290
00:10:25,360 --> 00:10:26,630
И это позволит линейной

291
00:10:26,980 --> 00:10:29,000
регрессии работать гораздо лучше для большего количества задач.

292
00:10:30,060 --> 00:10:31,190
В следующем видео мы рассмотрим эту идею

293
00:10:31,390 --> 00:10:34,310
регуляризации и применим ее к логистической регрессии.

294
00:10:35,140 --> 00:10:36,170
С тем, чтобы вы смогли понять, как логистическая

295
00:10:36,280 --> 00:10:37,630
регрессия может помочь избежать избыточной

296
00:10:37,920 --> 00:10:39,830
точности и просто работать лучше.