Para este momento, ya has visto un par de algoritmos de aprendizaje diferentes, regresión lineal y regresión logística. Funcionan bien para muchos problemas, pero cuando se les aplica a ciertas aplicaciones de aprendizaje automático, pueden caer en un problema llamado sobreajuste que causa que tengan un desempeño muy malo. Lo que me gustaría hacer en este video es explicarte lo que es este problema de sobreajuste, y durante algunos de los siguientes videos después de este, vamos a hablar de una técnica llamada regularización, que nos permitirá mejorar o reducir este problema de sobreajuste y lograr que estos algoritmos de aprendizaje funcionen mucho mejor. ¿Qué es sobreajuste? Sigamos utilizando nuestro ejemplo de predicción de precios de vivienda con regresión lineal donde queremos predecir el precio en función del tamaño de la casa. Una cosa que podríamos hacer es ajustar una función lineal a estos datos, y si hacemos eso, probablemente obtengamos ese tipo de línea recta que se ajusta a los datos. Pero esto no es un buen modelo. Observando los datos, parece bastante claro que conforme el tamaño de la vivienda incrementa, la meseta de precios de la vivienda, se aplana conforme nos movemos hacia la derecha por lo que este algoritmo no se ajusta al entrenamiento y llamamos a este problema subajuste, y otro término para esto es que este algoritmo tiene una alta oscilación. Ambos casos más o menos quieren decir que ni siquiera se están ajustando muy bien los datos. Oscilación es un término histórico o técnico, pero la idea es que si una línea recta se está ajustando a los datos, entonces es como si el algoritmo tuviera una preconcepción muy fuerte, o una oscilación muy fuerte de que los precios de la vivienda van a variar linealmente con su tamaño a pesar de que los datos indiquen lo contrario. A pesar de la evidencia de lo contrario si la preconcepción todavía son oscilacilatorias, todavía se cierra para ajustarse a una línea recta y esto termina siendo un mal ajuste a los datos. Ahora, en el medio, podríamos ajustar una función cuadrática a los datos, con este conjunto de datos, ajustamos la función cuadrática, y tal vez obtengamos este tipo de curva y, eso funciona bastante bien. Y, el otro extremo, sería si fuéramos a ajustar, digamos, un cuarto polinomio a los datos. Así que, aquí tenemos cinco parámetros, de «theta» 0 a «theta» 4, y, con eso, en realidad podemos ajustar una curva que se procese a través de nuestros cinco ejemplos de entrenamiento. Podrías obtener una curva que se vea así. Que, por un lado, parece hacer un buen trabajo ajustando el conjunto de entrenamiento y, se procesa a través de todos mis datos, por lo menos. Pero esto es todavía una curva muy ondulada, ¿correcto? Entonces, va hacia arriba y hacia abajo por todas partes, y, en realidad no creemos que sea un buen modelo para predecir el precio de la vivienda. Así es que llamamos a este problema sobreajuste, y otro término para esto es que este algoritmo tiene varianza alta. El término varianza alta también es histórico o técnico. Pero la intuición es que, si estamos ajustando un polinomio de alto grado, entonces, la hipótesis se puede ajustar, como sabes, es casi como si pudiera ajustarse a casi cualquier función y esta cara de posibles hipótesis es demasiado grande, es demasiado variable. Y no tenemos suficientes datos para restringirla para darnos una buena hipótesis, por lo que le llamamos sobreajuste. Y en el medio, no hay realmente un nombre pero solo voy a escribir, ya sabes, justo. Un segundo polinomio de grado, la función cuadrática parece ser lo adecuado para el ajuste de estos datos. Recapitulando un poco el problema de sobreajuste se da cuando tenemos demasiadas variables, entonces la hipótesis de aprendizaje se puede ajustar muy bien al lado del entrenamiento. Así, tu función de costos en realidad puede estar muy cerca de cero o puede ser incluso exactamente cero, pero podrías terminar con una curva como esta que, como sabes, se esfuerza demasiado para ajustarse al conjunto de entrenamiento, así que incluso es incapaz de generalizar nuevos ejemplos y falla en predecir los precios de los nuevos ejemplos también, y aquí el término generalizar se refiere a qué tan bien se aplica una hipótesis incluso a nuevos ejemplos. Esto es para los datos de las casas que no se han visto en el conjunto de entrenamiento. En esta diapositiva, observamos el sobreajuste para el caso de la regresión lineal. Algo similar puede aplicarse a la regresión logística también. Aquí hay un ejemplo de regresión logística con dos variables x1 y x2. Algo que podemos hacer, es ajustar la regresión logística con una hipótesis simple como esta, en donde, como de costumbre, "G" es mi función sigmoidea. Y si lo haces, terminas con una hipótesis, tratando de utilizar, tal vez, solo una línea recta para separar los ejemplos positivos y negativos. Y esto no parece un muy buen ajuste para la hipótesis. Así que, una vez más, esto es un ejemplo de subajuste o de la hipótesis teniendo una oscilación alta. En contraste, si vas a agregar a tus variables estos términos cuadráticos, entonces, podrías obtener una frontera de decisión que podría parecerse más a esto. Y, como sabes, es un buen ajuste a los datos. Probablemente, lo mejor que podemos conseguir, en este conjunto de entrenamiento. Y, finalmente, en el otro extremo, si fueras a ajustar un polinomio de muy alto grado, si fueras a generar muchos términos polinomiales de alto grado, entonces, la regresión logística puede retorcerse, puede intentar demasiado encontrar una frontera de decisión que se ajuste a tus datos de entrenamiento o ir muy lejos para retorcerse en sí misma, para ajustarse a cada ejemplo de entrenamiento individual. Y, como sabes, si las variables x1 y x2 ofrecen predicción, tal vez, el cáncer, ya sabes, el cáncer son tumores mamarios malignos o benignos. Esto no, esto en realidad no parece una hipótesis muy buena para hacer predicciones. Y así, una vez más, esto es una instancia de sobreajuste y, de una hipótesis teniendo varianza alta y en realidad no, y, siendo poco probable que generalice bien los nuevos ejemplos. Más adelante, en este curso, cuando hablemos sobre la depuración y el diagnóstico de lo que pueden salir mal con los algoritmos de aprendizaje, te daremos herramientas específicas para reconocer cuándo el sobreajuste y también el subajuste puedan estar ocurriendo. Pero, por ahora, vamos a hablar sobre el problema, cuando pensamos que se está produciendo sobreajuste, ¿Qué podemos hacer para abordarlo? En los ejemplos anteriores, hemos tenido datos de una o dos dimensiones, así que podríamos trazar la hipótesis y ver lo que está pasando o seleccionar el polinomio de grado apropiado. Así que, antes para el ejemplo de los precios de la vivienda, podríamos simplemente trazar la hipótesis y, como sabes, tal vez ver que se ajusta a un tipo de función muy ondulada que va por todas partes para predecir el precio de la vivienda. Y entonces podríamos utilizar figuras como estas para seleccionar un polinomio de grado apropiado. Así es que trazar la hipótesis, podría ser una manera de intentar decidir qué polinomio de grado utilizar. Pero eso no siempre funciona. Y, de hecho más a menudo podríamos tener problemas de aprendizaje en donde tenemos un montón de variables. Y no es sólo una cuestión de seleccionar cual polinomio de grado. De hecho, cuando tenemos tantas variables, también se vuelve mucho más difícil trazar los datos y se hace mucho más difícil visualizarlos, para decidir qué variables mantener o no. Así es que específicamente, si estamos tratando de predecir los precios de la vivienda, a veces podemos tener un montón de variables diferentes. Y todas estas variables, como sabes, podrían parecer útiles. Pero, si tenemos muchas variables, y muy pocos datos de entrenamiento, entonces, el sobreajuste puede convertirse en un problema. Con el fin de abordar el sobreajuste, tenemos dos opciones principales que podemos realizar. La primera opción es tratar de reducir el número de variables. Específicamente, una cosa que podemos hacer es revisar manualmente la lista de variables, y, usar eso para tratar de decidir cuales son las variables más importantes, y, por lo tanto cuáles son las variables que deberíamos conservar, y cuáles son las variables que deberíamos desechar. Más adelante en este curso, también vamos a hablar de los algoritmos de selección de modelo, los cuales son algoritmos para decidir automáticamente cuales variables conservar y cuales variables desechar. Esta idea de reducir el número de variables puede funcionar bien, y puede reducir el sobreajuste. Y, cuando hablemos de selección de modelos, tocaremos este tema con mucha mayor profundidad. Pero, la desventaja es que, al desechar algunas variables, también estamos eliminando parte de la información que tenemos sobre un problema. Por ejemplo, tal vez, todas esas variables son realmente útiles para predecir el precio de una casa, entonces, tal vez no queremos deshacernos de parte de nuestra información o eliminar algunas de nuestras variables. La segunda opción, de la que vamos a hablar en algunos de los siguientes videos, es la regularización. Con ella vamos a mantener todas las variables, pero vamos a reducir la magnitud o el valor de los parámetros «theta» de "J". Y este método funciona bien, como vamos a ver, cuando tenemos muchas variables, y cada una de ellas contribuye un poco a predecir el valor de "y", como vimos en el ejemplo de predicción del precio de la vivienda, en donde podemos tener un montón de variables, cada una de las cuales son, como sabes, útiles de alguna manera, por lo que tal vez, no queremos deshacernos de ellas. Así que esto suscribe la idea de la regularización en un nivel muy alto. Y, me doy cuenta de que todos estos detalles probablemente no tienen sentido para ti todavía. Pero en el siguiente video, vamos a empezar a formular exactamente cómo aplicar la regularización y, exactamente lo que la regularización significa. Y, entonces empezarás a descubrir, cómo utilizar esto, para lograr que los algoritmos de aprendizaje funcionen bien y a evitar el sobreajuste.