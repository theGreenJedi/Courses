1
00:00:00,360 --> 00:00:01,753
Para este momento, ya has visto

2
00:00:01,760 --> 00:00:04,097
un par de algoritmos de aprendizaje diferentes,

3
00:00:04,097 --> 00:00:06,504
regresión lineal y regresión logística.

4
00:00:06,510 --> 00:00:08,583
Funcionan bien para muchos problemas,

5
00:00:08,583 --> 00:00:09,684
pero cuando se les aplica

6
00:00:09,684 --> 00:00:11,903
a ciertas aplicaciones de aprendizaje automático,

7
00:00:11,903 --> 00:00:13,889
pueden caer en un problema llamado

8
00:00:13,900 --> 00:00:18,052
sobreajuste que causa que tengan un desempeño muy malo.

9
00:00:18,052 --> 00:00:18,866
Lo que me gustaría hacer en

10
00:00:18,866 --> 00:00:20,393
este video es explicarte

11
00:00:20,393 --> 00:00:22,400
lo que es este problema de sobreajuste,

12
00:00:22,400 --> 00:00:24,083
y durante algunos de los

13
00:00:24,083 --> 00:00:25,861
siguientes videos después de este,

14
00:00:25,861 --> 00:00:27,759
vamos a hablar de una técnica llamada

15
00:00:27,760 --> 00:00:29,787
regularización, que nos permitirá

16
00:00:29,787 --> 00:00:31,529
mejorar o

17
00:00:31,529 --> 00:00:33,607
reducir este problema de sobreajuste y

18
00:00:33,607 --> 00:00:36,844
lograr que estos algoritmos de aprendizaje funcionen mucho mejor.

19
00:00:36,860 --> 00:00:39,607
¿Qué es sobreajuste?

20
00:00:39,607 --> 00:00:41,616
Sigamos utilizando nuestro

21
00:00:41,620 --> 00:00:44,030
ejemplo de predicción de precios de vivienda

22
00:00:44,050 --> 00:00:46,146
con regresión lineal

23
00:00:46,146 --> 00:00:47,123
donde queremos predecir el

24
00:00:47,123 --> 00:00:50,730
precio en función del tamaño de la casa.

25
00:00:50,730 --> 00:00:51,870
Una cosa que podríamos hacer es

26
00:00:51,910 --> 00:00:53,620
ajustar una función lineal a

27
00:00:53,620 --> 00:00:54,892
estos datos, y si

28
00:00:54,892 --> 00:00:56,296
hacemos eso, probablemente obtengamos

29
00:00:56,296 --> 00:00:58,913
ese tipo de línea recta que se ajusta a los datos.

30
00:00:58,913 --> 00:01:01,012
Pero esto no es un buen modelo.

31
00:01:01,012 --> 00:01:02,543
Observando los datos, parece

32
00:01:02,560 --> 00:01:04,100
bastante claro que conforme el

33
00:01:04,100 --> 00:01:06,274
tamaño de la vivienda incrementa, la

34
00:01:06,274 --> 00:01:08,268
meseta de precios de la vivienda,

35
00:01:08,270 --> 00:01:11,721
se aplana conforme nos movemos hacia la derecha por lo que

36
00:01:11,740 --> 00:01:14,020
este algoritmo no

37
00:01:14,020 --> 00:01:15,898
se ajusta al entrenamiento y

38
00:01:15,898 --> 00:01:19,166
llamamos a este problema subajuste, y

39
00:01:19,180 --> 00:01:20,494
otro término para esto es

40
00:01:20,500 --> 00:01:24,666
que este algoritmo tiene una alta oscilación.

41
00:01:25,140 --> 00:01:26,841
Ambos casos más o menos

42
00:01:26,890 --> 00:01:30,760
quieren decir que ni siquiera se están ajustando muy bien los datos.

43
00:01:30,760 --> 00:01:32,328
Oscilación es un término

44
00:01:32,328 --> 00:01:34,515
histórico o técnico,

45
00:01:34,515 --> 00:01:36,109
pero la idea es que

46
00:01:36,110 --> 00:01:37,303
si una línea recta se está ajustando a

47
00:01:37,303 --> 00:01:38,909
los datos, entonces es como

48
00:01:38,920 --> 00:01:40,290
si el algoritmo tuviera una

49
00:01:40,330 --> 00:01:42,638
preconcepción muy fuerte, o una

50
00:01:42,638 --> 00:01:44,633
oscilación muy fuerte de que los precios de la vivienda

51
00:01:44,650 --> 00:01:46,339
van a variar

52
00:01:46,339 --> 00:01:49,988
linealmente con su tamaño a pesar de que los datos indiquen lo contrario.

53
00:01:50,000 --> 00:01:51,281
A pesar de la evidencia de lo

54
00:01:51,290 --> 00:01:54,174
contrario si la preconcepción todavía

55
00:01:54,174 --> 00:01:55,413
son oscilacilatorias, todavía se cierra

56
00:01:55,440 --> 00:01:56,974
para ajustarse a una línea recta

57
00:01:56,974 --> 00:02:00,638
y esto termina siendo un mal ajuste a los datos.

58
00:02:00,638 --> 00:02:02,173
Ahora, en el medio, podríamos

59
00:02:02,210 --> 00:02:04,626
ajustar una función cuadrática a los datos,

60
00:02:04,626 --> 00:02:06,222
con este conjunto de datos, ajustamos la

61
00:02:06,222 --> 00:02:07,793
función cuadrática, y tal vez obtengamos

62
00:02:07,810 --> 00:02:10,211
este tipo de curva

63
00:02:10,211 --> 00:02:14,361
y, eso funciona bastante bien.

64
00:02:14,361 --> 00:02:17,543
Y, el otro extremo, sería si fuéramos a ajustar, digamos, un cuarto polinomio a los datos.

65
00:02:17,550 --> 00:02:19,442
Así que, aquí tenemos cinco parámetros,

66
00:02:19,470 --> 00:02:23,196
de «theta» 0 a «theta» 4,

67
00:02:23,210 --> 00:02:23,926
y, con eso, en realidad podemos ajustar una curva

68
00:02:23,926 --> 00:02:26,727
que se procese a través de nuestros cinco ejemplos de entrenamiento.

69
00:02:26,727 --> 00:02:29,507
Podrías obtener una curva que se vea así.

70
00:02:31,260 --> 00:02:32,454
Que, por un

71
00:02:32,460 --> 00:02:33,791
lado, parece hacer

72
00:02:33,791 --> 00:02:35,052
un buen trabajo ajustando el

73
00:02:35,052 --> 00:02:36,291
conjunto de entrenamiento y, se

74
00:02:36,291 --> 00:02:38,269
procesa a través de todos mis datos, por lo menos.

75
00:02:38,270 --> 00:02:40,284
Pero esto es todavía una curva muy ondulada, ¿correcto?

76
00:02:40,300 --> 00:02:41,660
Entonces, va hacia arriba y hacia abajo por

77
00:02:41,660 --> 00:02:43,430
todas partes, y, en realidad no

78
00:02:43,430 --> 00:02:46,996
creemos que sea un buen modelo para predecir el precio de la vivienda.

79
00:02:47,000 --> 00:02:48,924
Así es que llamamos a este problema

80
00:02:48,924 --> 00:02:51,967
sobreajuste, y otro

81
00:02:51,970 --> 00:02:53,165
término para esto es que

82
00:02:53,170 --> 00:02:57,304
este algoritmo tiene varianza alta.

83
00:02:57,890 --> 00:02:59,951
El término varianza alta también es

84
00:02:59,951 --> 00:03:02,110
histórico o técnico.

85
00:03:02,130 --> 00:03:03,797
Pero la intuición es que,

86
00:03:03,800 --> 00:03:05,080
si estamos ajustando un

87
00:03:05,080 --> 00:03:07,326
polinomio de alto grado, entonces, la

88
00:03:07,330 --> 00:03:08,603
hipótesis se puede ajustar, como sabes,

89
00:03:08,620 --> 00:03:09,584
es casi como si pudiera

90
00:03:09,584 --> 00:03:11,995
ajustarse a casi cualquier función y

91
00:03:11,995 --> 00:03:14,159
esta cara de posibles hipótesis

92
00:03:14,159 --> 00:03:16,601
es demasiado grande, es demasiado variable.

93
00:03:16,610 --> 00:03:18,052
Y no tenemos suficientes datos

94
00:03:18,052 --> 00:03:19,279
para restringirla para darnos

95
00:03:19,279 --> 00:03:22,714
una buena hipótesis, por lo que le llamamos sobreajuste.

96
00:03:22,740 --> 00:03:24,340
Y en el medio, no hay realmente

97
00:03:24,350 --> 00:03:26,990
un nombre pero solo voy a escribir, ya sabes, justo.

98
00:03:26,990 --> 00:03:29,911
Un segundo polinomio de grado, la función cuadrática

99
00:03:29,911 --> 00:03:32,559
parece ser lo adecuado para el ajuste de estos datos.

100
00:03:32,559 --> 00:03:34,684
Recapitulando un poco el

101
00:03:34,690 --> 00:03:37,042
problema de sobreajuste se da

102
00:03:37,042 --> 00:03:38,258
cuando tenemos

103
00:03:38,258 --> 00:03:40,729
demasiadas variables, entonces

104
00:03:40,729 --> 00:03:43,881
la hipótesis de aprendizaje se puede ajustar muy bien al lado del entrenamiento.

105
00:03:43,881 --> 00:03:46,023
Así, tu función de costos

106
00:03:46,023 --> 00:03:47,344
en realidad puede estar muy cerca

107
00:03:47,344 --> 00:03:48,446
de cero o puede ser

108
00:03:48,446 --> 00:03:50,750
incluso exactamente cero, pero

109
00:03:50,750 --> 00:03:52,063
podrías terminar con una

110
00:03:52,063 --> 00:03:53,950
curva como esta que, como

111
00:03:53,950 --> 00:03:55,314
sabes, se esfuerza demasiado para

112
00:03:55,314 --> 00:03:57,103
ajustarse al conjunto de entrenamiento, así que

113
00:03:57,110 --> 00:03:59,233
incluso es incapaz de generalizar

114
00:03:59,250 --> 00:04:01,117
nuevos ejemplos y falla en

115
00:04:01,120 --> 00:04:03,018
predecir los precios de los nuevos ejemplos

116
00:04:03,050 --> 00:04:04,337
también, y aquí el

117
00:04:04,350 --> 00:04:06,853
término generalizar se refiere a

118
00:04:06,853 --> 00:04:10,868
qué tan bien se aplica una hipótesis incluso a nuevos ejemplos.

119
00:04:10,868 --> 00:04:12,274
Esto es para los datos de

120
00:04:12,320 --> 00:04:16,467
las casas que no se han visto en el conjunto de entrenamiento.

121
00:04:16,600 --> 00:04:17,910
En esta diapositiva, observamos

122
00:04:17,910 --> 00:04:20,802
el sobreajuste para el caso de la regresión lineal.

123
00:04:20,810 --> 00:04:24,182
Algo similar puede aplicarse a la regresión logística también.

124
00:04:24,190 --> 00:04:26,090
Aquí hay un ejemplo de regresión logística

125
00:04:26,090 --> 00:04:28,871
con dos variables x1 y x2.

126
00:04:28,910 --> 00:04:30,136
Algo que podemos hacer, es

127
00:04:30,140 --> 00:04:31,522
ajustar la regresión logística con

128
00:04:31,522 --> 00:04:34,518
una hipótesis simple como esta,

129
00:04:34,530 --> 00:04:38,076
en donde, como de costumbre, "G" es mi función sigmoidea.

130
00:04:38,120 --> 00:04:39,334
Y si lo haces, terminas

131
00:04:39,334 --> 00:04:41,593
con una hipótesis, tratando de

132
00:04:41,600 --> 00:04:42,923
utilizar, tal vez, solo una línea recta

133
00:04:42,923 --> 00:04:45,713
para separar los ejemplos positivos y negativos.

134
00:04:45,713 --> 00:04:49,071
Y esto no parece un muy buen ajuste para la hipótesis.

135
00:04:49,100 --> 00:04:50,659
Así que, una vez más, esto

136
00:04:50,659 --> 00:04:52,577
es un ejemplo de subajuste

137
00:04:52,577 --> 00:04:56,040
o de la hipótesis teniendo una oscilación alta.

138
00:04:56,210 --> 00:04:57,504
En contraste, si vas

139
00:04:57,504 --> 00:04:59,146
a agregar a tus variables

140
00:04:59,170 --> 00:05:01,032
estos términos cuadráticos, entonces,

141
00:05:01,032 --> 00:05:02,613
podrías obtener una frontera de decisión

142
00:05:02,613 --> 00:05:05,620
que podría parecerse más a esto.

143
00:05:05,620 --> 00:05:07,784
Y, como sabes, es un buen ajuste a los datos.

144
00:05:07,784 --> 00:05:10,838
Probablemente, lo

145
00:05:10,860 --> 00:05:13,991
mejor que podemos conseguir, en este conjunto de entrenamiento.

146
00:05:14,010 --> 00:05:15,157
Y, finalmente, en el otro

147
00:05:15,170 --> 00:05:16,169
extremo, si fueras a

148
00:05:16,169 --> 00:05:18,207
ajustar un polinomio de muy alto grado, si

149
00:05:18,207 --> 00:05:20,036
fueras a generar muchos

150
00:05:20,036 --> 00:05:22,461
términos polinomiales de alto grado,

151
00:05:22,490 --> 00:05:24,730
entonces, la regresión logística puede retorcerse,

152
00:05:24,750 --> 00:05:26,551
puede intentar

153
00:05:26,560 --> 00:05:28,233
demasiado encontrar una

154
00:05:28,233 --> 00:05:31,742
frontera de decisión que se ajuste

155
00:05:31,742 --> 00:05:33,013
a tus datos de entrenamiento o ir

156
00:05:33,030 --> 00:05:35,006
muy lejos para retorcerse en sí misma,

157
00:05:35,006 --> 00:05:37,689
para ajustarse a cada ejemplo de entrenamiento individual.

158
00:05:37,700 --> 00:05:38,757
Y, como sabes, si las

159
00:05:38,757 --> 00:05:39,547
variables x1 y

160
00:05:39,550 --> 00:05:41,435
x2 ofrecen predicción, tal vez,

161
00:05:41,435 --> 00:05:43,350
el cáncer,

162
00:05:43,390 --> 00:05:46,448
ya sabes, el cáncer son tumores mamarios malignos o benignos.

163
00:05:46,448 --> 00:05:47,988
Esto no, esto en realidad no

164
00:05:47,988 --> 00:05:51,893
parece una hipótesis muy buena para hacer predicciones.

165
00:05:51,930 --> 00:05:53,463
Y así, una vez más, esto es

166
00:05:53,463 --> 00:05:55,432
una instancia de sobreajuste

167
00:05:55,432 --> 00:05:57,128
y, de una hipótesis teniendo

168
00:05:57,128 --> 00:05:59,403
varianza alta y en realidad no,

169
00:05:59,403 --> 00:06:04,243
y, siendo poco probable que generalice bien los nuevos ejemplos.

170
00:06:04,560 --> 00:06:06,158
Más adelante, en este curso, cuando

171
00:06:06,158 --> 00:06:08,453
hablemos sobre la depuración y el diagnóstico

172
00:06:08,460 --> 00:06:09,794
de lo que pueden salir mal con

173
00:06:09,810 --> 00:06:11,490
los algoritmos de aprendizaje, te daremos

174
00:06:11,490 --> 00:06:13,297
herramientas específicas para reconocer

175
00:06:13,297 --> 00:06:14,953
cuándo el sobreajuste y también

176
00:06:14,953 --> 00:06:17,503
el subajuste puedan estar ocurriendo.

177
00:06:17,503 --> 00:06:18,775
Pero, por ahora, vamos a hablar sobre

178
00:06:18,780 --> 00:06:20,342
el problema, cuando

179
00:06:20,360 --> 00:06:22,206
pensamos que se está produciendo sobreajuste,

180
00:06:22,250 --> 00:06:24,864
¿Qué podemos hacer para abordarlo?

181
00:06:24,864 --> 00:06:26,640
En los ejemplos anteriores, hemos tenido

182
00:06:26,660 --> 00:06:28,701
datos de una o dos dimensiones, así que

183
00:06:28,701 --> 00:06:31,335
podríamos trazar la hipótesis y ver lo que está

184
00:06:31,335 --> 00:06:34,612
pasando o seleccionar el polinomio de grado apropiado.

185
00:06:34,620 --> 00:06:36,836
Así que, antes para el ejemplo de los precios de la vivienda,

186
00:06:36,836 --> 00:06:38,405
podríamos simplemente

187
00:06:38,410 --> 00:06:40,597
trazar la hipótesis y, como

188
00:06:40,600 --> 00:06:41,628
sabes, tal vez ver que

189
00:06:41,628 --> 00:06:42,830
se ajusta a un tipo de

190
00:06:42,830 --> 00:06:46,339
función muy ondulada que va por todas partes para predecir el precio de la vivienda.

191
00:06:46,339 --> 00:06:47,701
Y entonces podríamos utilizar figuras

192
00:06:47,740 --> 00:06:50,667
como estas para seleccionar un polinomio de grado apropiado.

193
00:06:50,680 --> 00:06:54,166
Así es que trazar la hipótesis, podría

194
00:06:54,166 --> 00:06:55,728
ser una manera de intentar

195
00:06:55,750 --> 00:06:58,160
decidir qué polinomio de grado utilizar.

196
00:06:58,160 --> 00:07:00,163
Pero eso no siempre funciona.

197
00:07:00,180 --> 00:07:02,019
Y, de hecho más a menudo

198
00:07:02,019 --> 00:07:06,075
podríamos tener problemas de aprendizaje en donde tenemos un montón de variables.

199
00:07:06,075 --> 00:07:07,563
Y no es sólo

200
00:07:07,563 --> 00:07:10,599
una cuestión de seleccionar cual polinomio de grado.

201
00:07:10,630 --> 00:07:12,147
De hecho, cuando

202
00:07:12,170 --> 00:07:13,779
tenemos tantas variables, también

203
00:07:13,779 --> 00:07:15,593
se vuelve mucho más difícil trazar

204
00:07:15,630 --> 00:07:17,698
los datos y se hace

205
00:07:17,710 --> 00:07:19,211
mucho más difícil visualizarlos,

206
00:07:19,211 --> 00:07:22,396
para decidir qué variables mantener o no.

207
00:07:22,420 --> 00:07:24,142
Así es que específicamente, si estamos tratando

208
00:07:24,160 --> 00:07:27,849
de predecir los precios de la vivienda, a veces podemos tener un montón de variables diferentes.

209
00:07:27,880 --> 00:07:31,373
Y todas estas variables, como sabes, podrían parecer útiles.

210
00:07:31,373 --> 00:07:32,609
Pero, si tenemos

211
00:07:32,609 --> 00:07:34,123
muchas variables, y muy pocos

212
00:07:34,123 --> 00:07:35,820
datos de entrenamiento, entonces, el

213
00:07:35,840 --> 00:07:37,776
sobreajuste puede convertirse en un problema.

214
00:07:37,776 --> 00:07:39,180
Con el fin de abordar el

215
00:07:39,180 --> 00:07:40,651
sobreajuste, tenemos dos

216
00:07:40,651 --> 00:07:43,780
opciones principales que podemos realizar.

217
00:07:43,780 --> 00:07:45,759
La primera opción es tratar de

218
00:07:45,770 --> 00:07:47,976
reducir el número de variables.

219
00:07:47,990 --> 00:07:49,337
Específicamente, una cosa que

220
00:07:49,337 --> 00:07:51,383
podemos hacer es revisar manualmente

221
00:07:51,383 --> 00:07:53,236
la lista de variables, y, usar

222
00:07:53,236 --> 00:07:54,894
eso para tratar de decidir cuales

223
00:07:54,894 --> 00:07:57,256
son las variables más importantes, y, por lo tanto

224
00:07:57,256 --> 00:07:58,476
cuáles son las variables que deberíamos

225
00:07:58,476 --> 00:08:01,844
conservar, y cuáles son las variables que deberíamos desechar.

226
00:08:01,844 --> 00:08:03,401
Más adelante en este curso, también

227
00:08:03,401 --> 00:08:06,018
vamos a hablar de los algoritmos de selección de modelo,

228
00:08:06,040 --> 00:08:08,361
los cuales son algoritmos para

229
00:08:08,361 --> 00:08:09,788
decidir automáticamente cuales variables

230
00:08:09,800 --> 00:08:12,500
conservar y cuales variables desechar.

231
00:08:12,500 --> 00:08:13,987
Esta idea de reducir el

232
00:08:13,987 --> 00:08:15,562
número de variables puede funcionar

233
00:08:15,562 --> 00:08:17,853
bien, y puede reducir el sobreajuste.

234
00:08:17,853 --> 00:08:19,383
Y, cuando hablemos de selección de modelos,

235
00:08:19,383 --> 00:08:22,534
tocaremos este tema con mucha mayor profundidad.

236
00:08:22,534 --> 00:08:24,386
Pero, la desventaja es que, al

237
00:08:24,386 --> 00:08:25,603
desechar algunas

238
00:08:25,603 --> 00:08:27,010
variables, también estamos eliminando

239
00:08:27,370 --> 00:08:30,615
parte de la información que tenemos sobre un problema.

240
00:08:30,650 --> 00:08:31,942
Por ejemplo, tal vez, todas

241
00:08:31,942 --> 00:08:33,760
esas variables son realmente útiles

242
00:08:33,780 --> 00:08:35,050
para predecir el precio de una

243
00:08:35,070 --> 00:08:36,636
casa, entonces, tal vez no

244
00:08:36,640 --> 00:08:37,687
queremos deshacernos de parte

245
00:08:37,687 --> 00:08:40,990
de nuestra información o eliminar algunas de nuestras variables.

246
00:08:41,540 --> 00:08:44,515
La segunda opción, de la que vamos a

247
00:08:44,515 --> 00:08:45,995
hablar en algunos de los

248
00:08:46,010 --> 00:08:49,268
siguientes videos, es la regularización.

249
00:08:49,268 --> 00:08:50,390
Con ella vamos a mantener

250
00:08:50,390 --> 00:08:52,579
todas las variables, pero

251
00:08:52,579 --> 00:08:55,063
vamos a reducir la magnitud

252
00:08:55,063 --> 00:08:56,506
o el valor de los parámetros

253
00:08:56,520 --> 00:08:58,745
«theta» de "J". Y este

254
00:08:58,750 --> 00:09:00,690
método funciona bien, como vamos a ver,

255
00:09:00,690 --> 00:09:01,925
cuando tenemos muchas

256
00:09:01,925 --> 00:09:03,822
variables, y cada una de ellas contribuye

257
00:09:03,822 --> 00:09:05,502
un poco a predecir

258
00:09:05,502 --> 00:09:07,723
el valor de "y", como

259
00:09:07,740 --> 00:09:10,283
vimos en el ejemplo de predicción del precio de la vivienda,

260
00:09:10,283 --> 00:09:11,413
en donde podemos tener un montón

261
00:09:11,413 --> 00:09:12,720
de variables, cada una de las cuales

262
00:09:12,750 --> 00:09:16,902
son, como sabes, útiles de alguna manera, por lo que tal vez, no queremos deshacernos de ellas.

263
00:09:16,930 --> 00:09:19,247
Así que esto suscribe la

264
00:09:19,250 --> 00:09:22,790
idea de la regularización en un nivel muy alto.

265
00:09:22,790 --> 00:09:24,354
Y, me doy cuenta de que todos

266
00:09:24,360 --> 00:09:26,763
estos detalles probablemente no tienen sentido para ti todavía.

267
00:09:26,763 --> 00:09:28,316
Pero en el siguiente video, vamos

268
00:09:28,316 --> 00:09:30,960
a empezar a formular exactamente cómo

269
00:09:30,960 --> 00:09:35,117
aplicar la regularización y, exactamente lo que la regularización significa.

270
00:09:35,140 --> 00:09:36,810
Y, entonces empezarás a

271
00:09:36,810 --> 00:09:38,310
descubrir, cómo utilizar esto,

272
00:09:38,310 --> 00:09:40,412
para lograr que los algoritmos de aprendizaje funcionen

273
00:09:40,412 --> 00:09:42,460
bien y a evitar el sobreajuste.