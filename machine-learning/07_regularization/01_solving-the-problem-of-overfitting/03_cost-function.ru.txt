В этом видео я хотел бы показать основные моменты как работает регуляризация И так же мы запишем функцию затрат, которую мы будем использовать, когда будем использовать регуляризацию. Я полагаю, что при помощи примеров, которые я нарисовал в этой лекции, вы отчасти сможете получить представление о регуляризации. Но гораздо лучше будет, если вы не просто прослушаете лекции, а  реализуете регуляризацию самостоятельно и увидите сами, как она работает. Если вы выполните все упражнения, относящиеся к этому разделу, то вы сможете увидеть регуляризацию в действии. Итак, приступим. В предыдущем видео мы могли заметить, что приближение квадратичной функцией дает довольно хорошие результаты. Но если нам нужно использовать полиномиальную функцию гораздо большего порядка, мы сталкиваемся с тем, что кривая очень хорошо покрывает обучающую выборку, но плохо переобучается, нет возможности обобщения. Предположим следующее: нам нужно придумать штраф с тем, чтобы добиться максимального уменьшения параметров тета-3 и тета-4. Вот что я имею в виду. Возьмем нашу целевую функцию, или задачу оптимизации, в которой мы минимизируем обычный квадрат отклонения функции затрат. Эту целевую функцию мы немного изменяем: мы прибавляем 100 тета-3 в квадрате, плюс 1000 тета-4 в квадрате. 1000 я беру как просто какое-то большое число. Теперь, если нам надо минимизировать эту функцию, то единственным способом сделать ее минимальной - это уменьшить тета-3 и тета-4 тоже. Верно? Потому что в противном случаеу нас будет тета-3, умноженная на 1000, что дает большое значение целевой функции. Таким образом, минимизируя новую целевую функцию мы получаем тета-3 близкое к нулю, тета-4 близкое к нулю и таким образом избавляемся от них совсем. А если мы это сделаем, то при тета-3 и тета-4 близких к нулю, мы получаем квадратичную функцию мы выравниваем данные это, как вы понимаете, квадратичная функция плюс, возможно, небольшие добавочные значения тета-3 и тета-4, близкие к нулю. И мы получаем в сущности квадратичную функцию, что хорошо. Потому что это лучшая для нас гипотеза. В этом примере мы рассмотрели результат применения штрафа к двум параметрам, которые были достаточно большие. В целом, в этом и состоит идея регуляризации. Идея заключается в том, что если у нас есть маленькие значения параметров, то имея маленькие значения параметров, мы обычно упрощаем соответствующую им гипотезу. Итак, для нашего последнего примера мы добавили штраф только к тета-3 и тета-4 и сделали их близкими к нулю. Мы свели результат к более простой гипотезе, просто к квадратичной функции. В более широком смысле, мы можем наложить штраф на все параметры, полагая, что это даст нам более простую гипотезу, как в нашем последнем примере, где устремление параметров к нулю привело нас к квадратичной функции. Но также можно показать, что меньшие значения параметров соответствуют обычно более гладким функциям, а также более простым. Которые поэтому хуже поддаются переобучению. Я понимаю, почему хочется, чтобы все параметры были небольшими. Почему это соответствует более простой гипотезе. Я также понимаю, что для вас это возможно пока не очевидно. Довольно сложно объяснять до тех пор, пока вы сами не реализуете и не увидите результат. Но я надеюсь, что пример с тета-3 и тета-4, устремленными к нулю и то как нам это дало более простую гипотезу, я надеюсь, что это поможет вам понять, хотя бы отчасти, почему же это так происходит. Давайте рассмотрим на конкретном примере. Для нашего примера с ценой на жилье мы можем рассмотреть все наши сто характеристик, о которых мы говорили, среди которых может быть x1 - размер, x2 - количества спальных комнат, x3 - количество этажей и так далее И у нас может быть сотня характеристик. И в отличие от нашего полиномиального примера мы не знаем, что тета-3 и тета-4 - это составляющие полинома высокого порядка. Если у нас есть просто багаж, набор из сотни признаков, то сложно выбрать заранее те, которые скорее всего нам подходят. У нас сто или сто один параметр. Мы не знаем, какой взять. Мы не знаем, какие параметры надо учитывать в большей или меньшей степени. Итак, для регуляризации, вот что мы сделаем: мы возьмем функцию затрат (вот функция затрат для линейной регресии). Я собираюсь изменить функцию затрат, чтобы уменьшить все параметры, потому что, я не знаю, какой из них (один или два) стоит уменьшать. Я собираюсь изменить функцию затрат, добавив элемент в конце. *закрываем квадратную скобку* Когда я добавляю новый элемент регуляризации в конце для
того, чтобы уменьшить каждый параметр. И этот элемент в конце призван уменьшить все мои параметры: тета-1, тета-2, тета-3 итд до тета-100. Кстати, по соглашению сумма начинается с единицы, таким образом, я не налагаю штраф на тета-0, которое может быть большим. Это всего лишь соглашение, о том, с какого числа считать i с нуля до N или с единицы до N. На практике же, не имеет особого значения, включено ли тета-0 или нет. Это очень слабо отражается на результатах. Но давайте по умолчанию производить регуляризацию  в диапазоне  от тета-1 до тета-100. Записываем нашу задачу оптимизации, нашу регуляризованную целевую функцию снова. Вот она. Здесь J от тета, где этот элемент справа - это элемент регуляризации ламбда - так называемый параметр и ламбда нужна для того, чтобы задавать соотношение между двумя разными целями. Первая цель, выраженная первым слагаемым, - это то, что мы тренируем,то чему должна соответствовать наша обучающая выборка. Мы хотим соответствовать обучающей выборке. А вторая цель - это мы хотим сохранить параметры небольшими и это достигается при помощи второго слагаемого при помощи регуляризационной цели.При помощи элемента регуляризации. А для чего нужна ламбда, параметр регуляризации, так для того, чтобы регулировать соотношение между этими целями. То есть целью соответствовать обучающему множеству и целью сдерживания роста значений параметров для поддержания простоты гипотезы, чтобы предотвратить переобучение. Для нашего примера с ценами на жилье где мы изначально хотели использовать полином высокого порядка мы пришли к тому что у нас получилась такая вот волнистая изогнутая функция. Если вы все еще хотите использовать полином со всеми свойствами полинома. Но вместо этого вы просто используете  одну регуляризированную целевую функцию получаете кривую, которая не совсем является квадратной функцией, но она гораздо более гладкая и гораздо проще, примерно как красная линия, которая дает нам  гораздо лучшую гипотезу для этих данных. Опять-таки, я понимаю, что может вызвать у вас сложности понимание того,  что ограничение параметров может  оказать такое влияние, но если вы сами  реализуете регуляризацию, то вы увидите сами,  что такой эффект имеет место. В регуляризированной линейной регресии в случае если регуляризационный коэффициент очень большой,то штраф к параметрам тета-1,  тета-2, тета-3, тета-4  очень большой. Это в том случае, если наша гипотеза вот эта вот, которая внизу. Если мы штрафуем тета-1, тета-2, тета-3,тета-4 сильно, то в результате мы получаем эти параметры близкие к нулю. Тета-1 будет близка к нулю, тета-2 будет близка к нулю. Тета-3 и тета-4 будут близки к нулю тоже. И если мы так сделаем, если мы избавимся от этих компонентов в гипотезе, то у нас получится гипотеза,  о которой мы только что говорили. Согласно ей, цена на жилье равна тета-0,  и это отображается на графике как горизонтальная прямая. И это пример недостаточного соответствия точности. Эта гипотеза,  эта прямая линия не соответствует  даже тренировочной  выборке в достаточной степени. Это просто жирная прямая линия, которая не проходит даже близко от наших точек. Она не проходит близко от большинства тренировочных примеров. Иными словами, гипотеза слишком сильно привязана, слишком сильно убеждена,  что цена на жилье равна тета-0,  что игнорирует сами данные, которые это опровергают, и предпочитает быть просто  плоской линией, простой горизонтальной линией.Я не очень хорошо это нарисовал. Это просто прямая горизонтальная линия к данным. Чтобы регуляризация хорошо работала,  нам нужно позаботиться о том,  чтобы правильно выбрать  параметр регуляризации ламбда. Когда мы будем говорить про множественный выбор в этом курсе чуть позже, мы рассмотрим несколько способов автоматического выбора  регуляризационного параметра ламбда. Итак, это идея регуляризации высоких степеней и пересмотра функции затрат с тем,  чтобы использовать регуляризацию. В следующих двух видео,  мы применим эти идеи  к линейным регрессиям и к логистическим регрессиям, таким образом, чтобы в них мы тоже смогли избежать избыточной точности.