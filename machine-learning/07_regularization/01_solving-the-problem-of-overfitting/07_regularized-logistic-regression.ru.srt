1
00:00:00,160 --> 00:00:01,480
Ранее мы говорили о двух типах алгоритмов оптимизации

2
00:00:02,110 --> 00:00:04,730
для логистической регрессии.

3
00:00:05,190 --> 00:00:06,190
Мы рассказали как использовать градиентный спуск чтобы оптимизировать

4
00:00:06,560 --> 00:00:09,210
значение функции J от тета.

5
00:00:09,690 --> 00:00:10,770
Мы также рассказали о продвинутых методах

6
00:00:11,120 --> 00:00:12,730
оптимизации.

7
00:00:13,520 --> 00:00:14,670
Единственное что требуется это что вы обеспечите

8
00:00:14,790 --> 00:00:16,300
возможность вычислить значение

9
00:00:16,940 --> 00:00:18,160
функции затрат J от тета и возможность вычислить

10
00:00:18,420 --> 00:00:20,920
производные.

11
00:00:22,450 --> 00:00:23,920
В этом видео мы покажем, как вы можете адаптировать

12
00:00:24,190 --> 00:00:25,420
оба эти способа, оба метода градиентного спуска, и

13
00:00:25,500 --> 00:00:27,570
более передовые методы

14
00:00:27,720 --> 00:00:29,350
оптимизации, с целью заставить их работать в

15
00:00:30,280 --> 00:00:31,770
регуляризованной логистической

16
00:00:31,950 --> 00:00:33,550
регрессии.

17
00:00:35,430 --> 00:00:36,670
Итак, вот сама идея.

18
00:00:37,260 --> 00:00:38,770
Мы увидели ранее что логистическая регрессия может быть склонна к переобучению,

19
00:00:39,190 --> 00:00:40,490
если вы, скажем, аппроксимируете ее

20
00:00:40,850 --> 00:00:42,540
полиномом признаков высокого

21
00:00:42,810 --> 00:00:44,090
порядка

22
00:00:44,290 --> 00:00:45,890
такие как этот.

23
00:00:46,470 --> 00:00:48,250
Где G есть сигмоидная функция.

24
00:00:48,480 --> 00:00:49,970
И в данном случае, вы, в

25
00:00:50,030 --> 00:00:51,330
конце концов, придете к

26
00:00:51,530 --> 00:00:53,020
гипотезе, граница разрешимости

27
00:00:53,150 --> 00:00:54,120
которой будет такой чересчур

28
00:00:54,360 --> 00:00:55,930
сложной и весьма кривой функцией,

29
00:00:56,620 --> 00:00:58,600
определенно не лучшей для данного

30
00:00:58,820 --> 00:00:59,680
учебного набора.

31
00:00:59,790 --> 00:01:01,000
Вообще, если у вас есть

32
00:01:01,350 --> 00:01:02,990
логистическая регрессия со

33
00:01:03,120 --> 00:01:04,890
многими признаками, --

34
00:01:05,150 --> 00:01:06,630
не обязательно полиномиальными, просто со многими -- вы,

35
00:01:06,790 --> 00:01:07,510
в конечном итоге, можете

36
00:01:07,670 --> 00:01:09,720
получить переобучение.

37
00:01:11,620 --> 00:01:14,010
Это была наша функция затрат для логистической регрессии.

38
00:01:14,810 --> 00:01:16,210
И если мы хотим использовать регуляризацию,

39
00:01:16,740 --> 00:01:18,820
то все, что нам для этого нужно - добавить

40
00:01:18,950 --> 00:01:20,630
следующее слагаемое lambda,

41
00:01:20,820 --> 00:01:22,290
деленное на 2M и умноженное на

42
00:01:22,650 --> 00:01:24,860
сумму по J от 1,

43
00:01:25,110 --> 00:01:26,580
где суммирование по J

44
00:01:26,730 --> 00:01:29,670
начинается с 1.

45
00:01:29,800 --> 00:01:31,000
А не с 0, от квадратов тета

46
00:01:31,550 --> 00:01:33,670
с индексом j.

47
00:01:34,330 --> 00:01:35,470
А это, в свою очередь,

48
00:01:35,750 --> 00:01:36,960
оказывает ограничивающий эффект на

49
00:01:37,650 --> 00:01:39,140
параметры тета 1 и тета 2  и т.д.

50
00:01:39,570 --> 00:01:42,600
до тета N, предупреждая их неограниченный рост.

51
00:01:43,610 --> 00:01:44,720
Если вы проделаете это, даже несмотря на

52
00:01:45,720 --> 00:01:46,450
использование полиномов очень

53
00:01:46,750 --> 00:01:48,870
высокого порядка

54
00:01:49,250 --> 00:01:51,500
со множеством параметров, эффект сохранится.

55
00:01:52,210 --> 00:01:53,240
Пока вы применяете регуляризацию и сохраняете

56
00:01:53,910 --> 00:01:55,090
параметры малыми, вы, скорее всего,

57
00:01:55,850 --> 00:01:57,580
получите границу разрешения.

58
00:01:58,830 --> 00:02:00,040
Которая может быть более похожа на это.

59
00:02:00,320 --> 00:02:01,460
Кажется, более разумным

60
00:02:02,500 --> 00:02:03,740
для отделения положительных и отрицательных примеров.

61
00:02:05,300 --> 00:02:06,970
Таким образом, когда

62
00:02:08,140 --> 00:02:09,080
используется регуляризация даже

63
00:02:09,220 --> 00:02:11,110
со многими признаками,

64
00:02:11,620 --> 00:02:13,500
это может помочь избавиться от проблемы переобучения.

65
00:02:14,740 --> 00:02:15,790
Как мы можем реализовать это на практике?

66
00:02:16,720 --> 00:02:18,280
Ну, это и было усовершенствование для исходного метода

67
00:02:18,710 --> 00:02:20,380
градиентного спуска.

68
00:02:20,670 --> 00:02:22,300
Мы последовательно произведем следующее обновление

69
00:02:22,750 --> 00:02:24,610
для тета с индексом j.

70
00:02:24,740 --> 00:02:26,940
Этот слайд во многом похож на предыдущий для линейной регрессии.

71
00:02:27,510 --> 00:02:28,460
Но то, что я собираюсь сделать – записать обновление

72
00:02:29,210 --> 00:02:31,390
тета c индексом 0 отдельно.

73
00:02:31,670 --> 00:02:32,930
Итак, первая

74
00:02:33,060 --> 00:02:34,110
строка - обновление

75
00:02:34,230 --> 00:02:35,470
тета с индексом 0, а вторая

76
00:02:35,590 --> 00:02:36,730
строка – моё обновление для тета с индексами от 1 до N. Поскольку я

77
00:02:36,880 --> 00:02:38,470
собираюсь рассмотреть тета c

78
00:02:38,900 --> 00:02:40,740
индексом 0 отдельно.

79
00:02:41,700 --> 00:02:43,140
Чтобы модифицировать

80
00:02:43,700 --> 00:02:45,370
этот алгоритм и использовать

81
00:02:46,770 --> 00:02:48,480
регуляризованную функцию затрат, все что мне

82
00:02:49,100 --> 00:02:50,510
нужно сделать - это просто

83
00:02:50,950 --> 00:02:51,810
изменить второе измененное правило

84
00:02:51,930 --> 00:02:53,700
похожим на

85
00:02:53,870 --> 00:02:55,620
то, что мы делали для

86
00:02:55,890 --> 00:02:57,480
линейной регрессии, таким образом, а именно.

87
00:02:58,510 --> 00:02:59,800
Повторюсь, это, внешне похоже

88
00:03:00,380 --> 00:03:02,080
на тот алгоритм, что

89
00:03:02,230 --> 00:03:03,720
у нас был для линейной регрессии.

90
00:03:04,580 --> 00:03:05,580
Но это, естественно, не тот же

91
00:03:05,660 --> 00:03:06,590
самый алгоритм, как

92
00:03:06,890 --> 00:03:08,370
ранее, потому что сейчас гипотеза

93
00:03:08,780 --> 00:03:10,420
определена, используя вот это.

94
00:03:10,860 --> 00:03:12,550
Итак, это не такой же алгоритм,

95
00:03:13,130 --> 00:03:14,390
как для регуляризованной линейной регрессии.

96
00:03:14,830 --> 00:03:16,340
Поскольку гипотеза другая.

97
00:03:16,940 --> 00:03:18,360
Несмотря на те изменения, что я записал.

98
00:03:18,630 --> 00:03:20,160
На самом деле, внешне выглядит

99
00:03:20,350 --> 00:03:22,130
так же как и то, что было раньше.

100
00:03:22,480 --> 00:03:25,310
Когда мы получали метод градиентного спуска для регуляризованной линейной регрессии.

101
00:03:26,690 --> 00:03:27,720
И, чтобы

102
00:03:27,830 --> 00:03:29,360
резюмировать

103
00:03:29,560 --> 00:03:30,860
обсуждение, это выражение в

104
00:03:31,130 --> 00:03:32,330
квадратных скобках, вот здесь - это новая

105
00:03:32,670 --> 00:03:35,120
частная

106
00:03:35,410 --> 00:03:36,750
производная

107
00:03:37,210 --> 00:03:38,590
по тета с индексом j от новой функции стоимости J

108
00:03:38,660 --> 00:03:41,420
от тета.

109
00:03:42,300 --> 00:03:43,480
Где J от тета - функция затрат, использующая регуляризацию,

110
00:03:43,700 --> 00:03:44,980
которую мы определили на

111
00:03:45,180 --> 00:03:48,100
предыдущем слайде

112
00:03:49,770 --> 00:03:52,060
А это - метод градиентного спуска для регуляризованной линейной регрессии.

113
00:03:55,200 --> 00:03:56,430
Теперь давайте поговорим о том, как заставить

114
00:03:56,580 --> 00:03:58,290
работать регуляризованную линейную

115
00:03:58,950 --> 00:04:00,010
регрессию, используя продвинутые методы

116
00:04:00,360 --> 00:04:02,070
оптимизации.

117
00:04:03,180 --> 00:04:05,590
И только для того, чтобы напомнить о том, что

118
00:04:05,840 --> 00:04:06,800
необходимо делать в этих методах, - нужно

119
00:04:07,080 --> 00:04:08,390
было определить функцию, которая

120
00:04:08,450 --> 00:04:09,460
называется функцией затрат

121
00:04:09,640 --> 00:04:11,160
и принимает в качестве

122
00:04:11,280 --> 00:04:13,660
аргумента вектор параметров тета, при этом во всех

123
00:04:13,790 --> 00:04:16,180
уравнениях, которые мы здесь

124
00:04:16,770 --> 00:04:19,030
писали, использовались вектора, начинающиеся с индекса 0.

125
00:04:19,510 --> 00:04:20,690
Итак, у нас была тета с индексами

126
00:04:21,180 --> 00:04:22,810
от 0 до N. Но из-за того,

127
00:04:23,020 --> 00:04:25,920
что Octave индексирует вектора начиная с индекса 1.Тета с индексом 0 записывается

128
00:04:26,820 --> 00:04:28,240
в Octave в виде тета

129
00:04:28,560 --> 00:04:29,990
с индексом 1.

130
00:04:30,120 --> 00:04:31,630
Тета с индексом 2 записывается

131
00:04:31,860 --> 00:04:32,930
как тета 2, и так далее

132
00:04:33,280 --> 00:04:35,070
до тета с

133
00:04:36,270 --> 00:04:36,650
индексом N+1.

134
00:04:36,740 --> 00:04:38,450
И то, что нам нужно было сделать - определить

135
00:04:38,600 --> 00:04:40,240
функцию.

136
00:04:41,170 --> 00:04:42,370
Давайте определим функцию, называемую функцией затрат, которую бы мы затем

137
00:04:42,780 --> 00:04:44,140
подставили в то, что

138
00:04:44,360 --> 00:04:46,920
у нас есть и что мы видели ранее.

139
00:04:47,300 --> 00:04:48,490
Мы будем

140
00:04:49,060 --> 00:04:50,310
использовать fmiunc для функции затрат, и

141
00:04:50,540 --> 00:04:52,160
так далее,

142
00:04:54,830 --> 00:04:55,430
правильно?

143
00:04:55,600 --> 00:04:56,870
Здесь, fminunc - функция поиска минимум

144
00:04:57,030 --> 00:04:58,060
F min без ограничения параметров.

145
00:04:58,280 --> 00:04:59,310
Мы будем работать с fminunc,

146
00:04:59,650 --> 00:05:01,230
которая просто  возьмет функцию затрат

147
00:05:01,310 --> 00:05:02,300
и минимизирует

148
00:05:02,540 --> 00:05:04,340
ее для нас.

149
00:05:05,950 --> 00:05:07,050
Итак, две основные вещи, которые функция затрат должна

150
00:05:07,170 --> 00:05:08,600
возвращать, это,

151
00:05:08,700 --> 00:05:10,620
во-первых J-val.

152
00:05:11,280 --> 00:05:12,400
Для этого нам необходимо

153
00:05:12,720 --> 00:05:13,950
написать код вычисления функции J от

154
00:05:14,020 --> 00:05:15,710
тета.

155
00:05:17,130 --> 00:05:19,030
Теперь, когда мы

156
00:05:19,450 --> 00:05:20,920
используем логистическую регрессию, функция затрат

157
00:05:20,990 --> 00:05:21,960
J от тета, конечно же, меняется и, в частности,

158
00:05:22,280 --> 00:05:23,450
теперь необходимо так же включить это

159
00:05:24,480 --> 00:05:25,760
дополнительное регуляризующее слагаемое

160
00:05:25,870 --> 00:05:29,580
в конце в функцию затрат.

161
00:05:29,850 --> 00:05:30,930
Убедитесь, что включили

162
00:05:31,030 --> 00:05:33,410
это слагаемое в конце, когда вычисляете J от тета.

163
00:05:34,590 --> 00:05:35,520
А также другие вещи, необходимые

164
00:05:36,050 --> 00:05:37,240
в функции затрат для

165
00:05:37,690 --> 00:05:39,010
получения градиента.

166
00:05:39,530 --> 00:05:41,170
Так,

167
00:05:41,400 --> 00:05:42,570
градиент с

168
00:05:42,660 --> 00:05:44,080
индексом 0 - частная производная J

169
00:05:44,240 --> 00:05:45,520
от  тета по тета с индексом 0, градиент с

170
00:05:45,690 --> 00:05:47,170
индексом 2 - вот это, и так

171
00:05:47,580 --> 00:05:49,520
далее.

172
00:05:49,780 --> 00:05:50,900
Повторюсь, индекс смещен на единицу.

173
00:05:51,220 --> 00:05:52,850
Правильно, из-за индексирования с единицы, которое

174
00:05:53,110 --> 00:05:54,450
использует Octave.

175
00:05:55,940 --> 00:05:56,780
И, глядя на эти слагаемые.

176
00:05:57,850 --> 00:05:58,680
Этот термин здесь.

177
00:05:59,410 --> 00:06:00,640
Мы уже получали его на предыдущем слайде, что это,

178
00:06:00,720 --> 00:06:02,840
вообще-то, равно вот этому.

179
00:06:03,230 --> 00:06:03,640
Оно не меняется.

180
00:06:04,120 --> 00:06:07,250
Поскольку производная по тета  с индексом 0 не меняется.

181
00:06:07,650 --> 00:06:09,540
По сравнению с версией без регуляризации.

182
00:06:10,960 --> 00:06:13,210
А другие слагаемые меняются.

183
00:06:13,840 --> 00:06:16,340
В частности, производные по тета с индексом 1.

184
00:06:17,010 --> 00:06:18,830
Мы также получали это на предыдущем слайде.

185
00:06:19,110 --> 00:06:20,670
Оно равняется изначальному слагаемому

186
00:06:20,890 --> 00:06:22,560
минус lambda,

187
00:06:23,450 --> 00:06:24,870
деленная на M и умножить на тета с индексом 1.

188
00:06:25,310 --> 00:06:27,140
Таким образом, убеждаемся, что провели это без ошибок.

189
00:06:27,800 --> 00:06:29,370
И мы можем добавить скобки сюда.

190
00:06:29,830 --> 00:06:30,980
Так, что суммирование не распространяется.

191
00:06:31,570 --> 00:06:33,160
И, аналогично, это

192
00:06:33,380 --> 00:06:34,800
другое

193
00:06:35,130 --> 00:06:36,180
слагаемое вот здесь выглядит как то с дополнительным

194
00:06:37,070 --> 00:06:37,950
слагаемым на предыдущем слайде, которое соответствует градиенту

195
00:06:38,030 --> 00:06:39,770
от их регуляризованной

196
00:06:39,950 --> 00:06:41,450
целевой функции.

197
00:06:42,230 --> 00:06:43,650
Так что если вы реализуете эту новую функцию затрат и передадите в

198
00:06:43,820 --> 00:06:45,140
fminuic или в один из

199
00:06:45,720 --> 00:06:47,370
тех продвинутых оптимизаторов,

200
00:06:48,190 --> 00:06:49,160
которые минимизируют новую

201
00:06:50,050 --> 00:06:51,940
регуляризованную

202
00:06:52,540 --> 00:06:55,990
функцию затрат J от тета.

203
00:06:56,990 --> 00:06:58,220
И параметры, которые вы получите, будут теми,

204
00:06:59,530 --> 00:07:00,740
которые соответствуют логистической

205
00:07:01,450 --> 00:07:02,940
регрессии с регуляризацией.

206
00:07:04,410 --> 00:07:05,540
Теперь вы знаете, как реализовать регуляризованную

207
00:07:05,780 --> 00:07:08,210
логистическую регрессию.

208
00:07:09,780 --> 00:07:10,920
Если пройтись по Кремниевой Долине, а я живу в Кремниевой Долине,

209
00:07:11,380 --> 00:07:12,900
то здесь много инженеров,

210
00:07:13,100 --> 00:07:14,900
которые зарабатывают кучу денег для

211
00:07:15,420 --> 00:07:16,490
своих компаний, используя методы

212
00:07:16,610 --> 00:07:18,090
машинного обучения

213
00:07:19,180 --> 00:07:20,390
Я знаю, что мы с вами изучаем

214
00:07:20,600 --> 00:07:22,860
это совсем недавно.

215
00:07:23,620 --> 00:07:25,410
Но если

216
00:07:26,510 --> 00:07:28,360
вы к настоящему моменту поняли линейную

217
00:07:29,210 --> 00:07:30,710
регрессию, продвинутые алгоритмы

218
00:07:30,950 --> 00:07:32,520
оптимизации и регуляризацию, то,

219
00:07:32,950 --> 00:07:34,270
честно говоря, вы возможно

220
00:07:35,010 --> 00:07:36,290
уже знаете

221
00:07:36,750 --> 00:07:38,050
намного больше о машинном обучении,

222
00:07:38,180 --> 00:07:39,580
конечно сейчас, намного больше,

223
00:07:40,240 --> 00:07:41,670
чем значительная часть преуспевающих инженеров Кремниевой

224
00:07:41,820 --> 00:07:44,760
Долины.

225
00:07:45,300 --> 00:07:46,420
Которые зарабатывают кучу денег для компаний.

226
00:07:47,050 --> 00:07:49,250
Или создают программные продукты используя алгоритмы машинного обучения.

227
00:07:50,370 --> 00:07:50,960
Итак, - мои поздравления.

228
00:07:52,080 --> 00:07:53,120
Вы проделали большой путь.

229
00:07:53,490 --> 00:07:54,550
Теперь вы знаете достаточно, чтобы применить эти подходы

230
00:07:54,780 --> 00:07:55,990
на практике для решения

231
00:07:56,310 --> 00:07:58,210
множества задач.

232
00:07:59,260 --> 00:08:00,580
Поздравляю Вас с этим.

233
00:08:00,780 --> 00:08:01,880
Конечно, предстоит

234
00:08:02,350 --> 00:08:03,280
изучить еще многое и в

235
00:08:03,400 --> 00:08:05,180
следующих видео мы начнем

236
00:08:05,380 --> 00:08:06,540
изучать мощные

237
00:08:06,560 --> 00:08:07,850
инструменты нелинейной

238
00:08:08,030 --> 00:08:10,890
классификации .

239
00:08:11,680 --> 00:08:13,350
Тогда, как в линейной регрессии, логистической регрессии

240
00:08:13,690 --> 00:08:14,940
вы можете формировать

241
00:08:15,080 --> 00:08:17,310
полиномиальные выражения,

242
00:08:17,460 --> 00:08:18,350
оказывается что существуют более

243
00:08:18,510 --> 00:08:21,150
мощные нелинейные квантификаторы,

244
00:08:21,460 --> 00:08:23,650
которые могут затем выделить полиномиальную регрессию.

245
00:08:24,640 --> 00:08:25,780
И в следующих за этим видео я начну рассказывать

246
00:08:25,810 --> 00:08:28,280
вам о них.

247
00:08:28,510 --> 00:08:29,560
Чтобы у вас были еще более мощные алгоритмы,

248
00:08:29,760 --> 00:08:30,440
чем сейчас, и чтобы вы могли применить

249
00:08:31,380 --> 00:08:32,870
их и к другим задачам.