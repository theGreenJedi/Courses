अब तक, आपने देखे है कुछ भिन्न-भिन्न लर्निंग अल्गोरिद्म्स, लिनीअर रेग्रेशन और लॉजिस्टिक रिग्रेशन. वे सही काम करते हैं बहुत सी प्राब्लम्ज़ के लिए, लेकिन जब आप अप्लाई करते हैं उन्हें कुछ निश्चित मशीन लर्निंग ऐप्लिकेशन्स के लिए, वे जा सकते हैं कठिनाई में जिसे कहते हैं ओवरफ़िटिंग जो कारण बन सकती है उनकी ख़राब पर्फ़ॉर्मन्स का. मैं क्या करना चाहता हूँ इस वीडियो में है समझाना आपको कि क्या है यह ओवरफ़िटिंग की समस्या, और इस के बाद के अगले कुछ वीडियो में, हम बात करेंगे एक तकनीक की जिसे कहते हैं रेगुलराइज़ेशन, जो करने देगा हमें सुधार या कमी इस ओवरफ़िटिंग समस्या में और कराएगा इन लर्निंग अल्गोरिद्म्स को शायद काम बेहतर. तो क्या है ओवरफ़िटिंग? चलो इस्तेमाल करते रहते हैं हमारा हमेशा का उदाहरण प्रिडिक्ट करने वाला घरों की क़ीमतें लिनीअर रेग्रेशन से जहाँ हम प्रिडिक्ट करना चाहते हैं क़ीमत घर के साइज़ के एक फ़ंक्शन की तरह. एक काम जो हम कर सकते हैं है कि फ़िट करें एक लिनीअर फ़ंक्शन एक डेटा को, और यदि हम वह करते हैं, शायद हमें मिलती है उस तरह की सीधी लाइन जो फ़िट होती है इस डेटा को. लेकिन यह नहीं है एक अच्छा मॉडल डेटा को देखते हुए, यह प्रतीत होता है काफ़ी स्पष्ट कि जैसे साइज़ घर का बढ़ता है, घर की क़ीमत एक समभूमि तक पहुँचती है, या एक प्रकार से समतल होती जाती है जैसे हम जाते हैं दाईं तरफ़ और इसलिए यह अल्गोरिद्म नहीं फ़िट करता ट्रेनिंग को और हम कहते हैं इसे अंडरफ़िटिंग की समस्या, और एक अन्य टर्म इसके लिए है कि इस अल्गोरिद्म में हाई बाइयस है. इन दोनो का लगभग मतलब है कि यह फ़िट ही नहीं कर रहा ट्रेनिंग डेटा को ढंग से. टर्म है एक प्रकार से एक ऐतिहासिक या तकनीकी, लेकिन विचार है कि यदि फ़िट करने से एक सीधी लाइन डेटा को, तब, यह है जैसे कि अल्गोरिद्म में है एक बहुत अधिक मज़बूत पूर्व धारणा, या एक बहुत मजबूत बाइयस कि घरों की क़ीमतें होंगी तबदील लिनीअरली उनके साइज़ के हिसाब से और जबकि डेटा इसके विपरीत सबूत होने के बावजूद कि विपरीत है पूर्व धारणा अभी भी है बाइयस, अभी भी समाप्त करता है इसे फ़िट करते हुए एक सीधी लाइन और यह हो जाता है एक ख़राब फ़िट डेटा को. अब, मध्य में, हम कर सकते थे फ़िट एक क्वाड्रैटिक फ़ंक्शन और, इस डेटा सेट के साथ, हम फ़िट करते हैं क्वाड्रैटिक फ़ंक्शन, शायद, हमें मिले उस तरह का कर्व और, वह बेहतर काम करता है. और दूसरी तरफ़ अति होती, यदि हमें करना होता फ़िट, मान लो एक चार डिग्री का पालिनोमीयल डेटा को. तो, यहाँ हैं हमारे पाँच पेरमिटर्स, थीटा ज़ीरो से थीटा चार तक, और, उस के साथ, हम वास्तव में फ़िट कर सकते हैं एक कर्व जो जाता है हमारे सारे पाँच ट्रेनिंग इग्ज़ाम्पल्ज़ से. आपको शायद मिले एक कर्व जो दिखता है ऐसा. वह, जहाँ एक तरफ़, प्रतीत होता है करता हुआ एक बहुत अच्छा काम फ़िट करते हुए ट्रेनिंग सेट को और, जो जाता है मेरे पूरे डेटा से, कम से कम. लेकिन, यह अभी भी एक बहुत घुमावदार कर्व, ठीक है? तो, जा रहा है ऊपर और नीचे सब जगह, और, हम नहीं वास्तव में सोचते कि वह है एक अच्छा मॉडल प्रिडिक्ट करने के लिए घर की कीमत. तो यह समस्या, हम कहते हैं ओवरफ़िटिंग, और, एक अन्य इसके लिए है कि इस अल्गोरिद्म में हाई वेरीयन्स है. टर्म हाई वेरीयन्स है एक अन्य एक ऐतिहासिक या तकनीकी टर्म. लेकिन अनुभव यह है कि, हम फ़िट कर रहे हैं इतना अधिक डिग्री का पालिनोमीयल, तब, हायपॉथिसस फ़िट कर सकती है, आप जानते हैं, यह है लगभग कि यह कर सकती है फ़िट लगभग कोई भी फ़ंक्शन और यह फ़ेस सम्भावित हायपॉथिसस का है बहुत बड़ा, यह है अधिक वेरीयबल. और हमारे पास पर्याप्त डेटा नहीं है इसे रोकने का कि यह दे हमें एक बेहतर हायपॉथिसस तो उसे कहते हैं ओवरफ़िटिंग. और मध्य में, वहाँ नहीं है वास्तव में एक नाम लेकिन मैं सिर्फ़ लिखूँगा, आप जानते हैं, बस सही. जहाँ एक द्वितीय डिग्री का पालिनोमीयल, क्वाड्रैटिक फ़ंक्शन प्रतीत होता एकदम सही फ़िट करने के लिए इस डेटा को. दोहराते हुए थोड़ा सा समस्या ओवरफ़िटिंग की आती है जब हमारे पास हैं बहुत अधिक फ़ीचर्ज़, तब लर्न हायपॉथिसस शायद फ़िट करे ट्रेनिंग सेट को ढंग से. तो, आपका कॉस्ट फ़ंक्शन शायद वास्तव में हो काफ़ी नज़दीक ज़ीरो के या शायद जो हो बिल्कुल ज़ीरो ही, लेकिन आपको शायद मिले एक कर्व ऐसा जो, आप जानते हैं कोशिश करता है बहुत अधिक फ़िट करने के लिए ट्रेनिंग सेट को, ताकि यह विफल भी हो जाता है जनरलाइज होने में नए इग्ज़ाम्पल्ज़ के लिए और विफल रहता है प्रिडिक्ट करने में क़ीमतें नए इग्ज़ाम्पल्ज़ के लिए भी, और यहाँ टर्म जनरलाइज्ड का मतलब है कितनी अच्छी तरह से हायपॉथिसस अप्लाई करती है नए इग्ज़ाम्पल्ज़ को. मतलब कि डेटा को घरों के जो नहीं थे ट्रेनिंग सेट में. इस स्लाइड पर, हमने देखा ओवर फ़िटिंग लिनीअर रेग्रेशन के लिए. एक ऐसी ही चीज़ अप्लाई करती हैं लजिस्टिक रेग्रेशन के लिए भी. यहाँ है एक लॉजिस्टिक रिग्रेशन का उदाहरण दो फ़ीचर्ज़ के साथ x1 और x2. एक काम जो हम कर सकते हैं, है फ़िट करें लॉजिस्टिक रिग्रेशन केवल एक सरल हायपॉथिसस से इस तरह, जहाँ, हमेशा की तरह, G है मेरा सिग्मोईड फ़ंक्शन. और यदि आप वह करते हैं, आपको मिलती है एक हायपॉथिसस, प्रयास करते हुए इस्तेमाल करने का, शायद सिर्फ़ एक सीधी लाइन जो अलग करती है पॉज़िटिव या नेगेटिव इग्ज़ाम्पल को. और यह नहीं दिखती है एक बहुत अच्छा फ़िट हायपॉथिसस को. तो, एक बार फिर, यह है एक उदाहरण अंडरफ़िटिंग का या हायपॉथिसस का जिसमें हाई बाइयस है. इसके विपरीत, यदि आपको जोड़ने होते आपके फ़ीचर्ज़ ये क्वाड्रैटिक टर्म्ज़, तब, आपको मिल सकती एक निर्णायक सीमा जो शायद दिखे अधिक इस जैसी. और, आप जानते हैं, वह है डेटा के लिए एक बहुत अच्छा फिट. शायद, लगभग उतना अच्छा जितना हमें मिल सकता था, इस ट्रेनिंग सेट पर. और अंत में, दूसरी तरफ़, यदि आपको करना होता फ़िट एक बहुत बड़ी डिग्री का पालिनोमीयल, यदि आपको बनाने होते बहुत से ऊँची डिग्री के पालिनोमीयल, तब, तब, लजिस्टिक रेग्रेशन शायद बिगड़ जाता ख़ुद, शायद कोशिश करता बहुत अधिक ढूँढने की एक निर्णय सीमा है जो फ़िट करती है आपके ट्रेनिंग डेटा को या बहुत कोशिश करता मरोड़ने की अपने आप को, फ़िट करने के लिए प्रत्येक ट्रेनिंग इग्ज़ाम्पल को सही तरीक़े से. और, आप जानते हैं, यदि फ़ीचर्ज़ x1 और x2 प्रिडिक्ट करते हैं, शायद, कैंसर को, आप जानते हैं, कैन्सर हैं एक घातक, सौम्य स्तन ट्यूमर को. यह नहीं है, यह वास्तव में नहीं दिखती एक बहुत अच्छी हायपॉथिसस, प्रिडिक्शन्स करने के लिए. और तो, एक बार फिर, यह है एक उदाहरण ओवर फ़िटिंग का और एक हायपॉथिसस का जिसमें हाई वेरीयन्स है और नहीं है वास्तव में, और, संभावना नहीं है कि जनरलाइज करें अच्छी तरह से नए इग्ज़ाम्पल्ज़ को. बाद में, इस पाठ्यक्रम में, जब हम बात करेंगे डीबगिंग और डायग्नोसिस करने की चीज़ों को जो ग़लत हो सकती हैं लर्निंग अल्गोरिद्म्स के साथ, हम देंगे आपको विशेष टूलस पहचान करने के लिए कि कब ओवर फ़िटिंग और, कब अंडर फ़िटिंग शायद हो रही है. लेकिन, अभी के लिए, चलो बात करते हैं समस्या की, यदि हम सोचते हैं ओवर फ़िटिंग हो रही है. हम क्या कर सकते हैं इसे सम्बोधित करने के लिए? पिछले उदाहरणों में, हमारे पास था एक या दो आयामी / डिमेन्शनल डेटा इसलिए, हम कर पाए प्लॉट हायपॉथिसस को और देख पाए कि क्या हो रहा था और चुन पाए उचित डिग्री का पालिनोमीयल. अत:, पहले घरों की क़ीमत के उदाहरण में, हम कर पाए केवल प्लॉट हायपाथिसस को और, आप जानते हैं, शायद देख पाए कि यह फ़िट कर रहा था एक प्रकार से बहुत घुमावदार फ़ंक्शन जो जा रहा था सब तरफ़ प्रिडिक्ट करने के लिए घरों की क़ीमतें. और हम तब इस्तेमाल कर पाए चित्र इस तरह के और चुन पाए उचित डिग्री का पालिनोमीयल. अत: प्लॉट करना हायपॉथिसस, हो सकता हैं एक तरीक़ा कोशिश में तय करने की कि कौन सी डिग्री पालिनोमीयल की लेनी चाहिए. लेकिन वह हमेश काम नहीं करता. और, वैसे तो अक्सर हमारे पास शायद हो सकती हैं लर्निंग प्राब्लम्ज़ ज़िनमे हमारे पास होते हैं बहुत से फ़ीचर्ज़. और वहाँ नही है प्रश्न सिर्फ़ चुनने के कौन सी डिग्री पालिनोमीयल की. और, वैसे तो जब हमारे पास हैं बहुत अधिक फ़ीचर्ज़, यह हो जाता है कठिन भी प्लॉट करना डेटा को और यह हो जाता है कठिनतर इसे विज़ूअलाइज करना, तय करने के लिए कि कौन से फ़ीचर्ज़ इस्तेमाल रखने हैं या नही. तो वस्तुतः, यदि हम प्रयास कर रहे हैं प्रिडिक्ट करने का घरों की क़ीमत कभी-कभी हमारे पास हो सकते हैं बहुत से भिन्न फ़ीचर्ज़. और सारे वे फ़ीचर्ज़ प्रतीत होते हैं, आप जानते हैं, शायद वे प्रतीत होते हैं उपयोगी. लेकिन, यदि हमारे पास हैं बहुत से फ़ीचर्ज़, और, बहुत कम ट्रेनिंग डेटा, तब, ओवर फिटिंग एक समस्या बन सकती है. सम्बोधित करने के लिए ओवर फ़िटिंग, वहाँ दो मुख्य विकल्प हैं चीज़ों के लिए जो हम कर सकते हैं. पहला विकल्प है, कोशिश करें कम करने के लिए संख्या फ़ीचर्ज़ की. वस्तुतः, एक काम जो हम कर सकते हैं कि ख़ुद से देखें फ़ीचर्ज़ की लिस्ट को, और, इस्तेमाल करें उसे तय करने की कोशिश में कि कौन से हैं सबसे अधिक महत्वपूर्ण फ़ीचर्ज़, और, इसलिए, कौन से फ़ीचर्ज़ हैं जो हमें चाहिए रखने, और कौन से हैं फ़ीचर्ज़ जो हमें हटा देने चाहिए. बाद में, इस पाठ्यक्रम में, हम बात करेंगे मॉडल सिलेक्शन अल्गोरिद्म्स की भी. कौन से अल्गोरिद्म्स हैं स्वचालित रूप से निर्णय लेने के लिए कि कौन से फ़ीचर्ज़ रखने हैं और कौन से हटाने हैं. यह सुझाव काम करने का संख्या फ़ीचर्ज़ की काम कर सकता है सही, और, कम कर सकता है ओवर फ़िटिंग. और, जब हम बात करेंगे मॉडल सिलेक्शन की, हम जाएँगे इसमें गहराई में. लेकिन, नुकसान यह है कि, फेंक देने से कुछ इनमें से फ़ीचर्ज़, का मतलब है फेंक देना कुछ इन्फ़र्मेशन जो आपके पास है प्रॉब्लम के बारे में. उदाहरण के लिए, शायद, सभी वे फ़ीचर्ज़ हैं वास्तव में उपयोगी प्रिडिक्ट करने के लिए क़ीमत एक घर की, तो, शायद, हम वास्तव में नहीं चाहते फेंकना कुछ हमारी इन्फ़र्मेशन या फेंकना हमारे कुछ फ़ीचर्ज़. दूसरा विकल्प है, जिसकी हम बात करेंगे अगले कुछ वीडियो में, है रेगुलराइज़ेशन. यहाँ, हम रखेंगे सारे फ़ीचर्ज़, लेकिन हम काम करेंगे मग्निट्यूड / परिमाण वैल्यूज़ पेरमिटर्स थीटा j की. और, यह विधि काम करती है अच्छी तरह से, हम देखेंगे जब हमारे पास हैं बहुत से फ़ीचर्ज़, उनमें से प्रत्येक योगदान देता है थोड़ा बहुत प्रिडिक्ट करने के लिए y की वैल्यू, जैसे हमने देखा घर की क़ीमत प्रिडिक्ट करने के उदाहरण में. जहाँ हमारे पास हो सकते हैं अधिक संख्या में फ़ीचर्ज़, प्रत्येक उनमें से है, आप जानते हैं, कुछ हद तक उपयोगी, तो, शायद, हम नहीं फेंकना चाहते उन्हें. तो, यह देता है रेगुलराइज़ेशन का सुझाव मोटे तौर पर. और, मैं समझता हूँ कि, सारी ये विस्तृत जानकारी शायद आपको समझ नहीं आयी होगी अभी. लेकिन, अगले वीडियो में, हम शुरू करेंगे फ़ॉर्म्युलेट करना कि वास्तव में कैसे अप्लाई करना हैं रेगुलराइज़ेशन करना और, वास्तव में रेगुलराइज़ेशन का क्या मतलब है. और, तब हम शुरू करेंगे समझना, कि कैसे इसे इस्तेमाल करना है, कैसे लर्निंग अल्गोरिद्म को काम करवाना है बेहतर और कैसे बचना है ओवर फ़िटिंग से.