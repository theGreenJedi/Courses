Neste vídeo, eu gostaria de mostrar as principais intuições por atrás de como "Regularização" funciona. E também iremos anotar a função de custo que utilizaremos,
quando estivermos usando Regularização. Com os exemplos escritos a mão que temos nesses slides, acredito que poderei transmitir parte da intuição. Mas, uma maneira ainda melhor de ver, por si próprio, como Regularização funciona, é implementá-la, você mesmo. E, se você fizer corretamente o exercício depois disso, você vai ter a chance de ver Regularização em ação, por si próprio. Assim, aqui está a intuição. No vídeo anterior, vimos que, se tivéssemos que ajustar uma função quadrática a esses dados, isso nos daria um excelente ajuste para os dados. Enquanto que, se tivéssemos que ajustar um polinômio de grau muito maior, terminaríamos com uma curva que se ajustaria
muito bem aos dados de treino, com uma curva que se ajustaria
muito bem aos dados de treino, mas, que na realidade, se ajustaria muito mal os dados, não generalizando muito bem. Considere o seguinte: suponha que nós vamos penalizar, e tornar os parâmetros "θ₃" e "θ₄" bem pequenos. Aqui está o que quero dizer: aqui está nosso objetivo de otimização, ou nosso problema, onde minimizamos nossa função de custo de erro quadrático habitual. Digamos que eu pegue esse objetivo, e modifique, adicionando a ele "1000·θ₃²+1000·θ₄²". a ele "1000·θ₃²+1000·θ₄²". "1000" serve apenas como um número grande. Agora, se formos minimizar essa função, a única maneira de fazer essa nova função de custo menor, é se "θ₃" e "θ₄" forem pequenos, certo? Porque, caso contrário, tendo "1000·θ₃", essa nova função de custo será grande. Assim, quando minimizarmos essa nova função, terminaremos com "θ₃" perto de "0", e "θ₄" perto de "0". Assim, acabamos nos livrando desses dois termos, de maior grau. E se fizermos isso, se "θ₃" e "θ₄" forem próximos de "0", terminaremos com uma função quadrática. Assim, teremos um ajuste aos dados, que é uma função quadrática, e uma mínima contribuição dos pequenos termos "θ₃" e "θ₄", que são quase 0. E assim, essencialmente terminamos com uma função quadrática, o que é muito bom. Porque essa é uma hipótese bem melhor. Nesse exemplo, em particular, nós vimos o efeito de penalizarmos 2 dos valores de parâmetros que são grandes. De modo mais geral, aqui está a ideia por trás da Regularização. A ideia é que, se tivermos valores pequenos para os parâmetros, isso, de alguma forma, corresponde a ter uma hipótese mais simples. Assim, para o exemplo anterior, nós penalizamos "θ₃" e "θ₄". E quando ambos estavam próximos de "0", nós ficamos com uma hipótese bem mais simples: uma função quadrática.
Mas, de forma geral, podemos pensar em penalizar todos os parâmetros como uma tentativa de achar uma hipótese mais simples. Porque, quando esses parâmetros forem próximos de "0" - esse exemplo resultou em uma função quadrática - mas, em geral, é possivel mostrar que, ter valores menores para os parâmetros, corresponde também, geralmente, a suavizar as funções mais simples. E , portanto, tornando-as menos
sujeitas a sobreajuste ("overfitting"). Eu imagino que o raciocínio do porquê ter todos os parâmetros pequenos, corresponde a ter hipóteses mais simples, não esteja completamente claro para você ainda. E é um pouco difícil de explicar, a menos que você implemente e veja, por si próprio. Mas, espero que esse exemplo de ter "θ₃" e "θ₄" pequenos, e de como isso nos dá uma hipótese mais simples, eu espero que isso ajude a explicar o porquê, e dê alguma intuição de como isso pode ser verdadeiro. Vamos ver um exemplo específico. Para a predição de preços de casas, podemos ter centenas de recursos, como já dissemos. Onde, "x₁" pode ser o tamanho, "x₂", o número de quartos, "x₃", o número de andares, e assim por diante. E podemos ter uma centena de parâmetros. E, diferentemente do exemplo do polinômio, nós não sabemos se "θ₃" e "θ₄" são os termos de ordem mais alta do polinômio. Assim, se tivermos apenas um conjunto de centenas de parâmetros, é difícil saber, antecipadamente, quais são aqueles que, provavelmente, são menos relevantes. Portanto, temos 101 parâmetros, e não sabemos quais parâmetros devemos escolher, para tentar comprimir. Assim, em Relgularização, o que vamos fazer, é pegar nossa função de custo. Aqui está minha função de custo para Regressão Linear. E o que vou fazer é, modificar essa função de custo para simplificar todos os meus parâmetros, porque eu não sei qual, ou quais, posso tentar comprimir. Então, vou modificá-la, para adicionar um termo no final. Desse modo, [nós temos colchetes aqui também] eu adiciono um termo extra de regularização, para comprimir todos os parâmetros. Então, esse termo irá comprimir todos os meus parâmetros: "θ₁", "θ₂", "θ₃", ..., até "θ₁₀₀". A propósito, por convenção, esse somatório começa em "1", então eu não vou penalizar "θ₀", sendo grande. Isso é uma convenção, que a soma é de "i=1" até "n", ao invés de "0" a "n". Mas, na prática, isso faz pouca diferença e, se você incluir, "θ₀" ou não, na prática, vai fazer pouca
diferença no resultado. Mas, por convenção, geralmente regularizamos apenas "θ₁"até "θ₁₀₀". Vamos escrever o nosso objetivo de otimização, nossa Função de Custo Regularizada. Aqui está ela, "J(θ)", onde esse termo à direita, é o termo de Regularização. E esse "λ" é chamado "parâmetro de regularização". E o que "λ" faz é controlar o equilibrio entre dois objetivos diferentes. O primeiro objetivo, capturado pelo primeiro termo no objetivo, é que gostariamos de ter um bom ajuste aos dados de treino. Queremos os dados de treino bem ajustados. E o segundo objetivo é, que queremos manter os parâmetros pequenos, e isso é capturado pelo segundo termo,
pelo objetivo de Regularização. E o parâmetro de regularização, "λ", controla o equilibrio entre esses dois objetivos. Entre se ajustar bem aos dados de treino, e manter os parâmetros pequenos, e assim manter a hipotese relativamente simples, evitando o sobreajuste ("overfitting"). Para o nosso exemplo de predição de preços de casas, onde, previamente, se tivessemos que ajustar um polinomio de ordem muito alta, poderíamos terminar com uma função curvilínia como essa. Se você ajustar um polinomio de grau alto, usando todos os seus parâmetros. Mas, ao invés disso, você pode garantir que esse objetivo de Regularização é utilizado. Assim, você pode ter, de fato, uma curva que não é bem uma função quadrática, mas é bem mais simples, e suave, como a curva em magenta, que dá uma hipótese bem melhor para esses dados. Novamente, eu sei que pode ser difícil ver porque encolher esses parâmetros, pode dar esse efeito. Mas se você implementar esse algoritmo
com a Regularização você poderá ver esse efeito, claramente. Na Regressão Linear regularizada, se o parâmetro de Regularização, "λ", for muito grande, o que acontece é que nós iremos penalizar os parâmetros "θ₁", "θ₂", "θ₃", "θ₄", ..., muito fortemente. E assim, se nossa hipótese for essa, aqui embaixo, e se penalizarmos fortemente "θ₁", "θ₂", "θ₃", "θ₄", ..., então teremos todos esses
parâmetros proximos de zero. então teremos todos esses
parâmetros proximos de zero. "θ₁" será quase 0; "θ₂" será quase 0, "θ₃" e "θ₄" também ficarão proximos de 0. E se fizermos isso, removeremos todos os termos da nossa hipótese, ficando apenas com
uma hipótese como essa: ficando apenas com
uma hipótese como essa: "h(x)=θ₀". Ou seja, o preço das casas são iguais a "θ₀". e isso se assemelha a ajustar os dados a uma linha reta horizontal. E isso é um exemplo de ajuste para baixo ("underfitting"), e em particular, essa hipótese, essa linha reta, é falha em ajustar-se bem ao conjunto de treino. É apenas uma linha reta, e não chega perto da maioria dos exemplos de treino. Outra maneira de dizer isso, é que essa hipótese tem um viés tão alto, que os preços das casas são iguais a "θ₀". E, apesar dos dados mostrarem o contrário, esse foi o ajuste escolhido,
apenas essa linha reta, esse foi o ajuste escolhido,
apenas essa linha reta, horizontal - que eu não desenhei muito bem. Essa é apenas uma linha horizontal para os dados. Portanto, para a Regularização funcionar direito, alguns cuidados precisam ser tomados, para fazer boas escolhas, para o parâmetro "λ" da Regularização. E quando falarmos mais tarde, sobre multi-seleção, falaremos sobre alguns modos de escolher, automaticamente, o parâmetro da Regularização, "λ".
Então, essa é a ideia por trás de Regularização, o parâmetro da Regularização, "λ".
Então, essa é a ideia por trás de Regularização, e da função de custo que usamos,
para utilizar Regularização. e da função de custo que usamos,
para utilizar Regularização. Nos próximos 2 vídeos, vamos pegar essas idéias e aplicá-las a Regressão Linear, e a Regressão Logística, a fim de pode usá-las para evitar problemas de sobreajuste.
Tradução: Carlos Lacerda | Revisão: Pablo de Morais Andrade