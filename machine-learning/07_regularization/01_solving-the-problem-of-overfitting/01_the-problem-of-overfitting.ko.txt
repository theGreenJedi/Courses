여기까지 왔다면 이제 몇 가지 종류의 학습 알고리즘을 배웠을 겁니다 로지스틱 회귀(logistic regression)과<br />선형 회귀(linear regression) 말이죠 이 알고리즘들은 많은 머신러닝 문제들에 적용시킬 수 있습니다만 과적합(overfitting)이라는 문제에 빠져 알고리즘의 성능이 잘 안나올 수 있습니다 이번 영상에서는 과적합(overfitting)이 무엇인지 설명하고 이후로 몇 개의 영상에서는 정규화(regularization)라는 기술에 대해서 얘기할건데 이 기술은 과적합 문제를 개선하여 학습 알고리즘이 더욱 좋은 성능을 낼 수 있도록 합니다 자 그렇다면 과적합(overfitting)이란 뭘까요? 우리가 계속 사용하던 집값 예측 예제를 봅시다 선형 회귀(linear regression)으로 집의 크기로부터 집값을 예측하는 문제였죠 이 문제를 풀 수 있는 방법으로는 선형 함수(1차 함수, 직선)를 데이터에 맞게끔 하는 것이고 이 문제를 해결하고나면 데이터를 가로지르는 선을 볼 수 있겠죠 그러나 이 모델은 별로 좋지 못한 것 같습니다 데이터를 보면 집값이 커지면서 가격이 올라가는게 보이지만 집의 사이즈가 커질수록 집값이 천천히 올라 결국 평탄화 되죠 그러다 보니 우리의 알고리즘은 이 데이터에 잘 들어맞지 않습니다 이런 문제를 과소적합(underfitting)이라고 하고 다른 말로는 알고리즘이 high bias(높은 편향)을 갖고 있다고 합니다 이 2가지 용어는 모두 대략적으로 우리의 모델이 데이터에 제대로<br />맞지 않는다는 것을 의미합니다 이 용어는 역사적으로 혹은 기술적으로 정해진 용어입니다 그러나 둘다 표현하고자 하는 아이디어는 직선을 데이터에 맞추는 경우 이런 경우에 우리는 알고리즘이 강한 선입견(preconception)을 갖고 있다 혹은 강한 편향(bias)를 갖고 있다라고 얘기하고 집 값이 집 크기에 따라서 데이터의 분포와 상관없이 선형적으로<br />변화할 것이라는 것입니다 데이터가 예측에 반대하는 증거를 보여줌에도 불구하고 데이터를 직선에 맞추려다보니 편향(bias) 혹은 선입견(preconception)이 생기고 따라서 형편없는 결과가 나옵니다 그러면 여기 가운데 처럼 2차 함수를 끼워넣을 수도 있겠죠 이런 데이터에 2차 함수를 넣으면 이렇 모양의 커브를 얻을 수 있겠죠 그리고 이건 꽤 잘 맞습니다 그리고 마지막으로 좀 극단적인 예제를 보면<br />4차 함수를 데이터에 맞추고 있습니다 그래서 여기는 세타0 부터 세타5까지 5개의 파라미터를 이용해 5개의 데이터 모두를 선을 이용해 맞출 수 있습니다 그러면 이런 선을 얻을 수 있겠죠 한편으로 보면 이 선은 데이터에 잘 맞으니 적어도 훈련용 데이터에 한해서는 꽤 좋은 성능을 낸다고 볼 수 있죠 근데 보면 엄청나게 꼬아져 있는 선이죠? 이 선을 보고 우리는 이 선이 제대로 된 집 값을 예측할거라고 생각되지 않습니다 이런 문제를 보고 과적합(overfitting)이라고 합니다 그리고 이걸 다른 말로 알고리즘이 high variance(높은 분산)을<br />갖고 있다고 합니다 high variance도 역사적 혹은 기술적으로 탄생한 용어입니다 어쨋건 직관적으로 우리는 어떤 고차 함수를 데이터에 맞추려고 하고 사실 완벽히 맞는 함수를 만들고자 하면 만들 수 있지만 그렇게 되면 커다란 변동성을 갖게 되죠 그리고 우리는 저런 변동성을 막아줄 엄청난 데이터를 가지고 있지 않죠 이런 상황을 보고 과적합(overfitting)이라고 합니다 여기 가운데 같은 경우는 어떤 이름이 따로 있는 것은 아니고<br />그냥 잘 맞는다라고 표현합니다 2차 함수가 데이터에 딱 잘 맞고 있죠 정리를 좀 하자면 과적합 문제는 많은 특성(feature)들이 존재할 때 우리의 가설이 학습용 데이터에만 잘 맞아서 학습용 데이터를 대상으로는 비용함수가 거의 0에 가까운 값을 혹은 0이 나오고 대신에 엄청 복잡한 커브로 학습 데이터에 맞춰질 것이고 학습 데이터를 제외한 다른 새로운 예제가 들어올 경우 새로운 예제에 대한 일반화(generalized)된 예측은 제대로 하지 못하는 경우가 발생합니다 일반화(generalized)라는 용어는 가설이 새로운 데이터에도 얼마나<br />잘 맞냐라고 생각하면 됩니다 여기서 새로운 데이터는라는 것은 학습용 데이터에 없던<br />집 크기에 대한 집 가격 정보겠죠 우리는 앞의 슬라이드에서 선형 회귀에 대한 과적합 케이스를 봤는데요 이것은 로지스틱 회귀에도 마찬가지입니다 여기 특성 x1, x2에 대한 로지스틱 회귀 예제가 있습니다 한가지 방법으로는 이렇게 간단한 가설(hypothesis)를 로지스틱 회귀에 사용할 수 있습니다 여기 g는 시그모이드 함수입니다 이런 가설을 가지고 로지스틱 회귀를 하면 이런 간단한 선으로 양성(positive)과 음성(negative)를 나누겠죠 그러나 이건 그다지 잘 맞아 떨어지지 않는거 같습니다 다시 말하자면 이런 케이스는 과소적합(underfitting) 케이스입니다 혹은 high bias(높은 편향)을 가지고 있다고 하죠 좀 더 나아가서 좀 더 고차의 특성들을 가설에 추가적으로 이용하면 이런 decision boundary를 얻을 수 있겠죠 보다시피 좀더 데이터에 잘 맞는걸 볼 수 있죠 아마도 이 선이 학습용 데이터로부터 얻을 수 있는 최적일거 같네요 그리고 마지막으로 극단적인 예제가 있죠 엄청 나게 많은 고차항을 생성하여 엄청 높은 차수의 다항식을 만들면 로지스틱 회귀는 데이터에 맞는 엄청 자잘자잘하게 꼬아진 decision boundary를 구해내게 될 것입니다 자신의 예제를 모두 맞추기 위해 엄청 길게 꼬여있겠죠 만약에 특성 x1과 x2가 암이 악성인지 아닌지를 판단하는데 쓰인다고 하면 이 가설은 정말 좋지 못한 예측을 하는 것이죠 다시 말하지만 이런 케이스를 과적합이라고 하고 high varicance(높은 분산)이라고도 합니다 그리고 새로운 예제에 잘 일반화되지 않습니다 이 코스의 뒤쪽에서는 학습 알고리즘이 과적합인지 디버깅과 분석하는 툴에 대해서 알아볼 것이고 마찬가지로 과소적합에 대해서도 할 것입니다 일단 과적합 케이스의 문제가 있다고 가정하고 어떻게 이게 과적합인지 알 수 있을까요? 우리의 예제들은 모두 1차원 혹은 2차원이었으므로 그냥 그래프를 그려 무슨 일이 일어나는지 보면서 적당한 차원의 다항식을 선택하면 됩니다 앞에서 본 집 값 문제를 보면 우리의 가설을 바로 그래프로 그려볼 수 있죠 그려면 다음과 같이 적당히 꼬불꼬불한 선이 우리의 데이터를 지나가겠죠 우리는 이런 그래프를 보고 적당한 수준의 다항식을 선택할 수 있죠 따라서 가설을 그래프로 표현하여 어떤 다항식을 사용하는지 보는 것은 한가지 방법이 되겠죠 하지만 이 방법은 항상 동작하지 않습니다 다항식의 차수가 높은 문제가 아니더라도 엄청나게 많은 특성이 있는 경우에는 다항식의 차수를 선택할 여지조차 없죠 게다가 특성이 많아질수록 그래프를 통해 시각적으로 표현하기가 더욱 어려워집니다 그래서 어떤 특성을 남겨야 할지 선택하기 어렵죠 구체적으로 얘기하면 집 값을 예측할건데 특성이 엄청 많은 경우입니다 게다가 특성 하나하나가 예측에 도움이 될 것 같은 경우죠 이렇게 특성이 많은데 학습 데이터가 적은 경우 과적합 문제가 발생할 수 있습니다 과적합 문제를 해결하기 위해서는 크게 2가지 해결법이 있습니다 첫번째로는 특성의 갯수를 줄이는 방법입니다 구체적으로 얘기하면 특성들을 쭉 보고 이거는 쓸만해 이거는 안쓸만해 해서 어떤 특성은 남기고 어떤 특성은 버리는 식으로 제거할 수 있겠죠 이 코스의 뒤쪽에서는 모델 선택 알고리즘을 배울겁니다 알고리즘이 어떤 특성을 사용할 건지 자동으로 선택하거나 버릴지 선택하지요 여튼 특성의 수를 줄이는 것이 과적합 문제에 해결책으로 이용될 수 있습니다 모델 선택 알고리즘을 애기하면서 여기에 대해 좀더 자세히 얘기하도록 하겠습니다 그러나 단점으로는 몇 가지 특성을 버림으로 인해 문제에 포함된 정보를 같이 버리게 된다는 것입니다 예를 들어 모든 특성들이 집 값을 예측하는데 도움이 되는경우 우리는 그 어떤 정보도 버리고 싶지 않을겁니다 두번째 해결법으로는 정규화(regularization)이 있습니다 뒤의 영상들에서 배울겁니다 모든 특성들을 남기되 각각의 특성이 갖는 영향 규모를 줄이는 겁니다 다르게 얘기하면 세타값이 미치는 영향을 줄이는 거죠 이 방법은 과적합 문제를 잘 해결해줍니다 엄청 많은 특성들이 있을 때 그리고 각 특성들이 예측에 엄청 작은 영향을 미치는 경우의 문제가 있다고 가정해봅시다 앞서 얘기했던 집값 예측 같은거 말이죠 엄청 많은 특성이 있고 각각의 특성이 예측값에 영향을 주죠 그래서 특성을 버리기 싫은겁니다 이로써 정규화에 대한 큰 그림을 설명드렸습니다 아직은 어떤 의미인지 잘 와닿지 않을 수 있습니다 그러나 다음 영상에서 수학적으로 정규화가 어떻게 동작하는지 또 어떻게 적용하는지 알아보겠습니다 그 다음에 정규화를 어떻게 사용해야 우리의 학습 알고리즘이 좋아지고 과적합을 피할 수 있는지 배우겠습니다