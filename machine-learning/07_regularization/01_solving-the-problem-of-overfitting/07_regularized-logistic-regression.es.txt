Para la regresión logística, hablamos anteriormente de dos tipos de algoritmos de optimización. Hablamos sobre cómo usar gradiente de descenso para optimizar como función de costo J de «theta». Y también hablamos sobre métodos avanzados de optimización. Unas que requieren que tú proporciones una manera de calcular tu función de costo J de «theta», y que proporciones una forma de calcular las derivadas. En este video, te mostraré cómo puedes adaptar ambas técnicas, gradiente de descenso y las técnicas de optimización más avanzadas para ponerlas a trabajar para la regresión logística regularizada. Entonces, esta es la idea. Vimos anteriormente que la regresión logística también es propensa a ser sobreajustada, si la ajustas con variables polinomiales de alto orden como éstas. En donde G es la función sigmoidea y, particularmente, terminas con una hipótesis, ya sabes, cuyo límite de decisión es una especie de función demasiado compleja y extremadamente contortiva que realmente no es una hipótesis tan buena para este conjunto de capacitación y, más generalmente, si tienes una regresión logística con muchas variables. No necesariamente polinomiales, pero con una gran cantidad de variables, puedes terminar con un sobreajuste. Esta fue nuestra función de costo para la regresión logística. y si queremos modificarla para usar regularización, todo lo que tenemos que hacer es sumarle el siguiente término lambda positiva sobre 2m, sumatoria desde J es igual a 1 y, como siempre, la suma de J es igual a 1. En lugar de la suma de J igual a 0, de «theta» J al cuadrado. Y esto tiene el efecto de penalizar los parámetros «theta» 1 «theta» 2 y así sucesivamente hasta «theta» N para evitar que sea demasiado grande. Y si haces esto, entonces tendrá el efecto de que, a pesar de que estás ajustando un polinomio de orden muy elevado con muchos parámetros, siempre que apliques regularización y mantengas los parámetros pequeños, es más probable que obtengas un límite de decisión. Ya sabes, que tal vez se parezca más a esto. Se ve más razonable para separar los ejemplos positivos y negativos. Entonces, al usar regularización incluso cuando tienes muchas variables, la regularización puede ayudar a resolver el problema del sobreajuste. ¿Cómo implementamos esto? Bien, para el algoritmo original del gradiente de descenso, esta era la actualización que teníamos. Haremos repetidamente la siguiente actualización a «theta» J. Este lado se ve muy parecido al anterior para la regresión lineal. Pero lo que voy a hacer es escribir la actualización para «theta» 0 por separado. Entonces, la primera línea es la actualización para «theta» 0 y una segunda línea es ahora mi actualización para «theta» 1 hasta «theta» N, porque yo voy a tratar «theta» 0 por separado. Y con el fin de modificar este algoritmo para usar una función de costo regularizada, todo lo que necesito hacer es muy parecido a lo que hicimos para la regresión lineal que, de hecho, sólo es modificar esta segunda regla de actualización de la siguiente manera. Y, nuevamente, esto, se ve cosméticamente igual a lo que teníamos para la regresión lineal. Pero, desde luego, no es el mismo algoritmo que teníamos, porque ahora la hipótesis se define usando esto. Entonces, no es el mismo algoritmo que el de la regresión lineal regularizada. Debido a que la hipótesis es diferente, incluso a pesar de que la actualización que escribí, de hecho, se ve cosméticamente igual a la que teníamos anteriormente. Estamos trabajando la gradiente de descenso para la regresión lineal regularizada. Y, desde luego, sólo para resumir la discusión, este término aquí entre corchetes, este término es, desde luego, la nueva derivada parcial con respecto a «theta» J de la nueva función de costo J de «theta». Donde J de «theta» aquí es la función de costo que definimos en una diapositiva anterior y que usa la regularización. Entonces, este es el gradiente de descenso para la regresión lineal regularizada. Hablemos sobre cómo hacer que trabaje la regresión lineal usando los más avanzados métodos avanzados de optimización. Y sólo para recordarte, lo que necesitábamos hacer para estos métodos era definir la función denominada función de costo, lo que requiere que introduzcamos el vector del parámetro «theta» y, nuevamente, en las ecuaciones que hemos estado escribiendo aquí hemos usado vectores con índice 0. Así que teníamos «theta» 0 hasta «theta» N. Pero, porque Octave indexa los vectores a partir de1. «theta» 0 se escribe como «theta» 1 en Octave. «theta» 1 se escribe en Octave como «theta» 2, y así hasta «theta» N más 1. Y lo que necesitábamos hacer era proporcionar una función. Vamos a proporcionar una función llamada función de costo que después haremos pasar por lo que tenemos, lo que hemos visto antes. Vamos a utilizar el fminunc y luego, ya sabes, en la función de costo, y así sucesivamente. Pero el fminunc fue el f min sin restricciones y esto trabajará con fminunc, lo que tomará la función de costo y la minimizará para nosotros. Así que, las dos cosas principales que la función de costo necesitaba para regresar, primero era J-val. Y, para esto, tenemos que escribir el código para calcular la función de costo J de «theta». Ahora, cuando usamos la regresión logística regularizada, desde luego, la función de costo j de «theta» cambia y, particularmente, ahora la función de costo debe incluir este término de regularización adicional al final también. Entonces, cuando calcules j de «theta», asegúrate de incluir ese término al final. Y después, la otra cosa que esta función de costo necesita para derivar con un gradiente. Entonces, la gradiente uno debe establecerse como la derivada parcial de J de «theta» con respecto a «theta» cero; la gradiente dos debe establecerse como eso, y así sucesivamente. Una vez más, el índice está desplazado por uno. Así es, debido a la indexación desde uno que utiliza Octave. Y, viendo estos términos. Este término aquí. En realidad trabajamos esto en una diapositiva anterior y, en realidad, es igual a éste. No cambia. Debido a que la derivada para «theta» cero no cambia. En comparación con la versión sin regularización. Y los otros términos sí cambian. Y, en particular, la derivada con respecto a «theta» uno. Esto también lo trabajamos en una diapositiva anterior. Es igual a, ya sabes, el término original y después menos lambda M veces «theta» 1. Sólo para asegurarnos de que hemos pasado esto correctamente. Y podemos añadir paréntesis aquí. Bien, así la sumatoria no se extiende. Y, del mismo modo, ya sabe, este otro término aquí se ve como éste, con este término adicional que teníamos en la diapositiva anterior, que corresponde a la gradiente de su objetivo de regularización. Entonces, si implementas esta función de costo y pasas esto a fminunc o a una de las técnicas de optimización avanzadas, se minimizará la nueva función de costo regularizada J de «theta». Y los parámetros que obtendrás serán los que correspondan a la regresión logística con regularización. Así es que ahora sabes cómo implementar la regresión logística regularizada. Cuando me paseo por Silicon Valley, Yo vivo aquí en Silicon Valley, hay muchos ingenieros que, francamente, están ganando un montón de dinero para sus compañías usando algoritmos de aprendizaje automático. Y, sé que sólo hemos, ya sabes, estudiado esto por un corto tiempo. Pero si comprendes la regresión lineal, los algoritmos avanzados de optimización y la regularización, por ahora, francamente, probablemente sabes mucho más sobre aprendizaje automático de lo que muchos saben ahora, pero probablemente sabes bastante más sobre aprendizaje automático ahora que, francamente, muchos de los Los ingenieros de Silicon Valley que tienen carreras muy exitosas. Ya sabes, que están ganando toneladas de dinero para las compañías. O construyendo productos usando algoritmos de aprendizaje automático. Así que, felicitaciones. Realmente has llegado muy lejos. Y, de hecho, ahora sabes suficiente para aplicar estas cosas y ponerte a trabajar para resolver muchos problemas. Así que, felicitaciones por eso. Pero, desde luego, hay mucho más que queremos enseñarte y, en la siguiente serie de videos, comenzaremos a hablar sobre un clasificador no lineal muy poderoso. Entonces, con la regresión lineal y la regresión logística, ya sabes, puedes formar términos polinomiales, pero resulta que hay cuantificadores no lineales mucho más potentes que pueden clasificar la regresión polinomial. Y en la siguiente serie de videos, voy empezar a hablarte sobre éstos. Para que tengas algoritmos de aprendizaje aún más poderosos de los que tienes ahora para aplicarlos a diferentes problemas.