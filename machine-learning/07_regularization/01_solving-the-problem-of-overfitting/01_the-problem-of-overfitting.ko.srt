1
00:00:00,360 --> 00:00:01,753
여기까지 왔다면

2
00:00:01,760 --> 00:00:04,097
이제 몇 가지 종류의 학습 알고리즘을 배웠을 겁니다

3
00:00:04,097 --> 00:00:06,504
로지스틱 회귀(logistic regression)과<br />선형 회귀(linear regression) 말이죠

4
00:00:06,510 --> 00:00:08,583
이 알고리즘들은

5
00:00:08,583 --> 00:00:09,684
많은 머신러닝 문제들에

6
00:00:09,684 --> 00:00:11,903
적용시킬 수 있습니다만

7
00:00:11,903 --> 00:00:13,889
과적합(overfitting)이라는 문제에 빠져

8
00:00:13,900 --> 00:00:18,052
알고리즘의 성능이 잘 안나올 수 있습니다

9
00:00:18,052 --> 00:00:18,866
이번 영상에서는

10
00:00:18,866 --> 00:00:20,393
과적합(overfitting)이

11
00:00:20,393 --> 00:00:22,400
무엇인지 설명하고

12
00:00:22,400 --> 00:00:24,083
이후로 몇 개의

13
00:00:24,083 --> 00:00:25,861
영상에서는

14
00:00:25,861 --> 00:00:27,759
정규화(regularization)라는

15
00:00:27,760 --> 00:00:29,787
기술에 대해서 얘기할건데

16
00:00:29,787 --> 00:00:31,529
이 기술은 과적합 문제를

17
00:00:31,529 --> 00:00:33,607
개선하여 학습 알고리즘이

18
00:00:33,607 --> 00:00:36,844
더욱 좋은 성능을 낼 수 있도록 합니다

19
00:00:36,860 --> 00:00:39,607
자 그렇다면 과적합(overfitting)이란 뭘까요?

20
00:00:39,607 --> 00:00:41,616
우리가 계속 사용하던

21
00:00:41,620 --> 00:00:44,030
집값 예측 예제를 봅시다

22
00:00:44,050 --> 00:00:46,146
선형 회귀(linear regression)으로

23
00:00:46,146 --> 00:00:47,123
집의 크기로부터 집값을

24
00:00:47,123 --> 00:00:50,730
예측하는 문제였죠

25
00:00:50,730 --> 00:00:51,870
이 문제를 풀 수 있는

26
00:00:51,910 --> 00:00:53,620
방법으로는 선형 함수(1차 함수, 직선)를

27
00:00:53,620 --> 00:00:54,892
데이터에 맞게끔 하는 것이고

28
00:00:54,892 --> 00:00:56,296
이 문제를 해결하고나면

29
00:00:56,296 --> 00:00:58,913
데이터를 가로지르는 선을 볼 수 있겠죠

30
00:00:58,913 --> 00:01:01,012
그러나 이 모델은 별로 좋지 못한 것 같습니다

31
00:01:01,012 --> 00:01:02,543
데이터를 보면

32
00:01:02,560 --> 00:01:04,100
집값이 커지면서

33
00:01:04,100 --> 00:01:06,274
가격이 올라가는게 보이지만

34
00:01:06,274 --> 00:01:08,268
집의 사이즈가 커질수록

35
00:01:08,270 --> 00:01:11,721
집값이 천천히 올라 결국 평탄화 되죠

36
00:01:11,740 --> 00:01:14,020
그러다 보니 우리의 알고리즘은

37
00:01:14,020 --> 00:01:15,898
이 데이터에 잘 들어맞지 않습니다

38
00:01:15,898 --> 00:01:19,166
이런 문제를 과소적합(underfitting)이라고 하고

39
00:01:19,180 --> 00:01:20,494
다른 말로는

40
00:01:20,500 --> 00:01:24,666
알고리즘이 high bias(높은 편향)을 갖고 있다고 합니다

41
00:01:25,140 --> 00:01:26,841
이 2가지 용어는 모두 대략적으로

42
00:01:26,890 --> 00:01:30,760
우리의 모델이 데이터에 제대로<br />맞지 않는다는 것을 의미합니다

43
00:01:30,760 --> 00:01:32,328
이 용어는

44
00:01:32,328 --> 00:01:34,515
역사적으로 혹은 기술적으로 정해진 용어입니다

45
00:01:34,515 --> 00:01:36,109
그러나 둘다 표현하고자 하는 아이디어는

46
00:01:36,110 --> 00:01:37,303
직선을 데이터에 맞추는 경우

47
00:01:37,303 --> 00:01:38,909
이런 경우에 우리는

48
00:01:38,920 --> 00:01:40,290
알고리즘이

49
00:01:40,330 --> 00:01:42,638
강한 선입견(preconception)을 갖고 있다

50
00:01:42,638 --> 00:01:44,633
혹은 강한 편향(bias)를 갖고 있다라고

51
00:01:44,650 --> 00:01:46,339
얘기하고 집 값이 집 크기에 따라서

52
00:01:46,339 --> 00:01:49,988
데이터의 분포와 상관없이 선형적으로<br />변화할 것이라는 것입니다

53
00:01:50,000 --> 00:01:51,281
데이터가 예측에 반대하는

54
00:01:51,290 --> 00:01:54,174
증거를 보여줌에도 불구하고

55
00:01:54,174 --> 00:01:55,413
데이터를 직선에 맞추려다보니

56
00:01:55,440 --> 00:01:56,974
편향(bias) 혹은 선입견(preconception)이 생기고

57
00:01:56,974 --> 00:02:00,638
따라서 형편없는 결과가 나옵니다

58
00:02:00,638 --> 00:02:02,173
그러면 여기 가운데 처럼

59
00:02:02,210 --> 00:02:04,626
2차 함수를 끼워넣을 수도 있겠죠

60
00:02:04,626 --> 00:02:06,222
이런 데이터에 2차 함수를 넣으면

61
00:02:06,222 --> 00:02:07,793
이렇 모양의

62
00:02:07,810 --> 00:02:10,211
커브를 얻을 수 있겠죠

63
00:02:10,211 --> 00:02:14,361
그리고 이건 꽤 잘 맞습니다

64
00:02:14,361 --> 00:02:17,543
그리고 마지막으로 좀 극단적인 예제를 보면<br />4차 함수를 데이터에 맞추고 있습니다

65
00:02:17,550 --> 00:02:19,442
그래서 여기는 세타0 부터 세타5까지

66
00:02:19,470 --> 00:02:23,196
5개의 파라미터를 이용해

67
00:02:23,210 --> 00:02:23,926
5개의 데이터 모두를

68
00:02:23,926 --> 00:02:26,727
선을 이용해 맞출 수 있습니다

69
00:02:26,727 --> 00:02:29,507
그러면 이런 선을 얻을 수 있겠죠

70
00:02:31,260 --> 00:02:32,454
한편으로 보면

71
00:02:32,460 --> 00:02:33,791
이 선은 데이터에 잘 맞으니

72
00:02:33,791 --> 00:02:35,052
적어도 훈련용

73
00:02:35,052 --> 00:02:36,291
데이터에 한해서는

74
00:02:36,291 --> 00:02:38,269
꽤 좋은 성능을 낸다고 볼 수 있죠

75
00:02:38,270 --> 00:02:40,284
근데 보면 엄청나게 꼬아져 있는 선이죠?

76
00:02:40,300 --> 00:02:41,660
이 선을 보고

77
00:02:41,660 --> 00:02:43,430
우리는 이 선이 제대로 된

78
00:02:43,430 --> 00:02:46,996
집 값을 예측할거라고 생각되지 않습니다

79
00:02:47,000 --> 00:02:48,924
이런 문제를 보고

80
00:02:48,924 --> 00:02:51,967
과적합(overfitting)이라고 합니다

81
00:02:51,970 --> 00:02:53,165
그리고 이걸 다른 말로

82
00:02:53,170 --> 00:02:57,304
알고리즘이 high variance(높은 분산)을<br />갖고 있다고 합니다

83
00:02:57,890 --> 00:02:59,951
high variance도 역사적 혹은 기술적으로

84
00:02:59,951 --> 00:03:02,110
탄생한 용어입니다

85
00:03:02,130 --> 00:03:03,797
어쨋건 직관적으로

86
00:03:03,800 --> 00:03:05,080
우리는 어떤 고차 함수를

87
00:03:05,080 --> 00:03:07,326
데이터에 맞추려고 하고

88
00:03:07,330 --> 00:03:08,603
사실 완벽히

89
00:03:08,620 --> 00:03:09,584
맞는 함수를

90
00:03:09,584 --> 00:03:11,995
만들고자 하면

91
00:03:11,995 --> 00:03:14,159
만들 수 있지만

92
00:03:14,159 --> 00:03:16,601
그렇게 되면 커다란 변동성을 갖게 되죠

93
00:03:16,610 --> 00:03:18,052
그리고 우리는 저런

94
00:03:18,052 --> 00:03:19,279
변동성을 막아줄 엄청난 데이터를 가지고 있지 않죠

95
00:03:19,279 --> 00:03:22,714
이런 상황을 보고 과적합(overfitting)이라고 합니다

96
00:03:22,740 --> 00:03:24,340
여기 가운데 같은 경우는

97
00:03:24,350 --> 00:03:26,990
어떤 이름이 따로 있는 것은 아니고<br />그냥 잘 맞는다라고 표현합니다

98
00:03:26,990 --> 00:03:29,911
2차 함수가 데이터에

99
00:03:29,911 --> 00:03:32,559
딱 잘 맞고 있죠

100
00:03:32,559 --> 00:03:34,684
정리를 좀 하자면

101
00:03:34,690 --> 00:03:37,042
과적합 문제는

102
00:03:37,042 --> 00:03:38,258
많은 특성(feature)들이

103
00:03:38,258 --> 00:03:40,729
존재할 때

104
00:03:40,729 --> 00:03:43,881
우리의 가설이 학습용 데이터에만 잘 맞아서

105
00:03:43,881 --> 00:03:46,023
학습용 데이터를 대상으로는

106
00:03:46,023 --> 00:03:47,344
비용함수가

107
00:03:47,344 --> 00:03:48,446
거의 0에 가까운 값을

108
00:03:48,446 --> 00:03:50,750
혹은 0이 나오고

109
00:03:50,750 --> 00:03:52,063
대신에

110
00:03:52,063 --> 00:03:53,950
엄청 복잡한 커브로

111
00:03:53,950 --> 00:03:55,314
학습 데이터에

112
00:03:55,314 --> 00:03:57,103
맞춰질 것이고

113
00:03:57,110 --> 00:03:59,233
학습 데이터를 제외한

114
00:03:59,250 --> 00:04:01,117
다른 새로운 예제가 들어올 경우

115
00:04:01,120 --> 00:04:03,018
새로운 예제에 대한 일반화(generalized)된

116
00:04:03,050 --> 00:04:04,337
예측은 제대로 하지 못하는 경우가 발생합니다

117
00:04:04,350 --> 00:04:06,853
일반화(generalized)라는 용어는

118
00:04:06,853 --> 00:04:10,868
가설이 새로운 데이터에도 얼마나<br />잘 맞냐라고 생각하면 됩니다

119
00:04:10,868 --> 00:04:12,274
여기서 새로운 데이터는라는 것은

120
00:04:12,320 --> 00:04:16,467
학습용 데이터에 없던<br />집 크기에 대한 집 가격 정보겠죠

121
00:04:16,600 --> 00:04:17,910
우리는 앞의 슬라이드에서

122
00:04:17,910 --> 00:04:20,802
선형 회귀에 대한 과적합 케이스를 봤는데요

123
00:04:20,810 --> 00:04:24,182
이것은 로지스틱 회귀에도 마찬가지입니다

124
00:04:24,190 --> 00:04:26,090
여기 특성 x1, x2에 대한

125
00:04:26,090 --> 00:04:28,871
로지스틱 회귀 예제가 있습니다

126
00:04:28,910 --> 00:04:30,136
한가지 방법으로는

127
00:04:30,140 --> 00:04:31,522
이렇게 간단한 가설(hypothesis)를

128
00:04:31,522 --> 00:04:34,518
로지스틱 회귀에 사용할 수 있습니다

129
00:04:34,530 --> 00:04:38,076
여기 g는 시그모이드 함수입니다

130
00:04:38,120 --> 00:04:39,334
이런 가설을 가지고

131
00:04:39,334 --> 00:04:41,593
로지스틱 회귀를 하면

132
00:04:41,600 --> 00:04:42,923
이런 간단한 선으로

133
00:04:42,923 --> 00:04:45,713
양성(positive)과 음성(negative)를 나누겠죠

134
00:04:45,713 --> 00:04:49,071
그러나 이건 그다지 잘 맞아 떨어지지 않는거 같습니다

135
00:04:49,100 --> 00:04:50,659
다시 말하자면

136
00:04:50,659 --> 00:04:52,577
이런 케이스는 과소적합(underfitting) 케이스입니다

137
00:04:52,577 --> 00:04:56,040
혹은 high bias(높은 편향)을 가지고 있다고 하죠

138
00:04:56,210 --> 00:04:57,504
좀 더 나아가서

139
00:04:57,504 --> 00:04:59,146
좀 더 고차의

140
00:04:59,170 --> 00:05:01,032
특성들을 가설에 추가적으로 이용하면

141
00:05:01,032 --> 00:05:02,613
이런 decision boundary를

142
00:05:02,613 --> 00:05:05,620
얻을 수 있겠죠

143
00:05:05,620 --> 00:05:07,784
보다시피 좀더 데이터에 잘 맞는걸 볼 수 있죠

144
00:05:07,784 --> 00:05:10,838
아마도 이 선이

145
00:05:10,860 --> 00:05:13,991
학습용 데이터로부터 얻을 수 있는 최적일거 같네요

146
00:05:14,010 --> 00:05:15,157
그리고 마지막으로

147
00:05:15,170 --> 00:05:16,169
극단적인 예제가 있죠

148
00:05:16,169 --> 00:05:18,207
엄청 나게 많은 고차항을

149
00:05:18,207 --> 00:05:20,036
생성하여

150
00:05:20,036 --> 00:05:22,461
엄청 높은 차수의 다항식을 만들면

151
00:05:22,490 --> 00:05:24,730
로지스틱 회귀는

152
00:05:24,750 --> 00:05:26,551
데이터에 맞는

153
00:05:26,560 --> 00:05:28,233
엄청 자잘자잘하게 꼬아진

154
00:05:28,233 --> 00:05:31,742
decision boundary를

155
00:05:31,742 --> 00:05:33,013
구해내게 될 것입니다

156
00:05:33,030 --> 00:05:35,006
자신의 예제를 모두 맞추기

157
00:05:35,006 --> 00:05:37,689
위해 엄청 길게 꼬여있겠죠

158
00:05:37,700 --> 00:05:38,757
만약에

159
00:05:38,757 --> 00:05:39,547
특성 x1과

160
00:05:39,550 --> 00:05:41,435
x2가

161
00:05:41,435 --> 00:05:43,350
암이 악성인지

162
00:05:43,390 --> 00:05:46,448
아닌지를 판단하는데 쓰인다고 하면

163
00:05:46,448 --> 00:05:47,988
이 가설은

164
00:05:47,988 --> 00:05:51,893
정말 좋지 못한 예측을 하는 것이죠

165
00:05:51,930 --> 00:05:53,463
다시 말하지만

166
00:05:53,463 --> 00:05:55,432
이런 케이스를

167
00:05:55,432 --> 00:05:57,128
과적합이라고 하고

168
00:05:57,128 --> 00:05:59,403
high varicance(높은 분산)이라고도 합니다

169
00:05:59,403 --> 00:06:04,243
그리고 새로운 예제에 잘 일반화되지 않습니다

170
00:06:04,560 --> 00:06:06,158
이 코스의 뒤쪽에서는

171
00:06:06,158 --> 00:06:08,453
학습 알고리즘이

172
00:06:08,460 --> 00:06:09,794
과적합인지

173
00:06:09,810 --> 00:06:11,490
디버깅과 분석하는

174
00:06:11,490 --> 00:06:13,297
툴에 대해서

175
00:06:13,297 --> 00:06:14,953
알아볼 것이고

176
00:06:14,953 --> 00:06:17,503
마찬가지로 과소적합에 대해서도 할 것입니다

177
00:06:17,503 --> 00:06:18,775
일단

178
00:06:18,780 --> 00:06:20,342
과적합 케이스의

179
00:06:20,360 --> 00:06:22,206
문제가 있다고 가정하고

180
00:06:22,250 --> 00:06:24,864
어떻게 이게 과적합인지 알 수 있을까요?

181
00:06:24,864 --> 00:06:26,640
우리의 예제들은

182
00:06:26,660 --> 00:06:28,701
모두 1차원 혹은 2차원이었으므로

183
00:06:28,701 --> 00:06:31,335
그냥 그래프를 그려 무슨 일이 일어나는지

184
00:06:31,335 --> 00:06:34,612
보면서 적당한 차원의 다항식을 선택하면 됩니다

185
00:06:34,620 --> 00:06:36,836
앞에서 본 집 값 문제를 보면

186
00:06:36,836 --> 00:06:38,405
우리의 가설을

187
00:06:38,410 --> 00:06:40,597
바로 그래프로 그려볼 수 있죠

188
00:06:40,600 --> 00:06:41,628
그려면 다음과 같이

189
00:06:41,628 --> 00:06:42,830
적당히 꼬불꼬불한

190
00:06:42,830 --> 00:06:46,339
선이 우리의 데이터를 지나가겠죠

191
00:06:46,339 --> 00:06:47,701
우리는 이런 그래프를 보고

192
00:06:47,740 --> 00:06:50,667
적당한 수준의 다항식을 선택할 수 있죠

193
00:06:50,680 --> 00:06:54,166
따라서 가설을

194
00:06:54,166 --> 00:06:55,728
그래프로 표현하여

195
00:06:55,750 --> 00:06:58,160
어떤 다항식을 사용하는지 보는 것은 한가지 방법이 되겠죠

196
00:06:58,160 --> 00:07:00,163
하지만 이 방법은 항상 동작하지 않습니다

197
00:07:00,180 --> 00:07:02,019
다항식의 차수가 높은 문제가

198
00:07:02,019 --> 00:07:06,075
아니더라도 엄청나게 많은

199
00:07:06,075 --> 00:07:07,563
특성이 있는 경우에는

200
00:07:07,563 --> 00:07:10,599
다항식의 차수를 선택할 여지조차 없죠

201
00:07:10,630 --> 00:07:12,147
게다가 특성이

202
00:07:12,170 --> 00:07:13,779
많아질수록

203
00:07:13,779 --> 00:07:15,593
그래프를 통해

204
00:07:15,630 --> 00:07:17,698
시각적으로 표현하기가

205
00:07:17,710 --> 00:07:19,211
더욱 어려워집니다

206
00:07:19,211 --> 00:07:22,396
그래서 어떤 특성을 남겨야 할지 선택하기 어렵죠

207
00:07:22,420 --> 00:07:24,142
구체적으로 얘기하면

208
00:07:24,160 --> 00:07:27,849
집 값을 예측할건데 특성이 엄청 많은 경우입니다

209
00:07:27,880 --> 00:07:31,373
게다가 특성 하나하나가 예측에 도움이 될 것 같은 경우죠

210
00:07:31,373 --> 00:07:32,609
이렇게 특성이 많은데

211
00:07:32,609 --> 00:07:34,123
학습 데이터가 적은 경우

212
00:07:34,123 --> 00:07:35,820
과적합 문제가

213
00:07:35,840 --> 00:07:37,776
발생할 수 있습니다

214
00:07:37,776 --> 00:07:39,180
과적합 문제를

215
00:07:39,180 --> 00:07:40,651
해결하기 위해서는

216
00:07:40,651 --> 00:07:43,780
크게 2가지 해결법이 있습니다

217
00:07:43,780 --> 00:07:45,759
첫번째로는 특성의 갯수를

218
00:07:45,770 --> 00:07:47,976
줄이는 방법입니다

219
00:07:47,990 --> 00:07:49,337
구체적으로 얘기하면

220
00:07:49,337 --> 00:07:51,383
특성들을 쭉 보고

221
00:07:51,383 --> 00:07:53,236
이거는 쓸만해

222
00:07:53,236 --> 00:07:54,894
이거는 안쓸만해

223
00:07:54,894 --> 00:07:57,256
해서 어떤 특성은

224
00:07:57,256 --> 00:07:58,476
남기고 어떤 특성은

225
00:07:58,476 --> 00:08:01,844
버리는 식으로 제거할 수 있겠죠

226
00:08:01,844 --> 00:08:03,401
이 코스의 뒤쪽에서는

227
00:08:03,401 --> 00:08:06,018
모델 선택 알고리즘을 배울겁니다

228
00:08:06,040 --> 00:08:08,361
알고리즘이 어떤 특성을

229
00:08:08,361 --> 00:08:09,788
사용할 건지 자동으로

230
00:08:09,800 --> 00:08:12,500
선택하거나 버릴지 선택하지요

231
00:08:12,500 --> 00:08:13,987
여튼 특성의 수를 줄이는 것이

232
00:08:13,987 --> 00:08:15,562
과적합 문제에 해결책으로

233
00:08:15,562 --> 00:08:17,853
이용될 수 있습니다

234
00:08:17,853 --> 00:08:19,383
모델 선택 알고리즘을 애기하면서

235
00:08:19,383 --> 00:08:22,534
여기에 대해 좀더 자세히 얘기하도록 하겠습니다

236
00:08:22,534 --> 00:08:24,386
그러나 단점으로는

237
00:08:24,386 --> 00:08:25,603
몇 가지 특성을 버림으로 인해

238
00:08:25,603 --> 00:08:27,010
문제에 포함된 정보를

239
00:08:27,370 --> 00:08:30,615
같이 버리게 된다는 것입니다

240
00:08:30,650 --> 00:08:31,942
예를 들어

241
00:08:31,942 --> 00:08:33,760
모든 특성들이

242
00:08:33,780 --> 00:08:35,050
집 값을 예측하는데

243
00:08:35,070 --> 00:08:36,636
도움이 되는경우

244
00:08:36,640 --> 00:08:37,687
우리는 그 어떤 정보도

245
00:08:37,687 --> 00:08:40,990
버리고 싶지 않을겁니다

246
00:08:41,540 --> 00:08:44,515
두번째 해결법으로는

247
00:08:44,515 --> 00:08:45,995
정규화(regularization)이 있습니다

248
00:08:46,010 --> 00:08:49,268
뒤의 영상들에서 배울겁니다

249
00:08:49,268 --> 00:08:50,390
모든 특성들을 남기되

250
00:08:50,390 --> 00:08:52,579
각각의 특성이 갖는

251
00:08:52,579 --> 00:08:55,063
영향 규모를 줄이는 겁니다

252
00:08:55,063 --> 00:08:56,506
다르게 얘기하면 세타값이 미치는 영향을 줄이는 거죠

253
00:08:56,520 --> 00:08:58,745
이 방법은 과적합 문제를

254
00:08:58,750 --> 00:09:00,690
잘 해결해줍니다

255
00:09:00,690 --> 00:09:01,925
엄청 많은 특성들이 있을 때

256
00:09:01,925 --> 00:09:03,822
그리고 각 특성들이 예측에

257
00:09:03,822 --> 00:09:05,502
엄청 작은 영향을

258
00:09:05,502 --> 00:09:07,723
미치는 경우의 문제가 있다고 가정해봅시다

259
00:09:07,740 --> 00:09:10,283
앞서 얘기했던 집값 예측 같은거 말이죠

260
00:09:10,283 --> 00:09:11,413
엄청 많은 특성이 있고

261
00:09:11,413 --> 00:09:12,720
각각의 특성이 예측값에 영향을 주죠

262
00:09:12,750 --> 00:09:16,902
그래서 특성을 버리기 싫은겁니다

263
00:09:16,930 --> 00:09:19,247
이로써 정규화에 대한

264
00:09:19,250 --> 00:09:22,790
큰 그림을 설명드렸습니다

265
00:09:22,790 --> 00:09:24,354
아직은 어떤 의미인지

266
00:09:24,360 --> 00:09:26,763
잘 와닿지 않을 수 있습니다

267
00:09:26,763 --> 00:09:28,316
그러나 다음 영상에서

268
00:09:28,316 --> 00:09:30,960
수학적으로 정규화가 어떻게 동작하는지

269
00:09:30,960 --> 00:09:35,117
또 어떻게 적용하는지 알아보겠습니다

270
00:09:35,140 --> 00:09:36,810
그 다음에 정규화를

271
00:09:36,810 --> 00:09:38,310
어떻게 사용해야

272
00:09:38,310 --> 00:09:40,412
우리의 학습 알고리즘이 좋아지고

273
00:09:40,412 --> 00:09:42,460
과적합을 피할 수 있는지 배우겠습니다