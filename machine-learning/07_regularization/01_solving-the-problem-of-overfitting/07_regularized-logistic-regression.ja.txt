ロジスティック回帰については 以前2つの種類の最適化アルゴリズムについて議論した。 1つ目は最急降下法を使って コスト関数Jのシータを最適化する方法。 2つ目として、アドバンスドな最適化関数を使う 手法についても議論した。 それはあなたが コスト関数Jのシータの 計算方法と、 偏微分の計算方法を提供する必要がある物だった。 このビデオでは、これらの どちらのテクニックにも 適用する方法、つまり最急降下法と よりアドバンスドな最適化技法の両方で それらを正規化したロジスティック回帰で 使えるようにする方法をお見せしたい。 さて、そのアイデアはこんなだ。 前にロジスティック回帰は とても高い次数の このような多項式を含む場合は オーバーフィットしがち だという事を見た。 ここでgは sigmoid関数だ。 つまり、最終的に とても、過度に複雑な まったく直感に反するような 決定境界の仮説になるような物の場合には、という事。 つまりはそんな仮説は このトレーニングセットに対して そんなに良い仮説とは 思えない。 より一般的に言うと、 たくさんのフィーチャーのロジスティック回帰の時には、 そのフィーチャーは必ずしも多項式で無くても、 たくさんフィーチャーがある時には ロジスティック回帰はオーバーフィットしがちだ。 これが我らのロジスティック回帰のコスト関数だ。 そしてそれを正規化を用いるように 修正したければ、 我らがやらなくてはいけない事は そこに以下の項を追加するだけだ、 + ラムダ/2m の 和を取る事のj=1から、、、 ここでいつも通り、和はj=1から取る、 j=0からじゃなくて。 で、シータjの二乗 そしてこれはつまり シータ1, シータ2, ...とシータnまでが 大きすぎる場合に ペナルティを課す、という事を意味する。 これを行えば、 とても高次の 多項式でたくさんのパラメータがあるような物に フィッティングしたとしても、 正規化を適用しておけば、 それによってパラメータを小さいままに保っておけば、 より、こんな感じの決定境界を 得られる可能性が高まる。 こちらの方が、陽性と陰性の手本を分離するには よりリーズナブルと言えよう。 つまり、正規化を使えば、 たくさんのフィーチャーがある時でも、 正規化がオーバーフィッティングの問題を 対処してくれる。 実際にはどうやって実装したらいいか？ もともとの最急降下法のアルゴリズムでは、 これが我らの得たアップデートだった。 我らは繰り返し、以下のアップデートを シータjに対してほどこすのだった。 このスライドは前回の線形回帰についてのスライドと、多くの点で似ている。 とにかく、私がやる事は、まずシータ0のアップデートを 別個に書く。 こうして、最初の行がシータ0の アップデートで、 二行目が、いまや シータ1からシータnまでの アップデートとなった。 何故ならシータ0は別扱いだから。 そしてこのアルゴリズムを 修正して 正規化したコスト関数を用いるようにする為には、 私が行わなくてはいけない事は 線形回帰の時と 極めて似ている。それは この二番目のアップデートルールを 以下のように変更するだけだ。 そしてまた、見た目でも 線形回帰の頃にあった物と 同一に見える。 だがもちろん、以前のアルゴリズムと 同じアルゴリズムという訳では無い。 何故なら今や、仮説は これを用いて定義されているから。 だからこれは、正規化された線形回帰と 同じアルゴリズムという訳では無い。 何故なら仮説が違うから。 たしかにここに書きだしたアップデートは 表面上は以前に得た物と まったく同一ではあるが。 正規化した線形回帰の時に導出した物と。 そしてもちろん、、、 この議論をまとめると、 この大カッコにくくられた この項、つまりこのここの項は、 この項は、 もちろん、新しいコスト関数Jのシータの シータjでの偏微分という 新しい偏微分項だ。 ここで、このJのシータは 前のスライドで定義した、正規化項ありの コスト関数だ。 以上が正規化した線形回帰だ（訳注：ロジスティック回帰の間違いと思われる、以下同様） ここからは、正規化した線形回帰を アドバンスドな最適化関数と どう使っていくかを 議論していこう。 ちょっとこれらの関数を 思い出しておく為に触れておくと、 これらの関数の為に我らがしなくてはいけない事は、 costFuncitonという関数を定義する事だった、 それは入力に パラメータベクトルのシータを受け取り、 ここで今回も、この等式では 0インデックスのベクトルとして書いた。 だからシータ0から シータnまである。 だがOctaveはベクトルのインデックスを1から始めるから、 シータ0はOctave上では theta1と書く。 シータ1はOctave上では theta2と書く。 そんな風にtheta(n+1)まで 降りていく。 そして我らがやるべき事は、 以下のような関数を提供する事。 costFunctionという関数を 提供する事としよう、それを 以前に見た奴に渡す。 fminuncを使って、 それに対し、引数を、 アットマークにコスト関数。 などとする。 fminuncは fminのunconstrained（制約無し） だった。そしてこの fminuncは、 最小化する対象を受け取って 我らの為に最小化してくれる。 コスト関数が返さなくてはいけない 物は主に二つ、 最初はjVal。 それの為には コスト関数Jのシータを計算するコードを 書かなくてはならない。 ここでは正規化したロジスティック回帰を使っているのだから、 当然コスト関数Jのシータも 前とは変わっている。 具体的には、 今回はコスト関数には、 末尾に追加の正規化項を含む必要がある。 だから、Jのシータを計算する時に 最後に項を追加する事を忘れないでくれ。 そしてその次に このコスト関数が提供しなくてはいけない物は gradientだ。 gradient1には、 Jのシータの シータ0による偏微分を セットする。 gradient2には これをセットする。などなど。 ここでも、インデックスは1ずれている。 何故ならOctaveは1からのインデックスを 使うから。 そしてこれらの項を見ると、 ここのこの項は 前回のスライドで計算した物と 同じで、これに等しい。 変わってない。 何故ならシータ0による微分は前と同じだから。 正規化をしてなかったバージョンと。 そしてそれ以外の項は、変わる。 具体的には、シータ1に関する微分は、 前回のスライドでやったのと同様で、 イコール、 元の項に、そこからマイナスの ラムダ/m 掛ける シータ1。 これをちゃんと渡している事をしっかり確認してくれ。 そしてここにカッコをつけられる、 和を取るのがここまで行ってしまわないように。 同様に、 この他の項も、これも こんな感じで、この追加の項があり、 これは前のスライドの物と同じだ、 同様で、これは正規化の目的関数の 微分から来ている。 さて、このcostFunctionを 実装して、これを このfminuncなり それ以外のアドバンスドな最適化技法の一つなりに渡せば、 それがこの新しく作った正規化したコスト関数、Jのシータを 最小化してくれる事になる。 そして得られるパラメータは、 ロジスティック回帰に正規化を含めた物に 対応した物となる。 さて、ここまでで正規化されたロジスティック回帰を どう実装するのかを知った訳だ。 シリコンバレーを回っていると、 私はシリコンバレーに住んでいるのだが、 機械学習のアルゴリズムを用いて 会社に巨万の富をもたらしているエンジニアが たくさん居る。 そしてここまでで我らが学んで来た期間は まだちょっとしか経っていない訳だが、 だが線形回帰を理解し、ロジスティック回帰を理解し、 アドバンスドな最適化アルゴリズムを理解し、 正規化を理解した今、 率直にいってたぶん、 いまや、比較的、かなり多く 機械学習について知っている事になると思う、ほとんどのエンジニアより、というと言い過ぎだが、 だがたぶん、現時点まででも既に、 機械学習について多くのエンジニアよりも、もっと多くの事を知っている、 シリコンバレーでとても成功したキャリアを得ている エンジニア達の多くと比べて、 会社に巨万の富を生み出しているエンジニアたちや 機械学習のアルゴリズムを使ってプロダクトを作っているエンジニア達と比べて。 つまり、、、おめでとう！ すでに貴方は、随分遠くまで来たって事だ。 そしてあなたは実際に たくさんの問題にこれらの事を適用する為の 十分な知識を得ているのだ。 だからそれについて、おめでとう！と言いたい。 だがもちろん、まだ教えたい事は 他にもいろいろある。 そこでこれに続く 一連のビデオで とても強力な非線型の分類器の クラスについて、議論を開始したい。 線形回帰やロジスティック回帰にも すでに知っての通り、 多項式の項をいれこむ事が出来るが、 多項式の回帰よりもよりパワフルな 非線型の分類器がある事が 既に知られている。 そしてこの後の一連の動画で それらについて教えていきたい。 様々な問題に適用出来る 今の手持ちのアルゴリズム以上に強力な アルゴリズムを得る為に。