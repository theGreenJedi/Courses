Até agora, você viu alguns algoritmos de aprendizagem diferentes, regressão linear e regressão logística. Eles funcionam bem para muitos problemas, mas quando usados em a certas aplicações de aprendizado de máquina, eles podem esbarrar em um problema chamado sobreajuste, que pode levá-los a ter uma performance muito ruim. O que eu gostaria de fazer neste vídeo é explicar o que é esse problema de sobreajuste, e mais além falaremos sobre uma técnica chamada regularização, que nos permitirá melhorar ou reduzir o sobreajuste e fazer com que esses algoritmos funcionem talvez muito melhor. Então o que é sobreajuste? Vamos continuar usando nosso exemplo atual de predição de preços de casa com regressão linear em que queremos prever o preço em função do tamanho da casa. Uma coisa que poderíamos fazer é ajustar uma função linear a esses dados, e se fizermos isso, talvez obtenhamos uma linha reta dessa forma. Mas esse não é um modelo muito bom. Olhando para os dados, parece claro que conforme o tamanho da casa aumenta, os preços estabilizam, ou se achatam conforme vamos para a direita, então esse algoritmo não se ajusta ao treino e chamamos esse problema de sub-ajuste, e outro termo para isso é que esse algoritmo tem alto viés ("bias"). Ambos os termos significam que o algoritmo não está se ajustando muito bem aos dados. O termo é um tanto histórico ou técnico, mas a ideia é que se ajustarmos uma linha reta aos dados, é como se o algoritmo tivesse uma preconcepção muito forte, ou um viés muito forte de que preços de casas irão variar linearmente com seu tamanho apesar dos dados dizerem o contrário. Apesar da evidência do contrário, a preconcepção ainda é viesada, ainda leva a ajustar uma linha reta e acaba sendo um ajuste ruim. Agora, no meio, poderíamos ajustar funções quadráticas e, com esse conjunto de dados, se ajustarmos uma função quadrática, talvez obtenhamos esse tipo de curva, e funciona muito bem. E, no outro extremo, seria se ajustássemos, digamos, um polinômio de quarta ordem aos dados. Então, aqui temos cinco parâmetros, teta zero a teta quatro, e, com isso, podemos ajustar uma curva que passe por todos os cinco pontos de nossos exemplos de treinamento Você poderia obter uma curva como essa, que, por um lado parece fazer um ótimo trabalho ajustando o conjunto de treinamento, e pelo menos passa por todos os pontos. Mas, essa ainda é uma curva muito sinuosa, certo? Então, ela vai para cima e para baixo, e não parece ser um bom modelo para predição de preços de casas. Então, chamamos esse problema de sobreajuste, e, outro termo para isso é que esse algoritmo tem alta variância. O termo alta variância é outro termo histórico ou técnico. Mas a intuição diz que se estamos ajustando um polinômio de tão alta ordem, a hipótese pode se ajustar a quase qualquer função e o número de hipóteses possíveis é muito grande, muito variável. Não temos dados suficientes para levá-los a nos dar uma boa hipótese então isso é chamado de sobreajuste. E no meio, não há exatamente um nome, mas vou apenas escrever "certo". Onde uma função polinomial de segunda ordem, quadrática parece ser a correta para ajustar a esse dado. Para recapitular um pouco, o problema de sobreajuste acontece quando temos parâmetros demais, então a hipótese aprendida pode se ajustar ao conjunto de treino bem demais. Então, sua função de custo deve ser muito próxima de zero ou pode ser até mesmo exatamente zero, mas você acaba com uma curva como essa, que tenta se ajustar bem demais ao conjunto de treino, de forma que falha ao generalizar para novos exemplos e falha para predizer preços em novos exemplos também, e aqui o termo generalizar se refere a quão bem uma hipótese se aplica a novos exemplos, isto é, para dados sobre casas que não foram vistos no conjunto de treino. Nesse slide, vimos o sobreajuste para o caso de regressão linear. Algo similar pode ser aplicado também a regressão logística. Aqui está um exemplo de regressão logística com dois parâmetros X1 e X2. Algo que poderíamos fazer é ajustarmos regressão logística com uma hipótese simples como essa, onde, como sempre, G é uma função sigmóide. E se você fizer isso, acabará com uma hipótese, tentando usar apenas uma linha reta para separar os exemplos positivos e negativos. Isso não parece um bom ajuste para a hipótese. Então, novamente, esse é um exemplo de sub-ajuste ou de hipótese com alto viés ("bias"). Em contraste, se você fosse adicionar aos seus parâmetros esses termos quadráticos, então, você poderia obter uma superfície de decisão que se pareceria mais com isso. E, você sabe, esse é um ajuste muito bom aos dados. Provavelmente, o melhor que poderíamos ter nesse conjunto de treinamento. E, finalmente, no outro extremo, se você fosse ajustar um polinômio de alta ordem, se você fosse gerar muitos termos de polinômios de alta ordem, então, a regressão logística pode se contorcer, pode tentar muito fortemente encontrar uma superfície de contorno que se ajuste aos dados de treinamento, ou se estender por grandes distâncias e se contorcer para ajustar todos os pontos de exemplo bem. E, você sabe, se os parâmetros X1 e X2  predizem, digamos, câncer, como tumores de mama malignos ou benignos. Isso realmente não parece uma hipótese muito boa para fazer predições. Então, novamente, esse é um exemplo de sobreajuste e, de uma hipótese com alta variância e com pouca probabilidade de generalizar bem para novos exemplos. Mais tarde, neste curso, quando nós falarmos sobre depuração e diagnóstico de coisas que podem dar errado com algoritmos de aprendizagem, nós lhe daremos ferramentas específicas para reconhecer a ocorrência sobreajuste ou sub-ajuste. Mas, agora, vamos falar sobre o problema de, se estiver acontecendo sobreajuste, o que podemos fazer para corrigir? Nos exemplos anteriores, nós tivemos um ou dois dados dimensionais, então poderíamos simplesmente plotar a hipótese e ver o que estava acontecendo e selecionar o polinômio de grau apropriado. Então, para os os exemplos anteriores de preços de casa, poderíamos simplesmente plotar a hipótese e, talvez ver que estava ajustando o tipo de função sinuosa que vai para todo lado para prever preços de casas. E nós poderíamos então usar figuras como essas para selecionar um polinômio de grau apropriado. Plotar a hipótese poderia ser uma maneira de tentar decidir que grau de polinômio usar. Mas isso não funciona sempre. E, de fato, é mais frequente termos problemas de aprendizagem onde temos um monte de parâmetros. E não é apenas caso de selecionar o grau do polinômio. Quando temos tantos parâmetros, também fica muito mais difícil plotar os dados e fica muito mais difícil visualizá-los para decidir quais parâmetros manter ou não. Na prática, quando estamos tentando predizer preços de casas, às vezes podemos ter muitos parâmetros diferentes. E todos esses parâmetros parecem ser úteis. Mas, se tivermos muitos parâmetros e muito poucos dados de treinamento, o sobreajuste pode se tornar um problema. Para corrigir o sobreajuste, há duas alternativas principais de coisas que podemos fazer. A primeira opção é tentar reduzir o número de parâmetros. Na verdade, algo que nós poderíamos fazer é verificar manualmente a lista de parâmetros e usar isso para decidir quais são os parâmetros mais importantes, o quais deveríamos manter, e quais deveríamos descartar. Mais tarde neste curso, também vamos falar sobre algoritmos de seleção de modelos, que são algoritmos para decidir automaticamente quais parâmetros manter, e quais parâmetros descartar. A ideia de reduzir o número de parâmetros pode funcionar bem, e pode reduzir o sobreajuste. Quando falarmos sobre seleção de modelos, vamos detalhar isso com maior profundidade. Mas, a desvantagem é que descartar alguns dos parâmetros também é descartar parte da informação que temos sobre o problema. Por exemplo, talvez todos aqueles parâmetros sejam realmente úteis para predizer o preço de uma casa, então não iráimos querer descartar nenhuma informação ou nenhum dos parâmetros. A segunda opção, sobre a qual falaremos nos próximos vídeos, é a regularização. Aqui, vamos manter todos os parâmetros, mas vamos reduzir a magnitude ou os valores dos parâmetros teta J. E esse método funciona bem quando temos muitos parâmetros, onde cada um dos quais contribui um pouco para predizer o valor de Y, como vimos no exemplo de predição de casas, em que poderíamos ter muitos parâmetros, onde todos são úteis, então não queremos descartá-los. Isso descreve a ideia de regularização em um alto nível. E, percebo que todos esses detalhes provavelmente ainda não fazem sentido para você. Mas, no próximo vídeo, vamos começar a formular exatamente como aplicar a regularização e o que ela significa. E vamos começar a descobrir como usar isso para fazer algoritmos de aprendizagem funcionarem bem e evitar o sobreajuste.
Tradução: Paulo R. Ormenese | Revisão: Eduardo Bonet