1
00:00:00,360 --> 00:00:01,753
ここまでで、幾つかの

2
00:00:01,760 --> 00:00:04,097
異なる学習アルゴリズムを見てきた。

3
00:00:04,097 --> 00:00:06,504
線形回帰とロジスティック回帰。

4
00:00:06,510 --> 00:00:08,583
それらは様々な問題でうまく機能する。

5
00:00:08,583 --> 00:00:09,684
だがそれをある種の機械学習の問題に

6
00:00:09,684 --> 00:00:11,903
適用しようとする時に、

7
00:00:11,903 --> 00:00:13,889
オーバーフィッティングと呼ばれる問題に遭遇して

8
00:00:13,900 --> 00:00:18,052
とてもしょぼいパフォーマンスしか発揮出来ないハメになる事がある。

9
00:00:18,052 --> 00:00:18,866
このビデオで私がやりたい事は、

10
00:00:18,866 --> 00:00:20,393
あなたに、オーバーフィッティングの問題とは

11
00:00:20,393 --> 00:00:22,400
何かを説明し、

12
00:00:22,400 --> 00:00:24,083
そして続く

13
00:00:24,083 --> 00:00:25,861
一連のビデオで、

14
00:00:25,861 --> 00:00:27,759
正規化（regularization）と呼ばれるテクニックを議論していきたい。

15
00:00:27,760 --> 00:00:29,787
正規化は我らに

16
00:00:29,787 --> 00:00:31,529
オーバーフィッティングの問題を

17
00:00:31,529 --> 00:00:33,607
緩和したり減らす事を可能にしてくれて、

18
00:00:33,607 --> 00:00:36,844
これらの学習アルゴリズムをもっと良く機能するようにしてくれる事がある物だ。

19
00:00:36,860 --> 00:00:39,607
さて、オーバーフィッティングとはなんだろう？

20
00:00:39,607 --> 00:00:41,616
説明の為の例として、

21
00:00:41,620 --> 00:00:44,030
線形回帰で住居の価格を

22
00:00:44,050 --> 00:00:46,146
住居のサイズの関数として

23
00:00:46,146 --> 00:00:47,123
予測したい、という

24
00:00:47,123 --> 00:00:50,730
例を引き続き用いる事にしよう。

25
00:00:50,730 --> 00:00:51,870
一つ出来る事としては、

26
00:00:51,910 --> 00:00:53,620
このデータに線形関数を

27
00:00:53,620 --> 00:00:54,892
フィッティングする、というのがある。

28
00:00:54,892 --> 00:00:56,296
それをすると、こんな感じの

29
00:00:56,296 --> 00:00:58,913
データにフィットした直線が得られる。

30
00:00:58,913 --> 00:01:01,012
だがこれはとても良いモデルとは言えない。

31
00:01:01,012 --> 00:01:02,543
データを見てみると、

32
00:01:02,560 --> 00:01:04,100
住居のサイズが上昇するにつれて、

33
00:01:04,100 --> 00:01:06,274
住居の価格はだんだんと台地になっていく、

34
00:01:06,274 --> 00:01:08,268
あるいはある種、平坦になっていく、我らが右に移動していくに連れて、

35
00:01:08,270 --> 00:01:11,721
というのが、明らかに見て取れる。

36
00:01:11,740 --> 00:01:14,020
だからこのアルゴリズムはトレーニングセットに

37
00:01:14,020 --> 00:01:15,898
あまりフィット出来ない。

38
00:01:15,898 --> 00:01:19,166
この問題をアンダーフィッティングと呼ぶ。

39
00:01:19,180 --> 00:01:20,494
また別の用語としては、

40
00:01:20,500 --> 00:01:24,666
このアルゴリズムは高バイアスだ、とも言う。

41
00:01:25,140 --> 00:01:26,841
これらはどちらもだいたいは

42
00:01:26,890 --> 00:01:30,760
トレーニングデータにすら、あまり良くはフィッティング出来ていない、という事を意味している。

43
00:01:30,760 --> 00:01:32,328
用語はある種

44
00:01:32,328 --> 00:01:34,515
歴史的、技術的な物だが、

45
00:01:34,515 --> 00:01:36,109
そのアイデアは、

46
00:01:36,110 --> 00:01:37,303
データに直線を

47
00:01:37,303 --> 00:01:38,909
フィッティングさせると、

48
00:01:38,920 --> 00:01:40,290
アルゴリズムは、住居の価格が

49
00:01:40,330 --> 00:01:42,638
サイズに応じて線形に変わる、という

50
00:01:42,638 --> 00:01:44,633
強い前提あるいはバイアスを

51
00:01:44,650 --> 00:01:46,339
置く、という事を意味し、

52
00:01:46,339 --> 00:01:49,988
しかもデータはそれに反している、という事だ。

53
00:01:50,000 --> 00:01:51,281
反対の証拠があるにも関わらず、

54
00:01:51,290 --> 00:01:54,174
前提がバイアスされたままで

55
00:01:54,174 --> 00:01:55,413
直線にフィットさせる、という事に

56
00:01:55,440 --> 00:01:56,974
固執したままだと

57
00:01:56,974 --> 00:02:00,638
データにあまりよくフィットしない、というハメになる。

58
00:02:00,638 --> 00:02:02,173
ここで、真ん中は、

59
00:02:02,210 --> 00:02:04,626
二次関数でフィットさせるとする。

60
00:02:04,626 --> 00:02:06,222
このデータセットに、二次関数で

61
00:02:06,222 --> 00:02:07,793
フィッティングすると、

62
00:02:07,810 --> 00:02:10,211
こんな種類のカーブを得る。

63
00:02:10,211 --> 00:02:14,361
これはかなり良さそう。

64
00:02:14,361 --> 00:02:17,543
そして反対側の極端として、例えば四次の多項式でデータにフィッティングする、というのが考えられる。

65
00:02:17,550 --> 00:02:19,442
この場合5つのパラメータがある。

66
00:02:19,470 --> 00:02:23,196
シータ0からシータ4まで。

67
00:02:23,210 --> 00:02:23,926
それでもって、我らは実際に曲線を

68
00:02:23,926 --> 00:02:26,727
5つの全ての手本を通るようにフィッティング出来てしまう。

69
00:02:26,727 --> 00:02:29,507
たとえばこんな感じの曲線が得られる。

70
00:02:31,260 --> 00:02:32,454
これは一方では

71
00:02:32,460 --> 00:02:33,791
トレーニングセットに

72
00:02:33,791 --> 00:02:35,052
フィッティングするという点では

73
00:02:35,052 --> 00:02:36,291
とてもよい仕事をしているように見える、

74
00:02:36,291 --> 00:02:38,269
少なくともデータの上を全て通るのだから。

75
00:02:38,270 --> 00:02:40,284
だがまたこれは、とてもうねうねした曲線でもあるね？

76
00:02:40,300 --> 00:02:41,660
つまり上に行ったり下に行ったり

77
00:02:41,660 --> 00:02:43,430
あちこち通って、住居の価格を予測するのに

78
00:02:43,430 --> 00:02:46,996
そんなに良さそうには思えない。

79
00:02:47,000 --> 00:02:48,924
だから、この問題を

80
00:02:48,924 --> 00:02:51,967
オーバーフィッティングと呼んでいて、

81
00:02:51,970 --> 00:02:53,165
また別の用語としては、

82
00:02:53,170 --> 00:02:57,304
このアルゴリズムは高バリアンスだ、とも言う。

83
00:02:57,890 --> 00:02:59,951
この高バリアンスという用語はまた別の

84
00:02:59,951 --> 00:03:02,110
歴史的、技術的な物だ。

85
00:03:02,130 --> 00:03:03,797
だが感覚的には、

86
00:03:03,800 --> 00:03:05,080
そんなに高次の多項式に

87
00:03:05,080 --> 00:03:07,326
フィッティングすると、

88
00:03:07,330 --> 00:03:08,603
その場合仮説は

89
00:03:08,620 --> 00:03:09,584
まるでどんな関数にも

90
00:03:09,584 --> 00:03:11,995
フィッティング出来てしまい、

91
00:03:11,995 --> 00:03:14,159
可能な仮説の数が単純に

92
00:03:14,159 --> 00:03:16,601
多くなりすぎる、あまりにも変わりすぎるという問題に直面する事になる。

93
00:03:16,610 --> 00:03:18,052
そして我らが、仮説を

94
00:03:18,052 --> 00:03:19,279
良い物だけに制約出来るほどには

95
00:03:19,279 --> 00:03:22,714
データを持っていない。だからこれはオーバーフィッティングと呼ばれる。

96
00:03:22,740 --> 00:03:24,340
そして真ん中。これには名前は無いが、

97
00:03:24,350 --> 00:03:26,990
これはちょうど良い、という状態。

98
00:03:26,990 --> 00:03:29,911
二次の多項式、二次関数は

99
00:03:29,911 --> 00:03:32,559
このデータにフィッティングするのに、ただ単にちょうど良さそう。

100
00:03:32,559 --> 00:03:34,684
ちょっと復習しておこう。

101
00:03:34,690 --> 00:03:37,042
オーバーフィッティングの問題は

102
00:03:37,042 --> 00:03:38,258
フィーチャーが多すぎる時に

103
00:03:38,258 --> 00:03:40,729
起こり、その場合、仮説はトレーニングセットには

104
00:03:40,729 --> 00:03:43,881
とても良くフィットするように学習する。

105
00:03:43,881 --> 00:03:46,023
だから、あなたのコスト関数は

106
00:03:46,023 --> 00:03:47,344
実際にとても0に近い所に行くだろう、

107
00:03:47,344 --> 00:03:48,446
時には完全に0に

108
00:03:48,446 --> 00:03:50,750
一致する事もある。

109
00:03:50,750 --> 00:03:52,063
だがそれは、こんなカーブに

110
00:03:52,063 --> 00:03:53,950
なってしまっているかもしれない。

111
00:03:53,950 --> 00:03:55,314
つまりトレーニングセットにフィットさせようと

112
00:03:55,314 --> 00:03:57,103
あまりにも頑張りすぎてて、

113
00:03:57,110 --> 00:03:59,233
それは新しい手本に対して一般化する事に

114
00:03:59,250 --> 00:04:01,117
失敗していて、だから新しい手本に対して

115
00:04:01,120 --> 00:04:03,018
価格を予測する事にも失敗してしまう。

116
00:04:03,050 --> 00:04:04,337
ここで一般化という

117
00:04:04,350 --> 00:04:06,853
用語は、新規の手本に対して

118
00:04:06,853 --> 00:04:10,868
仮説がどれだけ良く適用出来るか、という事を意味する。

119
00:04:10,868 --> 00:04:12,274
それは、トレーニングセットに無い

120
00:04:12,320 --> 00:04:16,467
住居のデータについて、という事。

121
00:04:16,600 --> 00:04:17,910
このスライドでは線形回帰の場合での

122
00:04:17,910 --> 00:04:20,802
オーバーフィッティングを見てきた。

123
00:04:20,810 --> 00:04:24,182
似たような事はロジスティック回帰でもありうる。

124
00:04:24,190 --> 00:04:26,090
これは2つのフィーチャー、x1とx2による

125
00:04:26,090 --> 00:04:28,871
ロジスティック回帰の例だ。

126
00:04:28,910 --> 00:04:30,136
我らが出来る事としては、

127
00:04:30,140 --> 00:04:31,522
このような単純な仮説に

128
00:04:31,522 --> 00:04:34,518
ロジスティック回帰をフィッティングする、というのが考えられる。

129
00:04:34,530 --> 00:04:38,076
ここでgはいつも通りsigmoid関数だ。

130
00:04:38,120 --> 00:04:39,334
そしてこれをやると、結局

131
00:04:39,334 --> 00:04:41,593
単なる直線で

132
00:04:41,600 --> 00:04:42,923
陽性と陰性の手本を分割しようと試みる

133
00:04:42,923 --> 00:04:45,713
仮説が得られる事になる。

134
00:04:45,713 --> 00:04:49,071
これはとても良く仮説にフィットしてるようには、とても見えない。

135
00:04:49,100 --> 00:04:50,659
つまり再びこれも、

136
00:04:50,659 --> 00:04:52,577
アンダーフィッティングしている例、あるいは

137
00:04:52,577 --> 00:04:56,040
仮説が高バイアスになってる例だ。

138
00:04:56,210 --> 00:04:57,504
対照的に、もしあなたのフィーチャーに

139
00:04:57,504 --> 00:04:59,146
これらの二次の項を

140
00:04:59,170 --> 00:05:01,032
追加したら、

141
00:05:01,032 --> 00:05:02,613
その場合はこんな感じの

142
00:05:02,613 --> 00:05:05,620
決定境界が得られるだろう。

143
00:05:05,620 --> 00:05:07,784
そしてこれは、かなり良くデータにフィットしてるように見える。

144
00:05:07,784 --> 00:05:10,838
たぶん、このトレーニングセットに対しては

145
00:05:10,860 --> 00:05:13,991
もっとも良さそうに見える。

146
00:05:14,010 --> 00:05:15,157
そして最後に、反対側の極端として、

147
00:05:15,170 --> 00:05:16,169
とても高次の多項式に

148
00:05:16,169 --> 00:05:18,207
フィッティングすると、

149
00:05:18,207 --> 00:05:20,036
もし仮にたくさんの高次の項のフィーチャーを

150
00:05:20,036 --> 00:05:22,461
生成したとすると、

151
00:05:22,490 --> 00:05:24,730
その場合、ロジスティック回帰は

152
00:05:24,750 --> 00:05:26,551
自身をゆがめて、

153
00:05:26,560 --> 00:05:28,233
凄い懸命に

154
00:05:28,233 --> 00:05:31,742
あなたのトレーニングデータにフィットするように

155
00:05:31,742 --> 00:05:33,013
決定境界を見つけようとする、あるいは

156
00:05:33,030 --> 00:05:35,006
凄く長くなるように自分自身をゆがめて

157
00:05:35,006 --> 00:05:37,689
トレーニング手本一つ一つにとても良くフィットするように進んでいく。

158
00:05:37,700 --> 00:05:38,757
そしてもしフィーチャーの

159
00:05:38,757 --> 00:05:39,547
x1とx2が

160
00:05:39,550 --> 00:05:41,435
例えば癌の予測を

161
00:05:41,435 --> 00:05:43,350
提供するとすると、

162
00:05:43,390 --> 00:05:46,448
癌が悪性か、乳腺腫瘍になりそうかの予想を提供するなら、

163
00:05:46,448 --> 00:05:47,988
これも、とても良い仮説、という風には

164
00:05:47,988 --> 00:05:51,893
思えない、予測の為に使うには。

165
00:05:51,930 --> 00:05:53,463
だからここでも、

166
00:05:53,463 --> 00:05:55,432
これはオーバーフィッティングの例で、

167
00:05:55,432 --> 00:05:57,128
そして仮説は高バリアンスに

168
00:05:57,128 --> 00:05:59,403
なっていて、たぶん新規の手本に対して

169
00:05:59,403 --> 00:06:04,243
うまく一般化されそうには無い。

170
00:06:04,560 --> 00:06:06,158
このコースの後の方で、

171
00:06:06,158 --> 00:06:08,453
学習アルゴリズムが

172
00:06:08,460 --> 00:06:09,794
おかしな事になった時の

173
00:06:09,810 --> 00:06:11,490
デバッグや診断の話をする時に、

174
00:06:11,490 --> 00:06:13,297
どういう時にオーバーフィッティングが起きて

175
00:06:13,297 --> 00:06:14,953
どういう時にアンダーフィッティングが起きているかを識別する

176
00:06:14,953 --> 00:06:17,503
具体的なツールを紹介する。

177
00:06:17,503 --> 00:06:18,775
だが現時点では、まずオーバーフィッティングの問題が

178
00:06:18,780 --> 00:06:20,342
既に起こっているとして、

179
00:06:20,360 --> 00:06:22,206
それについてどう対処したらいいのか？を

180
00:06:22,250 --> 00:06:24,864
議論しよう。

181
00:06:24,864 --> 00:06:26,640
以前の例では、

182
00:06:26,660 --> 00:06:28,701
1次元とか2次元のデータだった。

183
00:06:28,701 --> 00:06:31,335
だから仮説をプロットして何が起きているのかを見る事が出来て、

184
00:06:31,335 --> 00:06:34,612
適切な次数の仮説を選ぶ事が出来た。

185
00:06:34,620 --> 00:06:36,836
だから以前の住居の価格の例では、

186
00:06:36,836 --> 00:06:38,405
仮説を単にプロットしてみて、

187
00:06:38,410 --> 00:06:40,597
そしてそれを見て

188
00:06:40,600 --> 00:06:41,628
それがある種の、

189
00:06:41,628 --> 00:06:42,830
とてもうねうねと波打った関数で

190
00:06:42,830 --> 00:06:46,339
全ての住居の価格を予測する点を通るようにフィッティングしてしまっている事を見る事が出来るかもしれない。

191
00:06:46,339 --> 00:06:47,701
そしてこのような図を用いて

192
00:06:47,740 --> 00:06:50,667
多項式の適切な次数を選ぶ事が出来るかもしれない。

193
00:06:50,680 --> 00:06:54,166
つまり仮説をプロットするのは、

194
00:06:54,166 --> 00:06:55,728
どの次数を使うべきかを決める

195
00:06:55,750 --> 00:06:58,160
一つの方法たりえる。

196
00:06:58,160 --> 00:07:00,163
だがそれがいつも使える訳でも無い。

197
00:07:00,180 --> 00:07:02,019
そして実のところ、多くの場合において、我らは

198
00:07:02,019 --> 00:07:06,075
たくさんのフィーチャーを含む学習問題を扱う事になる。

199
00:07:06,075 --> 00:07:07,563
そしてまた、これは単に

200
00:07:07,563 --> 00:07:10,599
多項式の次数を選ぶだけの問題では無い。

201
00:07:10,630 --> 00:07:12,147
そして実際、

202
00:07:12,170 --> 00:07:13,779
そんなにたくさんのフィーチャーがある時には

203
00:07:13,779 --> 00:07:15,593
データをプロットする事ももっと難しくなり

204
00:07:15,630 --> 00:07:17,698
だからそれを可視化するのも

205
00:07:17,710 --> 00:07:19,211
もっと困難となる、

206
00:07:19,211 --> 00:07:22,396
どのフィーチャーを維持すべきか、そうでないかを決める為に使うプロットを。

207
00:07:22,420 --> 00:07:24,142
具体的には、住居の価格を予測するのに

208
00:07:24,160 --> 00:07:27,849
様々なフィーチャーを使って予測する、という事がありうる。

209
00:07:27,880 --> 00:07:31,373
そしてこれら全てのフィーチャーが、役に立ちそうに思える事がある。

210
00:07:31,373 --> 00:07:32,609
だが、たくさんのフィーチャーが

211
00:07:32,609 --> 00:07:34,123
ある場合には、そしてトレーニングデータが

212
00:07:34,123 --> 00:07:35,820
少ない時には、

213
00:07:35,840 --> 00:07:37,776
その場合はオーバーフィッティングが問題になりうる。

214
00:07:37,776 --> 00:07:39,180
オーバーフィッティングの問題をなんとかする為に

215
00:07:39,180 --> 00:07:40,651
取れる手段としては大きく

216
00:07:40,651 --> 00:07:43,780
2つの選択肢がある。

217
00:07:43,780 --> 00:07:45,759
最初の選択肢は

218
00:07:45,770 --> 00:07:47,976
フィーチャーの数を減らすこと。

219
00:07:47,990 --> 00:07:49,337
具体的には、一つ出来る事としては

220
00:07:49,337 --> 00:07:51,383
人力でフィーチャーのリストを

221
00:07:51,383 --> 00:07:53,236
見ていって、

222
00:07:53,236 --> 00:07:54,894
そしてどれがもっとも重要なフィーチャー群で

223
00:07:54,894 --> 00:07:57,256
ゆえにどれが維持すべき物か

224
00:07:57,256 --> 00:07:58,476
そしてどれが捨て去るべき物かを

225
00:07:58,476 --> 00:08:01,844
決定する、というのが考えられる。

226
00:08:01,844 --> 00:08:03,401
このコースの後半では、

227
00:08:03,401 --> 00:08:06,018
モデル選択のアルゴリズムについて議論する。

228
00:08:06,040 --> 00:08:08,361
それは自動的に、どのフィーチャーを

229
00:08:08,361 --> 00:08:09,788
維持しつづけて、どのフィーチャーを

230
00:08:09,800 --> 00:08:12,500
捨て去るかを決定するアルゴリズムだ。

231
00:08:12,500 --> 00:08:13,987
このフィーチャーの数を減らす、

232
00:08:13,987 --> 00:08:15,562
というアイデアは

233
00:08:15,562 --> 00:08:17,853
うまく行くこともあり、オーバーフィッティングを低減しうる。

234
00:08:17,853 --> 00:08:19,383
そしてモデル選択の話をする時に

235
00:08:19,383 --> 00:08:22,534
この話ももっとつっこんで行う。

236
00:08:22,534 --> 00:08:24,386
だがここでは、その欠点の話をしていこう。

237
00:08:24,386 --> 00:08:25,603
フィーチャーを幾つか捨て去る事は、

238
00:08:25,603 --> 00:08:27,010
それは同時に問題について自分の持っている

239
00:08:27,370 --> 00:08:30,615
情報を捨て去る事でもある。

240
00:08:30,650 --> 00:08:31,942
例えば、それらのフィーチャーが全て

241
00:08:31,942 --> 00:08:33,760
実際に住居の価格を予測するのに

242
00:08:33,780 --> 00:08:35,050
有用だったとしよう。

243
00:08:35,070 --> 00:08:36,636
その場合我らは本当は

244
00:08:36,640 --> 00:08:37,687
我らの持つ情報の一部を捨て去りたくは、

245
00:08:37,687 --> 00:08:40,990
我らのフィーチャーを捨て去りたくは無いだろう。

246
00:08:41,540 --> 00:08:44,515
二番目の選択肢は、

247
00:08:44,515 --> 00:08:45,995
これは続く一連のビデオで扱う事になるが、

248
00:08:46,010 --> 00:08:49,268
それは正規化(regularization)を行う、という事。

249
00:08:49,268 --> 00:08:50,390
ここでは、全てのフィーチャーを

250
00:08:50,390 --> 00:08:52,579
維持しつづけて、

251
00:08:52,579 --> 00:08:55,063
だがパラメータシータjの

252
00:08:55,063 --> 00:08:56,506
倍率を下げる。

253
00:08:56,520 --> 00:08:58,745
そしてこの手法は

254
00:08:58,750 --> 00:09:00,690
あとで見るように、以下のようなケースではうまく行く：

255
00:09:00,690 --> 00:09:01,925
それは我らがたくさんのフィーチャーを持っていて、

256
00:09:01,925 --> 00:09:03,822
その各々がちょっとずつ

257
00:09:03,822 --> 00:09:05,502
Yの値を予測するのに

258
00:09:05,502 --> 00:09:07,723
貢献している、という場合だ。

259
00:09:07,740 --> 00:09:10,283
ちょうど住居の価格の予測の例で見たように。

260
00:09:10,283 --> 00:09:11,413
そこではたくさんのフィーチャーがありえて、

261
00:09:11,413 --> 00:09:12,720
それらはおのおの、

262
00:09:12,750 --> 00:09:16,902
いくらか有用で、それらを捨て去りたくは無い。

263
00:09:16,930 --> 00:09:19,247
以上は正規化のアイデアを

264
00:09:19,250 --> 00:09:22,790
とても高いレベルで記述した物だ。

265
00:09:22,790 --> 00:09:24,354
だからこれらの詳細の全てが

266
00:09:24,360 --> 00:09:26,763
あなたにピンと来る訳では無いという事は分かっている。

267
00:09:26,763 --> 00:09:28,316
だが次のビデオから、

268
00:09:28,316 --> 00:09:30,960
厳密にどう正規化を適用するのか、

269
00:09:30,960 --> 00:09:35,117
正規化とは何を意味するのかを定式化していく。

270
00:09:35,140 --> 00:09:36,810
その時には、我らはこれを

271
00:09:36,810 --> 00:09:38,310
どのように用いる事で、

272
00:09:38,310 --> 00:09:40,412
学習アルゴリズムをうまく機能させ

273
00:09:40,412 --> 00:09:42,460
オーバーフィッティングを避ける事が出来るか、知る事となろう。