Em regressão logística, nós previamente conversamos sobre dois tipos de algoritmo de otimização. Nós discutimos sobre como usar descida de gradiente para otimizar a função de custo "J(θ)". E nós também conversamos sobre métodos de otimização avançados. Métodos que requerem que você forneça uma maneira de calcular a sua função de custo "J(θ)" e sua derivadas parciais. Nesse vídeo, nós iremos mostrar adaptar ambas as técnicas, tanto descida de gradiente quanto as técnicas mais avançadas, de forma a fazê-los funcionar com regressão logística regularizada. O conceito é o seguinte: Nós vimos antes que Regessão Logística também está propensa a sobreajuste caso você utilize um polinônimo de atributos de grau muito alto, como esse, onde G é a função sigmóide, e nesse caso, você acabaria com uma hipótese, cuja fronteira de decisão é uma função muito complexa e extremamente contorcida que não é na verdade uma boa hipótese para esse conjunto de treino. Em geral, se você tiver regressão logística com muitas variáveis, não necessariamente polinomiais, mas apenas muitas variáveis, você pode acabar com sobreajuste. Aqui está a nossa função custo para a regressão logística. E se quisermos modificar e usar regularização, tudo o que precisamos fazer é adicionar o seguinte termo: "λ / (2 · m)" vezes a somatória indo, como sempre, de "j = 1", em vez de "j = 0", a "n", de "θj²". E isso gera o efeito, então, de penalizar os parâmetros "θ₁", "θ₂", até "θn", para que não sejam muito grandes. E se você fizer isso, então você verá que apesar de estar treinando um polinômio de grau muito alto com vários parâmetros, desde que você aplique regularização e mantenha os parâmetros pequenos, você terá mais chances de ter uma fronteira de decisão que se pareça com essa. Ela parece mais sensata para separar os exemplares positivos e negativos. Então, ao usar regularização, mesmo quando tendo muitos atributos, a regularização consegue ajudar a cuidar do problema de sobreajuste. Como implementamos isso? Para o algoritmo original de descida de gradiente, esta é a expressão para a atualização. Nós vamos efetuar repetidamente a seguinte atualização sobre "θj". Esse slide se parece muito com o anterior de regressão linear. Mas o que eu vou fazer é escrever "θ₀" separadamente. Então, a primeira linha é a expressão para "θ₀" e a segunda linha é a expressão de "θ₁" até "θn", já que eu vou tratar "θ₀" separadamente. Para chegar modificar esse algoritmo para usar a função de custo regularizada, o que precisamos fazer é muito parecido com o que fizemos para regressão linear, é só modificar essa segunda expressão de atualização da seguinte forma E, novamente, isso é aparentemente igual ao que tínhamos para regressão linear. Mas claro que esse não é o mesmo algoritmo do anterior, por que agora a hipótese é definida usando isso. Então, esse não é o mesmo algoritmo que regressão linear regularizada, já que a hipótese é diferente, mesmo que essa atualização que eu escrevi realmente se pareça com a que tínhamos antes, nós estamos desenvolvendo a descida de gradiente para regressão linear regularizada. E é claro que, só para terminar essa discussão, esse termo dentro dos colchetes, esse termo aqui, esse termo é a nova derivada parcial com relação a "θj" da nova função de custo "J(θ)", onde "J(θ)" é a função de custo que definimos num slide anterior que usa regularização. Então, isso é a descida de gradiente para regressão linear regularizada. Agora vamos discutir sobre como fazer a regressão linear regularizada funcionar usando métodos de otimização avançados. E só para lembrar, o que precisávamos para esses métodos era definir a função chamada função de custo, que recebe como entrada o vetor de parâmetros "θ" e nas equações nós indexamos vetores a partir de 0. Então temos "θ₀" até "θn". Mas como Octave indexa vetores começando por 1, "θ₀" em Octave é escrito como "θ₁". "θ₁" é escrito em Octave em Octave como "θ₂" e assim por diante até "θn₊₁". E o que faremos é fornecer uma função, vamos fornecer uma função chamada "costFunction" que iríamos passar para o que vimos anteriormente. Usaremos a função "fminunc" com o argumento "@costFunction" e assim por diante. Mas a função "fminunc" era "fmin" sem restrição, e isso irá usar "fminunc" para minimizar a função de custo para nós. Então, as duas coisas principais que a função de custo precisava retornar eram, primeiramente, "jVal", e para isso nós precisávamos criar uma método para calcular a função de custo "J(θ)". Agora, quando usarmos regressão logística regularizada, é claro que a função de custo J(θ) muda e, particularmente, agora a função de custo precisa incluir esse termo adicional de regularização no final. Então, quando você calcular "J(θ)" confira se incluiu esse termo no final. E a outra coisa que essa função de custo precisa calcular é o gradiente. Então, a "gradient(1)" é atribuído o valor da derivada parcial de "J(θ)" com relação a "θ₀", a "gradient(2)" será atribuído isso, e assim por diante. Novamente, o índice é sempre 1 a mais, por que a indexação do Octave começa em 1. E olhando para esses termos, esse termo aqui, nós calculamos em um slide anterior que é igual a isso. O termo não muda, por que a derivada com relação a "θ₀" não muda quando comparada à versão sem regularização. Mas os outros termos mudam. Em particular a derivada com respeito a "θ₁", nós calculamos no slide anterior também. Ela é igual ao termo original mais "λ/m · θ₁". Só para ter entender isso corretamente. Podemos adicionar parênteses aqui, para que a soma não inclua o último termo. Similarmente, esse outro termo aqui é isto, esse termo adicional que tínhamos no slide anterior, que corresponde ao gradiente do termo de regularização. Se você implementar essa função de custo e passar para a "fminunc" ou para alguma daquelas técnicas de otimização avançadas, a nova função de custo "J(θ)" regularizada será minimizada. E os parâmetros que você receber de volta serão os que correspondem à regressão logística com regularização. Agora você sabe como implementar regressão logística regularizada. Quando eu dou uma volta pelo Vale do Silício, eu moro aqui no Vale do Silício, existem muitos engenheiros ganhando muito dinheiro com suas empresas usando algoritmos de aprendizado de máquina. E eu sei que nós só estamos estudando isso faz pouco tempo, mas se você entender regressão linear, regressão logística, os algoritmos avançados de otimização e regularização, honestamente nesse momento você já sabe mais sobre aprendizado de máquina que muitos, não todos mas você provavelmente sabe muito mais de aprendizado de máquina agora do que muitos dos engenheiros do Vale do Silício com carreiras bem sucedidas, Dando muito dinheiro para as empresas, ou construindo produtos usando algoritmos de aprendizado de máquina Então parabéns, você já avançou um bom tanto, e você já sabe o suficiente para aplicar isso e fazer funcionar em vários problemas. Então parabéns por isso. Mas certamente existe muito mais que nós queremos te ensinar, e nos próximos vídeos nós começaremos a conversar sobre uma classe muito poderosa de classificador não-linear. Então com enquanto regressão linear e regressão logística você consegue usar termos polinomiais, existem classificadores não lineares muito mais poderosos do que esse tipo de regressão polinomial. E no próximo conjunto de vídeos eu vou começar a falar sobre eles. Para que você tenha algoritmos de aprendizagem ainda mais poderosos para aplicar a diferentes problemas.
Tradução: Eduardo Bonet | Revisão: Marcel Dall'Agnol