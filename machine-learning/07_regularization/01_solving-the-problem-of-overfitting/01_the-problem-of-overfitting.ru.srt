1
00:00:00,360 --> 00:00:01,753
Вы уже познакомились с двумя разными

2
00:00:01,760 --> 00:00:04,097
алгоритмами: линейной регрессии и логистической

3
00:00:04,097 --> 00:00:06,504
регрессии.

4
00:00:06,510 --> 00:00:08,583
Они работают хорошо для большинства задач,

5
00:00:08,583 --> 00:00:09,684
но в применении к  определенным приложениям

6
00:00:09,684 --> 00:00:11,903
машинного обучения эти алгоритмы могут столкнуться с

7
00:00:11,903 --> 00:00:13,889
проблемой, называемой переобучением,

8
00:00:13,900 --> 00:00:18,052
что может значительно ухудшить их производительность.

9
00:00:18,052 --> 00:00:18,866
В этой лекции я

10
00:00:18,866 --> 00:00:20,393
объясню вам,

11
00:00:20,393 --> 00:00:22,400
что такое

12
00:00:22,400 --> 00:00:24,083
переобучение. А в следующих лекциях мы

13
00:00:24,083 --> 00:00:25,861
обсудим технику, которая называется

14
00:00:25,861 --> 00:00:27,759
регуляризацией. Она поможет нам

15
00:00:27,760 --> 00:00:29,787
снизить влияние переобучения,

16
00:00:29,787 --> 00:00:31,529
что возможно приведет к улучшению работы

17
00:00:31,529 --> 00:00:33,607
алгоритма

18
00:00:33,607 --> 00:00:36,844
обучения.

19
00:00:36,860 --> 00:00:39,607
Итак, что такое переобучение?

20
00:00:39,607 --> 00:00:41,616
Давайте продолжим использовать наш пример

21
00:00:41,620 --> 00:00:44,030
предсказания стоимости дома с помощью

22
00:00:44,050 --> 00:00:46,146
линейной регрессии. В этом примере, мы

23
00:00:46,146 --> 00:00:47,123
пытаемся выразить цену дома как

24
00:00:47,123 --> 00:00:50,730
функцию от его размера.

25
00:00:50,730 --> 00:00:51,870
Один из возможных подходов состоит в

26
00:00:51,910 --> 00:00:53,620
подборе линейной функции для наших

27
00:00:53,620 --> 00:00:54,892
данных. Если мы сделаем это, мы

28
00:00:54,892 --> 00:00:56,296
получим какую-то прямую, подходящую

29
00:00:56,296 --> 00:00:58,913
под наши данные.

30
00:00:58,913 --> 00:01:01,012
Но это не очень хорошая модель.

31
00:01:01,012 --> 00:01:02,543
Глядя на график, кажется

32
00:01:02,560 --> 00:01:04,100
очевидным,

33
00:01:04,100 --> 00:01:06,274
что при увеличении размера дома, рост цены

34
00:01:06,274 --> 00:01:08,268
замедляется, при движении по

35
00:01:08,270 --> 00:01:11,721
графику вправо. Таким образом,

36
00:01:11,740 --> 00:01:14,020
этот алгоритм не подходит для обучения.

37
00:01:14,020 --> 00:01:15,898
Мы это называем недообучением, или,

38
00:01:15,898 --> 00:01:19,166
другими словами, этот алгоритм

39
00:01:19,180 --> 00:01:20,494
имеет большую систематическую

40
00:01:20,500 --> 00:01:24,666
ошибку.

41
00:01:25,140 --> 00:01:26,841
Грубо говоря, такой алгоритм плохо

42
00:01:26,890 --> 00:01:30,760
работает даже на данных для обучения.

43
00:01:30,760 --> 00:01:32,328
Термин сложился

44
00:01:32,328 --> 00:01:34,515
исторически или

45
00:01:34,515 --> 00:01:36,109
технически, но идея состоит в

46
00:01:36,110 --> 00:01:37,303
том, что при выборе прямой линии

47
00:01:37,303 --> 00:01:38,909
для графика, наш алгоритм будет

48
00:01:38,920 --> 00:01:40,290
предвзято считать изменение

49
00:01:40,330 --> 00:01:42,638
цены дома линейно от его

50
00:01:42,638 --> 00:01:44,633
размера, несмотря на то, что

51
00:01:44,650 --> 00:01:46,339
график показывает

52
00:01:46,339 --> 00:01:49,988
обратное.

53
00:01:50,000 --> 00:01:51,281
Несмотря на очевидное несоответствие, алгоритм настойчиво

54
00:01:51,290 --> 00:01:54,174
продолжает

55
00:01:54,174 --> 00:01:55,413
подгонять прямую линию,

56
00:01:55,440 --> 00:01:56,974
что ведет к плохой

57
00:01:56,974 --> 00:02:00,638
аппроксимации данных.

58
00:02:00,638 --> 00:02:02,173
Теперь, на среднем графике, мы можем попробовать

59
00:02:02,210 --> 00:02:04,626
подобрать квадратичную функцию.

60
00:02:04,626 --> 00:02:06,222
Подбирая для этих данных

61
00:02:06,222 --> 00:02:07,793
квадратичную функцию, мы можем

62
00:02:07,810 --> 00:02:10,211
получить такую кривую,

63
00:02:10,211 --> 00:02:14,361
которая неплохо работает.

64
00:02:14,361 --> 00:02:17,543
В последнем случае, рассмотрим что будет, если мы попробуем подобрать полином, скажем, четвертого порядка.

65
00:02:17,550 --> 00:02:19,442
Итак, у нас есть пять неизвестных от

66
00:02:19,470 --> 00:02:23,196
тета ноль до тета четыре.

67
00:02:23,210 --> 00:02:23,926
Таким образом, мы можем нарисовать

68
00:02:23,926 --> 00:02:26,727
кривую, которая проходит через все пять точек на графике.

69
00:02:26,727 --> 00:02:29,507
Вы можете получить кривую, похожую на эту.

70
00:02:31,260 --> 00:02:32,454
С одной стороны, кажется,

71
00:02:32,460 --> 00:02:33,791
что алгоритм хорошо справляется

72
00:02:33,791 --> 00:02:35,052
со своей работой на

73
00:02:35,052 --> 00:02:36,291
учебном наборе данных,

74
00:02:36,291 --> 00:02:38,269
то есть по крайней мере, проходит через все точки на графике.

75
00:02:38,270 --> 00:02:40,284
Но это очень волнистая кривая, верно?

76
00:02:40,300 --> 00:02:41,660
Итак, линия идет вверх и вниз на всем графике, и

77
00:02:41,660 --> 00:02:43,430
мы вряд ли считаем, что это

78
00:02:43,430 --> 00:02:46,996
хорошая модель для предсказания цен на жильё.

79
00:02:47,000 --> 00:02:48,924
Мы называем такую проблему переобучением.

80
00:02:48,924 --> 00:02:51,967
Или по-другому, этот алгоритм

81
00:02:51,970 --> 00:02:53,165
имеет большую

82
00:02:53,170 --> 00:02:57,304
дисперсию.

83
00:02:57,890 --> 00:02:59,951
Термин высокая дисперсия – еще одни исторический

84
00:02:59,951 --> 00:03:02,110
или технический термин.

85
00:03:02,130 --> 00:03:03,797
Интуитивно мы

86
00:03:03,800 --> 00:03:05,080
понимаем, что если

87
00:03:05,080 --> 00:03:07,326
мы подбираем полином

88
00:03:07,330 --> 00:03:08,603
большой степени, если наша гипотеза

89
00:03:08,620 --> 00:03:09,584
полностью соответствует данным,

90
00:03:09,584 --> 00:03:11,995
тогда наша гипотеза слишком

91
00:03:11,995 --> 00:03:14,159
сложная, слишком

92
00:03:14,159 --> 00:03:16,601
дисперсионна.

93
00:03:16,610 --> 00:03:18,052
И у нас не достаточно данных для

94
00:03:18,052 --> 00:03:19,279
ограничения и получения

95
00:03:19,279 --> 00:03:22,714
хорошей гипотезы. Это мы называем переобучением.

96
00:03:22,740 --> 00:03:24,340
И в центре я напишу "в самый раз", хотя это и

97
00:03:24,350 --> 00:03:26,990
не настоящее наименование.

98
00:03:26,990 --> 00:03:29,911
Где полином второй степени, квадратичная функция,

99
00:03:29,911 --> 00:03:32,559
представляется наиболее подходящим для аппроксимации этих данных.

100
00:03:32,559 --> 00:03:34,684
Итак, мы сталкиваемся с проблемой

101
00:03:34,690 --> 00:03:37,042
переобучения, если у нас много величин и

102
00:03:37,042 --> 00:03:38,258
наша гипотеза слишком точно

103
00:03:38,258 --> 00:03:40,729
соответствует

104
00:03:40,729 --> 00:03:43,881
обучающей выборке.

105
00:03:43,881 --> 00:03:46,023
Ваша стоимостная

106
00:03:46,023 --> 00:03:47,344
функция

107
00:03:47,344 --> 00:03:48,446
может

108
00:03:48,446 --> 00:03:50,750
приближаться к нулю или даже быть равна нулю, но кривая в

109
00:03:50,750 --> 00:03:52,063
таком случае выглядит как эта. Функция

110
00:03:52,063 --> 00:03:53,950
чрезмерно подходит

111
00:03:53,950 --> 00:03:55,314
обучающей выборке, но, в итоге,

112
00:03:55,314 --> 00:03:57,103
эта функция не подходит для

113
00:03:57,110 --> 00:03:59,233
обобщения и предсказания цен новых

114
00:03:59,250 --> 00:04:01,117
примеров. Здесь термин обобщение

115
00:04:01,120 --> 00:04:03,018
относится к

116
00:04:03,050 --> 00:04:04,337
тому, насколько хорошо

117
00:04:04,350 --> 00:04:06,853
гипотеза применима для новых

118
00:04:06,853 --> 00:04:10,868
данных.

119
00:04:10,868 --> 00:04:12,274
То есть тех данных, которые не

120
00:04:12,320 --> 00:04:16,467
представлены в обучающей выборке.

121
00:04:16,600 --> 00:04:17,910
На этом слайде мы видим переобучение

122
00:04:17,910 --> 00:04:20,802
для линейной регрессии.

123
00:04:20,810 --> 00:04:24,182
То же самое применимо и к логистической регрессии.

124
00:04:24,190 --> 00:04:26,090
Здесь пример логистической регрессии

125
00:04:26,090 --> 00:04:28,871
двух величин x1 и x2.

126
00:04:28,910 --> 00:04:30,136
Первое что мы можем попробовать, это

127
00:04:30,140 --> 00:04:31,522
представить логистическую регрессию

128
00:04:31,522 --> 00:04:34,518
очень простой гипотезой,

129
00:04:34,530 --> 00:04:38,076
подобной этой, где, обычно, g(x) - это сигмоид.

130
00:04:38,120 --> 00:04:39,334
И если вы сделаете это, вы

131
00:04:39,334 --> 00:04:41,593
получите гипотезу, которая

132
00:04:41,600 --> 00:04:42,923
использует просто прямую

133
00:04:42,923 --> 00:04:45,713
линию, разделяющую положительные и отрицательные примеры.

134
00:04:45,713 --> 00:04:49,071
И это выглядит не очень подходящей гипотезой.

135
00:04:49,100 --> 00:04:50,659
Это вновь пример недообучения,

136
00:04:50,659 --> 00:04:52,577
или гипотеза имеет большое

137
00:04:52,577 --> 00:04:56,040
смещение (bias).

138
00:04:56,210 --> 00:04:57,504
Напротив, если вы добавите

139
00:04:57,504 --> 00:04:59,146
в вашу функцию эти

140
00:04:59,170 --> 00:05:01,032
квадратичные одночлены, тогда

141
00:05:01,032 --> 00:05:02,613
вы можете получить решение,

142
00:05:02,613 --> 00:05:05,620
границы которого выглядят вот так.

143
00:05:05,620 --> 00:05:07,784
И это достаточно хорошее представление данных.

144
00:05:07,784 --> 00:05:10,838
Возможно это самое лучшее решение, которое мы

145
00:05:10,860 --> 00:05:13,991
можем получить на данной обучающей выборке.

146
00:05:14,010 --> 00:05:15,157
Наконец, другая крайность,

147
00:05:15,170 --> 00:05:16,169
если вы возьмете многочлен

148
00:05:16,169 --> 00:05:18,207
большой степени со

149
00:05:18,207 --> 00:05:20,036
всевозможными вариантами

150
00:05:20,036 --> 00:05:22,461
одночленов высокого порядка,

151
00:05:22,490 --> 00:05:24,730
логистическая регрессия может быть

152
00:05:24,750 --> 00:05:26,551
искажена. Поиск

153
00:05:26,560 --> 00:05:28,233
границ решения в таком

154
00:05:28,233 --> 00:05:31,742
случае может быть

155
00:05:31,742 --> 00:05:33,013
очень сложным. Вы

156
00:05:33,030 --> 00:05:35,006
получите кривую, которая

157
00:05:35,006 --> 00:05:37,689
соответствует каждому элементу обучающей выборки.

158
00:05:37,700 --> 00:05:38,757
Если признаки x1 и x2

159
00:05:38,757 --> 00:05:39,547
используются для предсказания

160
00:05:39,550 --> 00:05:41,435
рака,

161
00:05:41,435 --> 00:05:43,350
предсказания того, является ли опухоль

162
00:05:43,390 --> 00:05:46,448
груди злокачественной или доброкачественной

163
00:05:46,448 --> 00:05:47,988
Это не очень подходящая гипотеза для

164
00:05:47,988 --> 00:05:51,893
предсказаний.

165
00:05:51,930 --> 00:05:53,463
Это снова пример переобучения,

166
00:05:53,463 --> 00:05:55,432
гипотеза имеет высокую

167
00:05:55,432 --> 00:05:57,128
дисперсию

168
00:05:57,128 --> 00:05:59,403
и маловероятно, что

169
00:05:59,403 --> 00:06:04,243
она хорошо обобщается на новые примеры.

170
00:06:04,560 --> 00:06:06,158
Позже, когда мы будем говорить

171
00:06:06,158 --> 00:06:08,453
об отладке и диагностике

172
00:06:08,460 --> 00:06:09,794
вероятных проблем с

173
00:06:09,810 --> 00:06:11,490
алгоритмами обучения, мы

174
00:06:11,490 --> 00:06:13,297
покажем вам

175
00:06:13,297 --> 00:06:14,953
специальные инструменты,

176
00:06:14,953 --> 00:06:17,503
позволяющие определить возникновение ситуации как переобучения, так и недообучения алгоритмов.

177
00:06:17,503 --> 00:06:18,775
Но сейчас давайте поговорим о

178
00:06:18,780 --> 00:06:20,342
проблеме, если мы считаем,

179
00:06:20,360 --> 00:06:22,206
что возникает

180
00:06:22,250 --> 00:06:24,864
переобучение, то что мы можем с этим поделать?

181
00:06:24,864 --> 00:06:26,640
В предыдущих примерах у нас были

182
00:06:26,660 --> 00:06:28,701
одномерные или

183
00:06:28,701 --> 00:06:31,335
двумерные данные, так что

184
00:06:31,335 --> 00:06:34,612
мы могли взять и построить гипотезы, увидеть, что происходило, и выбрать соответствующую степень многочлена.

185
00:06:34,620 --> 00:06:36,836
Ранее, в примере с ценами домов,

186
00:06:36,836 --> 00:06:38,405
мы могли просто построить

187
00:06:38,410 --> 00:06:40,597
гипотезу и увидеть,

188
00:06:40,600 --> 00:06:41,628
что она аппроксимируется

189
00:06:41,628 --> 00:06:42,830
очень извилистой функцией,

190
00:06:42,830 --> 00:06:46,339
проходящей через все ценовые точки.

191
00:06:46,339 --> 00:06:47,701
Также мы могли бы использовать графики, как эти, чтобы

192
00:06:47,740 --> 00:06:50,667
выбрать соответствующую степень многочлена.

193
00:06:50,680 --> 00:06:54,166
Построение гипотез могло бы быть

194
00:06:54,166 --> 00:06:55,728
одним из способов попытаться определить, какую

195
00:06:55,750 --> 00:06:58,160
степень многочлена использовать.

196
00:06:58,160 --> 00:07:00,163
Но это не всегда работает.

197
00:07:00,180 --> 00:07:02,019
На практике у нас зачастую задачи обучения,

198
00:07:02,019 --> 00:07:06,075
где у нас много признаков.

199
00:07:06,075 --> 00:07:07,563
И дело не только в выборе того, какую

200
00:07:07,563 --> 00:07:10,599
степень многочлена использовать.

201
00:07:10,630 --> 00:07:12,147
Фактически, когда у нас много

202
00:07:12,170 --> 00:07:13,779
признаков,

203
00:07:13,779 --> 00:07:15,593
становится намного труднее визуализировать

204
00:07:15,630 --> 00:07:17,698
данные,

205
00:07:17,710 --> 00:07:19,211
чтобы решить какие признаки

206
00:07:19,211 --> 00:07:22,396
учитывать, а какие - нет.

207
00:07:22,420 --> 00:07:24,142
Конкретно, если мы пытаемся предсказывать цены домов,

208
00:07:24,160 --> 00:07:27,849
у нас может быть множество различных признаков.

209
00:07:27,880 --> 00:07:31,373
И, вроде бы, все эти факторы кажутся полезными.

210
00:07:31,373 --> 00:07:32,609
Но, если у нас много признаков и

211
00:07:32,609 --> 00:07:34,123
мало обучающих данных, то переобучение

212
00:07:34,123 --> 00:07:35,820
может стать

213
00:07:35,840 --> 00:07:37,776
проблемой.

214
00:07:37,776 --> 00:07:39,180
Существует два основных варианта

215
00:07:39,180 --> 00:07:40,651
того, что мы можем сделать,

216
00:07:40,651 --> 00:07:43,780
чтобы решить проблему переобучения.

217
00:07:43,780 --> 00:07:45,759
Первым вариант - попробовать сократить

218
00:07:45,770 --> 00:07:47,976
число признаков.

219
00:07:47,990 --> 00:07:49,337
А именно, мы могли бы сделать

220
00:07:49,337 --> 00:07:51,383
следующее - просмотреть

221
00:07:51,383 --> 00:07:53,236
список факторов вручную и

222
00:07:53,236 --> 00:07:54,894
использовать это, чтобы попытаться

223
00:07:54,894 --> 00:07:57,256
решить, какие из них более

224
00:07:57,256 --> 00:07:58,476
важные, и, следовательно,

225
00:07:58,476 --> 00:08:01,844
какие из них учитывать и какие - отбросить.

226
00:08:01,844 --> 00:08:03,401
Далее в этом курсе, где мы также поговорим

227
00:08:03,401 --> 00:08:06,018
об алгоритмах выбора модели.

228
00:08:06,040 --> 00:08:08,361
Это алгоритмы для автоматического определения

229
00:08:08,361 --> 00:08:09,788
того, какие из признаков оставить, а какие

230
00:08:09,800 --> 00:08:12,500
отбросить.

231
00:08:12,500 --> 00:08:13,987
Идея сокращения числа признаков может

232
00:08:13,987 --> 00:08:15,562
хорошо сработать и

233
00:08:15,562 --> 00:08:17,853
уменьшить переобучение.

234
00:08:17,853 --> 00:08:19,383
И, когда мы заговорим о выборе модели,

235
00:08:19,383 --> 00:08:22,534
будем рассматривать еще глубже.

236
00:08:22,534 --> 00:08:24,386
Но недостатком является то, что

237
00:08:24,386 --> 00:08:25,603
отбрасывание некоторых

238
00:08:25,603 --> 00:08:27,010
признаков - это также и отбрасывание некоторой

239
00:08:27,370 --> 00:08:30,615
информации, которая у вас есть о задаче.

240
00:08:30,650 --> 00:08:31,942
Например, возможно, что все эти

241
00:08:31,942 --> 00:08:33,760
признаки полезны при

242
00:08:33,780 --> 00:08:35,050
предсказании стоимости дома, так что

243
00:08:35,070 --> 00:08:36,636
мы скорее всего не захотим

244
00:08:36,640 --> 00:08:37,687
отбросить часть нашей информации или

245
00:08:37,687 --> 00:08:40,990
несколько признаков.

246
00:08:41,540 --> 00:08:44,515
Второй вариант, о котором мы будем говорить

247
00:08:44,515 --> 00:08:45,995
в нескольких следующих видео, - это

248
00:08:46,010 --> 00:08:49,268
регуляризация.

249
00:08:49,268 --> 00:08:50,390
Здесь, мы сохраним все признаки, но

250
00:08:50,390 --> 00:08:52,579
ограничим величину

251
00:08:52,579 --> 00:08:55,063
изменения значений

252
00:08:55,063 --> 00:08:56,506
параметров тета J. И этот метод хорошо

253
00:08:56,520 --> 00:08:58,745
работает, мы увидим, в случае когда у нас много

254
00:08:58,750 --> 00:09:00,690
признаков, каждый из

255
00:09:00,690 --> 00:09:01,925
которых дает небольшой вклад в

256
00:09:01,925 --> 00:09:03,822
предсказание значения Y,

257
00:09:03,822 --> 00:09:05,502
наподобие того,

258
00:09:05,502 --> 00:09:07,723
что мы видели в примере

259
00:09:07,740 --> 00:09:10,283
предсказания стоимости жилья.

260
00:09:10,283 --> 00:09:11,413
Где у нас могло быть много

261
00:09:11,413 --> 00:09:12,720
признаков, каждый из которых в

262
00:09:12,750 --> 00:09:16,902
некоторой степени полезен, так что нам бы не хотелось их отбрасывать.

263
00:09:16,930 --> 00:09:19,247
Таким образом, это в общих чертах

264
00:09:19,250 --> 00:09:22,790
описывает идею регуляризации.

265
00:09:22,790 --> 00:09:24,354
И я понимаю, что все эти детали, возможно, пока

266
00:09:24,360 --> 00:09:26,763
не имеют для вас никакого значения.

267
00:09:26,763 --> 00:09:28,316
Но в следующем видео мы начнем строго

268
00:09:28,316 --> 00:09:30,960
формулировать, как

269
00:09:30,960 --> 00:09:35,117
применять регуляризацию и какое, собственно, она имеет значение.

270
00:09:35,140 --> 00:09:36,810
А затем мы начнем выяснять, как это

271
00:09:36,810 --> 00:09:38,310
использовать, чтобы

272
00:09:38,310 --> 00:09:40,412
заставить обучающие алгоритмы

273
00:09:40,412 --> 00:09:42,460
хорошо работать и избежать переобучения.