Для линейной регрессии мы ранее разработали два алгоритма  обучения, базирующихся на градиентном спуске и на нормальном уравнении В данном видео мы возьмем эти два алгоритма и обобщим их к случаю регуляризованной линейной регрессии. Вот задача оптимизации, которую мы придумали в прошлый раз для регуляризованной линейной регрессии. Вот это первая часть - наша обычная задача для случая линейной регрессии, а теперь у нас появилось вот это дополнительное слагаемое,  где ламбда - это наш параметр регуляризации, и мы хотим найти параметры тета, которые минимизируют функцию затрат, вот эту регуляризованную функцию затрат, J от тета. Раньше мы использовали градиентный спуск для исходной функции затрат, без компонента регуляризации  и у нас был следующий алгоритм для обычной линейной регрессии без регуляризации. Мы будем последовательно вычислять параметры тета-j по этой формуле, где j=1,2 и так далее до n. Я сейчас запишу уравнение для тета-0 отдельно. Я собираюсь просто отделить формулу для тета-0 отдельно от формул для параметров тета-1, 2, 3 ... n.Ничего ведь не изменилось, верно? Мы просто записываем формулу для тета-0 отдельно от формул для тета-1, тета-2, тета-3 и так далее до тета-n. Вот почему я хочу это сделать. Вы возможно помните, что для нашей регуляризованной линейной регрессии мы накладываем штраф на параметры тета-1. тета-2, и так далее до тета-n, но мы не штрафуем тета-0. Таким образом, когда мы изменяем алгоритм для случая регуляризованной линейной регрессии, мы в конце концов будем  вычислять с тета-0 немного по-другому. А именно, чтобы изменить  этот алгоритм с использованием регуляризационной задачи,  все что нам нужно, это взять вот эту нижнюю формулу  и изменить ее следующим образом. Мы возьмем этот элемент и прибавим  минус ламбда деленная на M, умноженная на тета-j. Если мы так сделаем, то получим градиентный спуск для минимизации регуляризированной функции затрат J от тета  в частности (я не собираюсь делать вычисления, чтобы доказать это), но если вы посмотрите на выражение в квадратных скобках. Если произвести вычисления, можно  доказать, что это выражение является частной производной по J от тета, используя новое определение  J от тета  с регуляризационным компонентом. И, кстати, по поводу этого компонента в верхней формуле  который я обвожу другим цветом, это тоже частная производная от J от тета к тета-0. Если вы посмотрите на формулу расчета тета-j, то можете увидеть нечто примечательное. А именно, тета-j изменяется как  тета-j минус альфа умноженное на .. здесь еще один компонент зависимый от тета-j. Таким образом,  если сгруппировать все элементы зависящие от тета-j, то мы можем показать, что  эту формулу можно заменить эквивалентно следующим образом. Все что я сделал - это вынес тета-j, здесь тета-j умноженное на 1 и в этом компоненте ламбда деленная на M. Здесь еще и альфа, итак наконец получаем  альфа на ламбда деленная на m,  это все умножить на тета-j и это выражение следующее: единица минус альфа на ламбда делить на M - довольно интересный член дает интересный эффект. А именно, этот член:  единица минус альфа на ламбду делить на M  - это число, это ограничение, которое дает всегда число меньше единицы, верно? Так как альфа на ламбду делить на M  будет положительным и обычно при обучении коэффициент мал, а M большое Он обычно довольно мал. Итак. этот член будет числом, которое совсем немного меньше единицы. Мы будем его представлять числом типа 0,99 или что-то такое. Результат наших изменений тета-j таков, что мы  говорим, что тета-j  заменяется тета-j умноженная на 0,99. Хорошо, тета-j умноженная на 0,99  уменьшает тета-j приближает ее немного в сторону нуля. Таким образом делая тета-j немного меньше. Более формально эта квадратная норма тета-j меньше. А за ней, второй член многочлена представляет собой ровно такой же,  как и в исходном градиентном спуске, который мы вывели. До того, как мы добавили сюда регуляризацию. Итак, я надеюсь, что этот градиентный спуск,эти преобразования вам понятны, когда мы используем регуляризованную линейную регрессию, то мы для каждого J мы умножали данные на число,немного меньше единицы,  так мы уменьшали параметр совсем немного. А затем мы производим преобразование, такое же как и раньше. Конечно же, это упрощенное объяснение того, что это преобразование делает. Математически, это именно градиентный спуск функции затрат J от тета, который мы определили на прошлом слайде, с использованием регуляризационной составляющей. Градиентный спуск - это только один из двух алгоритмов подходящих для модели линейной регрессии. Второй алгоритм основан на нормальном уравнении, где мы записали матрицу плана "X", в которой каждой строке соответствует один обучающий пример. И мы записали вектор "Y",  размерности M, который состоит из меток обучающей выборки. А "x" - это матрица размерностью M на N+1. Y - это вектор размерности M Для того, чтобы минимизировать функцию затрат J, мы установили, что один из способов  - это приравнять ее к следующему выражению. X транспонированное на X ... обратная матрица на X транспонированная на Y. Я оставляю тут пустое место, чтобы потом его заполнить. Это значение тета минимизирует функцию затрат J от тета в том случае, когда мы не используем регуляризацию. Теперь будем использовать регуляризацию. Чтобы вычислить минимум надо... я только напомню, как вычислять минимум. Надо взять частные производные относительного каждого из параметров, приравнять это к нулю, а затем немного математики, и можно показать, что эта вот формула минимизирует функцию затрат. А именно. если вы используете регуляризацию, то эта формула меняется следующим образом В этих скобках мы получаем вот такую матрицу Ноль, один, один, один и так далее до конца. Итак, получилась матрица, у которой левый верхний элемент - это ноль. Единицы на главной диагонали, а все остальные нули. Я рисую немного небрежно Рассмотрим конкретный пример, при N=2. Тогда эта матрица будет матрицей три на три. Обобщенно, это матрица размерности N+1 на N+1. Соответственно когда N равно двум, то матрица будет выглядеть так: Ноль, затем единицы на главной диагонали, и нули на всех остальных диагоналях. И опять-таки я не буду вычислять эти производные Это откровенно длинно и сложно Однако можно доказать, что с использованием новой формулы для J от тета, с регуляризационной составляющей. То эта новая формула для тета - именно та, которая даст нам глобальный минимум J от тета. В конце, я хочу вкратце описать  аспект необратимости. Это довольно продвинутый материал Можно считать, что этот материал необязателен, так что можете его пропустить. А если вы его прослушаете, но ничего не поймете, не переживайте об этом. Ранее когда я говорил про метод нормального уравнения У нас уже было одно необязательное видео про необратимость матрицы. Так вот это еще одна необязательная часть, в дополнение к предыдущему необязательному видео про необратимость. Допустим, что M (количество примеров) меньше N (количества признаков). Если у нас примеров меньше, чем признаков, тогда матрица X транспонированная умноженное на X будет необратимой или исключительной, или другое название такой матрицы - вырожденная. Если Вы реализуете это на языке Octave и используете функцию pinv для того, чтобы найти псевдо-обратную матрицу Он выдаст нечто похожее на правду, это не совсем понятно, но он выдаст вам очень хорошую гипотезу пусть даже численно реализованная функция языка Octave pinv выдаст вам разумный результат Но если реализовать это на другом языке И если воспользоваться обычным обращением матрицы, которая в Octave представлена при помощи функции Inv Мы пытаемся обратить обычным образом матрицу X транспонированное на X, затем на этом шаге мы находим, что X транспонированное на X - это особенная, необратимая матрица и если мы попытаемся сделать это на другом языке программирования и с использованием какой-нибудь библиотеки линейной алгебры, попытаемся обратить эту матрицу. Оно вполне возможно и не заработает, так как матрица необратимая или особенная. К счастью, регуляризация заботится и об этом, а именно, до тех пор, пока параметр регуляризации строго больше нуля. Можно доказать, что эта матрица X транспонированная на X плюс ламбда умноженное на вот эту забавную матрицу, можно доказать, что эта матрица не будет особенной и то что она будет обратимой. Итак при использовании регуляризации также можно не думать о проблеме необратимости матрицы X транспонированная на X. Теперь вы знаете, как реализовать регуляризацию линейной регрессии. Используя ее, вы сможете избежать избыточной точности, даже когда у вас много признаков и сравнительно небольшая обучающая выборка. И это позволит линейной регрессии работать гораздо лучше для большего количества задач. В следующем видео мы рассмотрим эту идею регуляризации и применим ее к логистической регрессии. С тем, чтобы вы смогли понять, как логистическая регрессия может помочь избежать избыточной точности и просто работать лучше.