1
00:00:00,160 --> 00:00:01,480
ロジスティック回帰については

2
00:00:02,110 --> 00:00:04,730
以前2つの種類の最適化アルゴリズムについて議論した。

3
00:00:05,190 --> 00:00:06,190
1つ目は最急降下法を使って

4
00:00:06,560 --> 00:00:09,210
コスト関数Jのシータを最適化する方法。

5
00:00:09,690 --> 00:00:10,770
2つ目として、アドバンスドな最適化関数を使う

6
00:00:11,120 --> 00:00:12,730
手法についても議論した。

7
00:00:13,520 --> 00:00:14,670
それはあなたが

8
00:00:14,790 --> 00:00:16,300
コスト関数Jのシータの

9
00:00:16,940 --> 00:00:18,160
計算方法と、

10
00:00:18,420 --> 00:00:20,920
偏微分の計算方法を提供する必要がある物だった。

11
00:00:22,450 --> 00:00:23,920
このビデオでは、これらの

12
00:00:24,190 --> 00:00:25,420
どちらのテクニックにも

13
00:00:25,500 --> 00:00:27,570
適用する方法、つまり最急降下法と

14
00:00:27,720 --> 00:00:29,350
よりアドバンスドな最適化技法の両方で

15
00:00:30,280 --> 00:00:31,770
それらを正規化したロジスティック回帰で

16
00:00:31,950 --> 00:00:33,550
使えるようにする方法をお見せしたい。

17
00:00:35,430 --> 00:00:36,670
さて、そのアイデアはこんなだ。

18
00:00:37,260 --> 00:00:38,770
前にロジスティック回帰は

19
00:00:39,190 --> 00:00:40,490
とても高い次数の

20
00:00:40,850 --> 00:00:42,540
このような多項式を含む場合は

21
00:00:42,810 --> 00:00:44,090
オーバーフィットしがち

22
00:00:44,290 --> 00:00:45,890
だという事を見た。

23
00:00:46,470 --> 00:00:48,250
ここでgは

24
00:00:48,480 --> 00:00:49,970
sigmoid関数だ。

25
00:00:50,030 --> 00:00:51,330
つまり、最終的に

26
00:00:51,530 --> 00:00:53,020
とても、過度に複雑な

27
00:00:53,150 --> 00:00:54,120
まったく直感に反するような

28
00:00:54,360 --> 00:00:55,930
決定境界の仮説になるような物の場合には、という事。

29
00:00:56,620 --> 00:00:58,600
つまりはそんな仮説は

30
00:00:58,820 --> 00:00:59,680
このトレーニングセットに対して

31
00:00:59,790 --> 00:01:01,000
そんなに良い仮説とは 思えない。

32
00:01:01,350 --> 00:01:02,990
より一般的に言うと、

33
00:01:03,120 --> 00:01:04,890
たくさんのフィーチャーのロジスティック回帰の時には、

34
00:01:05,150 --> 00:01:06,630
そのフィーチャーは必ずしも多項式で無くても、

35
00:01:06,790 --> 00:01:07,510
たくさんフィーチャーがある時には

36
00:01:07,670 --> 00:01:09,720
ロジスティック回帰はオーバーフィットしがちだ。

37
00:01:11,620 --> 00:01:14,010
これが我らのロジスティック回帰のコスト関数だ。

38
00:01:14,810 --> 00:01:16,210
そしてそれを正規化を用いるように

39
00:01:16,740 --> 00:01:18,820
修正したければ、

40
00:01:18,950 --> 00:01:20,630
我らがやらなくてはいけない事は

41
00:01:20,820 --> 00:01:22,290
そこに以下の項を追加するだけだ、

42
00:01:22,650 --> 00:01:24,860
+ ラムダ/2m の

43
00:01:25,110 --> 00:01:26,580
和を取る事のj=1から、、、

44
00:01:26,730 --> 00:01:29,670
ここでいつも通り、和はj=1から取る、

45
00:01:29,800 --> 00:01:31,000
j=0からじゃなくて。

46
00:01:31,550 --> 00:01:33,670
で、シータjの二乗

47
00:01:34,330 --> 00:01:35,470
そしてこれはつまり

48
00:01:35,750 --> 00:01:36,960
シータ1, シータ2, ...とシータnまでが

49
00:01:37,650 --> 00:01:39,140
大きすぎる場合に

50
00:01:39,570 --> 00:01:42,600
ペナルティを課す、という事を意味する。

51
00:01:43,610 --> 00:01:44,720
これを行えば、

52
00:01:45,720 --> 00:01:46,450
とても高次の

53
00:01:46,750 --> 00:01:48,870
多項式でたくさんのパラメータがあるような物に

54
00:01:49,250 --> 00:01:51,500
フィッティングしたとしても、

55
00:01:52,210 --> 00:01:53,240
正規化を適用しておけば、

56
00:01:53,910 --> 00:01:55,090
それによってパラメータを小さいままに保っておけば、

57
00:01:55,850 --> 00:01:57,580
より、こんな感じの決定境界を

58
00:01:58,830 --> 00:02:00,040
得られる可能性が高まる。

59
00:02:00,320 --> 00:02:01,460
こちらの方が、陽性と陰性の手本を分離するには

60
00:02:02,500 --> 00:02:03,740
よりリーズナブルと言えよう。

61
00:02:05,300 --> 00:02:06,970
つまり、正規化を使えば、

62
00:02:08,140 --> 00:02:09,080
たくさんのフィーチャーがある時でも、

63
00:02:09,220 --> 00:02:11,110
正規化がオーバーフィッティングの問題を

64
00:02:11,620 --> 00:02:13,500
対処してくれる。

65
00:02:14,740 --> 00:02:15,790
実際にはどうやって実装したらいいか？

66
00:02:16,720 --> 00:02:18,280
もともとの最急降下法のアルゴリズムでは、

67
00:02:18,710 --> 00:02:20,380
これが我らの得たアップデートだった。

68
00:02:20,670 --> 00:02:22,300
我らは繰り返し、以下のアップデートを

69
00:02:22,750 --> 00:02:24,610
シータjに対してほどこすのだった。

70
00:02:24,740 --> 00:02:26,940
このスライドは前回の線形回帰についてのスライドと、多くの点で似ている。

71
00:02:27,510 --> 00:02:28,460
とにかく、私がやる事は、まずシータ0のアップデートを

72
00:02:29,210 --> 00:02:31,390
別個に書く。

73
00:02:31,670 --> 00:02:32,930
こうして、最初の行がシータ0の

74
00:02:33,060 --> 00:02:34,110
アップデートで、

75
00:02:34,230 --> 00:02:35,470
二行目が、いまや

76
00:02:35,590 --> 00:02:36,730
シータ1からシータnまでの

77
00:02:36,880 --> 00:02:38,470
アップデートとなった。

78
00:02:38,900 --> 00:02:40,740
何故ならシータ0は別扱いだから。

79
00:02:41,700 --> 00:02:43,140
そしてこのアルゴリズムを

80
00:02:43,700 --> 00:02:45,370
修正して

81
00:02:46,770 --> 00:02:48,480
正規化したコスト関数を用いるようにする為には、

82
00:02:49,100 --> 00:02:50,510
私が行わなくてはいけない事は

83
00:02:50,950 --> 00:02:51,810
線形回帰の時と

84
00:02:51,930 --> 00:02:53,700
極めて似ている。それは

85
00:02:53,870 --> 00:02:55,620
この二番目のアップデートルールを

86
00:02:55,890 --> 00:02:57,480
以下のように変更するだけだ。

87
00:02:58,510 --> 00:02:59,800
そしてまた、見た目でも

88
00:03:00,380 --> 00:03:02,080
線形回帰の頃にあった物と

89
00:03:02,230 --> 00:03:03,720
同一に見える。

90
00:03:04,580 --> 00:03:05,580
だがもちろん、以前のアルゴリズムと

91
00:03:05,660 --> 00:03:06,590
同じアルゴリズムという訳では無い。

92
00:03:06,890 --> 00:03:08,370
何故なら今や、仮説は

93
00:03:08,780 --> 00:03:10,420
これを用いて定義されているから。

94
00:03:10,860 --> 00:03:12,550
だからこれは、正規化された線形回帰と

95
00:03:13,130 --> 00:03:14,390
同じアルゴリズムという訳では無い。

96
00:03:14,830 --> 00:03:16,340
何故なら仮説が違うから。

97
00:03:16,940 --> 00:03:18,360
たしかにここに書きだしたアップデートは

98
00:03:18,630 --> 00:03:20,160
表面上は以前に得た物と

99
00:03:20,350 --> 00:03:22,130
まったく同一ではあるが。

100
00:03:22,480 --> 00:03:25,310
正規化した線形回帰の時に導出した物と。

101
00:03:26,690 --> 00:03:27,720
そしてもちろん、、、

102
00:03:27,830 --> 00:03:29,360
この議論をまとめると、

103
00:03:29,560 --> 00:03:30,860
この大カッコにくくられた

104
00:03:31,130 --> 00:03:32,330
この項、つまりこのここの項は、

105
00:03:32,670 --> 00:03:35,120
この項は、

106
00:03:35,410 --> 00:03:36,750
もちろん、新しいコスト関数Jのシータの

107
00:03:37,210 --> 00:03:38,590
シータjでの偏微分という

108
00:03:38,660 --> 00:03:41,420
新しい偏微分項だ。

109
00:03:42,300 --> 00:03:43,480
ここで、このJのシータは

110
00:03:43,700 --> 00:03:44,980
前のスライドで定義した、正規化項ありの

111
00:03:45,180 --> 00:03:48,100
コスト関数だ。

112
00:03:49,770 --> 00:03:52,060
以上が正規化した線形回帰だ（訳注：ロジスティック回帰の間違いと思われる、以下同様）

113
00:03:55,200 --> 00:03:56,430
ここからは、正規化した線形回帰を

114
00:03:56,580 --> 00:03:58,290
アドバンスドな最適化関数と

115
00:03:58,950 --> 00:04:00,010
どう使っていくかを

116
00:04:00,360 --> 00:04:02,070
議論していこう。

117
00:04:03,180 --> 00:04:05,590
ちょっとこれらの関数を

118
00:04:05,840 --> 00:04:06,800
思い出しておく為に触れておくと、

119
00:04:07,080 --> 00:04:08,390
これらの関数の為に我らがしなくてはいけない事は、

120
00:04:08,450 --> 00:04:09,460
costFuncitonという関数を定義する事だった、

121
00:04:09,640 --> 00:04:11,160
それは入力に

122
00:04:11,280 --> 00:04:13,660
パラメータベクトルのシータを受け取り、

123
00:04:13,790 --> 00:04:16,180
ここで今回も、この等式では

124
00:04:16,770 --> 00:04:19,030
0インデックスのベクトルとして書いた。

125
00:04:19,510 --> 00:04:20,690
だからシータ0から

126
00:04:21,180 --> 00:04:22,810
シータnまである。

127
00:04:23,020 --> 00:04:25,920
だがOctaveはベクトルのインデックスを1から始めるから、

128
00:04:26,820 --> 00:04:28,240
シータ0はOctave上では

129
00:04:28,560 --> 00:04:29,990
theta1と書く。

130
00:04:30,120 --> 00:04:31,630
シータ1はOctave上では

131
00:04:31,860 --> 00:04:32,930
theta2と書く。

132
00:04:33,280 --> 00:04:35,070
そんな風にtheta(n+1)まで

133
00:04:36,270 --> 00:04:36,650
降りていく。

134
00:04:36,740 --> 00:04:38,450
そして我らがやるべき事は、

135
00:04:38,600 --> 00:04:40,240
以下のような関数を提供する事。

136
00:04:41,170 --> 00:04:42,370
costFunctionという関数を

137
00:04:42,780 --> 00:04:44,140
提供する事としよう、それを

138
00:04:44,360 --> 00:04:46,920
以前に見た奴に渡す。

139
00:04:47,300 --> 00:04:48,490
fminuncを使って、

140
00:04:49,060 --> 00:04:50,310
それに対し、引数を、

141
00:04:50,540 --> 00:04:52,160
アットマークにコスト関数。

142
00:04:54,830 --> 00:04:55,430
などとする。

143
00:04:55,600 --> 00:04:56,870
fminuncは

144
00:04:57,030 --> 00:04:58,060
fminのunconstrained（制約無し）

145
00:04:58,280 --> 00:04:59,310
だった。そしてこの

146
00:04:59,650 --> 00:05:01,230
fminuncは、

147
00:05:01,310 --> 00:05:02,300
最小化する対象を受け取って

148
00:05:02,540 --> 00:05:04,340
我らの為に最小化してくれる。

149
00:05:05,950 --> 00:05:07,050
コスト関数が返さなくてはいけない

150
00:05:07,170 --> 00:05:08,600
物は主に二つ、

151
00:05:08,700 --> 00:05:10,620
最初はjVal。

152
00:05:11,280 --> 00:05:12,400
それの為には

153
00:05:12,720 --> 00:05:13,950
コスト関数Jのシータを計算するコードを

154
00:05:14,020 --> 00:05:15,710
書かなくてはならない。

155
00:05:17,130 --> 00:05:19,030
ここでは正規化したロジスティック回帰を使っているのだから、

156
00:05:19,450 --> 00:05:20,920
当然コスト関数Jのシータも

157
00:05:20,990 --> 00:05:21,960
前とは変わっている。

158
00:05:22,280 --> 00:05:23,450
具体的には、

159
00:05:24,480 --> 00:05:25,760
今回はコスト関数には、

160
00:05:25,870 --> 00:05:29,580
末尾に追加の正規化項を含む必要がある。

161
00:05:29,850 --> 00:05:30,930
だから、Jのシータを計算する時に

162
00:05:31,030 --> 00:05:33,410
最後に項を追加する事を忘れないでくれ。

163
00:05:34,590 --> 00:05:35,520
そしてその次に

164
00:05:36,050 --> 00:05:37,240
このコスト関数が提供しなくてはいけない物は

165
00:05:37,690 --> 00:05:39,010
gradientだ。

166
00:05:39,530 --> 00:05:41,170
gradient1には、

167
00:05:41,400 --> 00:05:42,570
Jのシータの

168
00:05:42,660 --> 00:05:44,080
シータ0による偏微分を

169
00:05:44,240 --> 00:05:45,520
セットする。

170
00:05:45,690 --> 00:05:47,170
gradient2には

171
00:05:47,580 --> 00:05:49,520
これをセットする。などなど。

172
00:05:49,780 --> 00:05:50,900
ここでも、インデックスは1ずれている。

173
00:05:51,220 --> 00:05:52,850
何故ならOctaveは1からのインデックスを

174
00:05:53,110 --> 00:05:54,450
使うから。

175
00:05:55,940 --> 00:05:56,780
そしてこれらの項を見ると、

176
00:05:57,850 --> 00:05:58,680
ここのこの項は

177
00:05:59,410 --> 00:06:00,640
前回のスライドで計算した物と

178
00:06:00,720 --> 00:06:02,840
同じで、これに等しい。

179
00:06:03,230 --> 00:06:03,640
変わってない。

180
00:06:04,120 --> 00:06:07,250
何故ならシータ0による微分は前と同じだから。

181
00:06:07,650 --> 00:06:09,540
正規化をしてなかったバージョンと。

182
00:06:10,960 --> 00:06:13,210
そしてそれ以外の項は、変わる。

183
00:06:13,840 --> 00:06:16,340
具体的には、シータ1に関する微分は、

184
00:06:17,010 --> 00:06:18,830
前回のスライドでやったのと同様で、

185
00:06:19,110 --> 00:06:20,670
イコール、

186
00:06:20,890 --> 00:06:22,560
元の項に、そこからマイナスの

187
00:06:23,450 --> 00:06:24,870
ラムダ/m 掛ける シータ1。

188
00:06:25,310 --> 00:06:27,140
これをちゃんと渡している事をしっかり確認してくれ。

189
00:06:27,800 --> 00:06:29,370
そしてここにカッコをつけられる、

190
00:06:29,830 --> 00:06:30,980
和を取るのがここまで行ってしまわないように。

191
00:06:31,570 --> 00:06:33,160
同様に、

192
00:06:33,380 --> 00:06:34,800
この他の項も、これも

193
00:06:35,130 --> 00:06:36,180
こんな感じで、この追加の項があり、

194
00:06:37,070 --> 00:06:37,950
これは前のスライドの物と同じだ、

195
00:06:38,030 --> 00:06:39,770
同様で、これは正規化の目的関数の

196
00:06:39,950 --> 00:06:41,450
微分から来ている。

197
00:06:42,230 --> 00:06:43,650
さて、このcostFunctionを

198
00:06:43,820 --> 00:06:45,140
実装して、これを

199
00:06:45,720 --> 00:06:47,370
このfminuncなり

200
00:06:48,190 --> 00:06:49,160
それ以外のアドバンスドな最適化技法の一つなりに渡せば、

201
00:06:50,050 --> 00:06:51,940
それがこの新しく作った正規化したコスト関数、Jのシータを

202
00:06:52,540 --> 00:06:55,990
最小化してくれる事になる。

203
00:06:56,990 --> 00:06:58,220
そして得られるパラメータは、

204
00:06:59,530 --> 00:07:00,740
ロジスティック回帰に正規化を含めた物に

205
00:07:01,450 --> 00:07:02,940
対応した物となる。

206
00:07:04,410 --> 00:07:05,540
さて、ここまでで正規化されたロジスティック回帰を

207
00:07:05,780 --> 00:07:08,210
どう実装するのかを知った訳だ。

208
00:07:09,780 --> 00:07:10,920
シリコンバレーを回っていると、

209
00:07:11,380 --> 00:07:12,900
私はシリコンバレーに住んでいるのだが、

210
00:07:13,100 --> 00:07:14,900
機械学習のアルゴリズムを用いて

211
00:07:15,420 --> 00:07:16,490
会社に巨万の富をもたらしているエンジニアが

212
00:07:16,610 --> 00:07:18,090
たくさん居る。

213
00:07:19,180 --> 00:07:20,390
そしてここまでで我らが学んで来た期間は

214
00:07:20,600 --> 00:07:22,860
まだちょっとしか経っていない訳だが、

215
00:07:23,620 --> 00:07:25,410
だが線形回帰を理解し、ロジスティック回帰を理解し、

216
00:07:26,510 --> 00:07:28,360
アドバンスドな最適化アルゴリズムを理解し、

217
00:07:29,210 --> 00:07:30,710
正規化を理解した今、

218
00:07:30,950 --> 00:07:32,520
率直にいってたぶん、

219
00:07:32,950 --> 00:07:34,270
いまや、比較的、かなり多く

220
00:07:35,010 --> 00:07:36,290
機械学習について知っている事になると思う、ほとんどのエンジニアより、というと言い過ぎだが、

221
00:07:36,750 --> 00:07:38,050
だがたぶん、現時点まででも既に、

222
00:07:38,180 --> 00:07:39,580
機械学習について多くのエンジニアよりも、もっと多くの事を知っている、

223
00:07:40,240 --> 00:07:41,670
シリコンバレーでとても成功したキャリアを得ている

224
00:07:41,820 --> 00:07:44,760
エンジニア達の多くと比べて、

225
00:07:45,300 --> 00:07:46,420
会社に巨万の富を生み出しているエンジニアたちや

226
00:07:47,050 --> 00:07:49,250
機械学習のアルゴリズムを使ってプロダクトを作っているエンジニア達と比べて。

227
00:07:50,370 --> 00:07:50,960
つまり、、、おめでとう！

228
00:07:52,080 --> 00:07:53,120
すでに貴方は、随分遠くまで来たって事だ。

229
00:07:53,490 --> 00:07:54,550
そしてあなたは実際に

230
00:07:54,780 --> 00:07:55,990
たくさんの問題にこれらの事を適用する為の

231
00:07:56,310 --> 00:07:58,210
十分な知識を得ているのだ。

232
00:07:59,260 --> 00:08:00,580
だからそれについて、おめでとう！と言いたい。

233
00:08:00,780 --> 00:08:01,880
だがもちろん、まだ教えたい事は

234
00:08:02,350 --> 00:08:03,280
他にもいろいろある。

235
00:08:03,400 --> 00:08:05,180
そこでこれに続く

236
00:08:05,380 --> 00:08:06,540
一連のビデオで

237
00:08:06,560 --> 00:08:07,850
とても強力な非線型の分類器の

238
00:08:08,030 --> 00:08:10,890
クラスについて、議論を開始したい。

239
00:08:11,680 --> 00:08:13,350
線形回帰やロジスティック回帰にも

240
00:08:13,690 --> 00:08:14,940
すでに知っての通り、

241
00:08:15,080 --> 00:08:17,310
多項式の項をいれこむ事が出来るが、

242
00:08:17,460 --> 00:08:18,350
多項式の回帰よりもよりパワフルな

243
00:08:18,510 --> 00:08:21,150
非線型の分類器がある事が

244
00:08:21,460 --> 00:08:23,650
既に知られている。

245
00:08:24,640 --> 00:08:25,780
そしてこの後の一連の動画で

246
00:08:25,810 --> 00:08:28,280
それらについて教えていきたい。

247
00:08:28,510 --> 00:08:29,560
様々な問題に適用出来る

248
00:08:29,760 --> 00:08:30,440
今の手持ちのアルゴリズム以上に強力な

249
00:08:31,380 --> 00:08:32,870
アルゴリズムを得る為に。