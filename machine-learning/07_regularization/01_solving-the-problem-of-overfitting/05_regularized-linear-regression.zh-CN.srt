1
00:00:00,260 --> 00:00:01,490
对于线性回归的求解 我们之前

2
00:00:01,680 --> 00:00:03,130
推导了两种学习算法

3
00:00:03,490 --> 00:00:05,010
一种基于梯度下降

4
00:00:05,180 --> 00:00:07,650
一种基于正规方程

5
00:00:08,750 --> 00:00:09,740
在这段视频中 我们将继续学习

6
00:00:09,890 --> 00:00:11,640
这两个算法 并把它们推广

7
00:00:12,290 --> 00:00:13,380
到正则化线性回归中去

8
00:00:14,330 --> 00:00:17,640
这是我们上节课推导出的

9
00:00:18,100 --> 00:00:19,540
正则化线性回归的

10
00:00:20,200 --> 00:00:22,380
优化目标

11
00:00:23,360 --> 00:00:24,580
前面的第一部分是

12
00:00:24,980 --> 00:00:27,240
一般线性回归的目标函数

13
00:00:28,170 --> 00:00:29,300
而现在我们有这个额外的

14
00:00:30,200 --> 00:00:31,750
正则化项 其中 λ

15
00:00:32,450 --> 00:00:34,960
是正则化参数

16
00:00:35,220 --> 00:00:36,690
我们想找到参数 θ

17
00:00:37,160 --> 00:00:38,550
能最小化代价函数

18
00:00:39,030 --> 00:00:41,280
即这个正则化代价函​​数 J(θ)

19
00:00:41,840 --> 00:00:43,030
之前 我们使用

20
00:00:43,440 --> 00:00:45,180
梯度下降求解原来

21
00:00:46,620 --> 00:00:48,060
没有正则项的代价函数

22
00:00:48,770 --> 00:00:49,820
我们用

23
00:00:50,060 --> 00:00:51,990
下面的算法求解常规的

24
00:00:52,370 --> 00:00:53,620
没有正则项的线性回归

25
00:00:54,660 --> 00:00:56,260
我们会如此反复更新

26
00:00:56,330 --> 00:00:57,670
参数 θj

27
00:00:58,270 --> 00:01:00,030
其中 j=0, 1, 2...n

28
00:01:00,400 --> 00:01:02,110
让我

29
00:01:02,530 --> 00:01:03,960
照这个把

30
00:01:04,240 --> 00:01:06,580
j=0 即 θ0 的情况单独写出来

31
00:01:07,210 --> 00:01:08,400
我只是把

32
00:01:08,720 --> 00:01:09,900
θ0 的更新

33
00:01:10,160 --> 00:01:12,500
分离出来

34
00:01:12,680 --> 00:01:14,380
剩下的这些参数θ1, θ2 到θn的更新

35
00:01:14,780 --> 00:01:17,090
作为另一部分

36
00:01:17,370 --> 00:01:19,760
所以 这样做其实没有什么变化 对吧？

37
00:01:19,970 --> 00:01:21,070
这只是把 θ0 的更新

38
00:01:21,300 --> 00:01:23,300
这只是把 θ0 的更新

39
00:01:23,550 --> 00:01:25,240
和 θ1 θ2 到 θn 的更新分离开来

40
00:01:25,510 --> 00:01:26,980
和 θ1 θ2 到 θn 的更新分离开来

41
00:01:27,040 --> 00:01:27,900
我这样做的原因是

42
00:01:28,230 --> 00:01:29,320
你可能还记得

43
00:01:29,880 --> 00:01:31,260
对于正则化的线性回归

44
00:01:32,620 --> 00:01:33,970
我们惩罚参数θ1

45
00:01:34,440 --> 00:01:35,540
θ2...一直到

46
00:01:35,860 --> 00:01:38,360
θn  但是我们不惩罚θ0

47
00:01:38,820 --> 00:01:40,250
所以 当我们修改这个

48
00:01:40,410 --> 00:01:42,400
正则化线性回归的算法时

49
00:01:42,750 --> 00:01:44,050
我们将对

50
00:01:44,710 --> 00:01:46,870
θ0 的方式将有所不同

51
00:01:48,560 --> 00:01:50,360
具体地说 如果我们

52
00:01:50,500 --> 00:01:52,170
要对这个算法进行

53
00:01:52,300 --> 00:01:53,780
修改 并用它

54
00:01:53,870 --> 00:01:55,630
求解正则化的目标函数 我们

55
00:01:55,740 --> 00:01:57,170
需要做的是

56
00:01:57,350 --> 00:02:00,010
把下边的这一项做如下的修改

57
00:02:00,460 --> 00:02:01,860
我们要在这一项上添加一项:

58
00:02:02,670 --> 00:02:05,310
λ 除以 m

59
00:02:06,330 --> 00:02:08,920
再乘以 θj 

60
00:02:09,100 --> 00:02:10,850
如果这样做的话 那么你就有了

61
00:02:11,000 --> 00:02:13,230
用于最小化

62
00:02:13,960 --> 00:02:15,920
正则化代价函数 J(θ) 

63
00:02:16,160 --> 00:02:18,200
的梯度下降算法

64
00:02:19,520 --> 00:02:20,570
我不打算用

65
00:02:20,680 --> 00:02:22,260
微积分来证明这一点

66
00:02:22,390 --> 00:02:23,480
但如果你看这一项

67
00:02:23,690 --> 00:02:26,580
方括号里的这一项

68
00:02:27,730 --> 00:02:28,930
如果你知道微积分

69
00:02:29,380 --> 00:02:31,150
应该不难证明它是

70
00:02:31,370 --> 00:02:33,150
J(θ) 对 θj 的偏导数

71
00:02:33,980 --> 00:02:35,400
这里的 J(θ) 是用的新定义的形式

72
00:02:35,660 --> 00:02:37,520
它的定义中

73
00:02:38,140 --> 00:02:39,330
包含正则化项

74
00:02:39,510 --> 00:02:42,490
而另一项

75
00:02:42,760 --> 00:02:43,960
上面的这一项

76
00:02:44,750 --> 00:02:45,570
我用青色的方框

77
00:02:45,680 --> 00:02:47,240
圈出来的这一项

78
00:02:48,000 --> 00:02:49,270
这也一个是偏导数

79
00:02:49,940 --> 00:02:52,700
是 J(θ)对 θ0 的偏导数

80
00:02:53,680 --> 00:02:54,900
如果你仔细看 θj 的更新

81
00:02:55,600 --> 00:02:56,710
你会发现一些

82
00:02:56,910 --> 00:02:59,190
有趣的东西 具体来说

83
00:02:59,860 --> 00:03:01,100
θj 的每次更新

84
00:03:01,280 --> 00:03:03,400
都是 θj 自己减去 α 乘以原来的无正则项

85
00:03:04,090 --> 00:03:05,010
然后还有这另外的一项

86
00:03:05,380 --> 00:03:06,730
这一项的大小也取决于 θj

87
00:03:06,910 --> 00:03:08,310
所以 如果你

88
00:03:08,420 --> 00:03:09,410
把所有这些

89
00:03:10,030 --> 00:03:11,690
取决于 θj 的合在一起的话

90
00:03:11,780 --> 00:03:13,190
可以证明 这个更新

91
00:03:13,670 --> 00:03:15,100
可以等价地写为

92
00:03:15,200 --> 00:03:16,160
如下的形式

93
00:03:16,470 --> 00:03:17,620
具体来讲 上面的 θj

94
00:03:18,310 --> 00:03:20,100
对应下面的 θj 乘以括号里的1

95
00:03:20,450 --> 00:03:21,950
而这一项是

96
00:03:22,910 --> 00:03:24,830
λ 除以 m 还有一个α

97
00:03:25,140 --> 00:03:25,990
把它们合在一起 所以你最终得到

98
00:03:26,180 --> 00:03:27,650
α 乘以 λ 再除以 m

99
00:03:27,970 --> 00:03:31,450
然后合在一起 乘以 θj

100
00:03:31,820 --> 00:03:33,660
而这一项

101
00:03:34,230 --> 00:03:36,300
1 减去 α 乘以 λ 除以 m

102
00:03:36,600 --> 00:03:39,470
这一项很有意思

103
00:03:42,310 --> 00:03:43,710
具体来说 这一项

104
00:03:43,890 --> 00:03:45,320
1 减去 α 乘以 λ 除以 m

105
00:03:45,730 --> 00:03:46,780
这一项的值

106
00:03:46,870 --> 00:03:48,740
通常是一个具体的实数

107
00:03:48,800 --> 00:03:50,390
而且小于1

108
00:03:50,610 --> 00:03:51,670
对吧？由于

109
00:03:51,920 --> 00:03:53,580
α 乘以 λ 除以 m

110
00:03:54,070 --> 00:03:55,920
通常情况下是正的 如果你的学习速率小 而 m 很大的话

111
00:03:58,650 --> 00:03:58,860
(1 - αλ/m) 这一项通常是很小的

112
00:03:59,650 --> 00:04:00,680
所以这里的一项

113
00:04:00,740 --> 00:04:03,060
一般来说将是一个比1小一点点的值

114
00:04:03,340 --> 00:04:04,150
所以我们可以把它想成

115
00:04:04,330 --> 00:04:05,860
一个像0.99一样的数字

116
00:04:07,380 --> 00:04:08,800
所以

117
00:04:09,120 --> 00:04:10,550
对 θj 更新的结果

118
00:04:10,690 --> 00:04:11,950
我们可以看作是

119
00:04:12,410 --> 00:04:15,420
被替换为 θj 的0.99倍

120
00:04:15,770 --> 00:04:17,500
也就是 θj

121
00:04:18,490 --> 00:04:20,940
乘以0.99

122
00:04:21,280 --> 00:04:23,560
把 θj 向 0 压缩了一点点

123
00:04:23,670 --> 00:04:25,690
所以这使得 θj 小了一点

124
00:04:26,220 --> 00:04:28,080
更正式地说

125
00:04:28,420 --> 00:04:29,750
θj 的平方范数

126
00:04:29,870 --> 00:04:31,580
更小了

127
00:04:31,720 --> 00:04:33,430
另外 这一项后边的第二项

128
00:04:33,910 --> 00:04:35,400
这实际上

129
00:04:35,980 --> 00:04:37,930
与我们原来的

130
00:04:38,050 --> 00:04:40,270
梯度下降更新完全一样

131
00:04:40,750 --> 00:04:42,840
跟我们加入了正则项之前一样

132
00:04:44,270 --> 00:04:46,920
好的 现在你应该对这个

133
00:04:47,380 --> 00:04:48,630
梯度下降的更新没有疑问了

134
00:04:48,880 --> 00:04:51,350
当我们使用正则化线性回归时

135
00:04:51,550 --> 00:04:52,920
我们需要做的就是

136
00:04:53,320 --> 00:04:55,210
在每一个被正规化的参数 θj 上

137
00:04:55,420 --> 00:04:56,310
乘以了一个

138
00:04:56,400 --> 00:04:57,300
比1小一点点的数字

139
00:04:57,400 --> 00:04:58,900
也就是把参数压缩了一点

140
00:04:59,230 --> 00:05:00,340
然后

141
00:05:00,500 --> 00:05:03,000
我们执行跟以前一样的更新

142
00:05:04,170 --> 00:05:05,460
当然 这仅仅是

143
00:05:05,610 --> 00:05:08,310
从直观上认识 这个更新在做什么

144
00:05:08,910 --> 00:05:10,130
从数学上讲

145
00:05:10,580 --> 00:05:12,950
它就是带有正则化项的 J(θ) 

146
00:05:13,130 --> 00:05:14,330
的梯度下降算法

147
00:05:15,150 --> 00:05:16,020
我们在之前的幻灯片

148
00:05:16,480 --> 00:05:18,820
给出了定义

149
00:05:19,780 --> 00:05:21,210
梯度下降只是

150
00:05:21,470 --> 00:05:23,050
我们拟合线性回归模型的两种算法

151
00:05:24,470 --> 00:05:25,530
的其中一个

152
00:05:26,630 --> 00:05:28,090
第二种算法是

153
00:05:28,160 --> 00:05:29,130
使用正规方程

154
00:05:29,680 --> 00:05:31,650
我们的做法

155
00:05:31,740 --> 00:05:32,980
是建立这个

156
00:05:33,060 --> 00:05:34,770
设计矩阵 X 其中每一行

157
00:05:35,080 --> 00:05:37,830
对应于一个单独的训练样本

158
00:05:38,520 --> 00:05:39,790
然后创建了一个向量 y

159
00:05:40,170 --> 00:05:41,780
向量 y 是一个

160
00:05:41,940 --> 00:05:43,320
m 维的向量

161
00:05:43,590 --> 00:05:45,520
m 维的向量

162
00:05:46,010 --> 00:05:47,750
包含了所有训练集里的标签

163
00:05:48,470 --> 00:05:49,600
所以 X 是一个

164
00:05:49,830 --> 00:05:52,660
m × (n+1) 维矩阵

165
00:05:53,590 --> 00:05:55,220
y 是一个 m 维向量

166
00:05:55,780 --> 00:05:57,550
y 是一个 m 维向量

167
00:05:58,030 --> 00:05:59,200
为了最小化代价函数 J

168
00:05:59,470 --> 00:06:00,940
我们发现

169
00:06:01,470 --> 00:06:03,000
一个办法就是

170
00:06:03,230 --> 00:06:04,440
一个办法就是

171
00:06:04,670 --> 00:06:06,790
让 θ 等于这个式子

172
00:06:07,540 --> 00:06:09,040
即 X 的转置乘以 X 再对结果取逆

173
00:06:10,860 --> 00:06:12,770
再乘以 X 的转置乘以Y

174
00:06:13,020 --> 00:06:13,920
我在这里留点空间

175
00:06:14,120 --> 00:06:17,160
等下再填满

176
00:06:17,650 --> 00:06:18,820
这个 θ 的值

177
00:06:19,180 --> 00:06:20,980
其实就是最小化

178
00:06:21,250 --> 00:06:22,710
代价函数 J(θ) 的θ值

179
00:06:22,840 --> 00:06:26,280
这时的代价函数J(θ)没有正则项

180
00:06:26,460 --> 00:06:28,580
现在如果我们用了是正则化

181
00:06:28,780 --> 00:06:30,290
我们想要得到最小值

182
00:06:30,520 --> 00:06:31,820
我们想要得到最小值

183
00:06:31,910 --> 00:06:32,760
我们来看看应该怎么得到

184
00:06:32,980 --> 00:06:34,110
我们来看看应该怎么得到

185
00:06:34,220 --> 00:06:35,220
推导的方法是

186
00:06:35,930 --> 00:06:37,910
取 J 关于各个参数的偏导数

187
00:06:38,340 --> 00:06:40,600
并令它们

188
00:06:40,830 --> 00:06:41,910
等于0 然后做些

189
00:06:42,060 --> 00:06:42,920
数学推导 你可以

190
00:06:43,100 --> 00:06:45,060
得到这样的一个式子

191
00:06:45,550 --> 00:06:47,640
它使得代价函数最小

192
00:06:48,590 --> 00:06:52,130
具体的说 如果你

193
00:06:52,240 --> 00:06:54,080
使用正则化

194
00:06:54,250 --> 00:06:56,320
那么公式要做如下改变

195
00:06:56,480 --> 00:06:59,120
括号里结尾添这样一个矩阵

196
00:06:59,460 --> 00:07:00,940
0 1 1 1 等等 

197
00:07:01,800 --> 00:07:03,520
直到最后一行

198
00:07:04,510 --> 00:07:05,510
所以这个东西在这里是

199
00:07:05,630 --> 00:07:07,810
一个矩阵 它的左上角的元素是0

200
00:07:08,560 --> 00:07:10,080
其余对角线元素都是1

201
00:07:10,190 --> 00:07:11,960
剩下的元素也都是 0

202
00:07:13,050 --> 00:07:14,020
我画的比较随意

203
00:07:15,180 --> 00:07:16,790
可以举一个例子

204
00:07:17,060 --> 00:07:18,210
如果 n 等于2

205
00:07:19,090 --> 00:07:21,110
那么这个矩阵

206
00:07:21,840 --> 00:07:23,500
将是一个3 × 3 矩阵

207
00:07:24,300 --> 00:07:26,210
更一般地情况 该矩阵是

208
00:07:26,360 --> 00:07:27,660
一个 (n+1) × (n+1) 维的矩阵

209
00:07:28,270 --> 00:07:30,290
一个 (n+1) × (n+1) 维的矩阵

210
00:07:31,620 --> 00:07:33,150
因此 n 等于2时

211
00:07:33,370 --> 00:07:35,410
矩阵看起来会像这样

212
00:07:35,980 --> 00:07:37,360
左上角是0 

213
00:07:37,640 --> 00:07:39,020
然后其他对角线上是1

214
00:07:39,160 --> 00:07:41,100
其余部分都是0

215
00:07:42,390 --> 00:07:43,990
同样地 我不打算对这些作数学推导

216
00:07:44,620 --> 00:07:46,280
坦白说这有点费时耗力

217
00:07:46,620 --> 00:07:47,530
但可以证明

218
00:07:47,970 --> 00:07:49,550
如果你采用新定义的 J(θ)

219
00:07:49,940 --> 00:07:50,770
如果你采用新定义的 J(θ)

220
00:07:51,250 --> 00:07:53,730
包含正则项的目标函数

221
00:07:54,780 --> 00:07:56,070
那么这个计算 θ 的式子

222
00:07:56,220 --> 00:07:57,180
能使你的 J(θ) 

223
00:07:57,390 --> 00:08:00,080
达到全局最小值

224
00:08:01,420 --> 00:08:02,460
所以最后

225
00:08:02,610 --> 00:08:05,460
我想快速地谈一下不可逆性的问题

226
00:08:06,800 --> 00:08:08,110
这部分是比较高阶的内容

227
00:08:08,600 --> 00:08:09,530
所以这一部分还是作为选学

228
00:08:09,770 --> 00:08:11,600
你可以跳过去

229
00:08:11,750 --> 00:08:12,520
或者你也可以听听

230
00:08:12,660 --> 00:08:14,180
如果听不懂的话

231
00:08:14,320 --> 00:08:15,680
也没有关系

232
00:08:16,400 --> 00:08:18,950
之前当我讲正规方程的时候

233
00:08:19,700 --> 00:08:20,920
我们也有一段选学视频

234
00:08:21,800 --> 00:08:22,960
讲不可逆的问题

235
00:08:23,700 --> 00:08:25,740
所以这是另一个选学内容

236
00:08:26,170 --> 00:08:27,070
可以作为上次视频的补充

237
00:08:27,700 --> 00:08:30,100
可以作为上次视频的补充

238
00:08:31,610 --> 00:08:33,350
现在考虑 m

239
00:08:33,850 --> 00:08:35,340
即样本总数

240
00:08:35,690 --> 00:08:37,530
小与或等于特征数量 n

241
00:08:38,650 --> 00:08:40,080
如果你的样本数量

242
00:08:40,200 --> 00:08:41,480
比特征数量小的话  那么这个矩阵

243
00:08:42,170 --> 00:08:43,870
X 转置乘以 X 将是

244
00:08:44,070 --> 00:08:47,770
不可逆或奇异的(singluar) 

245
00:08:48,060 --> 00:08:50,120
或者用另一种说法是

246
00:08:50,360 --> 00:08:51,470
这个矩阵是

247
00:08:51,530 --> 00:08:53,390
退化(degenerate)的

248
00:08:53,860 --> 00:08:54,780
如果你在 Octave 里运行它

249
00:08:55,300 --> 00:08:56,380
无论如何

250
00:08:56,620 --> 00:08:58,570
你用函数 pinv 取伪逆矩阵

251
00:08:58,850 --> 00:08:59,800
这样计算

252
00:09:00,080 --> 00:09:01,900
理论上方法是正确的

253
00:09:02,240 --> 00:09:03,450
但实际上

254
00:09:03,560 --> 00:09:04,570
你不会得到一个很好的假设

255
00:09:05,410 --> 00:09:07,720
尽管 Ocatve 会

256
00:09:08,370 --> 00:09:09,670
用 pinv 函数

257
00:09:10,020 --> 00:09:11,050
给你一个数值解

258
00:09:11,340 --> 00:09:13,210
看起来还不错

259
00:09:13,440 --> 00:09:15,460
但是 如果你是在一个不同的编程语言中

260
00:09:16,270 --> 00:09:17,590
如果在 Octave 中

261
00:09:17,710 --> 00:09:19,030
你用 inv 来取常规逆

262
00:09:20,470 --> 00:09:22,070
你用 inv 来取常规逆

263
00:09:23,240 --> 00:09:24,010
也就是我们要对

264
00:09:24,330 --> 00:09:25,620
X 转置乘以 X 取常规逆

265
00:09:26,300 --> 00:09:28,030
然后在这样的情况下

266
00:09:28,150 --> 00:09:30,340
你会发现 X 转置乘以 X

267
00:09:30,450 --> 00:09:32,750
是奇异的 是不可逆的

268
00:09:32,790 --> 00:09:33,740
即使你在不同的

269
00:09:33,990 --> 00:09:35,830
编程语言里计算 并使用一些

270
00:09:36,230 --> 00:09:39,160
线性代数库 试图计算这个矩阵的逆矩阵

271
00:09:39,840 --> 00:09:41,080
都是不可行的

272
00:09:41,220 --> 00:09:43,060
因为这个矩阵是不可逆的或奇异的

273
00:09:44,650 --> 00:09:47,110
幸运的是 正规化也

274
00:09:47,110 --> 00:09:49,850
为我们解决了这个问题 具体地说

275
00:09:50,010 --> 00:09:53,370
只要正则参数是严格大于0的

276
00:09:53,870 --> 00:09:55,220
实际上 可以

277
00:09:55,300 --> 00:09:56,840
证明该矩阵 X 转置 乘以 X

278
00:09:57,080 --> 00:09:58,690
加上 λ 乘以

279
00:09:59,080 --> 00:10:00,400
这里这个矩阵

280
00:10:00,970 --> 00:10:02,250
可以证明

281
00:10:02,470 --> 00:10:03,650
这个矩阵将不是奇异的

282
00:10:03,760 --> 00:10:05,710
即该矩阵将是可逆的

283
00:10:07,450 --> 00:10:09,430
因此 使用正则化还可以

284
00:10:09,700 --> 00:10:11,910
照顾一些 X 转置乘以 X 不可逆的问题

285
00:10:12,580 --> 00:10:14,470
照顾一些 X 转置乘以 X 不可逆的问题

286
00:10:15,260 --> 00:10:18,000
好的 你现在知道了如何实现正则化线性回归

287
00:10:18,870 --> 00:10:19,910
利用它 你就可以

288
00:10:20,300 --> 00:10:21,970
避免过度拟合

289
00:10:22,210 --> 00:10:24,720
即使你在一个相对较小的训练集里有很多特征

290
00:10:25,360 --> 00:10:26,630
这应该可以让你

291
00:10:26,980 --> 00:10:29,000
在很多问题上更好地运用线性回归

292
00:10:30,060 --> 00:10:31,190
在接下来的视频中 我们将

293
00:10:31,390 --> 00:10:34,310
把这种正则化的想法应用到逻辑回归

294
00:10:35,140 --> 00:10:36,170
这样你就可以

295
00:10:36,280 --> 00:10:37,630
让逻辑回归也避免过度拟合

296
00:10:37,920 --> 00:10:39,830
并让它表现的更好 【教育无边界字幕组】翻译：Badbye 校对：所羅門捷列夫 审核：Naplessss