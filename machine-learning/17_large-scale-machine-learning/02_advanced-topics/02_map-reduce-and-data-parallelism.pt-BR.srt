1
00:00:00,320 --> 00:00:01,510
Nos últimos vídeos, falamos

2
00:00:01,810 --> 00:00:03,430
sobre Gradiente Descendente Estocástico

3
00:00:03,620 --> 00:00:05,020
e outras variações

4
00:00:05,120 --> 00:00:06,530
do algoritmo Gradiente Descendente Estocástico,

5
00:00:06,910 --> 00:00:09,150
incluindo adaptações para

6
00:00:09,490 --> 00:00:10,420
aprendizado dinâmico, mas todos

7
00:00:10,610 --> 00:00:11,810
esses algoritmos rodam

8
00:00:12,110 --> 00:00:13,740
em apenas uma máquina, em um computador.

9
00:00:14,800 --> 00:00:15,870
E alguns problemas de aprendizado

10
00:00:16,310 --> 00:00:17,270
são grandes demais para rodar

11
00:00:17,520 --> 00:00:19,160
em uma máquina só, as vezes

12
00:00:19,300 --> 00:00:21,050
possuindo tantos dados

13
00:00:21,170 --> 00:00:22,350
que você nunca iria querer

14
00:00:22,670 --> 00:00:23,980
rodar tudo em uma única

15
00:00:24,100 --> 00:00:26,270
máquina, não importa qual o algoritmo usado.

16
00:00:28,470 --> 00:00:29,640
Nesse video,

17
00:00:29,740 --> 00:00:31,240
eu gostaria de discutir um método diferente

18
00:00:31,770 --> 00:00:33,610
para aprendizado de máquina em larga escala,

19
00:00:34,010 --> 00:00:36,190
chamado de MapReduce.

20
00:00:37,030 --> 00:00:38,080
E apesar de termos

21
00:00:38,380 --> 00:00:39,400
vários vídeos sobre

22
00:00:39,970 --> 00:00:41,230
Gradiente Descendente e

23
00:00:41,550 --> 00:00:43,100
que vamos passar menos tempo

24
00:00:43,460 --> 00:00:45,350
no MapReduce, não julgue

25
00:00:45,560 --> 00:00:46,750
a importância do MapReduce com relação

26
00:00:47,160 --> 00:00:48,240
ao Gradiente Descendente

27
00:00:48,690 --> 00:00:49,590
com base no tempo

28
00:00:49,660 --> 00:00:51,480
que eu gastei em cada método.

29
00:00:52,230 --> 00:00:53,380
Muitas Pessoas dirão que

30
00:00:53,790 --> 00:00:54,840
MapReduce é no mínimo

31
00:00:55,090 --> 00:00:56,330
tão importante quanto,

32
00:00:56,580 --> 00:00:57,850
e outras dirão que ainda mais importante

33
00:00:58,500 --> 00:01:00,620
que o Gradiente Descendente,

34
00:01:01,460 --> 00:01:03,040
apenas mais simples de explicar,

35
00:01:03,160 --> 00:01:04,620
que é o porquê

36
00:01:04,720 --> 00:01:05,580
de eu gastar menos tempo nisso,

37
00:01:05,830 --> 00:01:07,040
mas usando esses princípios

38
00:01:07,670 --> 00:01:08,400
você poderia escalar

39
00:01:09,070 --> 00:01:10,640
algoritmos de aprendizado

40
00:01:10,880 --> 00:01:12,520
para problemas muito maiores

41
00:01:12,630 --> 00:01:14,530
do que seria possível com Gradiente Descendente.

42
00:01:18,720 --> 00:01:19,000
Aqui está a ideia:

43
00:01:19,810 --> 00:01:21,020
Digamos que eu queira ajustar

44
00:01:21,490 --> 00:01:22,960
um modelo de regressão linear ou

45
00:01:23,140 --> 00:01:24,440
um modelo de regressão logística,

46
00:01:24,540 --> 00:01:26,100
começando com o

47
00:01:26,430 --> 00:01:27,660
Gradiente Descendente,

48
00:01:27,840 --> 00:01:30,300
então essa é sua regra de aprendizado.

49
00:01:31,240 --> 00:01:32,430
Para manter a escrita

50
00:01:32,850 --> 00:01:34,170
nesse slide legível,

51
00:01:34,340 --> 00:01:36,990
vou assumir que "m=400"

52
00:01:37,530 --> 00:01:39,560
Claro que para nossos padrões,

53
00:01:39,750 --> 00:01:40,850
olhando para problemas

54
00:01:41,090 --> 00:01:42,050
de larga escala, esse m

55
00:01:42,170 --> 00:01:43,210
é bem pequeno,

56
00:01:43,770 --> 00:01:45,390
então isso seria normalmente

57
00:01:45,870 --> 00:01:46,920
aplicado para problemas

58
00:01:47,050 --> 00:01:48,190
onde você teria algo em torno

59
00:01:48,740 --> 00:01:49,940
de 400 milhões de exemplos,

60
00:01:50,080 --> 00:01:51,310
mas só para manter

61
00:01:51,390 --> 00:01:52,330
o slide simples,

62
00:01:52,770 --> 00:01:55,000
vou fingir que temos só 400 exemplos.

63
00:01:55,690 --> 00:01:57,460
Então, nesse caso,

64
00:01:57,790 --> 00:01:59,080
a regra do Gradiente Descendente

65
00:01:59,570 --> 00:02:00,930
tem esse 400 e a essa

66
00:02:01,500 --> 00:02:02,930
soma de "i= 1 ... 400"

67
00:02:03,330 --> 00:02:05,050
através dos meus 400 exemplos,

68
00:02:05,590 --> 00:02:06,890
e se m for grande,

69
00:02:07,050 --> 00:02:09,780
esse passo seria muito pesado computacionalmente.

70
00:02:10,890 --> 00:02:12,830
O que o MapReduce

71
00:02:13,250 --> 00:02:14,470
faz é o seguinte,

72
00:02:14,890 --> 00:02:15,740
e devo mencionar que

73
00:02:15,950 --> 00:02:16,940
a ideia do MapReduce vem

74
00:02:17,680 --> 00:02:20,190
de dois pesquisadores, Jeff Dean

75
00:02:20,700 --> 00:02:22,060
e Sanjay Gimawat.

76
00:02:22,640 --> 00:02:23,490
Jeff Dean é um dos

77
00:02:24,190 --> 00:02:26,520
engenheiros mais lendários

78
00:02:26,660 --> 00:02:28,300
no Vale do Silício e

79
00:02:28,420 --> 00:02:29,530
construiu grande parte

80
00:02:29,820 --> 00:02:31,670
da infraestrutura

81
00:02:32,310 --> 00:02:34,770
sobre a qual o Goggle roda hoje.

82
00:02:36,000 --> 00:02:37,320
Mas essa é a ideia do MapReduce.

83
00:02:37,850 --> 00:02:38,570
Digamos que eu tenha um

84
00:02:38,700 --> 00:02:39,840
conjunto de treino,

85
00:02:39,900 --> 00:02:41,220
que indicaremos por essa caixa

86
00:02:41,610 --> 00:02:42,760
com pares (x, y),

87
00:02:44,250 --> 00:02:47,730
onde temos (x1,y1)

88
00:02:47,990 --> 00:02:49,640
por todos os meus 400 exemplos,

89
00:02:50,520 --> 00:02:51,660
até (xm, ym).

90
00:02:52,190 --> 00:02:53,780
Esse é o meu conjunto de treino

91
00:02:55,060 --> 00:02:56,550
No MapReduce,

92
00:02:56,690 --> 00:02:58,190
dividiríamos esse conjunto

93
00:02:58,570 --> 00:03:00,510
em diferentes subconjuntos.

94
00:03:01,890 --> 00:03:02,590
Argumentarei que

95
00:03:02,950 --> 00:03:04,150
assumir que

96
00:03:04,290 --> 00:03:05,530
possuo 4 computadores,

97
00:03:06,160 --> 00:03:07,160
ou 4 máquinas para rodar

98
00:03:07,300 --> 00:03:08,670
em paralelo no meu conjunto,

99
00:03:08,890 --> 00:03:10,570
que é o porquê de eu dividir isso em 4 partes.

100
00:03:10,920 --> 00:03:12,290
Se tivéssemos 10 máquinas ou

101
00:03:12,400 --> 00:03:13,810
100 máquinas, então dividiríamos

102
00:03:13,970 --> 00:03:15,890
nosso conjunto de treino em 10 partes ou 100 partes.

103
00:03:18,040 --> 00:03:19,710
A primeira das minhas

104
00:03:19,850 --> 00:03:20,840
4 máquinas irá

105
00:03:21,100 --> 00:03:23,170
usar

106
00:03:23,270 --> 00:03:25,170
um quarto do meu

107
00:03:25,300 --> 00:03:28,680
conjunto de treino, ou seja, 100 exemplos.

108
00:03:30,020 --> 00:03:31,440
E, particularmente,

109
00:03:31,480 --> 00:03:32,520
ela irá olhara para

110
00:03:32,630 --> 00:03:34,800
essa soma e calcular a soma

111
00:03:35,490 --> 00:03:38,560
para os primeiros 100 exemplos.

112
00:03:40,030 --> 00:03:40,960
Vou escrever isso.

113
00:03:41,110 --> 00:03:42,530
Vou calcular uma variável

114
00:03:43,560 --> 00:03:46,230
"temp" sobrescrito 1,

115
00:03:46,320 --> 00:03:49,410
pois é a primeira máquina,

116
00:03:50,450 --> 00:03:52,150
igual a soma de 1 até 100,

117
00:03:52,260 --> 00:03:53,160
e vou inserir aquele

118
00:03:53,500 --> 00:03:56,610
mesmo termo de cima,

119
00:03:57,260 --> 00:04:00,140
(h(x^i)-y(i))*x^i_j

120
00:04:01,800 --> 00:04:03,230
(h(x^i)-y(i))*x^i_j

121
00:04:03,740 --> 00:04:05,680
Este é só o o termo

122
00:04:05,910 --> 00:04:07,460
do Gradiente Descendente ali em cima.

123
00:04:08,300 --> 00:04:09,780
E, similarmente, vou

124
00:04:10,010 --> 00:04:11,330
pegar o segundo quarto

125
00:04:11,600 --> 00:04:13,130
dos meus dados e enviar para

126
00:04:13,320 --> 00:04:14,520
minhas segunda máquina,

127
00:04:14,690 --> 00:04:15,680
e minha segunda máquina vai

128
00:04:15,900 --> 00:04:18,750
usar os exemplos de 101 até 200,

129
00:04:19,350 --> 00:04:21,170
e calcular variáveis similares,

130
00:04:21,720 --> 00:04:22,880
no caso tempˆ2_j,

131
00:04:23,110 --> 00:04:24,450
que é a mesma soma para

132
00:04:24,890 --> 00:04:26,620
exemplos 101 a 200

133
00:04:26,840 --> 00:04:29,680
Da mesma forma, as máquinas 3 e 4

134
00:04:29,830 --> 00:04:32,720
usarão

135
00:04:32,830 --> 00:04:34,110
o terceiro e quarto quartos

136
00:04:34,570 --> 00:04:36,550
do meu conjunto de treino.

137
00:04:37,530 --> 00:04:38,950
Agora cada máquina precisa

138
00:04:39,190 --> 00:04:40,580
somar 100 números

139
00:04:41,060 --> 00:04:42,570
em vez de 400,

140
00:04:42,760 --> 00:04:43,750
realizando apenas um quarto

141
00:04:44,050 --> 00:04:45,220
do trabalho e teoricamente

142
00:04:45,900 --> 00:04:48,000
terminando quatro vezes mais rápido.

143
00:04:49,380 --> 00:04:50,630
Finalmente, depois de todas as máquinas

144
00:04:50,990 --> 00:04:51,740
terem terminado de calcular,

145
00:04:51,850 --> 00:04:53,560
Vou pegar as variáveis temp

146
00:04:55,350 --> 00:04:56,480
e juntá-las.

147
00:04:56,870 --> 00:04:58,400
Eu pego essas variáveis e

148
00:04:58,530 --> 00:04:59,950
envio todas para

149
00:05:00,090 --> 00:05:03,080
um servidor mestre central,

150
00:05:03,300 --> 00:05:04,750
onde esse mestre

151
00:05:05,140 --> 00:05:06,720
irá combinar esse valores.

152
00:05:07,360 --> 00:05:08,470
Em particular,

153
00:05:08,780 --> 00:05:10,780
atualizar meus parâmetros θj

154
00:05:11,000 --> 00:05:13,160
como θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

155
00:05:13,410 --> 00:05:14,720
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

156
00:05:15,730 --> 00:05:17,560
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

157
00:05:17,680 --> 00:05:19,510
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

158
00:05:20,120 --> 00:05:22,940
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

159
00:05:23,300 --> 00:05:27,410
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

160
00:05:27,760 --> 00:05:30,290
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

161
00:05:32,400 --> 00:05:35,470
θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400

162
00:05:35,560 --> 00:05:37,890
para j = (0....n),

163
00:05:37,980 --> 00:05:39,570
onde

164
00:05:39,820 --> 00:05:41,220
"n" é o número de variáveis

165
00:05:42,550 --> 00:05:45,420
Espero que o cálculo desta expressão esteja claro.

166
00:05:45,670 --> 00:05:47,870
O que ela faz

167
00:05:50,930 --> 00:05:53,220
é examente

168
00:05:53,290 --> 00:05:54,570
o mesmo que

169
00:05:54,660 --> 00:05:56,140
ter um servidor central

170
00:05:56,680 --> 00:05:57,950
que pega os resultados

171
00:05:58,040 --> 00:05:58,780
tempˆ1_j, tempˆ2_j,

172
00:05:59,000 --> 00:05:59,850
tempˆ3_j e tempˆ4_j,

173
00:05:59,970 --> 00:06:01,760
e soma eles,

174
00:06:02,030 --> 00:06:03,430
esses quatro

175
00:06:04,090 --> 00:06:04,960
valores.

176
00:06:06,360 --> 00:06:07,810
Isso é somente a soma disso,

177
00:06:08,060 --> 00:06:09,440
mais isso,

178
00:06:09,760 --> 00:06:11,490
mais a soma disso,

179
00:06:11,630 --> 00:06:13,000
mais a soma disso,

180
00:06:13,120 --> 00:06:14,290
e esses quatro valores

181
00:06:14,470 --> 00:06:15,830
somados 

182
00:06:15,920 --> 00:06:17,740
são iguais

183
00:06:17,880 --> 00:06:19,580
à regra original do Gradiente Descendente.

184
00:06:20,590 --> 00:06:21,550
Depois temos α/400 aqui,

185
00:06:21,860 --> 00:06:22,910
e α / 400

186
00:06:23,350 --> 00:06:24,690
aqui também, e isso é

187
00:06:25,020 --> 00:06:27,020
exatamente igual ao

188
00:06:27,140 --> 00:06:29,390
Gradiente Descendente,

189
00:06:29,910 --> 00:06:30,880
só que ao invés de somarmos

190
00:06:31,290 --> 00:06:32,540
dos os 400 exemplos de treino,

191
00:06:32,810 --> 00:06:33,900
em apenas uma máquina,

192
00:06:34,040 --> 00:06:35,280
podemos ao invés

193
00:06:35,760 --> 00:06:37,460
dividir o trabalho em 4 máquinas.

194
00:06:39,090 --> 00:06:40,190
Esse é o princípio geral

195
00:06:40,630 --> 00:06:43,410
que está por trás do MapReduce.

196
00:06:45,060 --> 00:06:46,510
Se tivermos alguns conjuntos de treino,

197
00:06:46,670 --> 00:06:48,200
e queremos paralelizar usando

198
00:06:48,420 --> 00:06:49,100
4 máquinas, vamos pegar

199
00:06:49,170 --> 00:06:51,670
o conjunto de treino e dividi-lo igualmente.

200
00:06:52,120 --> 00:06:54,640
Dividir o mais igualmente possível em 4 subconjuntos.

201
00:06:56,470 --> 00:06:57,110
Então pegamos

202
00:06:57,180 --> 00:06:59,560
esses 4 subconjuntos e enviamos para 4 computadores diferentes.

203
00:07:00,530 --> 00:07:01,660
E cada um desses 4 computadores

204
00:07:02,130 --> 00:07:03,570
pode calcular a soma

205
00:07:03,950 --> 00:07:04,850
de apenas um quarto do

206
00:07:04,910 --> 00:07:06,230
conjunto de dados,

207
00:07:06,340 --> 00:07:07,720
e finalmente pegar os

208
00:07:07,780 --> 00:07:09,310
resultados de cada computador,

209
00:07:09,580 --> 00:07:12,720
e enviar para um servidor central, que irá calcular o resultado.

210
00:07:13,570 --> 00:07:14,900
Na última linha

211
00:07:15,190 --> 00:07:16,540
do exemplo anterior,

212
00:07:16,800 --> 00:07:17,910
o trabalho pesado do Gradiente Descendente

213
00:07:18,330 --> 00:07:20,140
foi calcular a soma

214
00:07:20,430 --> 00:07:22,270
com i=1...400 dos valores,

215
00:07:22,670 --> 00:07:24,110
ou genericamente, a soma

216
00:07:24,370 --> 00:07:25,570
de i=1...m

217
00:07:26,320 --> 00:07:28,180
da fórmula do Gradiente Descendente.

218
00:07:29,160 --> 00:07:30,430
Agora, como cada um dos

219
00:07:30,550 --> 00:07:31,890
quatro computadores precisa

220
00:07:32,190 --> 00:07:33,800
realizar um quarto do trabalho,

221
00:07:34,350 --> 00:07:35,940
a velocidade pode ser 4x maior.

222
00:07:38,820 --> 00:07:39,980
Se não existissem delays

223
00:07:40,190 --> 00:07:41,900
por causa da rede

224
00:07:42,100 --> 00:07:42,970
e sem custos de comunicação

225
00:07:43,400 --> 00:07:44,450
para enviar e

226
00:07:44,600 --> 00:07:45,450
receber dados,

227
00:07:45,610 --> 00:07:47,820
a velocidade poderia ser 4x maior.

228
00:07:48,050 --> 00:07:49,410
Claro que na prática,

229
00:07:50,100 --> 00:07:52,080
devido à latência na rede,

230
00:07:52,810 --> 00:07:54,500
o sobrecusto de combinar

231
00:07:54,600 --> 00:07:55,880
os resultados no final e outros fatores,

232
00:07:56,640 --> 00:07:59,150
a velocidade não chega a ser 4x maior.

233
00:08:00,140 --> 00:08:01,280
Mas de qualquer maneira,

234
00:08:01,360 --> 00:08:02,710
o MapReduce oferece

235
00:08:03,110 --> 00:08:04,560
uma maneira de processar

236
00:08:04,870 --> 00:08:05,940
bases de dados muitos maiores

237
00:08:06,270 --> 00:08:07,550
do que o possível com um computador.

238
00:08:08,770 --> 00:08:10,060
Se você pensa em aplicar

239
00:08:10,730 --> 00:08:12,200
MapReduce para algum algoritmo de

240
00:08:12,350 --> 00:08:14,260
aprendizado, para acelerar o processo,

241
00:08:14,750 --> 00:08:16,160
parelelizando o cálculo

242
00:08:16,900 --> 00:08:18,480
através de vários computadores,

243
00:08:18,730 --> 00:08:20,040
a questão chave é se perguntar:

244
00:08:20,760 --> 00:08:22,190
"Será que é possível expressar o algoritmo"

245
00:08:22,880 --> 00:08:25,150
"como uma soma sobre o conjunto de treino"?

246
00:08:25,440 --> 00:08:26,430
E na verdade muitos algoritmos

247
00:08:26,670 --> 00:08:28,100
de aprendizado podem ser

248
00:08:28,410 --> 00:08:29,880
expressados como a soma

249
00:08:30,170 --> 00:08:31,820
de funções sobre o conjunto de treino,

250
00:08:32,610 --> 00:08:34,030
e o custo de rodá-los

251
00:08:34,250 --> 00:08:35,480
com grandes conjuntos de dados

252
00:08:35,600 --> 00:08:37,810
é muito alto por operarem sobre uma grande quantidade de exemplos.

253
00:08:38,620 --> 00:08:39,870
Então, sempre que seu algoritmo de treino

254
00:08:40,200 --> 00:08:41,350
puder ser escrito como

255
00:08:41,450 --> 00:08:42,410
uma soma no conjunto de treino

256
00:08:42,660 --> 00:08:43,760
e quando o grosso do trabalho

257
00:08:43,860 --> 00:08:44,810
do algoritmo de aprendizado

258
00:08:45,200 --> 00:08:46,170
pode ser escrito como

259
00:08:46,320 --> 00:08:47,780
uma soma, então MapReduce

260
00:08:48,030 --> 00:08:49,030
pode ser um bom candidato

261
00:08:50,100 --> 00:08:52,830
para escalar seu algoritmo para quantidades de dados muito grandes.

262
00:08:53,880 --> 00:08:54,910
Vamos analisar mais um exemplo.

263
00:08:56,020 --> 00:08:58,120
Digamos que eu quisesse usar um dos algoritmos de otimização avançados,

264
00:08:58,410 --> 00:08:59,430
como pos exemplo

265
00:08:59,550 --> 00:09:00,460
o LBFGS ou outros

266
00:09:00,900 --> 00:09:02,960
do gênero,

267
00:09:03,070 --> 00:09:05,190
e digamos que eu queira treinar a regressão logística do algoritmo.

268
00:09:06,080 --> 00:09:08,680
Para isso, precisamos calcular dois valores principais.

269
00:09:09,300 --> 00:09:10,460
Um seria para o algoritmo de

270
00:09:10,960 --> 00:09:13,520
otimização avançada.

271
00:09:14,310 --> 00:09:15,270
Precisamos fornecer

272
00:09:15,530 --> 00:09:17,210
uma função para calcular

273
00:09:17,460 --> 00:09:18,760
a função de custo do objetivo de otimização.

274
00:09:20,220 --> 00:09:21,690
Para a regressão logística,

275
00:09:21,820 --> 00:09:22,870
a função de custo

276
00:09:23,660 --> 00:09:24,700
tem essa soma sobre

277
00:09:24,960 --> 00:09:26,340
o conjunto de treino,

278
00:09:26,970 --> 00:09:28,980
então ao paralelizar para

279
00:09:29,110 --> 00:09:29,970
10 máquinas, você dividiria

280
00:09:30,310 --> 00:09:31,640
o conjunto de treino em

281
00:09:31,910 --> 00:09:33,150
10 partes e enviar uma para

282
00:09:33,360 --> 00:09:35,380
cada uma das 10 máquinas

283
00:09:35,860 --> 00:09:37,460
para calcular a soma

284
00:09:37,760 --> 00:09:38,660
sobre apenas um décimo dos dados

285
00:09:40,370 --> 00:09:40,370
dimensões.

286
00:09:40,670 --> 00:09:41,550
Depois, a ooutra coisa que os

287
00:09:42,110 --> 00:09:43,400
algoritmos avançados precisam

288
00:09:43,660 --> 00:09:44,790
é a rotina para calcular

289
00:09:45,190 --> 00:09:47,160
as derivadas parciais.

290
00:09:47,280 --> 00:09:48,980
Novamente, as derivadas

291
00:09:49,100 --> 00:09:50,350
para a regressão logística

292
00:09:50,540 --> 00:09:51,840
podem ser escritas como a soma

293
00:09:52,010 --> 00:09:53,130
sobre o conjunto de treino e,

294
00:09:53,330 --> 00:09:54,600
novamente, como no exemplo anterior,

295
00:09:54,950 --> 00:09:56,060
você teria cada

296
00:09:56,520 --> 00:09:57,800
máquina calculando a soma

297
00:09:58,800 --> 00:10:01,170
de apenas uma fração pequena do conjunto de treino.

298
00:10:02,440 --> 00:10:04,590
Finalmente, tendo calculado

299
00:10:05,050 --> 00:10:06,260
tudo isso, elas poderiam

300
00:10:06,400 --> 00:10:07,520
enviar os resultados para

301
00:10:07,680 --> 00:10:09,400
o servidor central,

302
00:10:09,640 --> 00:10:12,760
que iria juntar todas as somas parciais.

303
00:10:13,320 --> 00:10:14,410
Isso corresponde a somar

304
00:10:14,500 --> 00:10:17,000
aquelas variáveis temp^i

305
00:10:17,550 --> 00:10:21,880
ou temp^i_j,

306
00:10:22,100 --> 00:10:23,610
que seriam calculadas na

307
00:10:23,980 --> 00:10:25,390
máquina i,

308
00:10:25,420 --> 00:10:26,800
de forma que o servidor central

309
00:10:27,050 --> 00:10:28,220
somaria isso e encontraria

310
00:10:28,450 --> 00:10:30,230
a função de custo

311
00:10:30,870 --> 00:10:32,750
e as derivadas parciais

312
00:10:33,390 --> 00:10:35,710
que seriam repassadas para o algoritmo de otimização.

313
00:10:36,890 --> 00:10:38,100
De maneira mais geral,

314
00:10:39,080 --> 00:10:40,790
ao escrevermos outros algoritmos

315
00:10:41,020 --> 00:10:42,430
de aprendizado na forma de

316
00:10:42,720 --> 00:10:43,800
uma somatória ou como

317
00:10:44,340 --> 00:10:45,660
termos que calculam

318
00:10:45,990 --> 00:10:47,100
somas sobre os conjuntos de dados,

319
00:10:47,740 --> 00:10:49,290
pode-se usar MapReduce para

320
00:10:49,440 --> 00:10:51,420
paralelizar outros algoritmos de aprendizado,

321
00:10:51,710 --> 00:10:53,310
e escalá-los para conjuntos enormes.

322
00:10:54,340 --> 00:10:55,850
Um último comentário,

323
00:10:56,390 --> 00:10:57,170
até agora temos discutido

324
00:10:57,510 --> 00:10:59,630
algoritmos de MapReduce para

325
00:10:59,850 --> 00:11:01,400
permitir paralelizar através de

326
00:11:02,090 --> 00:11:03,630
vários computadores, talvez

327
00:11:03,940 --> 00:11:05,020
vários em um

328
00:11:05,220 --> 00:11:08,060
cluster de computadores na central de dados.

329
00:11:09,150 --> 00:11:10,580
Acontece que às vezes

330
00:11:10,770 --> 00:11:12,010
mesmo que você só tenha um computador,

331
00:11:13,090 --> 00:11:14,390
o MapReduce pode ser aplicado.

332
00:11:15,530 --> 00:11:16,970
Em particular, muitos dos

333
00:11:17,320 --> 00:11:18,510
computadores de hoje possuem

334
00:11:18,780 --> 00:11:20,520
múltiplos núcleos.

335
00:11:21,170 --> 00:11:21,860
Pode-se ter várias CPU's,

336
00:11:22,180 --> 00:11:23,120
e em cada CPU pode-se

337
00:11:23,240 --> 00:11:26,170
ter múltiplos núcleos.

338
00:11:26,310 --> 00:11:27,170
Se o conjunto de treino for grande,

339
00:11:27,520 --> 00:11:28,460
você poderia,

340
00:11:28,570 --> 00:11:29,540
digamos que você tenha

341
00:11:29,740 --> 00:11:31,520
um computador com 4

342
00:11:31,880 --> 00:11:33,400
núcleos, o que poderia

343
00:11:33,460 --> 00:11:34,390
ser feito é, mesmo sendo

344
00:11:34,550 --> 00:11:35,580
uma única máquina, pode-se dividir

345
00:11:35,760 --> 00:11:37,680
o conjunto entre os diferentes

346
00:11:37,810 --> 00:11:39,140
núcleos presentes nessa caixa,

347
00:11:39,660 --> 00:11:40,960
seja ela um computador normal

348
00:11:41,220 --> 00:11:42,570
ou um servidor e usar

349
00:11:43,240 --> 00:11:45,070
MapReduce para dividir a carga.

350
00:11:45,370 --> 00:11:47,200
Cada um dos núcleos poderia

351
00:11:48,000 --> 00:11:49,010
então fazer a

352
00:11:49,200 --> 00:11:50,240
somatória de, digamos,

353
00:11:50,950 --> 00:11:52,000
um quarto do seu conjunto

354
00:11:52,050 --> 00:11:53,440
de treino, e depois combinar

355
00:11:53,510 --> 00:11:55,090
as somatórias parciais,

356
00:11:55,510 --> 00:11:56,890
de forma a

357
00:11:57,220 --> 00:11:59,360
encontrar a soma para todo o conjunto de treino.

358
00:11:59,750 --> 00:12:01,280
A vantagem de pensar

359
00:12:01,600 --> 00:12:02,880
no MapReduce dessa maneira,

360
00:12:03,350 --> 00:12:04,760
parelelizando em uma única máquina

361
00:12:04,900 --> 00:12:06,720
em vez de paralelizar em

362
00:12:06,910 --> 00:12:08,480
vários computadores é

363
00:12:09,060 --> 00:12:09,970
não precisar se preocupar

364
00:12:10,100 --> 00:12:11,740
com a latência da rede,

365
00:12:12,020 --> 00:12:13,380
por que toda a comunicação,

366
00:12:13,460 --> 00:12:14,810
todo o envio e recebimento

367
00:12:15,890 --> 00:12:18,020
de dados, tudo acontece localmente.

368
00:12:18,420 --> 00:12:20,170
Assim, a latência se torna

369
00:12:20,590 --> 00:12:21,530
um problema muito menor

370
00:12:21,960 --> 00:12:23,050
quando comparado a usar

371
00:12:23,540 --> 00:12:26,080
através de diferentes computadores na central.

372
00:12:27,040 --> 00:12:27,930
Um último detalhe sobre

373
00:12:27,990 --> 00:12:30,740
paralelização em máquinas de vários núcleos.

374
00:12:31,580 --> 00:12:32,600
Dependendo dos detalhes

375
00:12:32,930 --> 00:12:34,290
da sua implementação,

376
00:12:34,610 --> 00:12:35,920
se você tiver uma máquina multi-core,

377
00:12:36,190 --> 00:12:38,130
existem algumas bibliotecas

378
00:12:39,350 --> 00:12:40,490
de álgebra linear que paralelizam

379
00:12:41,490 --> 00:12:43,940
automaticamente suas funções,

380
00:12:44,680 --> 00:12:47,500
utilizando os diversos núcleos.

381
00:12:48,770 --> 00:12:50,140
Se você tiver sorte

382
00:12:50,280 --> 00:12:51,300
de estar usando uma dessas bibliotecas,

383
00:12:51,710 --> 00:12:52,980
já que nem todas

384
00:12:53,640 --> 00:12:55,120
são assim,

385
00:12:55,830 --> 00:12:57,800
se você estiver usando uma dessas bibliotecas

386
00:12:58,200 --> 00:13:00,680
e tiver uma boa implementação vetorizada do algoritmo de aprendizado

387
00:13:01,720 --> 00:13:02,710
às vezes você pode usar

388
00:13:03,160 --> 00:13:05,060
o algoritmo básico na

389
00:13:05,150 --> 00:13:06,460
forma vetorizada e

390
00:13:06,710 --> 00:13:08,630
não se preocupar com a paralelização, pois as bibliotecas

391
00:13:10,030 --> 00:13:12,480
lidariam com isso para você.

392
00:13:12,620 --> 00:13:14,710
Assim você não precisa implementá-las.

393
00:13:14,860 --> 00:13:16,570
Mas para os outros problemas,

394
00:13:17,180 --> 00:13:18,660
usar o princípio do MapReduce,

395
00:13:19,240 --> 00:13:20,690
formular o problema

396
00:13:20,880 --> 00:13:22,070
com MapReduce e paralelizar

397
00:13:22,170 --> 00:13:23,410
através de vários núcleos

398
00:13:23,890 --> 00:13:24,970
pode ser uma boa ideia

399
00:13:25,070 --> 00:13:27,310
e poderia acelerar o algoritmo.

400
00:13:29,860 --> 00:13:31,390
Nesse vídeo, nós falamos

401
00:13:31,730 --> 00:13:33,650
sobre o MapReduce para paralelização

402
00:13:34,460 --> 00:13:35,850
de algoritmos de aprendizado de máquina

403
00:13:36,070 --> 00:13:37,450
ao pegar os dados e espalhá-los

404
00:13:37,830 --> 00:13:39,660
para diversos computadores na central de dados.

405
00:13:40,160 --> 00:13:41,930
E o princípio é o mesmo

406
00:13:42,290 --> 00:13:43,970
para paralelizar usando núcleos

407
00:13:44,290 --> 00:13:45,400
de uma única máquina

408
00:13:46,870 --> 00:13:47,150
também.

409
00:13:47,650 --> 00:13:48,600
Hoje existem boas implementações

410
00:13:49,260 --> 00:13:51,080
open-source de MapReduce,

411
00:13:51,440 --> 00:13:52,210
e existem vários usuários

412
00:13:52,710 --> 00:13:54,480
do sistema open-source

413
00:13:54,890 --> 00:13:55,820
chamado Hadoop e usando

414
00:13:56,010 --> 00:13:57,580
tanto a sua implementação quanto

415
00:13:57,850 --> 00:13:59,770
outra open-source,

416
00:13:59,920 --> 00:14:01,090
você pode usar essas ideias

417
00:14:01,410 --> 00:14:02,730
para paralelizar seus algoritmos

418
00:14:03,540 --> 00:14:04,580
e rodá-los com

419
00:14:04,950 --> 00:14:05,980
bases de dados muitos maiores

420
00:14:06,320 --> 00:14:07,770
do que numa única máquina.