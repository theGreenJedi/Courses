Nos últimos vídeos, falamos sobre Gradiente Descendente Estocástico e outras variações do algoritmo Gradiente Descendente Estocástico, incluindo adaptações para aprendizado dinâmico, mas todos esses algoritmos rodam em apenas uma máquina, em um computador. E alguns problemas de aprendizado são grandes demais para rodar em uma máquina só, as vezes possuindo tantos dados que você nunca iria querer rodar tudo em uma única máquina, não importa qual o algoritmo usado. Nesse video, eu gostaria de discutir um método diferente para aprendizado de máquina em larga escala, chamado de MapReduce. E apesar de termos vários vídeos sobre Gradiente Descendente e que vamos passar menos tempo no MapReduce, não julgue a importância do MapReduce com relação ao Gradiente Descendente com base no tempo que eu gastei em cada método. Muitas Pessoas dirão que MapReduce é no mínimo tão importante quanto, e outras dirão que ainda mais importante que o Gradiente Descendente, apenas mais simples de explicar, que é o porquê de eu gastar menos tempo nisso, mas usando esses princípios você poderia escalar algoritmos de aprendizado para problemas muito maiores do que seria possível com Gradiente Descendente. Aqui está a ideia: Digamos que eu queira ajustar um modelo de regressão linear ou um modelo de regressão logística, começando com o Gradiente Descendente, então essa é sua regra de aprendizado. Para manter a escrita nesse slide legível, vou assumir que "m=400" Claro que para nossos padrões, olhando para problemas de larga escala, esse m é bem pequeno, então isso seria normalmente aplicado para problemas onde você teria algo em torno de 400 milhões de exemplos, mas só para manter o slide simples, vou fingir que temos só 400 exemplos. Então, nesse caso, a regra do Gradiente Descendente tem esse 400 e a essa soma de "i= 1 ... 400" através dos meus 400 exemplos, e se m for grande, esse passo seria muito pesado computacionalmente. O que o MapReduce faz é o seguinte, e devo mencionar que a ideia do MapReduce vem de dois pesquisadores, Jeff Dean e Sanjay Gimawat. Jeff Dean é um dos engenheiros mais lendários no Vale do Silício e construiu grande parte da infraestrutura sobre a qual o Goggle roda hoje. Mas essa é a ideia do MapReduce. Digamos que eu tenha um conjunto de treino, que indicaremos por essa caixa com pares (x, y), onde temos (x1,y1) por todos os meus 400 exemplos, até (xm, ym). Esse é o meu conjunto de treino No MapReduce, dividiríamos esse conjunto em diferentes subconjuntos. Argumentarei que assumir que possuo 4 computadores, ou 4 máquinas para rodar em paralelo no meu conjunto, que é o porquê de eu dividir isso em 4 partes. Se tivéssemos 10 máquinas ou 100 máquinas, então dividiríamos nosso conjunto de treino em 10 partes ou 100 partes. A primeira das minhas 4 máquinas irá usar um quarto do meu conjunto de treino, ou seja, 100 exemplos. E, particularmente, ela irá olhara para essa soma e calcular a soma para os primeiros 100 exemplos. Vou escrever isso. Vou calcular uma variável "temp" sobrescrito 1, pois é a primeira máquina, igual a soma de 1 até 100, e vou inserir aquele mesmo termo de cima, (h(x^i)-y(i))*x^i_j (h(x^i)-y(i))*x^i_j Este é só o o termo do Gradiente Descendente ali em cima. E, similarmente, vou pegar o segundo quarto dos meus dados e enviar para minhas segunda máquina, e minha segunda máquina vai usar os exemplos de 101 até 200, e calcular variáveis similares, no caso tempˆ2_j, que é a mesma soma para exemplos 101 a 200 Da mesma forma, as máquinas 3 e 4 usarão o terceiro e quarto quartos do meu conjunto de treino. Agora cada máquina precisa somar 100 números em vez de 400, realizando apenas um quarto do trabalho e teoricamente terminando quatro vezes mais rápido. Finalmente, depois de todas as máquinas terem terminado de calcular, Vou pegar as variáveis temp e juntá-las. Eu pego essas variáveis e envio todas para um servidor mestre central, onde esse mestre irá combinar esse valores. Em particular, atualizar meus parâmetros θj como θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 θj = θj  - α * ( (tempˆ1_j) + (tempˆ2_j) + (tempˆ3_j) + (tempˆ4_j) ) / 400 para j = (0....n), onde "n" é o número de variáveis Espero que o cálculo desta expressão esteja claro. O que ela faz é examente o mesmo que ter um servidor central que pega os resultados tempˆ1_j, tempˆ2_j, tempˆ3_j e tempˆ4_j, e soma eles, esses quatro valores. Isso é somente a soma disso, mais isso, mais a soma disso, mais a soma disso, e esses quatro valores somados são iguais à regra original do Gradiente Descendente. Depois temos α/400 aqui, e α / 400 aqui também, e isso é exatamente igual ao Gradiente Descendente, só que ao invés de somarmos dos os 400 exemplos de treino, em apenas uma máquina, podemos ao invés dividir o trabalho em 4 máquinas. Esse é o princípio geral que está por trás do MapReduce. Se tivermos alguns conjuntos de treino, e queremos paralelizar usando 4 máquinas, vamos pegar o conjunto de treino e dividi-lo igualmente. Dividir o mais igualmente possível em 4 subconjuntos. Então pegamos esses 4 subconjuntos e enviamos para 4 computadores diferentes. E cada um desses 4 computadores pode calcular a soma de apenas um quarto do conjunto de dados, e finalmente pegar os resultados de cada computador, e enviar para um servidor central, que irá calcular o resultado. Na última linha do exemplo anterior, o trabalho pesado do Gradiente Descendente foi calcular a soma com i=1...400 dos valores, ou genericamente, a soma de i=1...m da fórmula do Gradiente Descendente. Agora, como cada um dos quatro computadores precisa realizar um quarto do trabalho, a velocidade pode ser 4x maior. Se não existissem delays por causa da rede e sem custos de comunicação para enviar e receber dados, a velocidade poderia ser 4x maior. Claro que na prática, devido à latência na rede, o sobrecusto de combinar os resultados no final e outros fatores, a velocidade não chega a ser 4x maior. Mas de qualquer maneira, o MapReduce oferece uma maneira de processar bases de dados muitos maiores do que o possível com um computador. Se você pensa em aplicar MapReduce para algum algoritmo de aprendizado, para acelerar o processo, parelelizando o cálculo através de vários computadores, a questão chave é se perguntar: "Será que é possível expressar o algoritmo" "como uma soma sobre o conjunto de treino"? E na verdade muitos algoritmos de aprendizado podem ser expressados como a soma de funções sobre o conjunto de treino, e o custo de rodá-los com grandes conjuntos de dados é muito alto por operarem sobre uma grande quantidade de exemplos. Então, sempre que seu algoritmo de treino puder ser escrito como uma soma no conjunto de treino e quando o grosso do trabalho do algoritmo de aprendizado pode ser escrito como uma soma, então MapReduce pode ser um bom candidato para escalar seu algoritmo para quantidades de dados muito grandes. Vamos analisar mais um exemplo. Digamos que eu quisesse usar um dos algoritmos de otimização avançados, como pos exemplo o LBFGS ou outros do gênero, e digamos que eu queira treinar a regressão logística do algoritmo. Para isso, precisamos calcular dois valores principais. Um seria para o algoritmo de otimização avançada. Precisamos fornecer uma função para calcular a função de custo do objetivo de otimização. Para a regressão logística, a função de custo tem essa soma sobre o conjunto de treino, então ao paralelizar para 10 máquinas, você dividiria o conjunto de treino em 10 partes e enviar uma para cada uma das 10 máquinas para calcular a soma sobre apenas um décimo dos dados dimensões. Depois, a ooutra coisa que os algoritmos avançados precisam é a rotina para calcular as derivadas parciais. Novamente, as derivadas para a regressão logística podem ser escritas como a soma sobre o conjunto de treino e, novamente, como no exemplo anterior, você teria cada máquina calculando a soma de apenas uma fração pequena do conjunto de treino. Finalmente, tendo calculado tudo isso, elas poderiam enviar os resultados para o servidor central, que iria juntar todas as somas parciais. Isso corresponde a somar aquelas variáveis temp^i ou temp^i_j, que seriam calculadas na máquina i, de forma que o servidor central somaria isso e encontraria a função de custo e as derivadas parciais que seriam repassadas para o algoritmo de otimização. De maneira mais geral, ao escrevermos outros algoritmos de aprendizado na forma de uma somatória ou como termos que calculam somas sobre os conjuntos de dados, pode-se usar MapReduce para paralelizar outros algoritmos de aprendizado, e escalá-los para conjuntos enormes. Um último comentário, até agora temos discutido algoritmos de MapReduce para permitir paralelizar através de vários computadores, talvez vários em um cluster de computadores na central de dados. Acontece que às vezes mesmo que você só tenha um computador, o MapReduce pode ser aplicado. Em particular, muitos dos computadores de hoje possuem múltiplos núcleos. Pode-se ter várias CPU's, e em cada CPU pode-se ter múltiplos núcleos. Se o conjunto de treino for grande, você poderia, digamos que você tenha um computador com 4 núcleos, o que poderia ser feito é, mesmo sendo uma única máquina, pode-se dividir o conjunto entre os diferentes núcleos presentes nessa caixa, seja ela um computador normal ou um servidor e usar MapReduce para dividir a carga. Cada um dos núcleos poderia então fazer a somatória de, digamos, um quarto do seu conjunto de treino, e depois combinar as somatórias parciais, de forma a encontrar a soma para todo o conjunto de treino. A vantagem de pensar no MapReduce dessa maneira, parelelizando em uma única máquina em vez de paralelizar em vários computadores é não precisar se preocupar com a latência da rede, por que toda a comunicação, todo o envio e recebimento de dados, tudo acontece localmente. Assim, a latência se torna um problema muito menor quando comparado a usar através de diferentes computadores na central. Um último detalhe sobre paralelização em máquinas de vários núcleos. Dependendo dos detalhes da sua implementação, se você tiver uma máquina multi-core, existem algumas bibliotecas de álgebra linear que paralelizam automaticamente suas funções, utilizando os diversos núcleos. Se você tiver sorte de estar usando uma dessas bibliotecas, já que nem todas são assim, se você estiver usando uma dessas bibliotecas e tiver uma boa implementação vetorizada do algoritmo de aprendizado às vezes você pode usar o algoritmo básico na forma vetorizada e não se preocupar com a paralelização, pois as bibliotecas lidariam com isso para você. Assim você não precisa implementá-las. Mas para os outros problemas, usar o princípio do MapReduce, formular o problema com MapReduce e paralelizar através de vários núcleos pode ser uma boa ideia e poderia acelerar o algoritmo. Nesse vídeo, nós falamos sobre o MapReduce para paralelização de algoritmos de aprendizado de máquina ao pegar os dados e espalhá-los para diversos computadores na central de dados. E o princípio é o mesmo para paralelizar usando núcleos de uma única máquina também. Hoje existem boas implementações open-source de MapReduce, e existem vários usuários do sistema open-source chamado Hadoop e usando tanto a sua implementação quanto outra open-source, você pode usar essas ideias para paralelizar seus algoritmos e rodá-los com bases de dados muitos maiores do que numa única máquina.