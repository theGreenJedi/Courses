ここ数回のビデオでは、 確率的最急降下法についてや、 その他の確率的最急降下法の変種について ーーオンライン学習アルゴリズムの適用などーー 議論して来た。 だがそれらは全て 一つのマシン、または一つのコンピュータで 実行出来る物だった。 幾つかの機械学習の問題は 一つのマシンで実行するには あまりにも大きくて、 時には単純にデータがあまりにも大きい為に 一つのコンピュータで走らせてみようとは 考えもしないような状況もあるかもしれない、 そのコンピュータでどんなアルゴリズムを使うにせよ、だ。 そこでこのビデオでは、 大規模機械学習における、 異なるアプローチである所の、Map Reduceアプローチと呼ばれる物を 議論したい。 我らは確率的最急降下法について 結構たくさんのビデオを費やし Map Reduceに関しては 相対的にはちょっとしか扱わないが、 それを持って確率的最急降下法に比べると Map Reduceはそんなに重要では無い、とは 判断しないでもらいたい、 このそれぞれのアイデアに関して私が費やす 時間を元に判断するのは。 多くの人々が、Map Reduceは 少なくとも同程度には、 そして人によってはもっと重要な アイデアだと言うだろう、確率的最急降下法と 比較した時に。 単にMap Reduceは相対的には説明するのが簡単なだけ、 その為にそれについて あんまり時間を使わないだけだ。 だがこのアイデアを用いる事で 確率的最急降下法を用いる事で可能な範囲よりも 学習アルゴリズムを ずっとスケールさせる事が 出来るかもしれない。 そのアイデアとはこうだ。 例えば線形回帰とか ロジスティック回帰とか、とにかくその辺のモデルに フィッティングしたい、としよう。 ここではバッチ最急降下法から 始める事としよう。 これが我らのバッチ最急降下法のルールとなる。 そしてこのスライドの記述が 追いやすいように、 ここではm=400の手本と仮定して進めていく事にする。 もちろん、我らの基準からすると 大規模スケールの機械学習という 観点からすれば、mは極めて小さい、と 言えるだろう。 どちらかといえばより一般的な状況としては 4億個とかそれに近い数字の方が より典型的な 数字と言える。 だが、スライドの記述を シンプルにする為に、 我らの手持ちの手本が400個であるフリをしてみよう。 その場合、 バッチ最急降下法の学習ルールは この400個に対して行われ、 和をi=1から400までの この400個の手本に渡って取る、 もしmが大きければこれは、 計算量的に高くつくステップとなる。 そこでMapReduceのアイデアが行う事は 以下のような事だ。 ここでMap Reduceのアイデアは 二人の研究者による物だと言及しておくべきだろう、 Jeff Deanと Sanjay Gimawatだ。 ところでJeff Deanは シリコンバレー中でも もっとも伝説的なエンジニアの一人で、 今日Googleで動いている アーキテクチャ的なインフラのかなりの部分を 創り上げた男だ。 話を戻して、これがMap Reduceというアイデアだ。 あるトレーニングセットが あるとして、 この箱でxyのペアを 表すとして、 これはx1, y1から400個の手本まで xm, ymまで 降りていく。 つまりこれがトレーニングセットで400個の手本がある。 Map Reduceのアイデアでは、一つのやり方としては、 このトレーニングセットを 別々のサブセットに分割する。 今回の例ではマシンは 4台だと 想定していこう。 言い換えると4台のマシンがトレーニングセットに渡って 並列に走る。 そんな訳だから4台のマシンに分割した。 もしあなたの手持ちが10台のマシンだったり 100台のマシンなら、その場合は それに応じてトレーニングセットを10個とか100個とか、持ってるマシンの台数に応じて分割する事になる。 そして4台のマシンの最初の一台が やるべき事は、 トレーニングセットのうちの 最初の1/4を用いて つまり最初の100個のトレーニング手本を用いて、 具体的に言うと、それがやる事は この和を見てくれ。 そして最初の100個のトレーニング手本に対して この和を計算する事だ。 書きだしてみよう。 temp上付き添字1 jという変数を 計算してみる。この1は最初のマシンを表す。 これはイコール 和を取る事の 1から100までの ここでここにある項を 代入する。つまり、 h シータのxi 引く事の yi、 掛けるx ij。いい？ つまりこれは単なる ここの最急降下法の項だ。 そして次に、同様に、 二番目の1/4のデータを 取ってきて、それを二番目のマシンに 送りつける。 二番目のマシンはトレーニング手本の 101番目から200番目を使う。 そして同様の値、temp(2) jを 計算する。それは同様の和を インデックスが101から200までに対する トレーニングセットに対し取った物だ。 そして以下同様にマシン3と4は トレーニングセットの 三番目の1/4と、四番目の1/4を 使う事になる。 つまりいまや、各マシンは 400に渡る和では無くて 100手本に対する和だけとなる。 つまりやらなくてはならない仕事が1/4となり、 つまり4倍早く終えられる事が 期待される。 最後に、これら全てのマシンが 仕事を終えたら、 これらのtemp変数を取り出して 一つに戻さないといけない。 だからこれらの変数を 中央のサーバーに 送りつける。 そしてマスターサーバーがやる事は これらの結果を一つに結合する事。 具体的には、パラメータのシータjを シータjを、以下のように更新する： シータj引く事の 学習率の アルファ 掛ける事の 1/400 掛ける事の temp (i) j 足す事の temp (2) j 足す事の temp (3) j 足すことのtemp(4) j。 そしてもちろん、これをj=0から nまで、nはフィーチャーの数だが、 それらのjに対してそれぞれ行わなくてはならない。 こんな風に複数の等式に分割出来る。 この式がやってる事は、 完全にこれと同じだ。 中央のマスターサーバーが これらの結果を 受け取り、つまりtemp 1j、 temp 2j、 temp 3j、temp 4jと、 それらを足し合わせると つまり当然これら4つの 和となる訳だ。 いいかい？これは単に これ、足す、 この和、足す、 この和、足す、 この和。そしてこれら4つの物を 足しあわせると、 この和と等しくなる。これは もともとのバッチ最急降下法で計算していた和だ。 そしてそこに アルファ 掛けることの 1/400、 アルファ掛ける1/400。 そしてこれは厳密に バッチ最急降下法と 等価である。 ただ、400個のトレーニング手本にたいして 和を取らなくては いけなかった代わりに、 ワークロードを4つのマシンに 分割出来るようになっている。 さて、これは、一般的なMap Reduceの テクニックがどんなかを表した図だ。 あるトレーニングセットがあって、 それを4つのマシンに渡って並列化したいとすると、 トレーニングセットを 同じサイズに分割する、 4つのサブセットに等しく分割する。 そして次に、この4つのトレーニングデータのサブセットを 4つの別々のコンピュータに送る。 そして4つのコンピュータはおのおの、 1/4のトレーニングセットについてだけ 和を計算する事が出来る。 そして次に、 最終的に各コンピュータの結果を 取り出して、中央のサーバーに 送る。そしてそこで結果を一つに結合する。 前のスライドの例では 最急降下法の 仕事の大半は、 i=1から400までの何かの和を 計算する事だった。 より一般的に言うと、 最急降下法の式の i=1からmまでの和。 そしてここで、各4つのコンピュータは その仕事の1/4しか 行わないので、 潜在的には4倍のスピードアップの可能性がある。 特に、もし仮に ネットワークの遅延も無く データをあちこちに送るのに ネットワークのコミュニケーションのコストも 存在しないとすると、 4倍のスピードアップの可能性がある。 もちろん、現実には、 ネットワークのレイテンシもあるし、 結果を結合するオーバーヘッドもあるし、 その他のファクターもあるので、 実際には4倍のスピードアップよりはわずかに少ないだろう。 だが、それにも関わらず、 この種のMap Reduceのアプローチは 単体のコンピュータのみを使う事に比べると より大きなデータセットを処理する方法を 提供してくれるアプローチと言える。 もしあなたがある学習アルゴリズムを 複数のコンピュータに渡って 計算を並列化する事で スピードアップをする為に Map Reduceを適用することを検討している時は、 自身に問うてみるべき鍵となる問いは、 あなたの使おうとしている学習アルゴリズムは トレーニングセットに渡る和の形で表現出来るのか？という事だ。 そして多くの学習アルゴリズムは 実際にトレーニングセットに渡って 関数の和を取る形に表現出来る事が 分かっている。 そして大きなデータセットに対して それらを走らせる時の計算量のコストは とても大きなトレーニングセットに渡って和を取る必要がある事に起因している。 だから、あなたの学習アルゴリズムがなんであれ、 トレーニングセットに渡る和として 表現する事が出来て、 そして学習アルゴリズムの 仕事の大部分が トレーニングセットに渡る和として 表現出来れば、 MapReduceはあなたの学習アルゴリズムを とても大きなデータセットに対してスケールさせてくれる為の、とても有力な候補となる。 もう一つ例を見てみよう。 アドバンスドな最適化のアルゴリズムの一つを使いたいとしよう。 つまり、L-BFGSとか conjugate gradientとか そういう奴。 そして、ロジスティック回帰をそのアルゴリズムを使って訓練したいとする。 その為には、我らは二つの主な値を計算する必要がある。 一つめは、 L-BFGSやconjugate gradientのような最適化アルゴリズムに対して 我らは最適化の目的関数である、 コスト関数を計算するルーチンを 渡してやらないといけない。 そしてロジスティック回帰においては コスト関数はこんな感じの物を トレーニングセットに渡って 和を取る物だった。 すると10台のマシンに渡って 並列化したいなら、トレーニングセットを 10分割して、10台のマシンに割り振り、 そして10台の各マシンが、 トレーニングデータの 十分の一に渡って この量の和を 計算する。 次に、他にアドバンスドな最適化アルゴリズムが 必要としている物としては、 これらの偏微分項を計算する ルーチンだ。 ふたたび、これらの微分項は、 ロジスティック回帰なら、 トレーニングセットの和として 表現する事が出来て、だからふたたび、 前の例と同様に、 各マシンに、トレーニングデータの 少しの部分ずつだけを 計算させる事が出来る。 そして最後に、これらを全て 計算し終えたら、各マシンが その結果を中央のサーバーに 送りつける。 そしてそこで部分和を足し合わせる事が出来る。 これはこれらのtemp i、 あるいはtemp ij変数を 足し合わせる事に対応する、 ここでiはマシン番号iを表し、そのマシンローカルで 計算された物。 つまり中央のサーバーはこれらの物を 足し合わせる事が出来て、 それで全体のコスト関数を得る事が出来る、 そこから全体の偏微分項が得られて、 それをアドバンスドな最適化アルゴリズムに渡す事が出来る。 より一般的に言うと、 その他の学習アルゴリズムでも、 それらを和の形に 表現すれば、あるいは トレーニングセットに渡る関数の和の 形に表現すれば、 その他のアルゴリズムでもMap Reduceのテクニックを用いて 同様に並列化する事が出来る。 そしてとても大きなトレーニングセットに対してスケールさせる事が出来る。 最後に、一つコメントを。 ここまで我らは Map Reduceのアルゴリズムを 複数のコンピューターに渡って、 またはコンピュータクラスタの複数のコンピュータ、 またはデータセンターの複数のコンピュータに渡って 並列化する為の物として議論してきた。 だが時には、一つしかコンピュータが無くても Map Reduceが適応可能な場合が ある事が分かっている。 具体的には、こんにちの多くの コンピュータは、 複数のプロセッサコアを持っている。 複数のCPUを持っている事があり得るし、 各CPU内にも 複数のプロセッサコアがある場合もある。 もし大きなトレーニングセットがある時に、 あなたがとれる手段としては、 もしあなたの手元に 4つの計算コアを持つ コンピュータがあったとすると、 それが一つのコンピュータでしか無かったとしても、 トレーニングセットを 複数のピースに分割して 一つのマシンの中の別々のコアに トレーニングセットを送り込む、という事が出来る。 一つのマシンとは一つのデスクトップコンピュータかもしれないし、 一つのサーバーかもしれない。 そしてMap Reduceをこんな風に使う事で、ワークロードを分散出来る。 そして各コアはトレーニングセットの 1/4に渡る和を 実行出来る。 そして次にそれらの部分和を 取ってきて結合する事が出来る、 トレーニングセット全体に渡る 和を得る為に。 Map Reduceをこんな風に、 一つのマシンの中の複数コアに対する並列化と 考えるメリットは、複数のマシンに渡る 並列化として 考えることに比べて、 ネットワークのレイテンシを 気にする必要が無くなる、という事がある。 何故なら全てのコミュニケーションは、 temp j変数を行ったり来たり送る事は、 それらは全て一つのマシン内で起こる事だから。 だからデータセンター内の別々のマシンを 使う事と比べると、 ネットワークのレイテンシは より重要度が低くなる。 最後にマルチコアのマシンでの 並列化の落とし穴の最後の一つを挙げておこう。 実装の詳細によっては、 もしマルチコアのマシンがあって、 もしある種の数値計算線形代数ライブラリが あるなら、 幾つかの数値計算線形代数ライブラリは 線形代数計算を自動的にマシン内の複数コアに 並列化する物がある。 つまり、それらの線形代数数値計算ライブラリの 一つを使えるような程度の幸運に恵まれたなら、 そしてこれは確かに どのライブラリでも適用出来るという訳でも無いが、 もしあなたがそれらのライブラリの一つを使っていて そしてとても良いベクトル化した実装の学習アルゴリズムを用いているなら、 ただ標準的な学習アルゴリズムを ベクトル化した形で 実装するだけで、 そして並列化について思い煩う事無く、数値計算線形代数ライブラリが そのうちのいくらかをあなたの代わりに受け持ってくれる。 だからMap Reduceで実装する必要は無い。 だがそれ以外の学習問題では、 この種のMap Reduceの実装を有効利用する事により、 このMap Reduceの定式化を用いる事で 明示的に複数コアに渡る並列化を 自分自身で行う事もまた 同様に良いアイデアだと思う事もあるだろう、 そしてそれを用いてあなたの学習アルゴリズムを高速化する事が可能かもしれない。 このビデオでは、機械学習を 並列化するためのアプローチとして Map Reduceを議論する。 データセンターのたくさんのマシンに対して データをばらまくやり方だ。 だが、このアイデアは 一つのマシン内の複数コアで 並列化する為にも 極めて重要な物である。 こんにちでは、Map Reduceの とても良いオープンソース実装がある。 Hadoopと呼ばれる オープンソースのシステムには たくさんのユーザーが居る。だから、自分の 独自実装を使うにせよ誰かの作った オープンソース実装を使うにせよ、 これらのアイデアを用いて 学習アルゴリズムを並列化する事が出来て、 それらを一つのマシンだけを使う場合と比べたら より大きなデータセットに対して 走らせる事が可能となる。