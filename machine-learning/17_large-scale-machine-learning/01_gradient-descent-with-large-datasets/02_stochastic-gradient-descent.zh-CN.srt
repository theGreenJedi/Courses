1
00:00:00,251 --> 00:00:05,622
对于很多机器学习算法 包括线性回归、逻辑回归、神经网络等等

2
00:00:05,622 --> 00:00:11,955
算法的实现都是通过得出某个代价函数 或者某个最优化的目标来实现的

3
00:00:11,955 --> 00:00:16,476
然后使用梯度下降这样的方法来求得代价函数的最小值

4
00:00:16,476 --> 00:00:22,461
当我们的训练集较大时 梯度下降算法则显得计算量非常大

5
00:00:22,461 --> 00:00:29,300
在这段视频中 我想介绍一种跟普通梯度下降不同的方法 随机梯度下降(stochastic gradient descent)

6
00:00:29,300 --> 00:00:37,841
用这种方法我们可以将算法运用到较大训练集的情况中

7
00:00:37,841 --> 00:00:41,928
假如你要使用梯度下降法来训练某个线性回归模型

8
00:00:41,928 --> 00:00:48,055
简单复习一下 我们的假设函数是这样的

9
00:00:48,055 --> 00:00:54,459
代价函数是你的假设在训练集样本上预测的平均平方误差的二分之一倍的和

10
00:00:54,459 --> 00:00:59,705
通常我们看到的代价函数都是像这样的弓形函数

11
00:00:59,705 --> 00:01:06,659
因此 画出以θ0和θ1为参数的代价函数J 就是这样的弓形函数

12
00:01:06,659 --> 00:01:10,999
这就是梯度下降算法 在内层循环中

13
00:01:10,999 --> 00:01:15,594
你需要用这个式子反复更新参数θ的值

14
00:01:15,594 --> 00:01:22,574
在这段视频剩下的时间里 我将依然以线性回归为例

15
00:01:22,574 --> 00:01:29,371
但随机梯度下降的思想也可以应用于其他的学习算法

16
00:01:29,371 --> 00:01:38,011
比如逻辑回归、神经网络或者其他依靠梯度下降来进行训练的算法中

17
00:01:38,011 --> 00:01:43,236
这张图表示的是梯度下降的做法 假设这个点表示了参数的初始位置

18
00:01:43,236 --> 00:01:50,072
那么在你运行梯度下降的过程中 多步迭代最终会将参数锁定到全局最小值

19
00:01:50,072 --> 00:01:55,193
迭代的轨迹看起来非常快地收敛到全局最小

20
00:01:55,193 --> 00:01:59,561
而梯度下降法的问题是 当m值很大时

21
00:01:59,561 --> 00:02:08,382
计算这个微分项的计算量就变得很大 因为需要对所有m个训练样本求和

22
00:02:08,382 --> 00:02:15,644
所以假如m的值为3亿 美国就有3亿人口

23
00:02:15,644 --> 00:02:20,783
美国的人口普查数据就有这种量级的数据记录

24
00:02:20,783 --> 00:02:26,715
所以如果想要为这么多数据拟合一个线性回归模型的话 那就需要对所有这3亿数据进行求和

25
00:02:26,715 --> 00:02:36,385
这样的计算量太大了 这种梯度下降算法也被称为批量梯度下降(batch gradient descent)

26
00:02:36,385 --> 00:02:41,352
“批量”就表示我们需要每次都考虑所有的训练样本

27
00:02:41,352 --> 00:02:44,303
我们可以称为所有这批训练样本

28
00:02:44,303 --> 00:02:51,853
也许这不是个恰当的名字 但做机器学习的人就是这么称呼它的

29
00:02:51,853 --> 00:02:57,157
想象一下 如果你真的有这3亿人口的数据存在硬盘里

30
00:02:57,157 --> 00:03:05,945
那么这种算法就需要把所有这3亿人口数据读入计算机 仅仅就为了算一个微分项而已

31
00:03:05,945 --> 00:03:11,508
你需要将这些数据连续传入计算机 因为计算机存不下那么大的数据量

32
00:03:11,508 --> 00:03:16,425
所以你需要很慢地读取数据 然后计算一个求和 再来算出微分

33
00:03:16,425 --> 00:03:21,452
所有这些做完以后 你才完成了一次梯度下降的迭代

34
00:03:21,452 --> 00:03:24,749
然后你又需要重新来一遍

35
00:03:24,749 --> 00:03:28,424
也就是再读取这3亿人口数据 做个求和

36
00:03:28,424 --> 00:03:32,578
然后做完这些 你又完成了梯度下降的一小步

37
00:03:32,578 --> 00:03:36,959
然后再做一次 你得到第三次迭代 等等

38
00:03:36,959 --> 00:03:40,819
所以 要让算法收敛 绝对需要花很长的时间

39
00:03:40,819 --> 00:03:45,375
相比于批量梯度下降 我们介绍的方法就完全不同了

40
00:03:45,375 --> 00:03:50,465
这种方法在每一步迭代中 不用考虑全部的训练样本

41
00:03:50,465 --> 00:03:55,118
只需要考虑一个训练样本

42
00:03:55,118 --> 00:03:59,617
在开始介绍新的算法之前 我把批量梯度下降算法再写在这里

43
00:03:59,617 --> 00:04:05,794
这里是代价函数 这里是迭代的更新过程

44
00:04:05,794 --> 00:04:10,678
梯度下降法中的这一项

45
00:04:10,678 --> 00:04:17,933
是最优化目标 代价函数Jtrain(θ) 关于参数θj的偏微分

46
00:04:17,933 --> 00:04:23,386
下面我们来看对大量数据来说更高效的这种方法

47
00:04:23,386 --> 00:04:26,489
为了更好地描述随机梯度下降算法

48
00:04:26,489 --> 00:04:32,657
代价函数的定义有一点区别 我们定义参数θ

49
00:04:32,657 --> 00:04:40,471
关于训练样本(x(i),y(i))的代价 等于二分之一倍的

50
00:04:40,471 --> 00:04:44,791
我的假设h(x(i))跟实际输出y(i)的误差的平方

51
00:04:44,791 --> 00:04:53,386
因此这个代价函数值实际上测量的是我的假设在某个样本(x(i),y(i))上的表现

52
00:04:53,386 --> 00:05:01,010
你可能已经发现 总体的代价函数Jtrain可以被写成这样等效的形式

53
00:05:01,010 --> 00:05:09,606
Jtrain(θ)就是我的假设函数 在所有m个训练样本中的每一个样本(x(i),y(i))上的代价函数的平均值

54
00:05:09,606 --> 00:05:13,522
用这样的方法应用到线性回归中

55
00:05:13,522 --> 00:05:17,636
我来写出随机梯度下降的算法

56
00:05:17,636 --> 00:05:26,940
随机梯度下降法的第一步是将所有数据打乱

57
00:05:26,940 --> 00:05:32,539
我说的随机打乱的意思是 将所有m个训练样本重新排列

58
00:05:32,539 --> 00:05:37,450
这就是标准的数据预处理过程 稍后我们再回来讲

59
00:05:37,450 --> 00:05:42,997
随机梯度下降的主要算法如下

60
00:05:42,997 --> 00:05:48,150
在i等于1到m中进行循环

61
00:05:48,150 --> 00:05:53,067
也就是对所有m个训练样本进行遍历 然后进行如下更新

62
00:05:53,067 --> 00:06:06,523
我们按照这样的公式进行更新 θj等于θj减α乘以h(x(i))减y(i)乘以x(i)j

63
00:06:06,523 --> 00:06:12,961
同样还是对所有j的值进行更新

64
00:06:12,961 --> 00:06:24,708
不难发现 这一项实际上就是我们批量梯度下降算法中 求和式里面的那一部分

65
00:06:24,708 --> 00:06:31,256
事实上 如果你数学比较好的话 你可以证明这一项 也就是这一项

66
00:06:31,256 --> 00:06:43,511
是等于这个cost函数关于参数θj的偏微分

67
00:06:43,511 --> 00:06:47,383
这个cost函数就是我们之前先定义的代价函数

68
00:06:47,383 --> 00:06:52,081
最后画上大括号结束算法的循环

69
00:06:52,081 --> 00:06:59,365
随机梯度下降的做法实际上就是扫描所有的训练样本

70
00:06:59,365 --> 00:07:04,349
首先是我的第一组训练样本(x(1),y(1))

71
00:07:04,349 --> 00:07:09,399
然后只对这第一个训练样本 对它的代价函数

72
00:07:09,399 --> 00:07:13,725
计算一小步的梯度下降

73
00:07:13,725 --> 00:07:15,717
换句话说 我们要关注第一个样本

74
00:07:15,717 --> 00:07:21,214
然后把参数θ稍微修改一点 使其对第一个训练样本的拟合变得好一点

75
00:07:21,214 --> 00:07:29,244
完成这个内层循环以后 再转向第二个训练样本

76
00:07:29,244 --> 00:07:33,848
然后还是一样 在参数空间中进步一小步

77
00:07:33,848 --> 00:07:39,682
也就是稍微把参数修改一点 然后让它对第二个样本的拟合更好一点

78
00:07:39,682 --> 00:07:44,130
做完第二个 再转向第三个训练样本

79
00:07:44,130 --> 00:07:51,722
同样还是修改参数 让它更好的拟合第三个训练样本

80
00:07:51,722 --> 00:07:55,114
以此类推 直到完成所有的训练集

81
00:07:55,114 --> 00:08:01,297
然后外部这个重复循环会多次遍历整个训练集

82
00:08:01,297 --> 00:08:07,346
从这个角度分析随机梯度下降算法 我们能更好地理解为什么一开始要随机打乱数据

83
00:08:07,346 --> 00:08:10,772
这保证了我们在扫描训练集时

84
00:08:10,772 --> 00:08:15,197
我们对训练集样本的访问是随机的顺序

85
00:08:15,197 --> 00:08:21,229
不管你的数据是否已经随机排列过 或者一开始就是某个奇怪的顺序

86
00:08:21,229 --> 00:08:26,391
实际上这一步能让你的随机梯度下降稍微快一些收敛

87
00:08:26,391 --> 00:08:30,985
所以为了保险起见 最好还是先把所有数据随机打乱一下

88
00:08:30,985 --> 00:08:34,056
如果你不知道是否已经随机排列过的话

89
00:08:34,056 --> 00:08:37,240
但随机梯度下降的更重要的一点是

90
00:08:37,240 --> 00:08:45,504
跟批量梯度下降不同 随机梯度下降不需要等到对所有m个训练样本

91
00:08:45,504 --> 00:08:50,624
求和来得到梯度项 而是只需要对单个训练样本求出这个梯度项

92
00:08:50,624 --> 00:08:54,810
我们已经在这个过程中开始优化参数了

93
00:08:54,810 --> 00:09:02,248
就不用等到把所有那3亿的美国人口普查的数据拿来遍历一遍

94
00:09:02,248 --> 00:09:05,632
不需要等到对所有这些数据进行扫描

95
00:09:05,632 --> 00:09:09,947
然后才一点点地修改参数 直到达到全局最小值

96
00:09:09,947 --> 00:09:14,975
对随机梯度下降来说 我们只需要一次关注一个训练样本

97
00:09:14,975 --> 00:09:22,188
而我们已经开始一点点把参数朝着全局最小值的方向进行修改了

98
00:09:22,188 --> 00:09:27,558
这里把这个算法再重新写一遍 第一步是打乱数据

99
00:09:27,558 --> 00:09:35,089
第二步是算法的关键 是关于某个单一的训练样本(x(i),y(i))来对参数进行更新

100
00:09:35,089 --> 00:09:40,139
让我们来看看 这个算法是如何更新参数θ的

101
00:09:40,139 --> 00:09:43,467
之前我们已经看到 当使用批量梯度下降的时候

102
00:09:43,467 --> 00:09:46,331
需要同时考虑所有的训练样本数据

103
00:09:46,331 --> 00:09:53,397
批量梯度下降的收敛过程 会倾向于一条近似的直线 一直找到全局最小值

104
00:09:53,397 --> 00:09:59,956
与此不同的是 在随机梯度下降中 每一次迭代都会更快

105
00:09:59,956 --> 00:10:03,108
因为我们不需要对所有训练样本进行求和

106
00:10:03,108 --> 00:10:07,259
每一次迭代只需要保证对一个训练样本拟合好就行了

107
00:10:07,259 --> 00:10:13,931
所以 如果我们从这个点开始进行随机梯度下降的话

108
00:10:13,931 --> 00:10:19,556
第一次迭代 可能会让参数朝着这个方向移动

109
00:10:19,556 --> 00:10:23,791
然后第二次迭代 只考虑第二个训练样本

110
00:10:23,791 --> 00:10:28,278
假如很不幸 我们朝向了一个错误的方向

111
00:10:28,278 --> 00:10:33,731
第三次迭代 我们又尽力让参数修改到拟合第三组训练样本

112
00:10:33,731 --> 00:10:36,418
可能最终会得到这个方向

113
00:10:36,418 --> 00:10:42,717
然后再考虑第四个训练样本 做同样的事 然后第五第六第七 等等

114
00:10:42,717 --> 00:10:46,725
在你运行随机梯度下降的过程中 你会发现

115
00:10:46,725 --> 00:10:52,923
一般来讲 参数是朝着全局最小值的方向被更新的 但也不一定

116
00:10:52,923 --> 00:11:00,117
所以看起来它是以某个比较随机、迂回的路径在朝全局最小值逼近

117
00:11:00,117 --> 00:11:07,630
实际上 你运行随机梯度下降 和批量梯度下降 两种方法的收敛形式是不同的

118
00:11:07,630 --> 00:11:15,196
实际上随机梯度下降是在某个靠近全局最小值的区域内徘徊

119
00:11:15,196 --> 00:11:18,740
而不是直接逼近全局最小值并停留在那点

120
00:11:18,740 --> 00:11:21,676
但实际上这并没有多大问题

121
00:11:21,676 --> 00:11:26,788
只要参数最终移动到某个非常靠近全局最小值的区域内

122
00:11:26,788 --> 00:11:32,164
只要参数逼近到足够靠近全局最小值 这也会得出一个较为不错的假设

123
00:11:32,164 --> 00:11:36,340
所以 通常我们用随机梯度下降法

124
00:11:36,340 --> 00:11:43,658
也能得到一个很接近全局最小值的参数 对于绝大部分实际应用的目的来说 已经足够了

125
00:11:43,658 --> 00:11:47,121
最后一点细节 在随机梯度下降中

126
00:11:47,121 --> 00:11:51,099
我们有一个外层循环 它决定了内层循环的执行次数

127
00:11:51,099 --> 00:11:53,892
所以 外层循环应该执行多少次呢

128
00:11:53,892 --> 00:11:59,336
这取决于训练样本的大小 通常一次就够了

129
00:11:59,336 --> 00:12:02,064
最多到10次 是比较典型的

130
00:12:02,064 --> 00:12:05,852
所以我们可以循环执行内层1到10次

131
00:12:05,852 --> 00:12:12,309
因此 如果我们有非常大量的数据 比如美国普查的人口数据

132
00:12:12,309 --> 00:12:15,260
我说的3亿人口数据的例子

133
00:12:15,260 --> 00:12:19,609
所以每次你只需要考虑一个训练样本

134
00:12:19,609 --> 00:12:23,073
这里的i就是从1到3亿了

135
00:12:23,073 --> 00:12:25,720
所以可能你每次只需要考虑一个训练样本

136
00:12:25,720 --> 00:12:29,872
你就能训练出非常好的假设

137
00:12:29,872 --> 00:12:36,613
这时 由于m非常大 那么内循环只用做一次就够了

138
00:12:36,613 --> 00:12:43,071
但通常来说 循环1到10次都是非常合理的

139
00:12:43,071 --> 00:12:45,439
但这还是取决于你训练样本的大小

140
00:12:45,439 --> 00:12:49,413
如果你跟批量梯度下降比较一下的话

141
00:12:49,413 --> 00:12:53,905
批量梯度下降在一步梯度下降的过程中

142
00:12:53,905 --> 00:12:57,034
就需要考虑全部的训练样本

143
00:12:57,034 --> 00:13:01,983
所以批量梯度下降就是这样微小的一次次移动

144
00:13:01,983 --> 00:13:05,776
这也是为什么随机梯度下降法要快得多

145
00:13:05,776 --> 00:13:10,880
这就是随机梯度下降了

146
00:13:10,880 --> 00:13:15,594
如果你应用它 应该就能在很多学习算法中应用大量数据了

147
00:13:15,594 --> 99:59:59,000
并且会得到更好的算法表现【教育无边界字幕组】翻译：所罗门捷列夫 校对：竹二个 审核：Naplessss