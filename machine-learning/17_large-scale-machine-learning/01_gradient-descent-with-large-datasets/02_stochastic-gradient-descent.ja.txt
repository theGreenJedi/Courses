多くの学習アルゴリズムにとって、線形回帰やロジスティック回帰やニューラルネットワークなどにとって、 アルゴリズムを導出する方法は、まずコスト関数、または目的関数を考えて、 そしてそれを最急降下法なりのアルゴリズムを使って最小化する、という物だった。 大量のトレーニングセットがある時は、最急降下法は極めて計算的に高価な手続きとなる。 このビデオでは、基本的な最急降下法のアルゴリズムを改変した、確率的最急降下法という物を議論する、 それはこれらのアルゴリズムをもっとビッグなトレーニングセットにスケール出来るようにする。 最急降下法で線形回帰のモデルをトレーニングしているとしよう。 軽く復習しておくと、仮説はこんな形、コスト関数はこんな感じ。 それは1/2の仮説の二乗誤差の平均の、mのトレーニング手本に渡る和を取った物だ。 そしてコスト関数は以前見たように、こんな弓なりの形の関数だ。 パラメータシータ0とシータ1の関数としてプロットすると、コスト関数Jは弓型の関数となる。 そして最急降下法はこんな感じだ。
最急降下法の内側のループでは、 パラメータシータをこの式を使って繰り返しアップデートしていく。 さて、このビデオの残りの部分で、線形回帰を実行出来る例として使い続ける。 だが確率的最急降下法のアイデア自体は完全に一般的な物で、それはその他の学習アルゴリズム、 ロジスティック回帰とかニューラルネットワークとか、その他なんでも特定のトレーニングセットに対して最急降下法で学習するアルゴリズムになら適用出来る。 さて、これは最急降下法が何をするかだ。
もしパラメータがここの点で初期化されたら、 最急降下法はパラメータをグローバル最小へと持っていく。 こんな感じの軌跡を通って、だいたいまっすぐグローバル最小へと向かう。 ここで、最急降下法の問題点としては、もしmが大きい時には、 この微分項を計算するのが、とても高価になってしまう、という事。
何故ならこれは全てのm手本に渡って和を取るから。 だからもしmが3億なら、、、アメリカ合衆国にはだいたい3億人の人がいる。 すると、US、またはアメリカ合衆国の国勢調査のデータは、そんなオーダーの数のレコードとなる。 すると、そこに線形回帰のモデルをフィッティングしたいとすると、3億のレコードに渡って和をとらなくてはならない。 それはとても高価だ。このアルゴリズムに名前をつけておく。
この特定の最急降下法のバージョンは、バッチ最急降下法とも呼ばれる。 ここで「バッチ」という単語は一度に全部のトレーニング手本を見るという事実を表している。 それをある種の、全てのトレーニング手本のバッチ、と呼ぶ。 それは実はあんまりいい名前じゃ、ベストな名前って訳じゃない。
だが、機械学習屋の人々がこのバージョンの最急降下法をそう呼んでるんだから仕方がない。 そしてこの3億の国勢調査のレコードをディスクに保存して退避させてしまってる事を想像してみよう。 このアルゴリズムのまわり方としては、この微分項を計算するのに3億のレコードを全てコンピュータのメモリに読み出す必要がある。 これらのレコード全てをコンピュータにストリーム処理しなくてはいけない。何故ならコンピュータメモリに全て保存する事は出来ないから。 だからそれらを読んでいき、ゆっくりと和を蓄積していく事で、はじめて微分が計算出来る。 そしてそれを全部終えたら、その結果最急降下法を1ステップだけ進める事が出来る、、、 そしてまた全体をやりなおさなくてはいけない。 つまり、3億のレコードを全部スキャンして、それらの和を蓄積していく。 その仕事を全て終えても、最急降下法のちょっとのステップがもう一歩進むだけ。 そしてまた、同じ事をする。そしてまた三歩目が進める。などなど。 つまり、アルゴリズムが収束するのに、凄い長い時間がかかる。 バッチ最急降下法と比較して、別のアルゴリズムを考え出していく、 それは各イテレーションごとに全てのトレーニング手本を見なくて良く、 一回のイテレーションでは一つのトレーニング手本単体だけを見れば良い。 新しいアルゴリズムにうつる前に、ここにバッチ最急降下法のアルゴリズムを再掲しておこう、 これがコスト関数で、これがアップデート、そしてここの項はもちろん 最急降下法で用いる、偏微分項だ、 パラメータシータjによる、我らが最適化の目的関数 J trainのシータの。 さて、大規模なデータセットにもっと効率的にスケールするアルゴリズムを見てみよう。 確率的最急降下法と呼ばれるアルゴリズムを用いる為に、 このベクトル、コスト関数をちょっと違う形にして、トレーニング手本、x(i), y(i)に関する パラメータをシータとするコストを定義する、それはイコール、1/2掛ける、二乗誤差の x(i), y(i)に対して仮説が引き起こす分。 つまりこのコスト関数の項は、私の仮説が、単体の手本、x(i)とy(i)に対してどれだけ良いかを実際に測っている。 今、全体のコスト関数、J trainは、等価な形でこう書ける。 つまりJ trainは、m個のトレーニング手本の仮説のコストの平均に過ぎない。 線形回帰のコスト関数をこうやってみる見方を身につけた上で、 確率的最急降下法が何をする物なのか、書き下してみよう。 確率的最急降下法の最初のステップは、データセットをランダムにシャッフルする。 ランダムにシャッフルする、という事の意味は、m個のトレーニング手本をランダムに並べ替える、という事。 これは普通の前処理だ。後でこの件については考える。 だが確率的最急降下法の主な部分は、その次に以下のように続く所だ。 i=1からmまで、リピートする事の、、、 つまりトレーニング手本を繰り返しスキャンして、以下のアップデートを実施する。 パラメータ、シータjを シータj 引くことのアルファ掛けるh(x(i))引くことのy(i)に掛けるx(i)j。 このアップデートをいつも通り、全てのjの値に対して行う。 ここで、ここの項はバッチ最急降下法の和の中にある物と、完全に一致する事が分かるだろう。 実のところ、もし解析学が得意なら、このここの項は、 costのシータとx(i)を、パラメータシータjで偏微分した物に等しい事が示せる。 ここでこのcostはもちろん、前に定義した物。 このアルゴリズムのまとめとして、、、その前に中括弧を閉じておこう。 さて、確率的最急降下法がやる事は、実際にトレーニング手本をスキャンして、 そして最初にトレーニング手本の最初のx(1), y(1)を見る時、 この最初の例だけを見て、最初の手本に関してだけのコストによる、 最急降下法の小さな一ステップを実行する。 言い換えると、最初の手本を見て、 最初の手本のデータだけにもうちょっとだけフィットするように、パラメータを少しだけ変更する。 これを終えたら、この内側のforループの中で、次の二番目のトレーニング手本に進む。 そこで行う事は、またもう一歩、パラメーター空間内を進む事、 つまりちょっとだけ良く二番目のトレーニング手本にフィットするようにパラメータを変更する。 それを終えたら、三番目のトレーニング手本に進む。 そして三番目のトーレニング手本にちょっとだけ良くフィットするように、パラメータを変更する。 これをトレーニングセット全体に渡って行う。 そしてこの外側のループが、トレーニングセット全体を複数回繰り返させる。 この確率的最急降下法の見方は、データセットをランダムにシャッフルする事から始める理由も分かる。 もしシャッフルせずにトレーニングセットをここからスキャンして行ったら、 むちゃくちゃにソートされてる順番にトレーニング手本を見ていく事になる、 その順番はデータが最初からランダムに来たか、変な風にソートされているかに寄ってしまう。 実際的には、ランダムにソートする事は確率的最急降下法をちょっとだけスピードアップする、ちょっとだけね。 だから念のため、それがランダムな並びと確信が持てる場合を除いて、普通は とりあえずデータセットをランダムにシャッフルしておく方が良い。 だがもっと重要な点として、確率的最急降下法のもう一つの見方として、 それは、通常の最急降下法ととても似ているが、だがこれらの微分項を全てのmトレーニング手本に渡って足すのではなく、 この微分項を単に一つのトレーニング手本に対してだけ取る、という事をしている、 そしてそこで既にパラメータの改善を開始してしまう。 つまり、全てのアメリカ合衆国の国勢調査のレコード3億件をなめるのを待つのでは無く、 パラメータをちょっとだけ改善してグローバル最小へとちょっとだけ歩を進める為に、 全てのトレーニング手本をスキャンする事を必要とするのでは無く、 確率的最急降下法では、手本は一つしか見る必要が無くて、 この場合のパラメータの改善を既に始めてしまって良い、パラメータをグローバル最小へと進めるという。 これがアルゴリズムをふたたび書き下した物だ。
最初のステップはデータをランダムにシャッフルする事で、 二番目のステップは実際の仕事をする所だが、そこでは一つのトレーニング手本、x(i)とy(i)に関してのみでアップデートしてしまう。 ではこのアルゴリズムがパラメータに何をしていくか、見てみよう。 前に、バッチ最急降下法を使っている時に、 それは全てのトレーニング手本を一度に見る物だという事を見た。 バッチ最急降下法はグローバル最小へと向かう、かなりまっすぐな軌跡を描く傾向になる。 それに対して確率的最急降下法は、各イテレーションはもっと早い、 何故なら全てのトレーニング手本を足し合わせる必要が無いからだが、 しかし各イテレーションは一つのトレーニング手本に対してだけより良くフィットするように試みるだけなので、 だからもし確率的最急降下法を始めると、あー、確率的最急降下法をこの点とかから始めたとしよう。 最初のイテレーションでは、この方向に進んだとする、 二番目のイテレーションでは、うーん、二番目のイテレーションは偶然、 ちょっとツイてなかったとしよう。そして実際には悪い方向にこんな感じでパラメータを進めてしまった。 三度目のイテレーションでは、三番目のトレーニング手本に対してだけもっと良くフィットするようにパラメータを変更する。 そしてこんな方向に向かったとする。 そして四番目のトレーニング手本を見て、同じ事をする。5番目、6番目、7番目、などなど。 そして確率的最急降下法を実行すると、こんな結果が見られる： だいたいはパラメータはグローバル最小の方向に向かうが、いつもそうだという訳では無い。 つまりもっとデタラメに見える、遠回りの軌跡を通ってグローバル最小を探す。 そして実のところ、確率的最急降下法は、バッチ最急降下法がするような意味では収束しない。 そして最終的には、それはグローバル最小のそばのある一定の範囲をうろちょろし続けるようになる。 だがグローバル最小にたどりついて留まりつづける、という挙動では無い。 だが現実的には、それはそんなに問題じゃない、何故なら、 パラメータがグローバル最小のきわめてそばの領域に居続けるなら、 パラメータは最終的にグローバル最小に極めて近いはずなので、それは仮説としてはとても良い物となるだろう。 だから通常、確率的最急降下法を実行すると、 グローバル最小のそばのパラメータを得る事になり、それは実質的にはほとんどの現実的な目的にとって十分に良い物だ。 最後に詳細を一つ。確率的最急降下法においては、 この内側のループを複数回実行するように指示する、外側のループがある。 では、何回外側のループは繰り返せば良い？ トレーニングセットのサイズによっては、このループは一回で十分かもしれない。 典型的には、10回までのどこかって所かな。 つまりこの内側のループを1回から10回の間のどこかの回数実行すれば良い。 つまり、もし我らが、真に大量のデータセット、例えばこのUS国勢調査のような物で、 3億の手本とかあるならば、 トレーニングセットを1パスだけなめる頃には、 つまりこのforでi=1から3億まで回せば、 そのデータセットを1パスなめ終わる頃には、 既に十分完璧な良い仮説に到達しているかもしれない。 その場合は、この内側のループは一回だけ実行すれば良い。凄い凄い大きなmなら。 だが一般的には、1と10の間の価数のパスだけデータセットをなめる。この辺が普通だ。 でもそれは本当にトレーニングセットのサイズに依存した話だ。 バッチ最急降下法と比較してみると、 バッチ最急降下法だと、一つのパスでトレーニングセット全体をなめて、 それだけやってたった一歩の最急降下法のステップしか進まない。 つまり最急降下法のこれらの小さなステップの、たった一歩のステップだけ。 そしてこれが、確率的最急降下法がもっと早くなりうる理由だ。 以上が確率的最急降下法アルゴリズムだ。 そしてこれを実装すれば、あなたは多くの学習アルゴリズムをスケールアップして、 よりビッグなデータセットに対して、もっと良いパフォーマンスが得られるように出来るだろう。