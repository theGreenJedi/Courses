1
00:00:00,493 --> 00:00:03,492
आप अब जानते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट अल्गोरिद्म.

2
00:00:03,492 --> 00:00:09,907
लेकिन जब आप रन कर रहे हैं अल्गोरिद्म, आप कैसे सुनिश्चित करते हैं कि यह पूरी तरह डीबग हो गया है और यह सही कन्वर्ज हो रहा है?

3
00:00:09,907 --> 00:00:15,813
उतना ही महतवपूर्ण है, कैसे आप ट्यून करते हैं लर्निंग रेट अल्फ़ा स्टोकस्टिक ग्रेडीयंट डिसेंट के साथ.

4
00:00:15,813 --> 00:00:25,950
इस विडीओ में हम बात करेंगे कुछ टेक्नीक्स की इन कामों को करने के लिए, सुनिश्चित करने के लिए कि यह कन्वर्ज हो रहा है और चुनने के लिए लर्निंग रेट अल्फ़ा.

5
00:00:25,950 --> 00:00:30,600
पहले जब हम कर रहे थे बैच ग्रेडीयंट डिसेंट, हमारा स्टैंडर्ड तरीक़ा सुनिश्चित करने के लिए कि 

6
00:00:30,600 --> 00:00:36,493
ग्रेडीयंट डिसेंट कन्वर्ज हो रहा है था कि हम प्लॉट कर सकते थे ऑप्टिमायज़ेशन कॉस्ट फ़ंक्शन को इटरेशन संख्या के फ़ंक्शन की तरह.

7
00:00:36,493 --> 00:00:44,366
तो वह था कॉस्ट फ़ंक्शन और हम तय कर पाए कि यह कॉस्ट फ़ंक्शन कम हो रहा है हर इटरेशन में.

8
00:00:44,366 --> 00:00:50,438
जब ट्रेनिंग सेट साइज़ छोटे थे, हम वह कर पाए क्योंकि हम कम्प्यूट कर पाए सम काफ़ी एफ़िशिएँट ढंग से.

9
00:00:50,438 --> 00:00:57,950
लेकिन जब आपके पास है एक वृहद / मैसिव ट्रेनिंग सेट साइज़ तब आप नहीं चाहते कि आपको रोकना पड़े आपका अल्गोरिद्म बीच बीच में॰

10
00:00:57,950 --> 00:01:04,045
आप नहीं चाहते कि आपको रोकना पड़े स्टोकस्टिक ग्रेडीयंट डिसेंट कम्प्यूट करने के लिए यह कॉस्ट फ़ंक्शन

11
00:01:04,045 --> 00:01:07,442
क्योंकि इसे चाहिए एक सम आपके पूरे ट्रेनिंग सेट का.

12
00:01:07,442 --> 00:01:12,466
और पूरा उद्देश्य स्टोकस्टिक ग्रेडीयंट डिसेंट का था कि आप चाहते थे शुरू करना प्रोग्रस्स 

13
00:01:12,466 --> 00:01:19,130
देखने के बाद केवल एक अकेला इग्ज़ाम्पल बिना आवश्यकता के देखने की आपका पूरा ट्रेनिंग सेट 

14
00:01:19,130 --> 00:01:25,583
एकदम मध्य में अल्गोरिद्म के, सिर्फ़ कम्प्यूट करने के लिए चीज़ें जैसे कॉस्ट फ़ंक्शन पूरे ट्रेनिंग सेट का.

15
00:01:25,583 --> 00:01:32,472
अत: स्टोकस्टिक ग्रेडीयंट डिसेंट में, चेक करने के लिए कि अल्गोरिद्म कन्वर्ज हो रहा है, यहाँ है जो आप कर सकते हैं इसके स्थान पर.

16
00:01:32,472 --> 00:01:36,367
लेते हैं परिभाषा कॉस्ट की जो हमारे पास थी पहले.

17
00:01:36,367 --> 00:01:42,647
अत: कॉस्ट पेरमिटर्स थीटा की विद रिस्पेक्ट टु एक ट्रेनिंग इग्ज़ाम्पल के है सिर्फ़ वन हाफ़ स्क्वेर्ड एरर उस ट्रेनिंग इग्ज़ाम्पल पर.

18
00:01:42,647 --> 00:01:49,754
तब, जब स्टोकस्टिक ग्रेडीयंट डिसेंट लर्न कर रहा है, उसके ठीक पहले हम ट्रेन करते हैं एक ख़ास इग्ज़ाम्पल पर.

19
00:01:49,754 --> 00:01:54,601
तो, स्टोकस्टिक ग्रेडीयंट डिसेंट में हम देखेंगे इग्ज़ाम्पल्ज़ एक्स आइ, वाय आइ पर, क्रम में, और 

20
00:01:54,601 --> 00:01:57,329
तब एक प्रकार से करते हैं एक छोटा अप्डेट इस इग्ज़ाम्पल के आधार पर.

21
00:01:57,329 --> 00:02:04,095
और हम जाते हैं अगले इग्ज़ाम्पल पर, एक्स आइ जमा 1, वाय आइ जमा 1, और इसी प्रकार आगे, सही?

22
00:02:04,095 --> 00:02:05,880
वह है जो स्टोकस्टिक ग्रेडीयंट डिसेंट करता है.

23
00:02:05,880 --> 00:02:15,024
अत:, जब अल्गोरिद्म देख रहा है इग्ज़ाम्पल एक्स आइ, वाय आइ पर, लेकिन इससे पहले कि यह करे अप्डेट पेरमिटर्स थीटा को 

24
00:02:15,024 --> 00:02:20,255
उस इग्ज़ाम्पल का इस्तेमाल करके, आओ कम्प्यूट करते हैं कॉस्ट उस इग्ज़ाम्पल की.

25
00:02:20,255 --> 00:02:23,577
सिर्फ़ इसी चीज़ को कहने के लिए दोबारा, लेकिन दूसरे शब्दों में.

26
00:02:23,577 --> 00:02:33,294
एक स्टोकस्टिक ग्रेडीयंट डिसेंट स्कैन कर रहा है हमारे ट्रेनिंग सेट में से ठीक पहले उससे जब हम  थीटा को अप्डेट करते हैं एक ख़ास इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर.

27
00:02:33,294 --> 00:02:38,198
चलिए कम्प्यूट करते हैं कि हमारी हायपॉथिसस कितना अच्छा कर रही है उस ट्रेनिंग इग्ज़ाम्पल पर.

28
00:02:38,198 --> 00:02:43,852
और हम यह करना चाहते हैं थीटा को अप्डेट करने से पहले क्योंकि यदि हमने अभी अप्डेट किया थीटा को इग्ज़ाम्पल इस्तेमाल करके,

29
00:02:43,852 --> 00:02:49,061
आप जानते हैं, कि शायद यह बेहतर कर रहा हो उस इग्ज़ाम्पल पर बजाय उसके जो कि प्रतिनिधित्व करता हो॰ 

30
00:02:49,061 --> 00:02:57,438
अंत में, चेक करने के लिए कन्वर्जेन्स स्टोकस्टिक ग्रेडीयंट डिसेंट की, हम क्या कर सकते हैं कि प्रत्येक, मान लो, प्रत्येक हज़ार इटरेशन्स पर, 

31
00:02:57,438 --> 00:03:01,511
हम प्लॉट कर सकते हैं ये कॉस्ट्स जो हम कम्प्यूट कर रहे थे पिछले स्टेप में.

32
00:03:01,511 --> 00:03:07,450
हम कर सकते हैं प्लॉट ये कॉस्ट्स ऐव्रिज करके, मान लो, पिछले हज़ार इग्ज़ाम्पल्ज़ पर जो प्रॉसेस किए हैं अल्गोरिद्म ने.

33
00:03:07,450 --> 00:03:12,714
और यदि आप यह करते हैं, यह आपको देता है एक प्रकार से एक रनिंग अनुमान कि अल्गोरिद्म  कितने अच्छे से काम कर रहा है

34
00:03:12,714 --> 00:03:17,049
आप जानते हैं, पिछले 1000 ट्रेनिंग इग्ज़ाम्पल्ज़ पर जो आपके अल्गोरिद्म ने देखे हैं.

35
00:03:17,049 --> 00:03:23,974
अत:, तुलना में कम्प्यूट करने में जे<u > ट्रेन, समय-समय पर जिसे चाहिए स्कैन करना पूरे ट्रेनिंग सेट से.</u>

36
00:03:23,974 --> 00:03:27,973
इस दूसरी प्रक्रिया से, अच्छा, स्टोकस्टिक ग्रेडीयंट के हिस्से की तरह, 

37
00:03:27,973 --> 00:03:32,965
इतना समय नहीं लगता इन कॉस्ट्स को कम्प्यूट करने के लिए ठीक पहले अप्डेट करने के पेरामिटर थीटा को.

38
00:03:32,965 --> 00:03:40,276
और हम यह सिर्फ़ प्रत्येक हजार इटरेशंन्स के बाद कर रहे हैं, हम सिर्फ़ ऐव्रिज करते हैं पिछली 1,000 कॉस्ट्स जो हमने कम्प्यूट की थी और उसे प्लॉट करते हैं.

39
00:03:40,276 --> 00:03:47,537
और उन प्लॉट्स को देखकर, यह चेक हो जाता है यदि स्टोकस्टिक ग्रेडीयंट डिसेंट कन्वर्ज हो रहा है.

40
00:03:47,537 --> 00:03:51,708
तो यहाँ हैं कुछ उदाहरण कि ये प्लॉट्स कैसे दिखते होंगे.

41
00:03:51,708 --> 00:03:55,519
मान लो आपने प्लॉट की है कोस्ट ऐव्रिज पिछले हज़ार इग्ज़ाम्पल्ज़ पर, 

42
00:03:55,519 --> 00:04:01,073
क्योंकि ये हैं ऐव्रिज किए हुए सिर्फ़ एक हज़ार इग्ज़ाम्पल्ज़ पर, वे होंगे थोड़े नोयज़ी और इसलिए,

43
00:04:01,073 --> 00:04:03,873
यह शायद कम न हो प्रत्येक इटरेशन में.

44
00:04:03,873 --> 00:04:07,828
तब यदि आपको मिलता है एक चित्र जो ऐसा दिखता हैं, अत: प्लॉट नोयज़ी है 

45
00:04:07,828 --> 00:04:11,721
क्योंकि यह ऐव्रिज है, आप जानते हैं, सिर्फ़ एक छोटे सब सेट पर, मान लो एक हज़ार ट्रेनिंग इग्ज़ाम्पल्ज़ पर.

46
00:04:11,721 --> 00:04:17,283
यदि आपको मिलता है एक चित्र जो ऐसा दिखता हैं, आप जानते हैं वह होगा एक अच्छा रन अल्गोरिद्म के साथ.

47
00:04:17,283 --> 00:04:24,195
शायद, जहाँ यह दिखता है कि कॉस्ट कम हुई है और तब यह प्लैटू जो दिखता है कि दबा दिया गया है, आप जानते हैं, शुरू करने पर उस पोईँट के आस पास से. 

48
00:04:24,195 --> 00:04:29,603
लगता है, यह है आपकी कॉस्ट तब शायद आपके लर्निंग अल्गोरिद्म ने कन्वर्ज कर लिया है.

49
00:04:29,603 --> 00:04:34,252
यदि आप चाहते हैं इस्तेमाल करना एक छोटी लर्निंग रेट, कुछ जो शायद आप देखे ऐसा कि 

50
00:04:34,252 --> 00:04:39,229
अल्गोरिद्म शुरू में शायद लेर्न करे धीरे अत: कॉस्ट कम होती जाती है ज्यादा धीरे॰

51
00:04:39,229 --> 00:04:47,585
लेकिन तब आखिरकार आपके पास एक छोटी लर्निंग रेट है जो वास्तव में संभव है आपके अल्गोरिद्म को पहुँचने में, शायद एक थोड़े बेहतर हल पर. 

52
00:04:47,585 --> 00:04:53,426
तो लाल लाइन शायद दर्शाती है आचरण स्टोकेस्टिक ग्रेडिएंट डिसेंट का एक छोटी लर्निंग रेट के साथ. 

53
00:04:53,426 --> 00:05:00,594
और कारण कि यह ऐसा क्योंकि, आपको स्मरण होगा, स्टोकेस्टिक ग्रेडिएंट डिसेंट केवल कनवर्ज नहीं होता ग्लोबल मिनिमम पर. 

54
00:05:00,594 --> 00:05:05,068
है कि यह क्या करता है कि पैरामीटर्स ओसीलेट / झूलते हैं थोड़ा बहुत ग्लोबल मिनिमम के इर्दगिर्द. 

55
00:05:05,068 --> 00:05:09,231
और इसलिए लेने से एक छोटी लर्निंग रेट आपके पास होंगीं छोटी ऑसिलेशन्स॰ 

56
00:05:09,231 --> 00:05:12,896
और कभी-कभी यह छोटा अंतर नगण्य होगा 

57
00:05:12,896 --> 00:05:19,686
और कभी-कभी छोटी रेट से मिल सकती है आपको एक बेहतर वैल्यू पैरामीटर्स की. 

58
00:05:19,686 --> 00:05:22,269
यहाँ है और कुछ चीजें जो हो सकती हैं. 

59
00:05:22,269 --> 00:05:27,986
मान लो आप रन करते हो स्टोकेस्टिक ग्रेडिएंट डिसेंट और आप एवरेज करते हो एक हज़ार से अधिक एग्जामपल्स पर प्लॉट करते समय ये कॉस्ट्स.

60
00:05:27,986 --> 00:05:32,369
तो, आप जानते हैं, यहाँ हो सकता है परिणाम उनमें से किसी एक प्लॉट का. 

61
00:05:32,369 --> 00:05:34,353
तब फिर से, यह एक प्रकार से कनवर्ज हो गया दिखता है. 

62
00:05:34,353 --> 00:05:42,119
यदि आपको लेनी होती यह संख्या, एक हज़ार, और बढ़ानी होती एवरेज करने के लिए 5 हज़ार  एग्जामपल्स पर.  

63
00:05:42,119 --> 00:05:47,913
तब यह संभव है शायद आपको मिले एक अधिक समतल कर्व जो दिखता है ऐसा. 

64
00:05:47,913 --> 00:05:56,547
और एवरेज करने से, मान लो 5,000 एग्जामपल्स पर, बजाय 1,000 के, आपको शायद मिल सकता है एक समतल कर्व ऐसा.

65
00:05:56,547 --> 00:06:00,248
और इसलिए वह है प्रभाव बढ़ाने का संख्या एग्जामपल्स की जिस पर आप एवरेज करते हो. 

66
00:06:00,248 --> 00:06:06,229
नुक्सान इसे ज्यादा बढ़ाने का निस्संदेह है कि अब आपको मिलता है केवल एक डेटा पॉइंट प्रत्येक 5,000 एग्जामपल्स पर.  

67
00:06:06,229 --> 00:06:12,001
तो फीड बैक जो आपको मिलता है कैसा आपका लर्निंग अल्गोरिद्म कर रहा है, एक प्रकार से, शायद यह काफी विलंबित है 

68
00:06:12,001 --> 00:06:17,681
क्योंकि आपको मिलता है एक डेटा पॉइंट आपके प्लॉट पर प्रत्येक 5,000 एग्जामपल्स पर बजाय 1,000 एग्जामपल्स पर. 

69
00:06:17,681 --> 00:06:23,911
इसी प्रकार कभी-कभी जब रन करेंगे ग्रेडिएंट डिसेंट और आपको मिलेगा एक प्लॉट जो ऐसा दिखता है. 

70
00:06:23,911 --> 00:06:32,079
और वैसे दिखने वाले एक प्लॉट से, आप जानते हैं, ऐसा लगता है कि कॉस्ट कम नहीं हो रही है बिल्कुल.

71
00:06:32,079 --> 00:06:34,023
ऐसा लगता है कि अल्गोरिद्म लर्न ही नहीं कर रहा है. 

72
00:06:34,023 --> 00:06:39,261
यह है केवल, ऐसा दिखता है यहाँ एक दबा हुआ कर्व और कॉस्ट बिलकुल कम नहीं हो रही है. 

73
00:06:39,261 --> 00:06:46,260
लेकिन फिर से यदि आपको इसे बढ़ाना होता ऐव्रिज करने के लिए इग्ज़ाम्पल्ज़ की एक बड़ी संख्या पर

74
00:06:46,260 --> 00:06:49,729
ऐसा सम्भव है कि आप देखें कुछ इस लाल लाइन जैसा

75
00:06:49,729 --> 00:06:55,127
ऐसा लगता है कॉस्ट वास्तव में कम हो रही है, यह है कि सिर्फ़ नीली लाइन ऐव्रिज हो रही है 2, 3 इग्ज़ाम्पल्ज़ पर,

76
00:06:55,127 --> 00:07:01,374
नीली लाइन इतनी नोयज़ी थी कि आप नहीं देख सकते थे असली ट्रेंड कॉस्ट में वास्तव में कम होते हुए 

77
00:07:01,374 --> 00:07:06,688
और शायद एवरेज करने से 5,000 एग्जामपल्स पर बजाय 1,000 के सहायक हो सकता है.

78
00:07:06,688 --> 00:07:12,358
अवश्य ही हमने ऐव्रिज किया इग्ज़ाम्पल्ज़ की बड़ी संख्या पर जो यहाँ हमने ऐव्रिज किया 5,000 इग्ज़ाम्पल्ज़ पर,

79
00:07:12,358 --> 00:07:16,998
मैं सिर्फ़ ले रहा हूँ एक अलग रंग, यह सम्भव है कि आप देखें एक लर्निंग कर्व जो शायद ऐसा हो.

80
00:07:16,998 --> 00:07:21,197
यह अभी भी दबा हुआ है जब आपने ऐव्रिज भी कर लिया है इग्ज़ाम्पल्ज़ कि बड़ी संख्या पर.

81
00:07:21,197 --> 00:07:25,908
और जब आपको वह मिलता है, तब वह शायद होगा एक अधिक मज़बूत प्रमाण कि 

82
00:07:25,908 --> 00:07:29,287
दुर्भाग्य से अल्गोरिद्म नहीं कुछ ज़्यादा लर्न कर पा रहा कारण जो भी हों.

83
00:07:29,287 --> 00:07:34,969
और आपको या तो बदलना चाहिए लर्निंग रेट या बदलने चाहिए फ़ीचर्ज़ या बदलना चाहिए कुछ और इस अल्गोरिद्म में.

84
00:07:34,969 --> 00:07:39,235
अंत में, एक आख़िरी चीज़ जो आप शायद देखें वह होगी यदि आपको प्लॉट करने हों ये कर्व्ज़ 

85
00:07:39,235 --> 00:07:43,273
और आप देखें एक कर्व जो ऐसा दिखता हैं, जहाँ ऐसा लगता है कि यह बढ़ रहा है.

86
00:07:43,273 --> 00:07:48,066
और ऐसी स्थिति है तब यह एक संकेत है कि अल्गोरिद्म डाईवर्ज हो रहा है.

87
00:07:48,066 --> 00:07:53,965
और आपको क्या वाक़ई में क्या करना चाहिए कि ले एक छोटी वैल्यू लर्निंग रेट अल्फ़ा की.

88
00:07:53,965 --> 00:07:58,143
आशा है, इससे आपको समझ आ गया होगा एक रेंज कि क्या-क्या आप देख सकते हैं 

89
00:07:58,143 --> 00:08:02,946
जब आप प्लॉट करते हैं ये कॉस्ट ऐव्रिज इग्ज़ाम्पल्ज़ की कुछ रेंज पर और 

90
00:08:02,946 --> 00:08:07,765
बता सकते हैं कि क्या क्या आप कर सकते हैं प्रतिक्रिया में इन विभिन्न प्लॉट्स को देख कर.

91
00:08:07,765 --> 00:08:15,070
तो यदि प्लॉट्स दिखते हैं काफ़ी नोयज़ी, या यदि यह अधिक टेढ़ा मेढ़ा होता है, तब कोशिश करें बढ़ाने की इग्ज़ाम्पल्ज़ की संख्या 

92
00:08:15,070 --> 00:08:18,734
जिस पर आप ऐव्रिज कर रहे हैं ताकि आप देख सके ओवरॉल ट्रेंड प्लॉट में बेहतर.

93
00:08:18,734 --> 00:08:25,836
और यदि आप देखते हैं एररज़ वास्तव में बढ़ रही है, कॉस्ट वास्तव में बढ़ रही है, कोशिश करें लेने की एक छोटी वैल्यू अल्फ़ा की.

94
00:08:25,836 --> 00:08:31,649
अंत में, लर्निंग रेट का पहलू थोड़ा और खोजने योग्य है.

95
00:08:31,649 --> 00:08:38,922
हमने देखा कि जब हम रन करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, अल्गोरिद्म शुरू होता है यहाँ और एक प्रकार से घूम-घूम कर पहुँचता है मिनिमम पर 

96
00:08:38,922 --> 00:08:43,494
और तब यह असल में कन्वर्ज नहीं होता, और इसके बजाय घूमता रहता है मिनिमम के इर्दगिर्द हमेशा के लिए.

97
00:08:43,494 --> 00:08:50,225
और इसलिए अंत में आपको मिलती है एक पेरामिटर वैल्यू जो है आशापूर्वक नज़दीक ग्लोबल मिनिमम के जो वास्तव में ग्लोबल मिनिमम नहीं है.

98
00:08:50,225 --> 00:08:57,991
स्टोकस्टिक ग्रेडीयंट डिसेंट की अधिकांश इम्प्लमेंटेशन्स में, लर्निंग रेट अमूमन रखा जाता है कॉन्स्टंट.

99
00:08:57,991 --> 00:09:02,022
और इसलिए आपको मिलती है एक पिक्चर ऐसी.

100
00:09:02,022 --> 00:09:06,523
और यदि आप चाहते हैं स्टोकेस्टिक ग्रेडिएंट डिसेंट को कनवर्ज करवाना ग्लोबल मिनिमम पर,

101
00:09:06,523 --> 00:09:11,825
एक काम जो आप कर सकते हैं वह है कि आप धीरे धीरे कम करें लर्निंग रेट अल्फ़ा को.

102
00:09:11,825 --> 00:09:22,240
तो, एक आम तरीक़ा उसे करने का होगा कि सेट करें अल्फ़ा को किसी कॉन्स्टंट 1 विभाजित इटरेशन संख्या जमा कॉन्स्टंट 2 से.

103
00:09:22,240 --> 00:09:28,169
अत:, इटरेशन संख्या हैं संख्या इटरेशन्स की जो आपने की हैं स्टोकस्टिक ग्रेडीयंट डिसेंट की,

104
00:09:28,169 --> 00:09:29,519
तो यह वास्तव में हैं संख्या ट्रेनिंग इग्ज़ाम्पल्ज़ की जो आपने देखें हैं 

105
00:09:29,519 --> 00:09:34,103
और कॉन्स्टंट 1 और कॉन्स्टंट 2 हैं अतिरिक्त पेरमिटर्स अल्गोरिद्म के 

106
00:09:34,103 --> 00:09:38,160
जिनमें शायद आपको थोड़ी फेर-बदल करनी पड़े पाने के लिए एक बेहतर पर्फ़ॉर्मन्स.

107
00:09:38,160 --> 00:09:43,004
एक कारण कि लोग यह नहीं करना चाहते क्योंकि थोड़ा समय लग सकता है 

108
00:09:43,004 --> 00:09:48,122
फेरबदल करने में इन 2 अतिरिक्त पेरमिटर्स, कोंस्टंट 1 और कोंस्टंट 2 में, और इसलिए यह करता है अल्गोरिद्म को अधिक तुनुकमिज़ाज. 

109
00:09:48,122 --> 00:09:52,113
आप जानते हैं, यह है सिर्फ़ अतिरिक्त पेरमिटर्स जिनमें फेरबदल करके आप अल्गोरिद्म को बेहतर कर सकते हैं.

110
00:09:52,113 --> 00:09:57,246
लेकिन यदि आप कर पाते हैं ट्यून पेरमिटर्स को अच्छे से, तब पिक्चर जो आपको मिलती है कि 

111
00:09:57,246 --> 00:10:02,834
अल्गोरिद्म वास्तव में घूम कर पहुँचता है मिनिमम की तरफ़, लेकिन जैसे यह नज़दीक पहुँचता है 

112
00:10:02,834 --> 00:10:07,024
क्योंकि आप कम कर रहे हैं लर्निंग रेट घुमाव होते जाएँगे और छोटे 

113
00:10:07,024 --> 00:10:12,729
जब तक यह ग्लोबल मिनिमम पर नहीं पहुँच जाता. उमीद है यह समझ आया होगा, सही?

114
00:10:12,729 --> 00:10:21,608
और कारण कि यह फ़ॉर्म्युला समझ आता है क्योंकि जैसे अल्गोरिद्म रन करता है, इटरेशन संख्या बड़ी होती जाती है और अल्फ़ा धीरे-धीरे कम होता जाता है,

115
00:10:21,608 --> 00:10:27,506
और इसलिए आप लेते हैं और छोटे स्टेप्स जब तक आशापूर्वक यह कन्वर्ज होता है ग्लोबल मिनिमम पर.

116
00:10:27,506 --> 00:10:33,484
तो यदि आप धीरे-धीरे कम करते हैं अल्फ़ा को ज़ीरो तक आपको मिलती है थोड़ी बेहतर हायपॉथिसस.

117
00:10:33,484 --> 00:10:40,078
लेकिन क्योंकि अतिरिक्त कार्य करना पड़ता है फेर-बदल करने में पेरमिटर्स को और क्योंकि स्पष्ट: हम काफ़ी ख़ुश है 

118
00:10:40,078 --> 00:10:43,892
किसी भी पेरामिटर वैल्यू से जो नज़दीक है ग्लोबल मिनिमम के.

119
00:10:43,892 --> 00:10:50,863
अमूमन यह प्रक्रिया घटाने की अल्फ़ा को धीरे-धीरे नहीं की जाती और रखना लर्निंग रेट अल्फ़ा कोंस्टंट 

120
00:10:50,863 --> 00:10:56,983
ज़्यादा आम ऐप्लिकेशन है स्टोकस्टिक ग्रेडीयंट डिसेंट की जबकि आप देखेंगे लोग इस्तेमाल करते हैं कोई भी वर्ज़न.

121
00:10:56,983 --> 00:11:03,595
सारांश में इस विडीओ में हमने बात की लगभग मॉनिटर करने की 

122
00:11:03,595 --> 00:11:08,256
कि स्टोकस्टिक ग्रेडिएंट डिसेन्ट कैसे कर रहा है ऑप्टिमायज़ कॉस्ट फ़ंक्शन को.

123
00:11:08,256 --> 00:11:17,043
और यह है एक ढंग जिसमें आवश्यकता नहीं स्कैन करने की पूरे ट्रेनिंग सेट को समय-समय पर कम्प्यूट करने के लिए कॉस्ट फ़ंक्शन पूरे ट्रेनिंग सेट पर.

124
00:11:17,043 --> 00:11:20,693
लेकिन इसके स्थान पर यह देखता है मान लो सिर्फ़ आख़िर के हज़ार इग्ज़ाम्पल्ज़ या ऐसा कुछ.

125
00:11:20,693 --> 00:11:27,592
और आप इस्तेमाल कर सकते हैं इस विधि को दोनो तय करने के लिए स्टोकस्टिक ग्रेडीयंट डिसेंट सही जा रहा है और कन्वर्ज कर रहा है 

126
00:11:27,592 --> 00:11:31,468
या प्रयोग कर सकते हैं इसे ट्यून करने के लिए लर्निंग रेट अल्फा को.