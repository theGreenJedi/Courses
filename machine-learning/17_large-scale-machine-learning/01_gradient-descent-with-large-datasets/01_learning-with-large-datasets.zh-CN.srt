1
00:00:00,332 --> 00:00:04,284
在接下来的几个视频里 我们会讲大规模的机器学习

2
00:00:04,284 --> 00:00:08,316
就是用来处理大数据的算法

3
00:00:08,316 --> 00:00:12,839
如果我们看近5到10年的机器学习的历史

4
00:00:12,839 --> 00:00:17,853
现在的学习算法比5年前的好很多

5
00:00:17,853 --> 00:00:22,657
其中的原因之一就是我们现在拥有很多可以训练算法的数据

6
00:00:22,657 --> 00:00:29,741
在接下来的几个视频中 我们会讨论这些运用大量数据的算法

7
00:00:32,926 --> 00:00:35,527
为什么我们喜欢用大的数据集呢?

8
00:00:35,527 --> 00:00:40,564
我们已经知道 得到一个高效的机器学习系统的最好的方式之一是

9
00:00:40,564 --> 00:00:46,168
用一个低偏差的学习算法 然后用很多数据来训练它

10
00:00:46,168 --> 00:00:53,561
早先看到的一个例子是区分易混淆词组

11
00:00:53,561 --> 00:01:00,726
在这个例子 早餐我吃了两个(TWO)鸡蛋

12
00:01:00,726 --> 00:01:06,436
我们知道只要你给算法很多训练的数据 它就能做好

13
00:01:06,436 --> 00:01:10,419
由于这些结果所以在机器学习领域有一个说法是

14
00:01:10,419 --> 00:01:15,151
通常不是最好的算法胜出 而是谁有最多的数据

15
00:01:15,151 --> 00:01:19,568
所以我们想用大的数据集 至少在我们可以得到大数据集的情况下

16
00:01:19,568 --> 00:01:27,027
但训练大的数据集也有它自己的问题 特别是计算量的问题

17
00:01:27,027 --> 00:01:33,870
假设我们的训练集的大小m是100,000,000

18
00:01:33,870 --> 00:01:37,934
这对于现代的数据集其实是很现实的

19
00:01:37,934 --> 00:01:40,518
如果我们看美国的人口普查数据集

20
00:01:40,518 --> 00:01:44,663
美国有3亿人 我们通常会得到上亿的记录

21
00:01:44,663 --> 00:01:47,856
如果我们看一下很受欢迎的网站的浏览量

22
00:01:47,856 --> 00:01:52,509
我们很容易得到至少上亿条的记录

23
00:01:52,509 --> 00:01:57,407
假设我们要训练一个线性回归模型 或者是逻辑回归模型

24
00:01:57,407 --> 00:02:01,692
这是梯度下降的规则

25
00:02:01,692 --> 00:02:05,372
当你在看需要些什么来算梯度的时候

26
00:02:05,372 --> 00:02:09,992
也就是这边的这个项 当m是一个亿的时候

27
00:02:09,992 --> 00:02:13,976
你需要加一亿个项

28
00:02:13,976 --> 00:02:18,977
来计算这些导数项和计算一步的梯度下降

29
00:02:18,977 --> 00:02:25,627
用求一亿个项目总和的计算量

30
00:02:25,627 --> 00:02:28,628
来计算仅仅一步的梯度下降

31
00:02:28,628 --> 00:02:31,530
在接下来的视频里我们会讲

32
00:02:31,530 --> 00:02:38,413
把这个算法换掉的或者是找一个效率更高的算法来算这个导数

33
00:02:38,413 --> 00:02:41,709
在这一系列讲大型的机器学习的视频后

34
00:02:41,709 --> 00:02:47,045
我们会知道如何拟合模型 线性回归 逻辑回归 神经网络等等

35
00:02:47,045 --> 00:02:50,990
包括上亿数据的例子

36
00:02:50,990 --> 00:02:56,035
当然 在我们训练一个上亿条数据的模型之前

37
00:02:56,035 --> 00:03:01,276
我们还应该问自己 为什么不用几千条数据呢

38
00:03:01,276 --> 00:03:04,923
也许我们可以随机从上亿条的数据集里选个一千条的子集

39
00:03:04,923 --> 00:03:10,254
然后用我们的算法计算

40
00:03:10,254 --> 00:03:16,076
在我们投资精力和开发软件来训练大数据的模型之前

41
00:03:16,076 --> 00:03:22,461
往往作为一个很好的检查是去看看用一千个数据是否合适

42
00:03:22,461 --> 00:03:29,731
来检查小一些的数据集是不是好用

43
00:03:29,731 --> 00:03:33,958
也就是说用一个小一些的m等于1000的训练集

44
00:03:33,958 --> 00:03:37,797
通常的方法是画学习曲线

45
00:03:37,797 --> 00:03:46,872
如果你画了学习曲线而且你的训练目标看上去像这样

46
00:03:46,872 --> 00:03:49,553
这是J_train(θ)

47
00:03:49,553 --> 00:03:56,422
如果你的j交叉验证集的目标 J_cv(θ)看上去像这个

48
00:03:56,422 --> 00:04:00,310
这看起来像高方差的学习算法

49
00:04:00,310 --> 00:04:05,913
我们会对增加训练集的大小来提高性能更有信心

50
00:04:05,913 --> 00:04:10,462
而相比之下如果你画的学习曲线是这样的

51
00:04:10,462 --> 00:04:20,339
你的训练目标是这样的 你的交叉验证是那样的

52
00:04:20,339 --> 00:04:24,292
这看起来像经典的高偏差学习算法

53
00:04:24,292 --> 00:04:28,084
在后一种例子里 如果我们把这个画到

54
00:04:28,084 --> 00:04:33,437
m等于1000 和当m大于500小于1000的时候

55
00:04:33,437 --> 00:04:39,400
看上去增加m到上亿会不一定会好很多

56
00:04:39,400 --> 00:04:42,736
我们继续用m等于1000就很好了

57
00:04:42,736 --> 00:04:47,000
而不是花很多精力去找出这个算法的规模

58
00:04:47,000 --> 00:04:51,029
当然 如果你的情况是如右图所示

59
00:04:51,029 --> 00:04:53,885
一个很自然的方法是多加一些特征

60
00:04:53,885 --> 00:04:58,484
或者在你的神经网络里加一些隐藏的单元等等

61
00:04:58,484 --> 00:05:04,627
所以最后你会变成一个像左边的图 也许这相当于m等于1000

62
00:05:04,627 --> 00:05:09,553
这给你更多的信心去花时间在添加基础设施来改进算法

63
00:05:09,553 --> 00:05:14,735
而不是用多于一千条数据来建模 会更加有效果

64
00:05:14,735 --> 00:05:19,642
所以在大规模的机器学习中 我们喜欢找到合理的计算量的方法

65
00:05:19,642 --> 00:05:24,026
或高效率的计算量的方法来处理大的数据集

66
00:05:24,026 --> 00:05:26,826
在接下来的视频里 我们会看到两个主要的想法

67
00:05:26,826 --> 00:05:33,464
第一个叫做随机的梯度下降 第二个叫做映射化简 来处理大数据集

68
00:05:33,464 --> 00:05:39,986
在学习完这些方法后 希望这可以帮助你的学习算法处理大数据

69
00:05:39,986 --> 00:05:43,986
并且让你在不同的应用上有更好的表现 【教育无边界字幕组】翻译：teddypear 校对/审核：所罗门捷列夫