1
00:00:00,251 --> 00:00:05,622
बहुत से लर्निंग अल्गोरिद्म्स में, जैसे लिनियर रिग्रेशन, लोजिस्टिक रिग्रेशन और न्यूरल नेटवर्क्स, 

2
00:00:05,622 --> 00:00:11,955
जिस तरह से हम डिराइव करते हैं अल्गोरिद्म वह था बनाना एक कोस्ट फ़ंक्शन या बनाना एक ऑप्टिमायज़ेशन अब्जेक्टिव.

3
00:00:11,955 --> 00:00:16,476
और तब इस्तेमाल करते हुए एक अल्गोरिद्म ग्रेडीयंट डिसेंट जैसा मिनमायज़ करने के लिए उस कॉस्ट फ़ंक्शन को.

4
00:00:16,476 --> 00:00:22,461
हमारे पास है एक बहुत बड़ा ट्रेनिंग सेट, ग्रेडीयंट डिसेंट हो जाता है एक कॉम्प्यूटेशनली खर्चीली  प्रक्रिया.

5
00:00:22,461 --> 00:00:29,300
इस विडीओ में, हम बात करेंगे एक बदलाव की मौलिक ग्रेडीयंट डिसेंट अल्गोरिद्म में जिसे कहते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट,

6
00:00:29,300 --> 00:00:37,841
जो हमें स्केल करने देता है इन अल्गोरिद्म्स को अधिक बड़े ट्रेनिंग सेट्स पर.

7
00:00:37,841 --> 00:00:41,928
मान लो आप ट्रेन कर रहे हैं एक लिनीअर रेग्रेशन मॉडल ग्रेडीयंट डिसेंट इस्तेमाल करके.

8
00:00:41,928 --> 00:00:48,055
एक पुनरावृति करते हुए, हायपॉथिसस दिखेगी ऐसी, और कॉस्ट फ़ंक्शन दिखेगा ऐसा,

9
00:00:48,055 --> 00:00:54,459
जो है सम आधा ऐव्रिज स्क्वेर्ड एरर्स का आपकी हायपॉथिसस का आपके एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर,

10
00:00:54,459 --> 00:00:59,705
और कॉस्ट फ़ंक्शन जो हमने पहले देखा है दिखता है इस प्रकार का धनुष के आकार का फ़ंक्शन.

11
00:00:59,705 --> 00:01:06,659
अत:, प्लॉट करने पर इसे पेरमिटर्स थीटा 0 और थीटा 1 के फ़ंक्शन की तरह, कोस्ट फ़ंक्शन जे है एक प्रकार का धनुष के आकार का फ़ंक्शन.

12
00:01:06,659 --> 00:01:10,999
और ग्रेडीयंट डिसेंट दिखता है ऐसा, जहाँ ग्रेडीयंट डिसेंट के अंदर का लूप

13
00:01:10,999 --> 00:01:15,594
आप बार बार अप्डेट करते हो पेरमिटर्स थीटा को उस इक्स्प्रेशन से.

14
00:01:15,594 --> 00:01:22,574
अब बाक़ी के इस विडीओ में, मैं इस्तेमाल करता रहूँगा लिनीअर रेग्रेशन को मेरे उदाहरण के लिए.

15
00:01:22,574 --> 00:01:29,371
लेकिन विचार यहाँ हैं, विचार स्टोकस्टिक ग्रेडीयंट डिसेंट का है पूर्णतया सार्वजनिक और अप्लाई करता है दूसरे लर्निंग अल्गोरिद्म्स को भी

16
00:01:29,371 --> 00:01:38,011
जैसे लजिस्टिक रेग्रेशन, न्यूरल नेटवर्क्स और दूसरे अल्गोरिद्म्स जो आधारित हैं ट्रेन करने में ग्रेडीयंट डिसेंट को एक विशेष ट्रेनिंग सेट पर.

17
00:01:38,011 --> 00:01:43,236
तो यहाँ है एक चित्र कि ग्रेडीयंट डिसेंट क्या करता है, यदि पेरमिटर्स इनिशलाइज्ड किए हैं पोईँट को वहाँ पर 

18
00:01:43,236 --> 00:01:50,072
तब जैसे आप रन करते हैं ग्रेडीयंट डिसेंट, ग्रेडीयंट डिसेंट की अलग इटरेशन्स ले जायेंगी पेरमिटर्स को ग्लोबल मिनिमम पर.

19
00:01:50,072 --> 00:01:55,193
अत: लो एक ट्रेजेक्टरी / प्रक्षेप पथ जो वैसा दिखता है और जाता है सीधा ग्लोबल मिनिमम पर.

20
00:01:55,193 --> 00:01:59,561
अब, समस्या ग्रेडीयंट डिसेंट के साथ है कि यदि एम बड़ा है.

21
00:01:59,561 --> 00:02:08,382
तब कम्प्यूट करना इस डेरिवेटिव टर्म को काफ़ी महँगा हो सकता है क्योंकि क़ीमत है सम करना सारे एम इग्ज़ाम्पल्ज़ पर.

22
00:02:08,382 --> 00:02:15,644
अत: यदि एम है 300 मिल्यन / 30 करोड़, सही. अत: द यूनाइटेड स्टेट्स में, क़रीब 300 मिल्यन लोग हैं.

23
00:02:15,644 --> 00:02:20,783
और इसलिए द यूएस या यूनाइटेड स्टेट्स का सेन्सस / जनगणना का डेटा शायद होगा स्तर का उतने रेकर्ड्ज़. 

24
00:02:20,783 --> 00:02:26,715
तो आप फ़िट करना चाहते हैं लिनीअर रेग्रेशन मॉडल उसे तब आपको करना पड़ेगा सम 300 मिल्यन रेकर्ड्ज़ पर.

25
00:02:26,715 --> 00:02:36,385
और वह बहुत महँगा है. अल्गोरिद्म को एक नाम देने के लिए, यह विशेष वर्ज़न ग्रेडीयंट डिसेंट का बैच ग्रेडीयंट डिसेंट भी कहलाता है.

26
00:02:36,385 --> 00:02:41,352
और टर्म बैच बताती है कि हम देख रहे हैं सारे ट्रेनिंग इग्ज़ाम्पल्ज़ को एक साथ.

27
00:02:41,352 --> 00:02:44,303
हम इसे कहते हैं एक प्रकार से बैच सारे ट्रेनिंग इग्ज़ाम्पल्ज़ का.

28
00:02:44,303 --> 00:02:51,853
और यह वास्तव में नहीं, शायद श्रेष्ठतम नाम लेकिन यह है वह जो मशीन लर्निंग के लोग ग्रेडीयंट डिसेंट के इस ख़ास वर्ज़न को कहते हैं.

29
00:02:51,853 --> 00:02:57,157
और यदि आप कल्पना करें कि आपके पास हैं 300 मिल्यन जनगणना के रेकर्ड्ज़ डिस्क पर स्टोर किये हुए.

30
00:02:57,157 --> 00:03:05,945
जिस तरह से यह अल्गोरिद्म काम करता है आपको चाहिए रीड करना आपकी कम्प्यूटर की मेमरी में सारे 300 मिल्यन रेकर्ड्ज़ कम्प्यूट करने के लिए यह डेरिवेटिव टर्म.

31
00:03:05,945 --> 00:03:11,508
आपको करने पड़ते हैं स्ट्रीम सारे ये रेकर्ड्ज़ कम्प्यूटर से क्योंकि कम्प्यूटर मेमरी सारे रेकर्ड्ज़ स्टोर नहीं कर सकती.

32
00:03:11,508 --> 00:03:16,425
तो आपको उनमें से रीड करना पड़ता है और धीरे-धीरे, आप जानते हैं इकट्ठा करते हैं सम कम्प्यूट करने के लिए डेरिवेटिव.

33
00:03:16,425 --> 00:03:21,452
और तब यह सब काम कर लेने के बाद, उससे आपका एक स्टेप होता है ग्रेडीयंट डिसेंट का.

34
00:03:21,452 --> 00:03:24,749
और तब आपको यह पूरा काम दोबारा करना पड़ता है.

35
00:03:24,749 --> 00:03:28,424
आप जानते हैं, स्कैन करना 300 मिल्यन रेकर्ड्ज़ से, इकट्ठा करना ये सम.

36
00:03:28,424 --> 00:03:32,578
और तब यह सब काम कर लेने के बाद, आप लेते हैं एक और स्टेप ग्रेडीयंट डिसेंट का.

37
00:03:32,578 --> 00:03:36,959
और तब दोबारा उसे करो. और तब आप लेते हैं एक तीसरा स्टेप. और इसी प्रकार आगे.

38
00:03:36,959 --> 00:03:40,819
और इसलिए यह लेगा बहुत अधिक समय अल्गोरिद्म को कन्वर्ज करने में.

39
00:03:40,819 --> 00:03:45,375
बैच ग्रेडीयंट डिसेंट के विपरीत, हम बनाएँगे एक भिन्न अल्गोरिद्म 

40
00:03:45,375 --> 00:03:50,465
जिसे आवश्यकता नहीं है देखने की सारे ट्रेनिंग इग्ज़ाम्पल्ज़ प्रत्येक इटरेशन में,

41
00:03:50,465 --> 00:03:55,118
लेकिन वह देखता हैं केवल एक अकेला ट्रेनिंग इग्ज़ाम्पल एक इटरेशन में,

42
00:03:55,118 --> 00:03:59,617
नए अल्गोरिद्म पर जाने से पहले, यहाँ है सिर्फ़ एक बैच ग्रेडीयंट डिसेंट अल्गोरिद्म लिखा हुआ फिर से 

43
00:03:59,617 --> 00:04:05,794
वह है कॉस्ट फ़ंक्शन और वह है अप्डेट और निश्चय ही यह टर्म यहाँ, 

44
00:04:05,794 --> 00:04:10,678
जो इस्तेमाल की जाती है ग्रेडीयंट डिसेंट रूल मैं, यह है पर्शियल डेरिवेटिव 

45
00:04:10,678 --> 00:04:17,933
विद रिस्पेक्ट टु पेरामिटरज़ थीटा जे हमारे ऑप्टिमायज़ेशन अब्जेक्टिव का, जे ट्रेन थीटा का.

46
00:04:17,933 --> 00:04:23,386
अब, चलिए देखते हैं एक अधिक एफ़िशिएँट अल्गोरिद्म जो स्केल बेहतर करता है बड़े डेटा सेट्स को.

47
00:04:23,386 --> 00:04:26,489
बनाने के लिए अल्गोरिद्म जिसे कहते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट,

48
00:04:26,489 --> 00:04:32,657
यह संचालित करता है कॉस्ट फ़ंक्शन को एक थोड़े अलग ढंग से तब वे परिभाषित करते हैं कॉस्ट पेरामिटर थीटा की 

49
00:04:32,657 --> 00:04:40,471
विद रिस्पेक्ट टु एक ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ) बराबर वन हाफ़ टाइम्ज़ स्क्वेर्ड एरर 

50
00:04:40,471 --> 00:04:44,791
जो मेरी हायपॉथिसस उठाती है उस ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर.

51
00:04:44,791 --> 00:04:53,386
अत: यह कॉस्ट फ़ंक्शन टर्म का वास्तव में मतलब है कि कितना सही है मेरी हायपॉथिसस एक अकेले इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर.

52
00:04:53,386 --> 00:05:01,010
अब आप ध्यान करें कि पूरा कॉस्ट फ़ंक्शन जे ट्रेन अब लिखा जा सकता है इसी समान.

53
00:05:01,010 --> 00:05:09,606
अत: जे ट्रेन हैं केवल ऐव्रिज मेरे एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर मेरी हायपॉथिसस की कॉस्ट का उस इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर.

54
00:05:09,606 --> 00:05:13,522
इस दृष्टिकोण के साथ कॉस्ट फ़ंक्शन पर लिनीअर रेग्रेशन के,

55
00:05:13,522 --> 00:05:17,636
अब मैं लिखता हूँ क्या स्टोकस्टिक ग्रेडीयंट डिसेंट करता है.

56
00:05:17,636 --> 00:05:26,940
पहला स्टेप स्टोकस्टिक ग्रेडीयंट डिसेंट का है कि क्रमरहित करना डेटा सेट को.

57
00:05:26,940 --> 00:05:32,539
अत: उससे मेरा मतलब है कि रैंडम ढंग से फेर-बदल करना या रैंडम ढंग से क्रम बदलना आपके एम ट्रेनिंग इग्ज़ाम्पल्ज़ का.

58
00:05:32,539 --> 00:05:37,450
यह एक प्रकार से स्टैंडर्ड प्री-प्रासेसिंग स्टेप हैं मैं थोड़ी देर में इस पर वापिस आऊँगा.

59
00:05:37,450 --> 00:05:42,997
लेकिन मुख्य काम होता है ग्रेडीयंट डिसेंट का निम्न प्रकार से.

60
00:05:42,997 --> 00:05:48,150
हम दोहराएँगे आइ बराबर 1 से एम तक.

61
00:05:48,150 --> 00:05:53,067
अत: हम बार बार जाएँगे होते हुए मेरे ट्रेनिंग इग्ज़ाम्पल्ज़ से और करेंगे निम्न अप्डेट.

62
00:05:53,067 --> 00:06:06,523
अप्डेट करना है पेरामिटर थीटा जे को थीटा जे माइनस अल्फ़ा टाइम्ज़ एच ऑफ़ एक्स(आइ) माइनस वाय(आइ) टाइम्ज़ एक्स(आइ)जे.

63
00:06:06,523 --> 00:06:12,961
और हम करेंगे यह अप्डेट हमेशा की तरह जे की सारी वैल्यूज़ के लिए. 

64
00:06:12,961 --> 00:06:24,708
अब, आप ध्यान दें कि यह टर्म यहाँ पर है बिल्कुल वैसी जो हमारे पास थी समेशन के अंदर बैच ग्रेडीयंट डिसेंट में.

65
00:06:24,708 --> 00:06:31,256
वैसे तो, आप में से वे जो जानते हैं कैल्क्युलुस यह दिखाना सम्भव है कि वह टर्म, जो है यह टर्म यहाँ,

66
00:06:31,256 --> 00:06:43,511
बराबर है पर्शियल डेरिवेटिव के विद रिस्पेक्ट टु मेरे पेरामिटर थीटा जे के कॉस्ट पेरामिटरज़ थीटा की एक्स(आइ), वाय (आइ) पर.

67
00:06:43,511 --> 00:06:47,383
जहाँ कॉस्ट है निस्संदेह यह चीज़ जो परिभाषित की गई थी पहले.

68
00:06:47,383 --> 00:06:52,081
और सिर्फ़ रैप अप करने के लिए इस अल्गोरिद्म को, मैं बंद करता हूँ मेरी कर्ली ब्रेसिज़ यहाँ पर.

69
00:06:52,081 --> 00:06:59,365
तो स्टोकस्टिक ग्रेडीयंट डिसेंट क्या करता है कि यह वास्तव में स्कैन करता है ट्रेनिंग इग्ज़ाम्पल्ज़ में से.

70
00:06:59,365 --> 00:07:04,349
और पहले यह देखता है मेरा पहला ट्रेनिंग इग्ज़ाम्पल एक्स(1), वाय(1).

71
00:07:04,349 --> 00:07:09,399
और तब देखते हुए सिर्फ़ इस पहले ट्रेनिंग इग्ज़ाम्पल को, यह लेता है जैसे एक मूलत: एक छोटा ग्रेडीयंट डिसेंट स्टेप 

72
00:07:09,399 --> 00:07:13,725
विद रिस्पेक्ट टु कॉस्ट सिर्फ़ पहले ट्रेनिंग इग्ज़ाम्पल की.

73
00:07:13,725 --> 00:07:15,717
अत: दूसरे शब्दों में, हम देखते हैं पहले इग्ज़ाम्पल पर 

74
00:07:15,717 --> 00:07:21,214
और बदलते हैं पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ पहला ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर.

75
00:07:21,214 --> 00:07:29,244
ऐसा कर चुकने के बाद अंदर यह इनर फ़ॉर-लूप तब जाएगा दूसरे ट्रेनिंग इग्ज़ाम्पल पर.

76
00:07:29,244 --> 00:07:33,848
और यह क्या करेगा कि वहाँ लेगा एक और छोटा स्टेप पेरामिटर स्पेस में, 

77
00:07:33,848 --> 00:07:39,682
अत: बदलेगा पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ एक दूसरा ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर.

78
00:07:39,682 --> 00:07:44,130
ऐसा कर लेने के बाद यह जाएगा तीसरे ट्रेनिंग इग्ज़ाम्पल पर.

79
00:07:44,130 --> 00:07:51,722
और बदलेगा पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ एक तीसरा ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर, और इसी प्रकार आगे

80
00:07:51,722 --> 00:07:55,114
जब तक आप जानते हैं, आप पूरे ट्रेनिंग सेट से गुज़र नहीं लेते.

81
00:07:55,114 --> 00:08:01,297
और तब यह बाहरी रिपीट लूप शायद इसे करवाएगा बहुत से पास्सेस पूरे ट्रेनिंग सेट के ऊपर.

82
00:08:01,297 --> 00:08:07,346
यह दृष्टिकोण स्टोकस्टिक ग्रेडीयंट डिसेंट का प्रेरित भी करता है कि क्यों हम चाहते थे शुरू करना क्रम रहित फेर-बदल करना अपने डेटा सेट को.

83
00:08:07,346 --> 00:08:10,772
यह हमें नहीं दिखाता कि जब हम स्कैन करते हैं डेटा सेट में यहाँ,

84
00:08:10,772 --> 00:08:15,197
कि हम जाते हैं ट्रेनिंग इग्ज़ाम्पल्ज़ पर एक प्रकार के क्रम रहित विधि से.

85
00:08:15,197 --> 00:08:21,229
निर्भर करते हुए कि क्या आपका डेटा पहले से ही क्रम रहित है या पहले से ही क्रम बद्ध है किसी भिन्न क्रम में,

86
00:08:21,229 --> 00:08:26,391
व्यावहारिक रूप में यह गति बढ़ा देता है स्टोकस्टिक ग्रेडीयंट डिसेंट की थोड़ी सी.

87
00:08:26,391 --> 00:08:30,985
अत: ध्यान रखते हुए सेफ़्टी का, यह अक्सर बेहतर होता है क्रम रहित फेर-बदल करना डेटा में यदि आप विश्वस्त नहीं हैं

88
00:08:30,985 --> 00:08:34,056
कि यह आपको मिला कर्म रहित तरीक़े से.

89
00:08:34,056 --> 00:08:37,240
परंतु, महत्वपूर्ण है एक और दृष्टिकोण स्टोकस्टिक ग्रेडीयंट डिसेंट का है 

90
00:08:37,240 --> 00:08:45,504
कि काफ़ी कुछ ग्रेडीयंट डिसेंट जैसा है लेकिन इंतज़ार करने के स्थान पर सम अप करने के सारी ग्रेडीयंट टर्म्ज़ का पूरे एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर, 

91
00:08:45,504 --> 00:08:50,624
हम क्या करते हैं कि हम लेते हैं यह ग्रेडीयंट टर्म इस्तेमाल करके सिर्फ़ एक अकेला ट्रेनिंग इग्ज़ाम्पल

92
00:08:50,624 --> 00:08:54,810
और हमने शुरू कर दिया है सुधार पेरमिटर्स में अभी से ही.

93
00:08:54,810 --> 00:09:02,248
अत: बजाय, आप जानते हैं, इंतज़ार करने के जाने का सारे 300,000 ट्रेनिंग इग्ज़ाम्पल्ज़ में से यूनाइटेड स्टेट्स के जनगणना रेकर्ड्ज़ में से,

94
00:09:02,248 --> 00:09:05,632
मतलब, बजाय इस आवश्यकता के कि स्कैन करें सारे ट्रेनिंग इग्ज़ाम्पल्ज़ 

95
00:09:05,632 --> 00:09:09,947
इससे पहले कि बदल सकते हैं पेरमिटर्स थोड़े थोड़े और जा सकते हैं ग्लोबल मिनिमम की तरफ़.

96
00:09:09,947 --> 00:09:14,975
स्टोकस्टिक ग्रेडीयंट डिसेंट में इसके स्थान पर हम सिर्फ़ देखते हैं एक अकेले ट्रेनिंग इग्ज़ाम्पल पर 

97
00:09:14,975 --> 00:09:22,188
और हम शुरू कर सकते हैं बदलाव पेरमिटर्स में इस केस में खिसकाते हुए पेरमिटर्स को ग्लोबल मिनिमम की तरफ़.

98
00:09:22,188 --> 00:09:27,558
तो, यहाँ है अल्गोरिद्म लिखा हुआ फिर से जहाँ पहला स्टेप है क्रमरहित करना डेटा को

99
00:09:27,558 --> 00:09:35,089
और दूसरा स्टेप हैं जहाँ असल में काम होता है जहाँ वह अप्डेट है विद रिस्पेक्ट टु एक अकेला ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ).

100
00:09:35,089 --> 00:09:40,139
तो देखते है क्या करता है यह अल्गोरिद्म पेरमिटर्स को.

101
00:09:40,139 --> 00:09:43,467
पहले, हमने देखा था जब हम कर रहे थे बैच ग्रेडीयंट डिसेंट,

102
00:09:43,467 --> 00:09:46,331
कि वह अल्गोरिद्म देखता है सारे ट्रेनिंग इग्ज़ाम्पल्ज़ को एक समय में,

103
00:09:46,331 --> 00:09:53,397
बैच ग्रेडीयंट डिसेंट लेगा, आप जानते हैं, लेता है एक उचित रूप से सीधी रेखा का पथ ग्लोबल मिनिमम पर जाने के लिए इस प्रकार.

104
00:09:53,397 --> 00:09:59,956
इसके विपरीत स्टोकस्टिक ग्रेडीयंट डिसेंट में प्रत्येक इटरेशन होगी अधिक शीघ्र 

105
00:09:59,956 --> 00:10:03,108
क्योंकि हमें आवश्यकता नहीं है सम करने की पूरे ट्रेनिंग इग्ज़ाम्पल्ज़ पर.

106
00:10:03,108 --> 00:10:07,259
लेकिन प्रत्येक इटरेशन कोशिश कर रही है फ़िट करने के लिए एक ट्रेनिंग इग्ज़ाम्पल को बेहतर.

107
00:10:07,259 --> 00:10:13,931
अत:, यदि हमें शुरू करना होता स्टोकस्टिक ग्रेडीयंट डिसेंट, ओह, चलो शुरू करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट उस पोईँट पर ऐसे ही.

108
00:10:13,931 --> 00:10:19,556
पहली इटरेशन, आप जानते हैं, शायद ले जाएगी पेरमिटर्स को उस दिशा में और 

109
00:10:19,556 --> 00:10:23,791
शायद दूसरी इटरेशन दख़ते हुए सिर्फ़ दूसरे इग्ज़ाम्पल को शायद सिर्फ़ संयोग से,

110
00:10:23,791 --> 00:10:28,278
हम होते हैं अधिक बदकिस्मत और असल में जाते हैं एक ग़लत दिशा में पेरमिटर्स की उस प्रकार.

111
00:10:28,278 --> 00:10:33,731
तीसरी इटरेशन में जहाँ हम कोशिश करते हैं बदलने की पेरमिटर्स को फ़िट करने के लिए सिर्फ़ तीसरा ट्रेनिंग इग्ज़ाम्पल बेहतर,

112
00:10:33,731 --> 00:10:36,418
शायद हम जाते हैं उस दिशा में.

113
00:10:36,418 --> 00:10:42,717
और तब हम देखते हैं चौथा ट्रेनिंग इग्ज़ाम्पल और हम वह करेंगे. पाँचवा, छठा, 7वाँ और आगे.

114
00:10:42,717 --> 00:10:46,725
और जैसे आप रन करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, आपको क्या मिलता है कि 

115
00:10:46,725 --> 00:10:52,923
यह अक्सर खिसकाएगा पेरमिटर्स को ग्लोबल मिनिमम की दिशा में, लेकिन हमेशा नहीं.

116
00:10:52,923 --> 00:11:00,117
और इसलिए लेगा कुछ अधिक अनियमित सा दिखता हुआ घुमावदार रास्ता ग्लोबल मिनिमम तक पहुँचने का.

117
00:11:00,117 --> 00:11:07,630
और असल में जैसे आप रन करते हो स्टोकस्टिक ग्रेडीयंट डिसेंट यह वास्तव में कन्वर्ज नहीं करता उसी मायने में जैसे बैच ग्रेडीयंट डिसेंट करता है 

118
00:11:07,630 --> 00:11:15,196
और यह इधर उधर होता है लगातार किसी क्षेत्र में जो किसी क्षेत्र में नज़दीक ग्लोबल मिनिमम के,

119
00:11:15,196 --> 00:11:18,740
लेकिन यह पहुँचता नहीं है ग्लोबल मिनिमम पर और रूकता नहीं है वहाँ.

120
00:11:18,740 --> 00:11:21,676
लेकिन व्यावहारिक रुप में, यह समस्या नहीं है क्योंकि, आप जानते हैं, जब 

121
00:11:21,676 --> 00:11:26,788
तक पेरमिटर्स पहुँचते हैं किसी क्षेत्र में जो शायद काफ़ी नज़दीक ग्लोबल मिनिमम के.

122
00:11:26,788 --> 00:11:32,164
अत:, जब पेरमिटर्स पहुँचते हैं काफी नज़दीक ग्लोबल मिनिमम के, वह होगी एक काफ़ी अच्छी हायपॉथिसस 

123
00:11:32,164 --> 00:11:36,340
और इसलिए रन करके स्टोकस्टिक ग्रेडीयंट डिसेंट 

124
00:11:36,340 --> 00:11:43,658
हमें मिलता हैं एक पेरामिटर नज़दीक ग्लोबल मिनिमम के और वह पर्याप्त है, आप जानते हैं, वास्तव में काम के उद्देश्य से.

125
00:11:43,658 --> 00:11:47,121
सिर्फ़ एक अंतिम गौण बात. स्टोकस्टिक ग्रेडीयंट डिसेंट में,

126
00:11:47,121 --> 00:11:51,099
हमारे पास था यह बाहरी लूप रिपीट जो कहता है करने को इस आंतरिक लूप को बहुत बार.

127
00:11:51,099 --> 00:11:53,892
तो, कितनी बार हम करते हैं रिपीट इस बाहरी लूप को?

128
00:11:53,892 --> 00:11:59,336
निर्भर करते हुए ट्रेनिंग सेट के साइज़ पर, इस लूप को एक बार करना भी शायद पर्याप्त होगा.

129
00:11:59,336 --> 00:12:02,064
और, आप जानते है, शायद 10 बार काफ़ी आम है 

130
00:12:02,064 --> 00:12:05,852
तो हम शायद करें रिपीट इस आंतरिक लूप को कोई एक से दस बार तक.

131
00:12:05,852 --> 00:12:12,309
अत: यदि हमारे पास है एक आप जानते है, वास्तव में मैसिव डेटा सेट जैसे यूएस जनगणना का उस उदाहरण में 

132
00:12:12,309 --> 00:12:15,260
जिसकी मैं बात करता रहा हूँ लगभग 300 मिल्यन इग्ज़ाम्पल्ज़ वाला,

133
00:12:15,260 --> 00:12:19,609
यह सम्भव है कि जब तक आप लेते हैं एक पास आपके ट्रेनिंग सेट से.

134
00:12:19,609 --> 00:12:23,073
अत: यह है आइ बराबर 1 से 300 मिल्यन तक.

135
00:12:23,073 --> 00:12:25,720
यह सम्भव है कि जब तक आप लेते हैं एक पास आपके ट्रेनिंग सेट से

136
00:12:25,720 --> 00:12:29,872
आपके पास हो एक सर्वथा अच्छी हायपॉथिसस.

137
00:12:29,872 --> 00:12:36,613
जिस स्थिति में, आप जानते हैं, यह आंतरिक लूप आपको शायद करना पड़े सिर्फ़ एक बार यदि एम है बहुत अधिक बड़ा.

138
00:12:36,613 --> 00:12:43,071
लेकिन सामान्य तौर पर लेना कोई 1 से 10 पास्सेस आपके डेटा से, आप जानते है, शायद काफ़ी आम है.

139
00:12:43,071 --> 00:12:45,439
परंतु वास्तव में यह निर्भर करता है साइज़ पर आपके डेटा सेट पर.

140
00:12:45,439 --> 00:12:49,413
और यदि आप तुलना करते हैं इसकी ग्रेडीयंट डिसेंट के साथ.

141
00:12:49,413 --> 00:12:53,905
बैच ग्रेडीयंट डिसेंट में, एक पास लेने के बाद आपके पूरे डेटा सेट से,

142
00:12:53,905 --> 00:12:57,034
आपने लिया होगा केवल एक अकेला ग्रेडीयंट डिसेंट स्टेप.

143
00:12:57,034 --> 00:13:01,983
अत: एक इनमें से छोटा स्टेप ग्रेडीयंट डिसेंट का होगा जहाँ आप लेते हैं एक छोटा ग्रेडीयंट डिसेंट स्टेप 

144
00:13:01,983 --> 00:13:05,776
और इसलिए स्टोकस्टिक ग्रेडीयंट डिसेंट हो सकता है काफ़ी तीव्र.

145
00:13:05,776 --> 00:13:10,880
तो वह था स्टोकस्टिक ग्रेडीयंट डिसेंट अल्गोरिद्म.

146
00:13:10,880 --> 00:13:15,594
और यदि आप इसे इम्प्लमेंट करते है, आशा है वह आपको स्केल अप करने देगा बहुत से आपके लर्निंग अल्गोरिद्म्स को 

147
00:13:15,594 --> 99:59:59,000
एक काफ़ी बड़े सेट्स पर और मिल सकती है बेहतर पर्फ़ॉर्मन्स उस तरह से.