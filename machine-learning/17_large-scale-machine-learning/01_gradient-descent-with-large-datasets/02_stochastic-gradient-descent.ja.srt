1
00:00:00,251 --> 00:00:05,622
多くの学習アルゴリズムにとって、線形回帰やロジスティック回帰やニューラルネットワークなどにとって、

2
00:00:05,622 --> 00:00:11,955
アルゴリズムを導出する方法は、まずコスト関数、または目的関数を考えて、

3
00:00:11,955 --> 00:00:16,476
そしてそれを最急降下法なりのアルゴリズムを使って最小化する、という物だった。

4
00:00:16,476 --> 00:00:22,461
大量のトレーニングセットがある時は、最急降下法は極めて計算的に高価な手続きとなる。

5
00:00:22,461 --> 00:00:29,300
このビデオでは、基本的な最急降下法のアルゴリズムを改変した、確率的最急降下法という物を議論する、

6
00:00:29,300 --> 00:00:37,841
それはこれらのアルゴリズムをもっとビッグなトレーニングセットにスケール出来るようにする。

7
00:00:37,841 --> 00:00:41,928
最急降下法で線形回帰のモデルをトレーニングしているとしよう。

8
00:00:41,928 --> 00:00:48,055
軽く復習しておくと、仮説はこんな形、コスト関数はこんな感じ。

9
00:00:48,055 --> 00:00:54,459
それは1/2の仮説の二乗誤差の平均の、mのトレーニング手本に渡る和を取った物だ。

10
00:00:54,459 --> 00:00:59,705
そしてコスト関数は以前見たように、こんな弓なりの形の関数だ。

11
00:00:59,705 --> 00:01:06,659
パラメータシータ0とシータ1の関数としてプロットすると、コスト関数Jは弓型の関数となる。

12
00:01:06,659 --> 00:01:10,999
そして最急降下法はこんな感じだ。
最急降下法の内側のループでは、

13
00:01:10,999 --> 00:01:15,594
パラメータシータをこの式を使って繰り返しアップデートしていく。

14
00:01:15,594 --> 00:01:22,574
さて、このビデオの残りの部分で、線形回帰を実行出来る例として使い続ける。

15
00:01:22,574 --> 00:01:29,371
だが確率的最急降下法のアイデア自体は完全に一般的な物で、それはその他の学習アルゴリズム、

16
00:01:29,371 --> 00:01:38,011
ロジスティック回帰とかニューラルネットワークとか、その他なんでも特定のトレーニングセットに対して最急降下法で学習するアルゴリズムになら適用出来る。

17
00:01:38,011 --> 00:01:43,236
さて、これは最急降下法が何をするかだ。
もしパラメータがここの点で初期化されたら、

18
00:01:43,236 --> 00:01:50,072
最急降下法はパラメータをグローバル最小へと持っていく。

19
00:01:50,072 --> 00:01:55,193
こんな感じの軌跡を通って、だいたいまっすぐグローバル最小へと向かう。

20
00:01:55,193 --> 00:01:59,561
ここで、最急降下法の問題点としては、もしmが大きい時には、

21
00:01:59,561 --> 00:02:08,382
この微分項を計算するのが、とても高価になってしまう、という事。
何故ならこれは全てのm手本に渡って和を取るから。

22
00:02:08,382 --> 00:02:15,644
だからもしmが3億なら、、、アメリカ合衆国にはだいたい3億人の人がいる。

23
00:02:15,644 --> 00:02:20,783
すると、US、またはアメリカ合衆国の国勢調査のデータは、そんなオーダーの数のレコードとなる。

24
00:02:20,783 --> 00:02:26,715
すると、そこに線形回帰のモデルをフィッティングしたいとすると、3億のレコードに渡って和をとらなくてはならない。

25
00:02:26,715 --> 00:02:36,385
それはとても高価だ。このアルゴリズムに名前をつけておく。
この特定の最急降下法のバージョンは、バッチ最急降下法とも呼ばれる。

26
00:02:36,385 --> 00:02:41,352
ここで「バッチ」という単語は一度に全部のトレーニング手本を見るという事実を表している。

27
00:02:41,352 --> 00:02:44,303
それをある種の、全てのトレーニング手本のバッチ、と呼ぶ。

28
00:02:44,303 --> 00:02:51,853
それは実はあんまりいい名前じゃ、ベストな名前って訳じゃない。
だが、機械学習屋の人々がこのバージョンの最急降下法をそう呼んでるんだから仕方がない。

29
00:02:51,853 --> 00:02:57,157
そしてこの3億の国勢調査のレコードをディスクに保存して退避させてしまってる事を想像してみよう。

30
00:02:57,157 --> 00:03:05,945
このアルゴリズムのまわり方としては、この微分項を計算するのに3億のレコードを全てコンピュータのメモリに読み出す必要がある。

31
00:03:05,945 --> 00:03:11,508
これらのレコード全てをコンピュータにストリーム処理しなくてはいけない。何故ならコンピュータメモリに全て保存する事は出来ないから。

32
00:03:11,508 --> 00:03:16,425
だからそれらを読んでいき、ゆっくりと和を蓄積していく事で、はじめて微分が計算出来る。

33
00:03:16,425 --> 00:03:21,452
そしてそれを全部終えたら、その結果最急降下法を1ステップだけ進める事が出来る、、、

34
00:03:21,452 --> 00:03:24,749
そしてまた全体をやりなおさなくてはいけない。

35
00:03:24,749 --> 00:03:28,424
つまり、3億のレコードを全部スキャンして、それらの和を蓄積していく。

36
00:03:28,424 --> 00:03:32,578
その仕事を全て終えても、最急降下法のちょっとのステップがもう一歩進むだけ。

37
00:03:32,578 --> 00:03:36,959
そしてまた、同じ事をする。そしてまた三歩目が進める。などなど。

38
00:03:36,959 --> 00:03:40,819
つまり、アルゴリズムが収束するのに、凄い長い時間がかかる。

39
00:03:40,819 --> 00:03:45,375
バッチ最急降下法と比較して、別のアルゴリズムを考え出していく、

40
00:03:45,375 --> 00:03:50,465
それは各イテレーションごとに全てのトレーニング手本を見なくて良く、

41
00:03:50,465 --> 00:03:55,118
一回のイテレーションでは一つのトレーニング手本単体だけを見れば良い。

42
00:03:55,118 --> 00:03:59,617
新しいアルゴリズムにうつる前に、ここにバッチ最急降下法のアルゴリズムを再掲しておこう、

43
00:03:59,617 --> 00:04:05,794
これがコスト関数で、これがアップデート、そしてここの項はもちろん

44
00:04:05,794 --> 00:04:10,678
最急降下法で用いる、偏微分項だ、

45
00:04:10,678 --> 00:04:17,933
パラメータシータjによる、我らが最適化の目的関数 J trainのシータの。

46
00:04:17,933 --> 00:04:23,386
さて、大規模なデータセットにもっと効率的にスケールするアルゴリズムを見てみよう。

47
00:04:23,386 --> 00:04:26,489
確率的最急降下法と呼ばれるアルゴリズムを用いる為に、

48
00:04:26,489 --> 00:04:32,657
このベクトル、コスト関数をちょっと違う形にして、トレーニング手本、x(i), y(i)に関する

49
00:04:32,657 --> 00:04:40,471
パラメータをシータとするコストを定義する、それはイコール、1/2掛ける、二乗誤差の

50
00:04:40,471 --> 00:04:44,791
x(i), y(i)に対して仮説が引き起こす分。

51
00:04:44,791 --> 00:04:53,386
つまりこのコスト関数の項は、私の仮説が、単体の手本、x(i)とy(i)に対してどれだけ良いかを実際に測っている。

52
00:04:53,386 --> 00:05:01,010
今、全体のコスト関数、J trainは、等価な形でこう書ける。

53
00:05:01,010 --> 00:05:09,606
つまりJ trainは、m個のトレーニング手本の仮説のコストの平均に過ぎない。

54
00:05:09,606 --> 00:05:13,522
線形回帰のコスト関数をこうやってみる見方を身につけた上で、

55
00:05:13,522 --> 00:05:17,636
確率的最急降下法が何をする物なのか、書き下してみよう。

56
00:05:17,636 --> 00:05:26,940
確率的最急降下法の最初のステップは、データセットをランダムにシャッフルする。

57
00:05:26,940 --> 00:05:32,539
ランダムにシャッフルする、という事の意味は、m個のトレーニング手本をランダムに並べ替える、という事。

58
00:05:32,539 --> 00:05:37,450
これは普通の前処理だ。後でこの件については考える。

59
00:05:37,450 --> 00:05:42,997
だが確率的最急降下法の主な部分は、その次に以下のように続く所だ。

60
00:05:42,997 --> 00:05:48,150
i=1からmまで、リピートする事の、、、

61
00:05:48,150 --> 00:05:53,067
つまりトレーニング手本を繰り返しスキャンして、以下のアップデートを実施する。

62
00:05:53,067 --> 00:06:06,523
パラメータ、シータjを シータj 引くことのアルファ掛けるh(x(i))引くことのy(i)に掛けるx(i)j。

63
00:06:06,523 --> 00:06:12,961
このアップデートをいつも通り、全てのjの値に対して行う。

64
00:06:12,961 --> 00:06:24,708
ここで、ここの項はバッチ最急降下法の和の中にある物と、完全に一致する事が分かるだろう。

65
00:06:24,708 --> 00:06:31,256
実のところ、もし解析学が得意なら、このここの項は、

66
00:06:31,256 --> 00:06:43,511
costのシータとx(i)を、パラメータシータjで偏微分した物に等しい事が示せる。

67
00:06:43,511 --> 00:06:47,383
ここでこのcostはもちろん、前に定義した物。

68
00:06:47,383 --> 00:06:52,081
このアルゴリズムのまとめとして、、、その前に中括弧を閉じておこう。

69
00:06:52,081 --> 00:06:59,365
さて、確率的最急降下法がやる事は、実際にトレーニング手本をスキャンして、

70
00:06:59,365 --> 00:07:04,349
そして最初にトレーニング手本の最初のx(1), y(1)を見る時、

71
00:07:04,349 --> 00:07:09,399
この最初の例だけを見て、最初の手本に関してだけのコストによる、

72
00:07:09,399 --> 00:07:13,725
最急降下法の小さな一ステップを実行する。

73
00:07:13,725 --> 00:07:15,717
言い換えると、最初の手本を見て、

74
00:07:15,717 --> 00:07:21,214
最初の手本のデータだけにもうちょっとだけフィットするように、パラメータを少しだけ変更する。

75
00:07:21,214 --> 00:07:29,244
これを終えたら、この内側のforループの中で、次の二番目のトレーニング手本に進む。

76
00:07:29,244 --> 00:07:33,848
そこで行う事は、またもう一歩、パラメーター空間内を進む事、

77
00:07:33,848 --> 00:07:39,682
つまりちょっとだけ良く二番目のトレーニング手本にフィットするようにパラメータを変更する。

78
00:07:39,682 --> 00:07:44,130
それを終えたら、三番目のトレーニング手本に進む。

79
00:07:44,130 --> 00:07:51,722
そして三番目のトーレニング手本にちょっとだけ良くフィットするように、パラメータを変更する。

80
00:07:51,722 --> 00:07:55,114
これをトレーニングセット全体に渡って行う。

81
00:07:55,114 --> 00:08:01,297
そしてこの外側のループが、トレーニングセット全体を複数回繰り返させる。

82
00:08:01,297 --> 00:08:07,346
この確率的最急降下法の見方は、データセットをランダムにシャッフルする事から始める理由も分かる。

83
00:08:07,346 --> 00:08:10,772
もしシャッフルせずにトレーニングセットをここからスキャンして行ったら、

84
00:08:10,772 --> 00:08:15,197
むちゃくちゃにソートされてる順番にトレーニング手本を見ていく事になる、

85
00:08:15,197 --> 00:08:21,229
その順番はデータが最初からランダムに来たか、変な風にソートされているかに寄ってしまう。

86
00:08:21,229 --> 00:08:26,391
実際的には、ランダムにソートする事は確率的最急降下法をちょっとだけスピードアップする、ちょっとだけね。

87
00:08:26,391 --> 00:08:30,985
だから念のため、それがランダムな並びと確信が持てる場合を除いて、普通は

88
00:08:30,985 --> 00:08:34,056
とりあえずデータセットをランダムにシャッフルしておく方が良い。

89
00:08:34,056 --> 00:08:37,240
だがもっと重要な点として、確率的最急降下法のもう一つの見方として、

90
00:08:37,240 --> 00:08:45,504
それは、通常の最急降下法ととても似ているが、だがこれらの微分項を全てのmトレーニング手本に渡って足すのではなく、

91
00:08:45,504 --> 00:08:50,624
この微分項を単に一つのトレーニング手本に対してだけ取る、という事をしている、

92
00:08:50,624 --> 00:08:54,810
そしてそこで既にパラメータの改善を開始してしまう。

93
00:08:54,810 --> 00:09:02,248
つまり、全てのアメリカ合衆国の国勢調査のレコード3億件をなめるのを待つのでは無く、

94
00:09:02,248 --> 00:09:05,632
パラメータをちょっとだけ改善してグローバル最小へとちょっとだけ歩を進める為に、

95
00:09:05,632 --> 00:09:09,947
全てのトレーニング手本をスキャンする事を必要とするのでは無く、

96
00:09:09,947 --> 00:09:14,975
確率的最急降下法では、手本は一つしか見る必要が無くて、

97
00:09:14,975 --> 00:09:22,188
この場合のパラメータの改善を既に始めてしまって良い、パラメータをグローバル最小へと進めるという。

98
00:09:22,188 --> 00:09:27,558
これがアルゴリズムをふたたび書き下した物だ。
最初のステップはデータをランダムにシャッフルする事で、

99
00:09:27,558 --> 00:09:35,089
二番目のステップは実際の仕事をする所だが、そこでは一つのトレーニング手本、x(i)とy(i)に関してのみでアップデートしてしまう。

100
00:09:35,089 --> 00:09:40,139
ではこのアルゴリズムがパラメータに何をしていくか、見てみよう。

101
00:09:40,139 --> 00:09:43,467
前に、バッチ最急降下法を使っている時に、

102
00:09:43,467 --> 00:09:46,331
それは全てのトレーニング手本を一度に見る物だという事を見た。

103
00:09:46,331 --> 00:09:53,397
バッチ最急降下法はグローバル最小へと向かう、かなりまっすぐな軌跡を描く傾向になる。

104
00:09:53,397 --> 00:09:59,956
それに対して確率的最急降下法は、各イテレーションはもっと早い、

105
00:09:59,956 --> 00:10:03,108
何故なら全てのトレーニング手本を足し合わせる必要が無いからだが、

106
00:10:03,108 --> 00:10:07,259
しかし各イテレーションは一つのトレーニング手本に対してだけより良くフィットするように試みるだけなので、

107
00:10:07,259 --> 00:10:13,931
だからもし確率的最急降下法を始めると、あー、確率的最急降下法をこの点とかから始めたとしよう。

108
00:10:13,931 --> 00:10:19,556
最初のイテレーションでは、この方向に進んだとする、

109
00:10:19,556 --> 00:10:23,791
二番目のイテレーションでは、うーん、二番目のイテレーションは偶然、

110
00:10:23,791 --> 00:10:28,278
ちょっとツイてなかったとしよう。そして実際には悪い方向にこんな感じでパラメータを進めてしまった。

111
00:10:28,278 --> 00:10:33,731
三度目のイテレーションでは、三番目のトレーニング手本に対してだけもっと良くフィットするようにパラメータを変更する。

112
00:10:33,731 --> 00:10:36,418
そしてこんな方向に向かったとする。

113
00:10:36,418 --> 00:10:42,717
そして四番目のトレーニング手本を見て、同じ事をする。5番目、6番目、7番目、などなど。

114
00:10:42,717 --> 00:10:46,725
そして確率的最急降下法を実行すると、こんな結果が見られる：

115
00:10:46,725 --> 00:10:52,923
だいたいはパラメータはグローバル最小の方向に向かうが、いつもそうだという訳では無い。

116
00:10:52,923 --> 00:11:00,117
つまりもっとデタラメに見える、遠回りの軌跡を通ってグローバル最小を探す。

117
00:11:00,117 --> 00:11:07,630
そして実のところ、確率的最急降下法は、バッチ最急降下法がするような意味では収束しない。

118
00:11:07,630 --> 00:11:15,196
そして最終的には、それはグローバル最小のそばのある一定の範囲をうろちょろし続けるようになる。

119
00:11:15,196 --> 00:11:18,740
だがグローバル最小にたどりついて留まりつづける、という挙動では無い。

120
00:11:18,740 --> 00:11:21,676
だが現実的には、それはそんなに問題じゃない、何故なら、

121
00:11:21,676 --> 00:11:26,788
パラメータがグローバル最小のきわめてそばの領域に居続けるなら、

122
00:11:26,788 --> 00:11:32,164
パラメータは最終的にグローバル最小に極めて近いはずなので、それは仮説としてはとても良い物となるだろう。

123
00:11:32,164 --> 00:11:36,340
だから通常、確率的最急降下法を実行すると、

124
00:11:36,340 --> 00:11:43,658
グローバル最小のそばのパラメータを得る事になり、それは実質的にはほとんどの現実的な目的にとって十分に良い物だ。

125
00:11:43,658 --> 00:11:47,121
最後に詳細を一つ。確率的最急降下法においては、

126
00:11:47,121 --> 00:11:51,099
この内側のループを複数回実行するように指示する、外側のループがある。

127
00:11:51,099 --> 00:11:53,892
では、何回外側のループは繰り返せば良い？

128
00:11:53,892 --> 00:11:59,336
トレーニングセットのサイズによっては、このループは一回で十分かもしれない。

129
00:11:59,336 --> 00:12:02,064
典型的には、10回までのどこかって所かな。

130
00:12:02,064 --> 00:12:05,852
つまりこの内側のループを1回から10回の間のどこかの回数実行すれば良い。

131
00:12:05,852 --> 00:12:12,309
つまり、もし我らが、真に大量のデータセット、例えばこのUS国勢調査のような物で、

132
00:12:12,309 --> 00:12:15,260
3億の手本とかあるならば、

133
00:12:15,260 --> 00:12:19,609
トレーニングセットを1パスだけなめる頃には、

134
00:12:19,609 --> 00:12:23,073
つまりこのforでi=1から3億まで回せば、

135
00:12:23,073 --> 00:12:25,720
そのデータセットを1パスなめ終わる頃には、

136
00:12:25,720 --> 00:12:29,872
既に十分完璧な良い仮説に到達しているかもしれない。

137
00:12:29,872 --> 00:12:36,613
その場合は、この内側のループは一回だけ実行すれば良い。凄い凄い大きなmなら。

138
00:12:36,613 --> 00:12:43,071
だが一般的には、1と10の間の価数のパスだけデータセットをなめる。この辺が普通だ。

139
00:12:43,071 --> 00:12:45,439
でもそれは本当にトレーニングセットのサイズに依存した話だ。

140
00:12:45,439 --> 00:12:49,413
バッチ最急降下法と比較してみると、

141
00:12:49,413 --> 00:12:53,905
バッチ最急降下法だと、一つのパスでトレーニングセット全体をなめて、

142
00:12:53,905 --> 00:12:57,034
それだけやってたった一歩の最急降下法のステップしか進まない。

143
00:12:57,034 --> 00:13:01,983
つまり最急降下法のこれらの小さなステップの、たった一歩のステップだけ。

144
00:13:01,983 --> 00:13:05,776
そしてこれが、確率的最急降下法がもっと早くなりうる理由だ。

145
00:13:05,776 --> 00:13:10,880
以上が確率的最急降下法アルゴリズムだ。

146
00:13:10,880 --> 00:13:15,594
そしてこれを実装すれば、あなたは多くの学習アルゴリズムをスケールアップして、

147
00:13:15,594 --> 99:59:59,000
よりビッグなデータセットに対して、もっと良いパフォーマンスが得られるように出来るだろう。