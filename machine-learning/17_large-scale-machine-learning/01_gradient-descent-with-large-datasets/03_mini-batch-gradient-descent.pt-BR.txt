No vídeo anterior, nós falamos sobre o gradiente decrescente estocástico, e como ele poderia ser bem mais veloz que o gradiente decrescente de lote completo. Neste vídeo, vamos falar sobre outra variação e que chama-se gradiente decrescente com lote parcial. E ele pode rodar às vezes até mais rápido que o gradiente descendente estocástico. Para resumir o algoritmo que fizemos uso até agora, no gradiente descendente de lote completo nós usamos todos os exemplos em cada passo. Enquanto na versão estocástica nós usamos um único exemplo em cada passo. O que o gradiente descendente de lote parcial faz é um meio termo entre esses dois. Especificamente, com esse algoritmo nós iremos usar "b" exemplos em cada iteração, onde b é um parâmetro chamado como "tamanho do lote". Então a idéia é que ele resulte em algo que fique entre os dois casos, onde usamos apenas um exemplo e onde usamos todos. E ele é exatamente igual o gradiente descendente completo, exceto pelo fato que usarei apenas uma parte dos exemplos. Uma escolha típica para o valor de b poderia ser algo como 10. E um típico intervalo para b seria algo entre 2 até 100. Estes seriam um intervalo bem comum para a escolha do gradiente descendente lote parcial. E o intuito é que ao invés de usarmos um exemplo por vez ou todos de uma só vez, que usemos b exemplos por vez. Vou escrever isso de maneira informal. Peguemos então b, neste caso, digamos que b seja igual a 10. Então pegaremos os 10 próximos exemplos do meu conjunto de dados, e isso será um determinado subconjunto xi, yi. Se forem 10 exemplos, então os índices devem ir até x (i+9), y (i+9), e esses serão esses 10 exemplos juntos, e então nós, basicamente, realizaremos um passo do gradiente decrescente usando esses 10 exemplos. Sendo a taxa de aprendizado vezes um sobre dez, vezes a somatoria através de k igual a i até i+9 de h de x(k) menos y(k) e vezes x(k). E então nessa expressão, onde a somatória acontece entre os meus dez exemplos. Esse dez, como vocês devem ver, é o tamanho do meu gradiente com lote parcial e esse i+9, o 9 vem da escolha do parâmetro b, e isso aqui será incrementado, e i vai passar pelos próximos 10 exemplos e continuar evoluindo dessa maneira. Para finalizar, só para escrever o algoritmo todo. Para simplificarmos a parte dos índices, irei assumir que temos um gradiente descendente com lote parcial de tamanho dez e conjunto de treinamento de tamanho de 1000, o que faremos é ter esse tipo de "for" com i igual a um, onze, vinte e um, pulando de 10 em 10, pois escolhemos realizar com 10 exemplos de cada vez. E depois realizamos uma atualização no gradiente decrescente usando os dez exemplos por vez, então esse 10 e esse i+9 são consequências da escolha do parâmetro b como 10. E você pode perceber que esse loop do For terminará no 991, pois se temos 1000 exemplos de treinamento, então eu preciso de 100 passos de tamanho 10 para percorre-los todos. E esse é o gradiente decrescente de lote parcial. Comparado com o completo, ele também possibilitará cálculos mais rápidos. E se pegarmos novamente o nosso exemplo do censo dos EUA com 300 milhões de exemplos de treinamento, então o que estou querendo mostrar aqui é que após utilizar os 10 primeiros exemplos, nós podemos começar a progredir na questão da definição dos parâmetros teta, nós não precisamos utilizar todos os milhões de dados. Nós só precisamos pegar os primeiros 10 exemplos e isso já nos permitirá avançar e então nós poderemos pegar os próximos dez e modificar novamente os parâmetros um pouquinho e assim por diante. E é por isso que o gradiente decrescente de lote parcial pode ser mais veloz que o completo. Basicamente, você pode começar a progredir na modificação dos parâmetros após verificar apenas dez exemplos ao invés de ter que esperar o algoritmo verificar todos os seus 300 milhões de dados. Agora a relação entre o gradiente descendente de lote parcial e o estocástico. Por que nós queremos verificar b exemplos por vez ao invés de pegar apenas um único exemplo como o gradiente decrescente estocástico? A resposta é vetorização. Particularmente, o gradiente decrescente de lote parcial tem uma grande chance de ter uma performance melhor que o estocástico apenas se for usada uma boa implementação vetorizada. Neste caso, a soma dos 10 exemplos pode ser realizada de forma vetorial, o que possibilitará o uso parcial de computação paralela para os dez exemplos. Em outras palavras, ao usar-se uma implementação vetorizada apropriada para calcular o restante dos termos, você pode usar bibliotecas de cálculo numérico e paralelizar o calculo do seu gradiente para os b exemplos, enquanto se você tivesse pegando apenas um único exemplo por vez com o caso estocástico, não haveria nada para botar em paralelo. Ou, ao menos, há menos para se colocar em paralelo. Uma desvantagem do gradiente decrescente de lote parcial é ter que lidar com um parâmetro extra b, o tamanho do lote parcial, com o qual você terá que lidar e isso talvez tome o seu tempo. Mas se você tiver uma implementação vetorizada boa, a velocidade do lote parcial pode ser ainda mais rápida que a da versão estocástica. E esse é o gradiente decrescente de lote parcial, que é um algoritmo que atua em um meio termo do que faz o gradiente descendente estocástico e o de lote completo. E se você escolher um bom valor para b, eu geralmente uso b igual a 10, mas outro valores, qualquer coisa entre 2 e 100, pode ser razoavelmente comum. Então escolhemos um valor para b e se usarmos uma boa implementação vetorial, teremos algumas vezes um resultado mais rápido que o gradiente descendente estocástico e que o gradiente descendente de lote completo.