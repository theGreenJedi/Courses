1
00:00:00,000 --> 00:00:07,306
在之前的视频中 我们讨论了随机梯度下降 以及它是怎样比批量梯度下降更快的

2
00:00:07,306 --> 00:00:12,866
在这次视频中 让我们讨论基于这些方法的另一种变形 叫做小批量梯度下降

3
00:00:12,866 --> 00:00:16,906
这种算法有时候甚至比随机梯度下降还要快一点

4
00:00:16,906 --> 00:00:22,046
首先来总结一下我们已经讨论过的算法

5
00:00:22,046 --> 00:00:26,619
在批量梯度下降中每次迭代我们都要用所有的m个样本

6
00:00:26,619 --> 00:00:31,792
然而在随机梯度下降中每次迭代我们只用一个样本

7
00:00:31,792 --> 00:00:36,120
小批量梯度下降做的介于它们之间

8
00:00:36,120 --> 00:00:46,559
准确地说 在这种方法中我们每次迭代使用b个样本 b是一个叫做"小批量规模"的参数

9
00:00:46,559 --> 00:00:52,688
所以这种算法介于随机梯度下降和批量梯度下降之间

10
00:00:52,688 --> 00:00:57,488
这就像批量梯度下降 只不过我会用小很多的批量规模

11
00:00:57,488 --> 00:01:08,672
b的一个标准取值可能是10 比如说 b的一个标准的取值可能是2到100之间的任何一个数

12
00:01:08,672 --> 00:01:13,668
因此那是小批量规模的一个非常典型的取值区间

13
00:01:13,668 --> 00:01:21,153
算法思想是我们每次用b个样本而不是每次用1个或者m个

14
00:01:21,153 --> 00:01:28,833
所以让我正式地把它写出来 我们将要确定b 例如我们假设b是10

15
00:01:28,833 --> 00:01:37,782
所以我们将要从训练集中取出接下来的10个样本 假设训练集是样本 (x(i).y(i)) 的集合

16
00:01:37,782 --> 00:01:46,114
如果是10个样本 最多索引值达到(x(i+9),y(i+9))

17
00:01:46,114 --> 00:01:57,794
这是全部的10个样本 然后我们将要用这10个样本做一个实际上是梯度下降的更新

18
00:01:57,794 --> 00:02:19,012
因此 就是学习率乘以1/10乘以 k 对 h (x(k)-y(k))×x(k)j 从i到i+9求和

19
00:02:19,012 --> 00:02:27,213
在这个表达式中 我们计算10个样本的梯度下降公式的和

20
00:02:27,229 --> 00:02:32,370
因此 这是数字10 就是小批量规模 i+9

21
00:02:32,370 --> 00:02:39,384
9来自参数b的选择 然后在这之后我们将要把 i 加10

22
00:02:39,384 --> 00:02:46,755
我们将要继续处理接下来的10个样本 然后像这样一直继续

23
00:02:46,755 --> 00:02:50,584
因此完整地写出整个算法

24
00:02:50,584 --> 00:02:55,231
为了简化刚才的这个索引

25
00:02:55,231 --> 00:02:59,843
我将假设我有小批量规模为10和一个大小为1000的训练集

26
00:02:59,843 --> 00:03:05,045
我们接下来要做的就是计算这个形式的和 从i等于1、11、21等等开始

27
00:03:05,045 --> 00:03:07,926
步长是10因为我们每次处理10个样本

28
00:03:07,926 --> 00:03:13,648
然后我们每次对10个样本使用这种梯度下降来更新

29
00:03:13,648 --> 00:03:21,566
所以这是10这是i+9 它们是小批量规模选取10带来的结果

30
00:03:21,566 --> 00:03:27,435
这是最后的for循环 这里在991结束因为

31
00:03:27,435 --> 00:03:34,457
如果我有1000个训练样本那么为了遍历整个训练集我需要100个步长为10的循环

32
00:03:34,457 --> 00:03:37,729
这就是小批量梯度下降

33
00:03:37,729 --> 00:03:43,219
相比批量梯度下降 这种算法也让我们进展快很多

34
00:03:43,219 --> 00:03:49,487
所以让我们再次处理美国3亿人的人口普查数据训练集

35
00:03:49,487 --> 00:03:55,621
然后我们要说的是在处理了前10个样本之后 我们可以开始优化参数θ

36
00:03:55,621 --> 00:04:00,873
因此我们不需要扫描整个训练集

37
00:04:00,873 --> 00:04:05,377
我们只要处理前10个样本然后这可以让我们有所改进

38
00:04:05,377 --> 00:04:09,289
接着我们可以处理第二组10个样本 再次对参数做一点改进 然后接着这样做

39
00:04:09,289 --> 00:04:14,186
因此 这就是小批量梯度下降比批量梯度下降快的原因

40
00:04:14,186 --> 00:04:19,578
你可以在只处理了10个样本之后就改进参数

41
00:04:19,578 --> 00:04:24,836
而不是需要等到你扫描完3亿个样本中的每一个

42
00:04:24,836 --> 00:04:29,699
那么 小批量梯度下降和随机梯度下降比较又怎么样呢？

43
00:04:29,699 --> 00:04:38,237
也就是说 为什么我们想要每次处理b个样本 而不是像随机梯度下降一样每次处理一个样本？

44
00:04:38,237 --> 00:04:42,044
答案是——向量化！

45
00:04:42,044 --> 00:04:47,450
具体来说 小批量梯度下降可能比随机梯度下降好

46
00:04:47,450 --> 00:04:50,817
仅当你有好的向量化实现时

47
00:04:50,817 --> 00:04:58,571
在那种情况下 10个样本求和可以用一种更向量化的方法实现

48
00:04:58,571 --> 00:05:05,376
允许你部分并行计算10个样本的和

49
00:05:05,376 --> 00:05:09,953
因此 换句话说 使用正确的向量化方法计算剩下的项

50
00:05:09,953 --> 00:05:18,565
你有时可以使用好的数值代数库来部分地并行计算b个样本

51
00:05:18,565 --> 00:05:24,152
然而如果你是用随机梯度下降每次只处理一个样本 那么你知道

52
00:05:24,152 --> 00:05:27,456
每次只处理一个样本没有太多的并行计算

53
00:05:27,456 --> 00:05:29,824
至少并行计算更少

54
00:05:29,824 --> 00:05:34,866
小批量梯度下降的一个缺点是有一个额外的参数b

55
00:05:34,866 --> 00:05:39,006
你需要调试小批量大小 因此会需要一些时间

56
00:05:39,006 --> 00:05:45,611
但是如果你有一个好的向量化实现这种方法有时甚至比随机梯度下降更快

57
00:05:45,611 --> 00:05:52,937
好了 这就是小批量梯度下降算法

58
00:05:52,937 --> 00:05:57,697
在某种意义上做的事情介于随机梯度下降和批量梯度下降之间

59
00:05:57,697 --> 00:06:02,626
如果你选择合理的b的值 我经常选择b等于10 但是

60
00:06:02,626 --> 00:06:07,343
你知道 别的值 比如2到100之间的任何一个数都可能合理

61
00:06:07,343 --> 00:06:11,917
因此我们选择b的值 如果你有一个好的向量化实现

62
00:06:11,917 --> 00:06:15,917
有时它可以比随机梯度下降和批量梯度下降更快 【教育无边界字幕组】翻译: zearom32 校对/审核: 所罗门捷列夫