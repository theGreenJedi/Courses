在之前的视频中 我们讨论了随机梯度下降 以及它是怎样比批量梯度下降更快的 在这次视频中 让我们讨论基于这些方法的另一种变形 叫做小批量梯度下降 这种算法有时候甚至比随机梯度下降还要快一点 首先来总结一下我们已经讨论过的算法 在批量梯度下降中每次迭代我们都要用所有的m个样本 然而在随机梯度下降中每次迭代我们只用一个样本 小批量梯度下降做的介于它们之间 准确地说 在这种方法中我们每次迭代使用b个样本 b是一个叫做"小批量规模"的参数 所以这种算法介于随机梯度下降和批量梯度下降之间 这就像批量梯度下降 只不过我会用小很多的批量规模 b的一个标准取值可能是10 比如说 b的一个标准的取值可能是2到100之间的任何一个数 因此那是小批量规模的一个非常典型的取值区间 算法思想是我们每次用b个样本而不是每次用1个或者m个 所以让我正式地把它写出来 我们将要确定b 例如我们假设b是10 所以我们将要从训练集中取出接下来的10个样本 假设训练集是样本 (x(i).y(i)) 的集合 如果是10个样本 最多索引值达到(x(i+9),y(i+9)) 这是全部的10个样本 然后我们将要用这10个样本做一个实际上是梯度下降的更新 因此 就是学习率乘以1/10乘以 k 对 h (x(k)-y(k))×x(k)j 从i到i+9求和 在这个表达式中 我们计算10个样本的梯度下降公式的和 因此 这是数字10 就是小批量规模 i+9 9来自参数b的选择 然后在这之后我们将要把 i 加10 我们将要继续处理接下来的10个样本 然后像这样一直继续 因此完整地写出整个算法 为了简化刚才的这个索引 我将假设我有小批量规模为10和一个大小为1000的训练集 我们接下来要做的就是计算这个形式的和 从i等于1、11、21等等开始 步长是10因为我们每次处理10个样本 然后我们每次对10个样本使用这种梯度下降来更新 所以这是10这是i+9 它们是小批量规模选取10带来的结果 这是最后的for循环 这里在991结束因为 如果我有1000个训练样本那么为了遍历整个训练集我需要100个步长为10的循环 这就是小批量梯度下降 相比批量梯度下降 这种算法也让我们进展快很多 所以让我们再次处理美国3亿人的人口普查数据训练集 然后我们要说的是在处理了前10个样本之后 我们可以开始优化参数θ 因此我们不需要扫描整个训练集 我们只要处理前10个样本然后这可以让我们有所改进 接着我们可以处理第二组10个样本 再次对参数做一点改进 然后接着这样做 因此 这就是小批量梯度下降比批量梯度下降快的原因 你可以在只处理了10个样本之后就改进参数 而不是需要等到你扫描完3亿个样本中的每一个 那么 小批量梯度下降和随机梯度下降比较又怎么样呢？ 也就是说 为什么我们想要每次处理b个样本 而不是像随机梯度下降一样每次处理一个样本？ 答案是——向量化！ 具体来说 小批量梯度下降可能比随机梯度下降好 仅当你有好的向量化实现时 在那种情况下 10个样本求和可以用一种更向量化的方法实现 允许你部分并行计算10个样本的和 因此 换句话说 使用正确的向量化方法计算剩下的项 你有时可以使用好的数值代数库来部分地并行计算b个样本 然而如果你是用随机梯度下降每次只处理一个样本 那么你知道 每次只处理一个样本没有太多的并行计算 至少并行计算更少 小批量梯度下降的一个缺点是有一个额外的参数b 你需要调试小批量大小 因此会需要一些时间 但是如果你有一个好的向量化实现这种方法有时甚至比随机梯度下降更快 好了 这就是小批量梯度下降算法 在某种意义上做的事情介于随机梯度下降和批量梯度下降之间 如果你选择合理的b的值 我经常选择b等于10 但是 你知道 别的值 比如2到100之间的任何一个数都可能合理 因此我们选择b的值 如果你有一个好的向量化实现 有时它可以比随机梯度下降和批量梯度下降更快 【教育无边界字幕组】翻译: zearom32 校对/审核: 所罗门捷列夫