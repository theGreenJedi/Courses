1
00:00:00,332 --> 00:00:04,284
続く幾つかのビデオで、大規模スケールの機械学習について話す。

2
00:00:04,284 --> 00:00:08,316
つまり、アルゴリズムなんだけれど、とてもビッグなデータセットを見る物だ。

3
00:00:08,316 --> 00:00:12,839
ここ最近5年とか10年の機械学習の歴史を振り返ると、

4
00:00:12,839 --> 00:00:17,853
5年前と比べても凄く学習アルゴリズムがうまく動作するようになった理由の一つには、

5
00:00:17,853 --> 00:00:22,657
アルゴリズムのトレーニングに使えるデータの量が単純に増えた、というのがある。

6
00:00:22,657 --> 00:00:29,741
ここからの幾つかのビデオでは、そんな巨大なデータセットがあった時にそれを扱うアルゴリズムについて議論していきたい。

7
00:00:32,926 --> 00:00:35,527
さて、そもそも何故そんな大きなデータセットを使いたいと思うのか？

8
00:00:35,527 --> 00:00:40,564
我らは既に、機械学習のシステムで高いパフォーマンスを得たい時には、ベストな方法の一つに

9
00:00:40,564 --> 00:00:46,168
低いバイアスの学習アルゴリズムを使い、大量のデータでトレーニングさせる、というのがあった。

10
00:00:46,168 --> 00:00:53,561
既に見た前出の例としては、このややこしい単語の分類の例がある。

11
00:00:53,561 --> 00:01:00,726
For breakfastには、I ateといえばtwoのeggsとなる。
この例の問題を解くと、こんな結果となる。

12
00:01:00,726 --> 00:01:06,436
見た所、データを大量に突っ込めば突っ込む程良く振舞うように見える。

13
00:01:06,436 --> 00:01:10,419
だから、これらのような結果から、以下のような事が良く言われる。

14
00:01:10,419 --> 00:01:15,151
機械学習においては、勝者はもっとも良いアルゴリズムを持つ物では無く、もっともデータを持っている人だ、と。

15
00:01:15,151 --> 00:01:19,568
だから大規模なデータセットから学習したい、少なくともそんなデータセットが入手可能ならば。

16
00:01:19,568 --> 00:01:27,027
しかし大規模なデータセットからの学習は、特有の問題もつきまとう。具体的には、計算的な問題だ。

17
00:01:27,027 --> 00:01:33,870
トレーニングセットのサイズmが100,000,000だとしよう。

18
00:01:33,870 --> 00:01:37,934
これはこんにち的なデータセットしては、普通に現実的な範囲だ。

19
00:01:37,934 --> 00:01:40,518
USの国勢調査のデータを見ると、そこには、

20
00:01:40,518 --> 00:01:44,663
3億の人がUSには居るから、普通に何億ってデータを扱う事になる。

21
00:01:44,663 --> 00:01:47,856
人気のあるwebサイトのトラフィックの量を見ると、

22
00:01:47,856 --> 00:01:52,509
簡単に億より多くの手本を得る事になる。

23
00:01:52,509 --> 00:01:57,407
線形回帰のモデルをトレーニングしたいとしよう。またはロジスティック回帰でも良い。

24
00:01:57,407 --> 00:02:01,692
その場合、これが最急降下法のルールとなる。

25
00:02:01,692 --> 00:02:05,372
そして勾配を計算する為に必要な事を見ると、

26
00:02:05,372 --> 00:02:09,992
それはここの項だ。そしてmが一億の時は、

27
00:02:09,992 --> 00:02:13,976
1億に渡る和を取る必要がある、

28
00:02:13,976 --> 00:02:18,977
これらの微分項を計算する為には、そして最急降下法の1ステップを実行する為には。

29
00:02:18,977 --> 00:02:25,627
1億に渡る和を取るという計算量的なコストの為に、

30
00:02:25,627 --> 00:02:28,628
最急降下法のたった1ステップを計算する為だけに。

31
00:02:28,628 --> 00:02:31,530
続く一連のビデオで、これを別の何かに置き換えるテクニックや、

32
00:02:31,530 --> 00:02:38,413
この微分項を計算するより効率的な方法について議論する。

33
00:02:38,413 --> 00:02:41,709
この大規模スケールの機械学習の一連のビデオを観終わった頃には、

34
00:02:41,709 --> 00:02:47,045
線形回帰とかロジスティック回帰とかニューラルネットワークのモデルのフィッティングを

35
00:02:47,045 --> 00:02:50,990
こんにち的なデータセット、つまり一億の手本とかに行う方法を理解する事になるだろう。

36
00:02:50,990 --> 00:02:56,035
もちろん、一億の手本でモデルをトレーニングするという努力を払う前に、

37
00:02:56,035 --> 00:03:01,276
単に1000の手本を使うだけでダメなのか？という事も自らに問うてみなくてはならない。

38
00:03:01,276 --> 00:03:04,923
時には1億の手本からランダムに1000の手本を

39
00:03:04,923 --> 00:03:10,254
ピックアップして、その1000の手本でトレーニングするだけ、でも良いかもしれない。

40
00:03:10,254 --> 00:03:16,076
だからこれらの巨大なモデルをトレーニングするのに必要なソフトウェアの開発などに投資する前に、

41
00:03:16,076 --> 00:03:22,461
1000の手本でトレーニングしてみる事は、良いサニティチェックとなる。

42
00:03:22,461 --> 00:03:29,731
より少ないトレーニングセットでサニティチェックを行う方法は、

43
00:03:29,731 --> 00:03:33,958
つまりより少ないm=1000のサイズのトレーニングセットを用いる方法は、

44
00:03:33,958 --> 00:03:37,797
通常の学習曲線をプロットする、という方法で良いだろう。

45
00:03:37,797 --> 00:03:46,872
もし学習曲線をプロットしてみて、トレーニングの目的関数がこんな感じなら、

46
00:03:46,872 --> 00:03:49,553
この目的関数はJ trainのシータだが、

47
00:03:49,553 --> 00:03:56,422
そしてクロスバリデーションセットの目的関数、Jcvのシータがこんな感じだったとする。

48
00:03:56,422 --> 00:04:00,310
その場合、高バリアンスな学習アルゴリズムのようなので、

49
00:04:00,310 --> 00:04:05,913
さらに追加でトレーニング手本を足す事は、パフォーマンスを改善すると確信が持てる。

50
00:04:05,913 --> 00:04:10,462
一方、対照的に、学習曲線をプロットしたら、

51
00:04:10,462 --> 00:04:20,339
トレーニングの目的関数がこんな感じで、クロスバリデーションの目的関数がこんな感じだと、

52
00:04:20,339 --> 00:04:24,292
これは高バイアスな学習アルゴリズムに見える。

53
00:04:24,292 --> 00:04:28,084
この後者の場合には、例えばこれがm=1000までのプロットだとして、

54
00:04:28,084 --> 00:04:33,437
この辺がm=500で、1000までとして、

55
00:04:33,437 --> 00:04:39,400
するとたぶん、データを1億に増やしてもたぶんあまり良くはならないだろう。

56
00:04:39,400 --> 00:04:42,736
それならばアルゴリズムのスケールを増やす為に努力を費やすよりは、

57
00:04:42,736 --> 00:04:47,000
m=1000のままにしておく方が良かろう。

58
00:04:47,000 --> 00:04:51,029
もちろん、この右側の図のような状況だったら、

59
00:04:51,029 --> 00:04:53,885
次に行うべき自然なステップとしては、追加のフィーチャーを足すとか、

60
00:04:53,885 --> 00:04:58,484
隠れユニットをニューラルネットワークに足すとか、そういう事で、

61
00:04:58,484 --> 00:05:04,627
そういう事を通してm=1000のままだと左側の状態に近づいていったら

62
00:05:04,627 --> 00:05:09,553
その時は初めて1億以上の手本を使うように、

63
00:05:09,553 --> 00:05:14,735
インフラを追加したり、アルゴリズムを変更したりする事に、より確信を持てるようになり、それは実際に良い自分の時間の使い方だと思えるだろう。

64
00:05:14,735 --> 00:05:19,642
さて、大規模スケールの機械学習においては、とてもビッグなデータを扱うのに、

65
00:05:19,642 --> 00:05:24,026
計算量的にリーズナブルな、または効率的な方法を知りたい。

66
00:05:24,026 --> 00:05:26,826
続く幾つかのビデオでは、二つの主なアイデアを見ていく。

67
00:05:26,826 --> 00:05:33,464
最初の物は確率的な最急降下法、そして二番目はMap Reduceと呼ばれる物。とてもビッグなデータを見るのに。

68
00:05:33,464 --> 00:05:39,986
そしてこれらの手法を学んだ後には、あなたの学習アルゴリズムをビッグにスケールアップ出来るようになり、

69
00:05:39,986 --> 00:05:43,986
様々な応用に対して、もっと良いパフォーマンスが得られるようになる事を祈る。