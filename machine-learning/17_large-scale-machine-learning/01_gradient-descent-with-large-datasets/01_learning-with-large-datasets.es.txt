En los próximos vídeos, vamos a hablar del aprendizaje automático a gran escala. Es decir, algoritmos que tratan grandes conjuntos de datos. Si miran hacia atrás, a la historia reciente del aprendizaje automático de hace 5 o 10 años, una de las razones por las que los algoritmos de aprendizaje funcionan mejor ahora que, incluso digamos, hace 5 años, es simplemente la enorme cantidad de datos que tenemos ahora y con los que podemos entrenar a nuestros algoritmos. En estos próximos vídeos, vamos a hablar de los algoritmos que vamos a usar cuando tenemos este tipo de conjuntos de datos masivos. Entonces, ¿por qué queremos utilizar estos grandes conjuntos de datos? Ya hemos visto que una de las mejores maneras de conseguir un sistema de aprendizaje automático de alto rendimiento, es que tomen un algoritmo de aprendizaje de baja oscilación y entrenen a éste sobre una gran cantidad de datos. Así que, uno de los primeros ejemplos que ya hemos visto fue este ejemplo para clasificar entre las palabras confundibles. Así, «Para el desayuno, comí dos (DOS) huevos.» En este ejemplo vimos este tipo de resultados, en donde siempre y cuando alimenten al algoritmo con una gran cantidad de datos, parece que funciona muy bien. Así que son los resultados como estos lo que ha llevado al dicho en el aprendizaje automático de que a menudo, no es quién tiene el mejor algoritmo el que gana, es quien tiene la mayor cantidad de datos. De modo que desean aprender de grandes conjuntos de datos, por lo menos cuando podemos conseguir estos grandes conjuntos de datos. Pero el aprendizaje con grandes conjuntos de datos viene con sus propios problemas singulares, en concreto, los problemas computacionales. Digamos que el tamaño de su conjunto de entrenamiento es «m» igual a 100 millones. Y esto es en realidad bastante realista para muchos conjuntos de datos modernos. Si se fijan en el conjunto de datos del Censo de EE.UU., hay, ya saben, 300 millones de personas en los EE.UU., por lo general pueden obtener cientos de millones de registros. Si observan la cantidad de tráfico que tienen los sitios web populares, fácilmente pueden obtener conjuntos de entrenamiento que son mucho más grandes que cientos de millones de ejemplos. Y digamos que quieren entrenar un modelo de regresión lineal, o tal vez un modelo de regresión logística, en cuyo caso esta es la regla del gradiente de descenso. Y si se fijan en lo que tienen que hacer para calcular el gradiente, que es este término por aquí, entonces cuando «m» es cien millones, tienen que realizar una suma de más de cien millones de términos, a fin de calcular estos términos derivados y realizar un solo paso de gradiente descendente. Debido al costo computacional por sumar más de cien millones de entradas, a fin de calcular sólo un paso del gradiente de descenso, en los próximos vídeos hablaremos acerca de las técnicas ya sea para sustituir este algoritmo por algo más, o para encontrar formas más eficientes para calcular esta derivada. Al final de esta secuencia de videos sobre el aprendizaje automático a gran escala, sabrán cómo ajustar modelos,  --regresión lineal, regresión logística, redes neuronales,etcétera-- inclusive hasta con conjuntos de datos de, digamos, cien millones de ejemplos. Por supuesto, antes de esforzarnos por entrenar un modelo con cien millones de ejemplos, también debemos preguntarnos, bueno, ¿por qué no utilizar solamente mil ejemplos?. Tal vez podamos escoger al azar los subconjuntos de un millar de ejemplos de cien millones de ejemplos y entrenar a nuestro algoritmo sólo con mil ejemplos. Así que antes de invertir esfuerzo en el desarrollo real y el software necesario para entrenar a estos modelos masivos, a menudo es una buena prueba de validez si el entrenamiento en sólo un millar de ejemplos pudiera resultar igual de bien. La forma para comprobar la validez del uso de un conjunto de entrenamiento mucho más pequeño que podría ser igual de bueno, es decir, si el uso de un conjunto de entrenamiento mucho más pequeño, «n» es igual a 1000, que pudiera hacer igual de bien, entonces este es el método habitual para trazar las curvas de aprendizaje; de manera que si fueran a trazar las curvas de aprendizaje, y si su objetivo de entrenamiento tuviera este aspecto, eso es «J entrena «theta»». Y si su objetivo de conjunto de validación cruzada «Jcv» de «theta», se viera de esta manera, entonces esto se parece a un algoritmo de aprendizaje de alta varianza, y estaremos más seguros de que la adición de ejemplos de entrenamiento adicionales mejoraría el desempeño, mientras que por el contrario, si fueran a trazar las curvas de aprendizaje, si su objetivo de entrenamiento se viera así, y si su objetivo de  validación cruzada luciera de esa manera, entonces esto se parece al algoritmo de aprendizaje clásico de alta oscilación. Y en este último caso si tuvieran que trazar esto hasta, por ejemplo, «m» es igual a 1000, de manera que «m» es igual a 500, hasta «m» es igual a 1000, entonces parece poco probable que el aumento de «m» a cien millones vaya a ser mucho mejor, y entonces estaría bien que se apegaran a «m» es igual a 1000, en lugar de invertir una gran cantidad de esfuerzo para encontrar la escala del algoritmo. Por supuesto, si estuvieran en la situación que se muestra en la figura a la derecha, entonces lo natural sería añadir variables adicionales, o añadir unidades ocultas adicionales a su red neuronal y así sucesivamente, de modo que terminen con una situación más cercana a la de la izquierda, en donde tal vez esto es hasta «m» es igual a 1000, y esto entonces les da más confianza que tratar de añadir infraestructura para cambiar el algoritmo para usar mucho más que miles de ejemplos que en realidad podría ser un buen uso de su tiempo. Así que en el aprendizaje automático a gran escala, nos gusta encontrar formas de cómputo razonables, o formas computacionalmente eficientes, para hacer frente a los conjuntos de datos muy grandes. En los próximos vídeos, veremos dos ideas principales. La primera se llama gradiente de descenso estocástico y la segunda se llama Reducción de Mapa, para tratar con conjuntos de datos muy grandes. Y después de que hayan aprendido acerca de estos métodos, con suerte eso les permitirá ampliar sus algoritmos de aprendizaje para muchos datos, y les permitirá obtener un mejor rendimiento en muchas aplicaciones diferentes.