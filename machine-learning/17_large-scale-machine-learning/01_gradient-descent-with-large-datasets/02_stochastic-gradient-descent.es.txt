Para muchos algoritmos de aprendizaje, entre ellos la regresión lineal, la regresión logística y las redes neuronales, la forma en que derivamos el algoritmo fue encontrando una función de costos, o encontrando un objetivo de optimización, y después usando un algoritmo como el gradiente de descenso para minimizar esa función de costos. Cuando tenemos un conjunto de entrenamiento muy grande, el gradiente de descenso se convierte en un procedimiento muy costoso computacionalmente. En este video, hablaremos de una modificación al algoritmo básico del gradiente de descenso llamado gradiente de descenso estocástico, que nos permitirá ampliar estos algoritmos para conjuntos de entrenamiento mucho mayores. Supongamos que están entrenando un modelo de regresión lineal utilizando el gradiente de descenso. Como un resumen rápido, la hipótesis se verá así, y la función de coste se verá así, que es la mitad del promedio de error cuadrado de su hipótesis sobre sus ejemplos de entrenamiento «m», y la función de costos que ya hemos visto, se parece a este tipo de función en forma de arco. Así, trazada como la función de los parámetros «theta» 0 y «theta» 1, la función de costo «J» es un tipo de función en forma de arco. Y el gradiente de descenso se ve así, en donde en el bucle interno del gradiente de descenso ustedes actualizan repetidamente los parámetros «theta» usando esa expresión. Ahora bien, en el resto de este video, voy a seguir usando la regresión lineal como el ejemplo en ejecución. Pero las ideas aquí, las ideas del gradiente de descenso estocástico son completamente generales y también aplican a otros algoritmos de aprendizaje, como la regresión logística, las redes neuronales y otros algoritmos que se basan en entrenar el gradiente de descenso  sobre un conjunto de entrenamiento específico. Así que aquí está una imagen de lo que hace el gradiente de descenso. Si los parámetros se inicializan hacia el al punto allí, entonces, a medida que ejecuten el gradiente de descenso, las diferentes iteraciones del gradiente de descenso tomarán los parámetros hasta el mínimo global. Así que tomen una trayectoria que se parezca a eso y que se dirige de manera muy directa al mínimo global. Ahora, el problema con el gradiente de descenso es que si «m» es grande, entonces el cálculo de este término derivado puede ser muy costoso ya que esto requiere sumar sobre todos los  ejemplos «m». De modo que si «m» es 300 millones, bien, entonces en los Estados Unidos hay cerca de 300 millones de personas. Así que los datos del censo de los EE.UU. o Estados Unidos pueden estar en el orden de esa cantidad de registros, de modo que ustedes desean ajustar el modelo de regresión lineal a eso y luego sumar más de 300 millones de registros. Y eso es muy caro. Para darle un nombre al algoritmo, esta versión particular de gradiente de descenso también se llama gradiente de descenso por lotes. Y el término lote se refiere al hecho de que estamos viendo todos los ejemplos de entrenamiento a la vez. Lo llamamos una especie de lote de todos los ejemplos de entrenamiento. Y tal vez no es en realidad el mejor nombre, pero esto es lo que la gente del aprendizaje automático llama a esta versión particular del gradiente de descenso. Y si realmente imaginan que tienen 300 millones de registros de los censos guardadas en el disco, la forma en que este algoritmo funciona es  que necesitan leer en la memoria de la computadora los 300 millones de registros a fin de calcular este término derivativo. Necesitan transmitir todos estos registros a través de la computadora porque no pueden almacenar todos sus archivos en la memoria de la computadora. Así que tienen que leer a través de ellos y, poco a poco, ya saben, acumular la suma con el fin de calcular la derivada. Y, después de haber hecho todo ese trabajo, eso les permite tomar un paso del gradiente de descenso. Y ahora tienen que hacer todo de nuevo, ya saben, buscar a través de los 300 millones de registros, acumular estas sumas, y después de haber hecho todo ese trabajo, pueden tomar otro pequeño paso usando el gradiente de descenso. Y luego, hacerlo de nuevo; y después, toman un tercer paso, y así sucesivamente. De modo que les va a tomar mucho tiempo conseguir que el algoritmo converja. En contraste con el gradiente de descenso por lotes, lo que vamos a hacer es encontrar un algoritmo diferente que no necesite que se vean todos los ejemplos de entrenamiento en cada iteración, sino que necesite observar solamente un solo ejemplo de entrenamiento en una iteración. Antes de pasar al nuevo algoritmo, aquí está sólo un algoritmo del gradiente de descenso por lotes escrito de nuevo, siendo esta la función de costos y siendo esto la  actualización y, por supuesto, este término aquí que se usa en la regla del gradiente de descenso, esa es la derivada parcial con respecto a los parámetros «theta» «J» de nuestro objetivo de optimización, «J entrenamiento» de «theta». Ahora, veamos el algoritmo más eficiente que se adapta mejor a los grandes conjuntos de datos. Con el fin de trabajar desde los algoritmos llamados gradientes de descenso estocásticos, escribamos la función de costos de una manera ligeramente diferente. Tenemos que encontrar el costo del parámetro «theta» con respecto a un ejemplo de entrenamiento «x(i), y(1)» para que sea igual a la mitad de las veces del error al cuadrado en la que incurre mi hipótesis en ese ejemplo, «x(i), y(i)». De modo que este término de la función de costos realmente mide qué tan bien está funcionando mi hipótesis sobre un solo ejemplo «x(i), y(i)». Ahora notan de que la función de costo «j entrenar» general se puede escribir de esta forma equivalente. Así que «j entrenar» es sólo el promedio sobre mis ejemplos de entrenamiento «m» del costo de mi hipótesis en ese ejemplo «x(i), y(i)». Armado con esta visión de la función de costos para la regresión lineal, permítanme escribir ahora lo que hace el gradiente de descenso estocástico. El primer paso del gradiente de descenso estocástico es mezclar al azar el conjunto de datos. Con esto sólo me refiero a  mezclar de manera aleatoria o a reordenar sus ejemplos «m» de entrenamiento. Es una especie de paso de pre-procesamiento estándar; regresaré a esto en un minuto. Pero el trabajo principal del gradiente de descenso estocástico se hace a continuación en lo siguiente. Vamos a repetir para «i» es igual a 1 hasta «m». Así que vamos a buscar repetidamente a través de mis ejemplos de entrenamiento y  realizar la siguiente actualización. Voy a actualizar el parámetro «theta» «j» como «theta» «j» menos «alfa» veces «h» de «x(i)» menos «y(i)» veces «x(i)j». Y vamos a hacer esta actualización como de costumbre para todos los valores de «j». Ahora, observamos que este término aquí es exactamente lo que teníamos dentro de la suma del gradiente de descenso por lotes. De hecho, para aquellos de ustedes que estén familiarizados con cálculo, es posible demostrar que ese término aquí, ese es este término aquí, es igual a la derivada parcial con respecto a mi parámetro «theta» «j» del costo de los parámetros «theta» sobre «x(i), y(i)». En donde el costo es, por supuesto, esto que se definió anteriormente. Y sólo para concluir el algoritmo, voy a cerrar mis corchetes allí. Así que lo que el gradiente de descenso estocástico está haciendo es en realidad escanear a través de los ejemplos de entrenamiento. Y primero va a ver mi primer ejemplo de entrenamiento «x(1), y(1)». Y después, viendo sólo este primer ejemplo, va a tomar básicamente como un paso pequeño del gradiente de descenso con respecto al costo de sólo este primer ejemplo de entrenamiento. De modo que, en otras palabras, vamos a ver el primer ejemplo y modificar los parámetros un poco para ajustar sólo el primer ejemplo de entrenamiento un poco mejor. Una vez hecho esto dentro de este bucle interior, se va a ir entonces al segundo ejemplo de entrenamiento. Y lo que va a hacer allí es dar otro pasito más en el espacio del parámetro, así que modificar los parámetros sólo un poco para ajustar solamente un segundo ejemplo de entrenamiento un poco mejor. Una vez hecho esto, se va a ir a mi tercer ejemplo de entrenamiento y modificar los parámetros para tratar de ajustar sólo el tercer ejemplo de entrenamiento un poco mejor, y así sucesivamente hasta que, ya saben, pasen a través de todo el conjunto de entrenamiento. Y después, este bucle exterior repetido puede ocasionar que pase múltiples veces sobre  el conjunto de entrenamiento completo. Esta visión del gradiente de descenso estocástico también motiva la razón por la que deseamos empezar por mezclar de manera aleatoria el conjunto de datos. Esto nos asegura que cuando escaneamos a través del conjunto de entrenamiento aquí, terminamos visitando los ejemplos de entrenamiento en algún tipo de orden mezclado al azar. Dependiendo de si sus datos ya estaban ordenados al azar, o si venían mezclados originalmente en un orden extraño, en la práctica esto sólo agilizaría las conversiones al gradiente de descenso estocástico sólo un poco. Así que, en aras de la seguridad, por lo general es mejor mezclar aleatoriamente el conjunto de datos si no están seguros si llegó a ustedes en un orden mezclado al azar o no. Pero más importante aún, otra vista del gradiente de descenso estocástico es que es muy parecido al  gradiente de descenso de lote, pero en lugar de esperar a sumar estos términos del gradiente sobre todos los ejemplos «m» de entrenamiento, lo que estamos haciendo es que estamos tomando este término del gradiente usando sólo un ejemplo de entrenamiento, y ya estamos empezando a hacer progreso para mejorar los parámetros. Así que en lugar de esperar hasta tomar una trayectoria a través de todos los 300,000 de registros de censo de Estados Unidos, es decir, en lugar de tener que buscar a través de todos los ejemplos de entrenamiento, antes de que podamos modificar los parámetros un poco y avanzar hacia un mínimo global, en vez de eso, para el gradiente de descenso estocástico, sólo tenemos que mirar a un solo ejemplo de entrenamiento y ya estamos empezando a realizar progresos en este caso de los parámetros, hacia mover los parámetros al mínimo global. Por lo tanto, aquí está el algoritmo escrito de nuevo en el que el primer paso es mezclar aleatoriamente los datos, y el segundo paso es en el que se hace el trabajo real, en donde está esa actualización con respecto a un único ejemplo de entrenamiento «x(i), y(i)». Por lo tanto, vamos a ver lo que este algoritmo hace a los parámetros. Anteriormente, vimos que cuando estamos usando gradiente de descenso por lotes, ese es el algoritmo que analiza todos los ejemplos de entrenamiento a la vez. El gradiente de descenso por lotes tenderá a tomar una trayectoria de línea  razonablemente recta para llegar al mínimo global de esta manera. En contraste con el gradiente de descenso estocástico, cada iteración va a ser mucho más rápida porque no necesitamos sumar sobre todos los ejemplos de entrenamiento, pero cada iteración sólo está tratando de ajustar mejor el único ejemplo de  entrenamiento. Así que, si empezáramos el gradiente de descenso estocástico, ¡oh! vamos a iniciar el gradiente de descenso estocástico en un punto de este modo. La primera iteración, ya saben, puede tomar los parámetros en esa dirección y tal vez la segunda iteración, que ve sólo el segundo ejemplo, tal vez sólo por casualidad, no tengamos suerte y en realidad nos dirigimos en una mala dirección con los parámetros de esta manera. En la tercera iteración en la que tratamos de modificar los  parámetros para ajustar mejor sólo los terceros ejemplos de entrenamiento, tal vez terminemos dirigiéndonos en esa dirección. Y luego vamos a ver el cuarto ejemplo de entrenamiento y haremos eso. El quinto ejemplo, sexto ejemplo, séptimo y así sucesivamente. Y a medida que ejecutan el gradiente de descenso estocástico, lo que encuentran es que generalmente moverá los parámetros en la dirección del mínimo global, pero no siempre. Así que tomará trayectorias mas aleatorias, tortuosas hacia el mínimo global. Y, de hecho, a medida que ejecutan el gradiente de descenso estocástico, éste no converge en realidad en el mismo mismo sentido en el que lo hace el gradiente de descenso por lotes y lo que termina haciendo es dar vueltas continuamente en alguna región que está en alguna región cerca del mínimo global, pero no sólo llega al mínimo global y se queda allí. Pero en la práctica esto no es un problema porque, ya saben, siempre y cuando los parámetros terminen en alguna región allí, tal vez están bastante cerca del mínimo global. Así que siempre y cuando los parámetros terminen muy cerca del mínimo global, esa será una muy buena hipótesis, y por lo general, al ejecutar el gradiente de descenso estocástico, obtenemos un parámetro cerca del mínimo global y eso es suficientemente bueno para esencialmente la mayoría de propósitos prácticos. Sólo un detalle final. En el gradiente de descenso estocástico tuvimos esta repetición del bucle exterior que nos dice que hagamos este bucle interior varias veces. Así que, ¿cuántas veces repetimos este bucle externo? Dependiendo del tamaño del conjunto de entrenamiento, podría ser suficiente si hacemos este bucle una sola vez. Y hasta, ya saben, tal vez 10 veces podría ser lo usual, de modo que podríamos terminar repitiendo este bucle interno de una a diez veces. Así que si tenemos un conjunto de datos verdaderamente masivo como este censo de los EE.UU. que nos dio ese ejemplo del que he estado hablando, con 300 millones de ejemplos, es posible que para cuando haya tomado un solo pase a través de su conjunto de entrenamiento. Así que, esto es para «i» es igual a 1 hasta 300 millones. Es posible que para cuando hayan tomado un solo pase a través de su conjunto de datos, ya tengan un hipótesis perfectamente buena, en cuyo caso puede que sólo tengan que hacer este bucle interno una sola vez si «m» es muy, muy grande. Pero, en general, tomando cualquier cantidad de entre 1 a 10 pases a través de su conjunto de datos puede ser bastante común, pero en realidad depende del tamaño de su conjunto de entrenamiento. Y si contrastan esto con el gradiente de descenso por lotes, con el gradiente de descenso por lotes, después de tomar un pase a través de todo el conjunto de entrenamiento, habrían tomado un solo paso del gradiente de descenso, así que uno de estos pequeños pasos de bebé del gradiente de descenso en el que toman sólo un pequeño paso del gradiente de descenso, y esta es la razón por la que el gradiente de descenso estocástico puede ser mucho más rápido. Por lo tanto, ese fue el algoritmo del gradiente de descenso estocástico. Y si lo implementan, les permitirá ampliar muchos de sus algoritmos de aprendizaje a conjuntos de datos mucho más grandes y obtener mucho más rendimiento de esa manera.