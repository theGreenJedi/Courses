对于很多机器学习算法 包括线性回归、逻辑回归、神经网络等等 算法的实现都是通过得出某个代价函数 或者某个最优化的目标来实现的 然后使用梯度下降这样的方法来求得代价函数的最小值 当我们的训练集较大时 梯度下降算法则显得计算量非常大 在这段视频中 我想介绍一种跟普通梯度下降不同的方法 随机梯度下降(stochastic gradient descent) 用这种方法我们可以将算法运用到较大训练集的情况中 假如你要使用梯度下降法来训练某个线性回归模型 简单复习一下 我们的假设函数是这样的 代价函数是你的假设在训练集样本上预测的平均平方误差的二分之一倍的和 通常我们看到的代价函数都是像这样的弓形函数 因此 画出以θ0和θ1为参数的代价函数J 就是这样的弓形函数 这就是梯度下降算法 在内层循环中 你需要用这个式子反复更新参数θ的值 在这段视频剩下的时间里 我将依然以线性回归为例 但随机梯度下降的思想也可以应用于其他的学习算法 比如逻辑回归、神经网络或者其他依靠梯度下降来进行训练的算法中 这张图表示的是梯度下降的做法 假设这个点表示了参数的初始位置 那么在你运行梯度下降的过程中 多步迭代最终会将参数锁定到全局最小值 迭代的轨迹看起来非常快地收敛到全局最小 而梯度下降法的问题是 当m值很大时 计算这个微分项的计算量就变得很大 因为需要对所有m个训练样本求和 所以假如m的值为3亿 美国就有3亿人口 美国的人口普查数据就有这种量级的数据记录 所以如果想要为这么多数据拟合一个线性回归模型的话 那就需要对所有这3亿数据进行求和 这样的计算量太大了 这种梯度下降算法也被称为批量梯度下降(batch gradient descent) “批量”就表示我们需要每次都考虑所有的训练样本 我们可以称为所有这批训练样本 也许这不是个恰当的名字 但做机器学习的人就是这么称呼它的 想象一下 如果你真的有这3亿人口的数据存在硬盘里 那么这种算法就需要把所有这3亿人口数据读入计算机 仅仅就为了算一个微分项而已 你需要将这些数据连续传入计算机 因为计算机存不下那么大的数据量 所以你需要很慢地读取数据 然后计算一个求和 再来算出微分 所有这些做完以后 你才完成了一次梯度下降的迭代 然后你又需要重新来一遍 也就是再读取这3亿人口数据 做个求和 然后做完这些 你又完成了梯度下降的一小步 然后再做一次 你得到第三次迭代 等等 所以 要让算法收敛 绝对需要花很长的时间 相比于批量梯度下降 我们介绍的方法就完全不同了 这种方法在每一步迭代中 不用考虑全部的训练样本 只需要考虑一个训练样本 在开始介绍新的算法之前 我把批量梯度下降算法再写在这里 这里是代价函数 这里是迭代的更新过程 梯度下降法中的这一项 是最优化目标 代价函数Jtrain(θ) 关于参数θj的偏微分 下面我们来看对大量数据来说更高效的这种方法 为了更好地描述随机梯度下降算法 代价函数的定义有一点区别 我们定义参数θ 关于训练样本(x(i),y(i))的代价 等于二分之一倍的 我的假设h(x(i))跟实际输出y(i)的误差的平方 因此这个代价函数值实际上测量的是我的假设在某个样本(x(i),y(i))上的表现 你可能已经发现 总体的代价函数Jtrain可以被写成这样等效的形式 Jtrain(θ)就是我的假设函数 在所有m个训练样本中的每一个样本(x(i),y(i))上的代价函数的平均值 用这样的方法应用到线性回归中 我来写出随机梯度下降的算法 随机梯度下降法的第一步是将所有数据打乱 我说的随机打乱的意思是 将所有m个训练样本重新排列 这就是标准的数据预处理过程 稍后我们再回来讲 随机梯度下降的主要算法如下 在i等于1到m中进行循环 也就是对所有m个训练样本进行遍历 然后进行如下更新 我们按照这样的公式进行更新 θj等于θj减α乘以h(x(i))减y(i)乘以x(i)j 同样还是对所有j的值进行更新 不难发现 这一项实际上就是我们批量梯度下降算法中 求和式里面的那一部分 事实上 如果你数学比较好的话 你可以证明这一项 也就是这一项 是等于这个cost函数关于参数θj的偏微分 这个cost函数就是我们之前先定义的代价函数 最后画上大括号结束算法的循环 随机梯度下降的做法实际上就是扫描所有的训练样本 首先是我的第一组训练样本(x(1),y(1)) 然后只对这第一个训练样本 对它的代价函数 计算一小步的梯度下降 换句话说 我们要关注第一个样本 然后把参数θ稍微修改一点 使其对第一个训练样本的拟合变得好一点 完成这个内层循环以后 再转向第二个训练样本 然后还是一样 在参数空间中进步一小步 也就是稍微把参数修改一点 然后让它对第二个样本的拟合更好一点 做完第二个 再转向第三个训练样本 同样还是修改参数 让它更好的拟合第三个训练样本 以此类推 直到完成所有的训练集 然后外部这个重复循环会多次遍历整个训练集 从这个角度分析随机梯度下降算法 我们能更好地理解为什么一开始要随机打乱数据 这保证了我们在扫描训练集时 我们对训练集样本的访问是随机的顺序 不管你的数据是否已经随机排列过 或者一开始就是某个奇怪的顺序 实际上这一步能让你的随机梯度下降稍微快一些收敛 所以为了保险起见 最好还是先把所有数据随机打乱一下 如果你不知道是否已经随机排列过的话 但随机梯度下降的更重要的一点是 跟批量梯度下降不同 随机梯度下降不需要等到对所有m个训练样本 求和来得到梯度项 而是只需要对单个训练样本求出这个梯度项 我们已经在这个过程中开始优化参数了 就不用等到把所有那3亿的美国人口普查的数据拿来遍历一遍 不需要等到对所有这些数据进行扫描 然后才一点点地修改参数 直到达到全局最小值 对随机梯度下降来说 我们只需要一次关注一个训练样本 而我们已经开始一点点把参数朝着全局最小值的方向进行修改了 这里把这个算法再重新写一遍 第一步是打乱数据 第二步是算法的关键 是关于某个单一的训练样本(x(i),y(i))来对参数进行更新 让我们来看看 这个算法是如何更新参数θ的 之前我们已经看到 当使用批量梯度下降的时候 需要同时考虑所有的训练样本数据 批量梯度下降的收敛过程 会倾向于一条近似的直线 一直找到全局最小值 与此不同的是 在随机梯度下降中 每一次迭代都会更快 因为我们不需要对所有训练样本进行求和 每一次迭代只需要保证对一个训练样本拟合好就行了 所以 如果我们从这个点开始进行随机梯度下降的话 第一次迭代 可能会让参数朝着这个方向移动 然后第二次迭代 只考虑第二个训练样本 假如很不幸 我们朝向了一个错误的方向 第三次迭代 我们又尽力让参数修改到拟合第三组训练样本 可能最终会得到这个方向 然后再考虑第四个训练样本 做同样的事 然后第五第六第七 等等 在你运行随机梯度下降的过程中 你会发现 一般来讲 参数是朝着全局最小值的方向被更新的 但也不一定 所以看起来它是以某个比较随机、迂回的路径在朝全局最小值逼近 实际上 你运行随机梯度下降 和批量梯度下降 两种方法的收敛形式是不同的 实际上随机梯度下降是在某个靠近全局最小值的区域内徘徊 而不是直接逼近全局最小值并停留在那点 但实际上这并没有多大问题 只要参数最终移动到某个非常靠近全局最小值的区域内 只要参数逼近到足够靠近全局最小值 这也会得出一个较为不错的假设 所以 通常我们用随机梯度下降法 也能得到一个很接近全局最小值的参数 对于绝大部分实际应用的目的来说 已经足够了 最后一点细节 在随机梯度下降中 我们有一个外层循环 它决定了内层循环的执行次数 所以 外层循环应该执行多少次呢 这取决于训练样本的大小 通常一次就够了 最多到10次 是比较典型的 所以我们可以循环执行内层1到10次 因此 如果我们有非常大量的数据 比如美国普查的人口数据 我说的3亿人口数据的例子 所以每次你只需要考虑一个训练样本 这里的i就是从1到3亿了 所以可能你每次只需要考虑一个训练样本 你就能训练出非常好的假设 这时 由于m非常大 那么内循环只用做一次就够了 但通常来说 循环1到10次都是非常合理的 但这还是取决于你训练样本的大小 如果你跟批量梯度下降比较一下的话 批量梯度下降在一步梯度下降的过程中 就需要考虑全部的训练样本 所以批量梯度下降就是这样微小的一次次移动 这也是为什么随机梯度下降法要快得多 这就是随机梯度下降了 如果你应用它 应该就能在很多学习算法中应用大量数据了 并且会得到更好的算法表现【教育无边界字幕组】翻译：所罗门捷列夫 校对：竹二个 审核：Naplessss