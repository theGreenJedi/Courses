अगले कुछ वीडियोज़ में हम बात करेंगे लार्ज स्केल मशीन लर्निंग की. वह कि, अल्गोरिद्म्स लेकिन दृष्टिकोण बड़े / बिग डाटा सेट्स का रखते हुए. यदि आप पीछे देखो हाल ही का 5 या 10 साल का इतिहास मशीन लर्निंग का. एक कारण कि लर्निंग अल्गोरिद्म्स इतना ज्यादा अच्छे से काम करते हैं अब 5 साल पहले के मुक़ाबले, है केवल बहुत मात्रा में डेटा जो अब हमारे पास है और जिन पर हम अपने अल्गोरिद्म्स को ट्रेन कर सकते हैं. इन अगले कुछ वीडियोज़ में, हम बात करेंगे अल्गोरिद्म्स की जो डील करते हैं इन मेस्सिव डेटा सेट्स को. तो क्यों हम चाहते है उपयोग करना इतने बड़े डेटा सेट्स को? हमने पहले देखा है कि श्रेष्ठतम उपाय एक अच्छी पर्फ़ॉर्मन्स वाला मशीन लर्निंग सिस्टम पाने का है अगर आप लेते हैं एक लो-बायस लर्निंग अल्गोरिद्म, और उसे ट्रेन करते हैं बहुत से डेटा पर. और इसलिए, एक शुरू का उदाहरण हमने पहले देखा वह था यह उदाहरण क्लैसिफ़ाई करने का कोनफ़्यूसेबल वर्ड्ज़ / शब्द॰
. अत:, फ़ोर ब्रेक्फ़स्ट आई ऐट टू (टीडबल्यूओ) एग्ज़ और हमने देखा इस उदाहरण में, इस प्रकार के परिणाम, जहाँ, आप जानते हैं, जब तक फ़ीड करते रहते हैं अल्गोरिद्म को बहुत सा डेटा, वह अच्छा करता प्रतीत होता है. और इसलिए इस तरह के परिणामों से, प्रचलित हुआ मशीन लर्निंग में कि अक्सर यह नहीं कि किसके पास श्रेष्ठतम अल्गोरिद्म है वही जीतता है. यह कि जिसके पास अधिकतम डेटा है. अत: आप लर्न करना चाहते हैं लार्ज डेटा सेट्स से, कम से कम तब जब हमें मिल सकते हैं इस प्रकार के लार्ज डेटा सेट्स. लेकिन लर्निंग लार्ज डेटा सेट्स के साथ आतीं हैं उससे ही सम्बंधित कुछ अलग कठिनाइयाँ, विशेषत: कॉम्प्यूटेशनल कठिनाइयाँ. मान लीजिए कि आपका ट्रेनिंग सेट साइज़ एम है 100,000,000. और यह वास्तव में सम्भव है आजकल के बहुत से डेटा सेट्स के लिए. अगर आप देखें द यूएस के सेंसस डेटा सेट को, अगर वहाँ है, आप जानते हैं, 300 मिल्यन लोग यूएस में, आपको प्रायः मिल सकते हैं कई सौ मिल्यन्स रेकोडर्स. अगर आप देखे ट्रैफ़िक संख्या जो लोकप्रिय / पॉप्युलर वेब साइट्स को मिलती है, आपको आसानी से ट्रेनिंग सेट मिल जाते है जो बहुत बड़े हैं सौ मिल्यंज़ के इग्ज़ैम्पल्ज़ से भी. और मान लीजिए आप ट्रेन करना चाहते हैं एक लिनीअर रेग्रेशन मॉडल, या शायद एक लजिस्टिक रेग्रेशन मॉडल, जिस केस में यह ग्रेडीयंट डिसेंट रूल है. और अगर आप देखें कि आपको क्या चाहिए ग्रेडीयंट कम्प्यूट करने के लिए, जो यह टर्म है यहाँ पर, तब जब एम एक सौ मिल्यन होता है, आपको करना पड़ता है समेशन एक सौ मिल्यन टर्म्ज़ से अधिक का, कम्प्यूट करने के लिए ये डेरिवेटिवज़ टर्म्ज़ और करने के एक अकेला स्टेप डिसेंट का. कॉम्प्यूटेशनल इक्स्पेन्स के कारण जो एक सौ मिल्यन से ज़्यादा टर्म्ज़ के समेशन से आता है कम्प्यूट करने के लिए एक अकेला स्टेप ग्रेडीयंट डिसेंट का, अगले कुछ विडीओज़ में मैंने बात की है तकनीकों की या तो हम इसे बदल दें किसी और चीज़ से या ढूँढें कार्यक्षम / इफ़िशंट तरीक़े कम्प्यूट करने के लिए यह डेरिवेटिव. लार्ज स्केल मशीन लर्निंग विडीओज़ के इस क्रम के अंत तक, आप जान गए है मॉडल कैसे फ़िट करते हैं, लिनियर रिग्रेशन, लोजिस्टिक रिग्रेशन, न्यूरल नेटवर्क्स इत्यादि यहाँ तक की आजकल के डेटा सेट्स से भी जिनमे हो सकते हैं शायद एक सौ मिल्यन इग्ज़ैम्पल्ज़. निस्संदेह, इससे पहले कि हम चेष्टा करें ट्रेन करने की एक मोडल एक सौ मिल्यन इग्ज़ैम्पल्ज़ के साथ, हमें अपने आप से यह भी पूछना चाहिए, क्यों न प्रयोग करें सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़. शायद हम रैंडम्ली / बिना किसी क्रम के ले सकते हैं सबसेट्स एक हज़ार इग्ज़ैम्पल्ज़ के एक सौ मिल्यन इग्ज़ैम्पल्ज़ में से और ट्रेन कर सकते हैं हमारा अल्गोरिद्म सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़ पर. अत: इससे पहले कि इतनी मेहनत करें वास्तव में सॉफ़्टवेयर डिवेलप करने की ट्रेन करने के लिए इन बड़े माडल्ज़ को अच्छा रहता है कि आप एक सैनिटी चेक कर ले, कि अगर ट्रेनिंग सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़ पर भी उतना अच्छा काम कर रही है. तरीक़ा सैनिटी चेक का कि एक छोटा ट्रेनिंग सेट भी उतना अच्छा काम कर रहा है कि अगर एक छोटा ट्रेनिंग सेट जहाँ एम है 1000 साइज़ का ट्रेनिंग सेट वह भी उतना अच्छा काम कर रहा है, वह है प्रचलित तरीक़ा प्लॉट करके देखने का लर्निंग कर्व्ज़, तो अगर आपको प्लॉट करने होंगें लर्निंग कर्व्ज़ और अगर आपका ट्रेनिंग अब्जेक्टिव ऐसा होगा, वह जे ट्रेन थीटा है. और अगर आपका क्रॉस वैलिडेशन सेट अब्जेक्टिव, जेसीवी ऑफ थीटा ऐसा होगा, तब यह लगता है एक हाई-वेरीयन्स लर्निंग अल्गोरिद्म, और हम होंगे अधिक विश्वस्त / कॉन्फ़िडेंट कि अतिरिक्त ट्रेनिंग इग्ज़ैम्पल्ज़ लेने से पर्फ़ॉर्मन्स बेहतर हो जाएगी. जबकि इसके विपरीत, यदि आपको प्लॉट करने होगे लर्निंग कर्व्ज़, अगर आपका ट्रेनिंग अब्जेक्टिव ऐसा होगा, और आपका क्रॉस वैलिडेशन सेट का अब्जेक्टिव ऐसा होगा, तब यह हो लगता है एक श्रेष्ठ हाई-बायस लर्निंग अल्गोरिद्म, और इस दूसरे केस में, आप जानते हैं, अगर आप प्लॉट करें, मान लीजिए, एम बराबर है 1000 तक और इसलिए वह है एम बराबर है 500 से एम बराबर है 1000 तक, तब ऐसा लगता है कि शायद सम्भव न है एम को सौ मिल्यन तक बढ़ाने से बेहतर कार्य होगा और तब अच्छा रहेगा आपका एम बराबर है 1000 तक ही बने रहना, इसके बजाय कि आप इतनी मेहनत करें यह जानने के लिए कि कैसे स्केल करना है अल्गोरिद्म को. निस्संदेह, अगर आप होते इस स्थिति में जो दिखाई गई है दाईं तरफ़ के चित्र में, तब यह स्वभाविक हो जाता और फ़ीचर्ज़ जोड़ लेना, या अतिरिक्त हिडन यूनिट्स जोड़ना आपके न्यूरल नेटवर्क में इत्यादि, ताकि आप उस स्थिति में आ पाएँ जो पास हो बाईं तरफ़ की तरह, जहाँ यह शायद है एम बराबर है 1000, और यह तब आपको अधिक आश्वासन देगा कि कोशिश और इंफ़्रा स्ट्रक्चर जोड़ने की अल्गोरिद्म को बदलने के लिए प्रयोग करने के लिए एक हज़ार से कहीं ज़्यादा इग्ज़ैम्पल्ज़ शायद आपके समय का अच्छा उपयोग होगा. तो लार्ज-स्केल मशीन लर्निंग में, हम चाहते है निकालना कॉम्प्यूटेशनली उचित रास्ते, या कॉम्प्यूटेशनली कार्यक्षम / इफ़िशंट रास्ते, डील करने के लिए बड़े /बिग डेटा सेट्स से. अगले कुछ विडीओज़ में हम देखेंगे दो प्रमुख सिद्धांतों को. पहले को कहते हैं स्टोकेस्टिक ग्रेडीयंट डिसेंट और दूसरे को कहते हैं मैप रिडयूस
 डील करने के लिए बड़े डेटा सेट्स से. और जब आप लर्न कर लेंगे इन तरीकों को, आशा है वह आपको करवा पाएँगे स्केल अप आपके लर्निंग अल्गोरिद्म्स को बिग डेटा के लिए और दिलवा पाएँगे अधिक बेहतर पर्फ़ॉर्मन्स विभिन्न ऐप्लिकेशन्स में.