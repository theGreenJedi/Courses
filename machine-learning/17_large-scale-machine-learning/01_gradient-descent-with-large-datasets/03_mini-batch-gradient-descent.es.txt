En el video anterior, hablamos del gradiente de descenso estocástico y de cómo éste puede ser mucho más rápido que el gradiente de descenso en lotes. En este vídeo vamos a hablar de otra variación sobre estas ideas que se llama gradiente de descenso de mini lote, que puede trabajar a veces incluso un poco más rápido que el gradiente de descenso estocástico. Para resumir el algoritmo del que hemos hablado hasta ahora, en el gradiente de descenso por lotes vamos a utilizar todos los ejemplos «m» en cada iteración, mientras que en el gradiente de descenso estocástico vamos a utilizar un solo ejemplo en cada iteración. Lo que hace el gradiente de descenso de mini lotes es algo en un punto intermedio. Específicamente con este algoritmo, vamos a utilizar ejemplos «b» en cada iteración, en donde «b» es un parámetro llamado "tamaño de mini lote", de modo que la idea es que está de alguna manera entre el gradiente de descenso por lotes y el gradiente de descenso estocástico. Esto es igual que el gradiente de descenso por lotes, excepto que voy a utilizar un tamaño de lote mucho más pequeño. Una elección típica para el valor de «b» podría ser que «b» es igual a 10, digamos, y un rango típico realmente podría estar en cualquier lugar desde «b» es igual a 2, hasta «b» es igual a 100. Así que ese será un rango de valores bastante típico para el lote de mini lotes. La idea es que, en lugar de utilizar un ejemplo a la vez, o «m» ejemplos a la vez, usaremos «b» ejemplos a la vez. De modo que escribiré esto de manera informal, vamos a obtener, digamos, «b». Para este ejemplo, digamos que «b» es igual a 10. Así que vamos a obtener los próximos 10 ejemplos de mi conjunto de entrenamiento, de modo ese pueda ser algún conjunto de ejemplos «xi, yi». Si se trata de 10 ejemplos entonces la indexación será hasta «x(i+9), y(i+9)», de modo que son 10 ejemplos en total y luego vamos a realizar básicamente una actualización del gradiente de descenso usando estos 10 ejemplos. Así que eso es frecuencia de aprendizaje multiplicado por un décimo multiplicado por la suma sobre «k» es igual a «i» hasta «i+9» de «h» subíndice «theta» de «x(k)» menos «y(k)» multiplicado por «x(k)j». De modo que en esta expresión, estamos sumando los términos del gradiente sobre mis diez ejemplos. Así que, el número diez, este es mi tamaño de mini lote y en «i+9» el 9 proviene de la elección del parámetro «b» y después de esto, aumentaremos «i» por el décimo. Vamos a pasar a los próximos diez ejemplos y luego nos seguiremos moviendo de esta manera. Sólo para escribir el algoritmo en su totalidad, a fin de simplificar la indexación para esto voy a suponer que tenemos un tamaño de mini lote de diez y un tamaño de conjunto de entrenamiento de mil. Lo que vamos a hacer es tener este tipo de fórmula: para «i» es igual a 1, 11, 21, así que escalonando en pasos de 10 porque observamos 10 ejemplos a la vez; y luego realizamos este tipo de actualización del gradiente de descenso usando diez ejemplos a la vez, de modo que este 10 y este i+9, esos son la consecuencia de haber elegido que mi mini lote sea 10. Y este último bucle de cuatro , éste conjunto n 991 aquí porque, si tengo 1000 muestras de entrenamiento, entonces necesito 100 pasos de tamaño 10 a fin de pasar a través de mi conjunto de entrenamiento. Así que este es el gradiente de descenso de mini lote. Comparado con el gradiente de descenso por lotes, éste también nos permite hacer progresos mucho más rápido. Así que tenemos de nuevo nuestro ejemplo de ejecución de, ya saben, los datos del censo de EE.UU. con 300 millones de ejemplos de entrenamiento, entonces lo que estamos diciendo después de ver tan sólo los 10 primeros ejemplos, podemos empezar a hacer progresos para mejorar los parámetros «theta» así que no tenemos que buscar a través de todo el conjunto de entrenamiento. Sólo tenemos que mirar a los primeros 10 ejemplos y esto nos permitirá empezar a hacer progresos y después podemos ver los segundos diez ejemplos y modificar los parámetros un poco de nuevo, y así sucesivamente. Por lo tanto, esa es la razón por la que el gradiente de descenso de mini lote puede ser más rápido que el gradiente de descenso por lotes, a saber, pueden empezar a hacer progresos en la modificación de los parámetros después de mirar sólo diez ejemplos, en lugar de tener que esperar hasta que hayan buscado cada ejemplo individual de los 300 millones de ellos. Así que, ¿qué hay del gradiente de descenso de mini lote, en comparación con el gradiente de descenso estocástico?. Así que, ¿por qué queremos observar los ejemplos «b» a la vez, en lugar de mirar un solo ejemplo a la vez, como en el gradiente de descenso estocástico? La respuesta está en la vectorización. En particular, es posible que el gradiente de descenso de mini lote supere el desempeño del gradiente de descenso estocástico sólo si tienen una buena implementación vectorizada. En ese caso, la suma sobre 10 ejemplos se puede realizar de una manera más vectorizada, que le permitirá paralelizar parcialmente sus cálculos en los diez ejemplos. Así que, en otras palabras, usando la vectorización apropiada para calcular los términos derivativos, algunas veces pueden usar parcialmente las buenas bibliotecas de álgebra lineal numérica y paralelizar sus cálculos del gradiente en los ejemplos «b», mientras que, si están viendo un solo ejemplo a la vez con el gradiente de descenso estocástico entonces, ya saben, sólo mirando un ejemplo a la vez, no hay mucho para paralelizar. Al menos, hay menos para paralelizar. Una desventaja del gradiente de descenso de mini lote es que ahora existe este parámetro extra «b», el tamaño de mini lote con el que quizás tengan que jugar y que, por tanto, podría llevar tiempo. Pero si tienen una buena implementación vectorizada, esto se puede ejecutar aún más rápido que el gradiente de descenso estocástico. Así que este fue el gradiente de descenso de mini lote que es un algoritmo que, en cierto sentido, hace algo que de alguna manera está entre lo que hace el gradiente de descenso  estocástico y lo que hace el gradiente de descenso por lotes. Y si eligen su valor razonable de «b», yo suelo usar «b» es igual a 10, pero, ya saben, otros valores, desde por ejemplo 2 hasta 100, serían razonablemente comunes. Así que si elegimos un valor de «b», y si usan una buena implementación vectorizada, a veces pudiera ser más rápido que el gradiente de descenso estocástico, y más rápido que el gradiente de descenso por lotes.