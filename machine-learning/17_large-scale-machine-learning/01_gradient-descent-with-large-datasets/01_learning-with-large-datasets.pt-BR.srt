1
00:00:00,332 --> 00:00:04,284
Nos próximos vídeos falaremos de aprendizado de máquina em larga escala.

2
00:00:04,284 --> 00:00:08,316
Ou seja, nossos algoritmos lidando com grandes quantidades de dados.

3
00:00:08,316 --> 00:00:12,839
Se olharmos para a história recente do campo, 5 ou 10 anos atrás,

4
00:00:12,839 --> 00:00:17,853
um dos motivos pelos quais os algoritmos de aprendizado funcionam bem melhor hoje do que, digamos, há 5 anos,

5
00:00:17,853 --> 00:00:22,657
é a quantidade enorme de dados que temos agora e que podemos usar para treinar os algoritmos.

6
00:00:22,657 --> 00:00:29,741
 Neste próximos vídeos, falaremos sobre algoritmos para lidar com quantidades massivas de dados.

7
00:00:32,926 --> 00:00:35,527
Por que gostaríamos de usar esses conjuntos grandes de dados?

8
00:00:35,527 --> 00:00:40,564
Nós vimos que uma das melhores maneiras de conseguir um sistema de aprendizado de máquina de alta performance,

9
00:00:40,564 --> 00:00:46,168
caso o seu algoritmo tenha um baixo desvio, é treiná-lo com muitos dados.

10
00:00:46,168 --> 00:00:53,561
E um dos exemplos que já vimos antes é esse de distinguir entre palavras similares que geralmente confundem-se.

11
00:00:53,561 --> 00:01:00,726
Seja: Para o café da manhã eu comi (dois) ovos, e nós vimos que nesse tipo de aplicação,

12
00:01:00,726 --> 00:01:06,436
contanto que você providencie muitos dados para o algoritmo, ele se sairá muito bem. 

13
00:01:06,436 --> 00:01:10,419
E são resultados como esses que nos levam a dizer que em aprendizado de máquina

14
00:01:10,419 --> 00:01:15,151
não é o melhor algoritmo que vence, mas aquele que dispõem de mais dados.

15
00:01:15,151 --> 00:01:19,568
Você quer então treinar o seu algoritmo com grandes conjuntos de dados, ao menos quando nós podemos ter esses grandes conjuntos.

16
00:01:19,568 --> 00:01:27,027
Mas o aprendizado com grandes conjuntos de dados também acompanha seus problemas típicos, especialmente, os computacionais.

17
00:01:27,027 --> 00:01:33,870
Digamos que o tamanho do seu conjunto de treinamento M seja 100.000.000.

18
00:01:33,870 --> 00:01:37,934
E esse é um número compatível com os conjuntos de dados atuais.

19
00:01:37,934 --> 00:01:40,518
Se você verificar os dados do senso dos EUA, uma vez que existe, aproximadamente,

20
00:01:40,518 --> 00:01:44,663
300 milhões de pessoas nos EUA, você pode provavelmente chegar em centenas de milhões de registros.

21
00:01:44,663 --> 00:01:47,856
Se você olhar para a quantidade de tráfego que sites populares tem,

22
00:01:47,856 --> 00:01:52,509
você pode facilmente conseguir conjuntos de treinamento que são muito maiores que centenas de milhões.

23
00:01:52,509 --> 00:01:57,407
Digamos que você queira treinar uma regressão linear, ou talvez uma regressão logística,

24
00:01:57,407 --> 00:02:01,692
em ambos os casos usaria o gradiente descendente.

25
00:02:01,692 --> 00:02:05,372
E se você for ver o que precisará para calcular o gradiente,

26
00:02:05,372 --> 00:02:09,992
que é esse termo aqui, então quando M é cem milhões,

27
00:02:09,992 --> 00:02:13,976
você terá que lidar com uma somatória através de cem milhões de termos.

28
00:02:13,976 --> 00:02:18,977
Para que possa calcular essas derivadas e realizar um único passo em direção ao mínimo.

29
00:02:18,977 --> 00:02:25,627
Devido o custo computacional da somatória de centena de milhões de termos,

30
00:02:25,627 --> 00:02:28,628
para que se possa calcular apenas um passo do gradiente decrescente,

31
00:02:28,628 --> 00:02:31,530
nos próximos vídeos nós iremos falar sobre técnicas

32
00:02:31,530 --> 00:02:38,413
para tanto substituir isso por algo melhor ou encontrar maneiras mais eficientes de calcular essa derivada.

33
00:02:38,413 --> 00:02:41,709
Ao final dessa sequência e vídeos sobre aprendizado em larga escala,

34
00:02:41,709 --> 00:02:47,045
você saberá como ajustar modelos de regressão lineares, regressão logística, redes neurais e assim por diante

35
00:02:47,045 --> 00:02:50,990
mesmo com os conjuntos de dados modernos, que possuem, digamos, cem milhões de exemplos.

36
00:02:50,990 --> 00:02:56,035
Obviamente, antes de nos esforçarmos para treinar um modelo com cem milhões de dados,

37
00:02:56,035 --> 00:03:01,276
nós devemos nos perguntar o porquê de não usarmos apenas, digamos, mil exemplos.

38
00:03:01,276 --> 00:03:04,923
Talvez possamos pegar aleatoriamente subconjuntos de mil exemplos

39
00:03:04,923 --> 00:03:10,254
desse conjunto maior de cem milhões e treinar o nosso algoritmo apenas com mil dados.

40
00:03:10,254 --> 00:03:16,076
Portanto, antes de investir esforço em desenvolver algo capaz de lidar com essa quantidade massiva de dados,

41
00:03:16,076 --> 00:03:22,461
é sempre bom conferir se treinar o modelo com mil exemplos não é bom o suficiente.

42
00:03:22,461 --> 00:03:29,731
A maneira de verificar se usar um conjunto de treinamento bem menor pode ter um efeito tão bom quanto,

43
00:03:29,731 --> 00:03:33,958
ou seja, que usando um número muito menor como um conjunto de tamanho 1000,

44
00:03:33,958 --> 00:03:37,797
que teria performance tão boa quanto, é através tão usado método de colocar no gráfico as curvas de aprendizado,

45
00:03:37,797 --> 00:03:46,872
então se você montar as curvas de aprendizado e se a sua função objetivo de treino parece-se com algo assim,

46
00:03:46,872 --> 00:03:49,553
essa é a J de teta de treino.

47
00:03:49,553 --> 00:03:56,422
E se a sua função objetivo para a validação, Jcv de teta parecer-se com isso,

48
00:03:56,422 --> 00:04:00,310
então teremos um caso de algoritmo de aprendizado com alta variância,

49
00:04:00,310 --> 00:04:05,913
e estaremos mais confiantes na melhoria de sua performance ao adicionarmos mais exemplos de treinamento.

50
00:04:05,913 --> 00:04:10,462
Enquanto isso, se você desenhar suas curvas de aprendizado,

51
00:04:10,462 --> 00:04:20,339
se sua curva de treino paracer-se com isso, e a de validação cruzada com isso aqui,

52
00:04:20,339 --> 00:04:24,292
então parece que o algoritmo sofre de alto desvio.

53
00:04:24,292 --> 00:04:28,084
E neste último caso, como sabemos, se desenharmos,

54
00:04:28,084 --> 00:04:33,437
digamos, para m igual a 1000, com m indo de 500 até igual a 1000,

55
00:04:33,437 --> 00:04:39,400
então parece-nos improvável que aumentar m até cem milhões trará uma melhoria considerável,

56
00:04:39,400 --> 00:04:42,736
então é melhor ficarmos mesmo com n igual a 1000,

57
00:04:42,736 --> 00:04:47,000
ao invés de investir muito esforço em como escalaríamos esse algoritmo.

58
00:04:47,000 --> 00:04:51,029
É claro que se você estiver na situação mostrada na figura da direita,

59
00:04:51,029 --> 00:04:53,885
então algo natural a fazer seria adicionar novas variáveis,

60
00:04:53,885 --> 00:04:58,484
ou adicionar novas unidades ocultas na sua rede neural e assim por diante,

61
00:04:58,484 --> 00:05:04,627
e assim você terminaria em uma situação mais próxima da presenta à esquerda, onde estejamos em uma situação, talvez, de n igual a 1000,

62
00:05:04,627 --> 00:05:09,553
e então isso nos daria mais confiança em tentar modificar o algoritmo

63
00:05:09,553 --> 00:05:14,735
para usar muito mais do que mil exemplos e isso então valeria o tempo investido.

64
00:05:14,735 --> 00:05:19,642
Portanto, em aprendizado de máquina em larga escala, nós queremos propor maneiras computacionalmente razoáveis,

65
00:05:19,642 --> 00:05:24,026
ou maneiras computacionalmente eficientes, para lidar com conjuntos de dados muito grandes.

66
00:05:24,026 --> 00:05:26,826
Nos próximos vídeos, nós veremos dois dos principais conceitos.

67
00:05:26,826 --> 00:05:33,464
O primeiro é chamado de gradiente decrescente estocástico e o segundo é a divisão do cálculo do gradiente para possibilitar cálculos paralelos.

68
00:05:33,464 --> 00:05:39,986
Depois de aprender esses métodos, você estará apto para escalar seus algoritmos de aprendizado com grandes conjuntos de dados

69
00:05:39,986 --> 00:05:43,986
e terá uma performance melhor em diferente aplicações.