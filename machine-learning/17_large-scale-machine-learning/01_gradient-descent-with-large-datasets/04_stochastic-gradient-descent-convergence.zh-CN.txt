现在你已经知道了随机梯度下降算法 但是当你运行这个算法时 你如何确保调试过程已经完成 并且能正常收敛呢？ 还有 同样重要的是 你怎样调整随机梯度下降中学习速率α的值 在这段视频中 我们会谈到一些方法来处理这些问题 确保它能收敛 以及选择合适的学习速率α 回到我们之前批量梯度下降的算法 我们确定梯度下降已经收敛的一个标准方法 是画出最优化的代价函数 关于迭代次数的变化 这就是代价函数 我们要保证这个代价函数在每一次迭代中 都是下降的 当训练集比较小的时候 我们不难完成 因为要计算这个求和是比较方便的 但当你的训练集非常大的时候 你不希望老是定时地暂停算法 来计算一遍这个求和 因为这个求和计算需要考虑整个的训练集 而随机梯度下降的算法是 你每次只考虑一个样本 然后就立刻进步一点点 不需要在算法当中 时不时地扫描一遍全部的训练集 来计算整个训练集的代价函数 因此 对于随机梯度下降算法 为了检查算法是否收敛 我们可以进行下面的工作 让我们沿用之前定义的cost函数 关于θ的cost函数 等于二分之一倍的训练误差的平方和 然后 在随机梯度下降法学习时 在我们对某一个样本进行训练前 在随机梯度下降中 我们要关注样本(x(i),y(i)) 然后关于这个样本更新一小步 进步一点点 然后再转向下一个样本 (x(i+1),y(i+1)) 随机梯度下降就是这样进行的 在算法扫描到样本(x(i),y(i)) 但在更新参数θ之前 使用这个样本 我们可以算出这个样本对应的cost函数 我再换一种方式表达一遍 当随机梯度下降法对训练集进行扫描时 在我们使用某个样本(x(i),y(i))来更新θ前 让我们来计算出 这个假设对这个训练样本的表现 我要在更新θ前来完成这一步 原因是如果我们用这个样本更新θ以后 再让它在这个训练样本上预测 其表现就比实际上要更好了 最后 为了检查随机梯度下降的收敛性 我们要做的是 每1000次迭代 我们可以画出前一步中计算出的cost函数 我们把这些cost函数画出来 并对算法处理的最后1000个样本的cost值求平均值 如果你这样做的话 它会很有效地帮你估计出 你的算法在最后1000个样本上的表现 所以 我们不需要时不时地计算Jtrain 那样的话需要所有的训练样本 随机梯度下降法的这个步骤 只需要在每次更新θ之前进行 也并不需要太大的计算量 要做的就是 每1000次迭代运算中 我们对最后1000个样本的cost值求平均然后画出来 通过观察这些画出来的图 我们就能检查出随机梯度下降是否在收敛 这是几幅画出来的图的例子 假如你已经画出了最后1000组样本的cost函数的平均值 由于它们都只是1000组样本的平均值 因此它们看起来有一点嘈杂 因此cost的值不会在每一个迭代中都下降 假如你得到一种这样的图像 看起来是有噪声的 因为它是在一小部分样本 比如1000组样本中求的平均值 如果你得到像这样的图 那么你应该判断这个算法是在下降的 看起来代价值在下降 然后从大概这个点开始变得平缓 这就是代价函数的大致走向 这基本说明你的学习算法已经收敛了 如果你想试试更小的学习速率 那么你很有可能看到的是 算法的学习变得更慢了 因此代价函数的下降也变慢了 但由于你使用了更小的学习速率 你很有可能会让算法收敛到一个好一点的解 红色的曲线代表随机梯度下降使用一个更小的学习速率 出现这种情况是因为 别忘了 随机梯度下降不是直接收敛到全局最小值 而是在局部最小附近反复振荡 所以使用一个更小的学习速率 最终的振荡就会更小 有时候这一点小的区别可以忽略 但有时候一点小的区别 你就会得到更好一点的参数 接下来再看几种其他的情况 假如你还是运行随机梯度下降 然后对1000组样本取cost函数的平均值 并且画出图像 那么这是另一种可能的图形 看起来这样还是已经收敛了 如果你把这个数 1000 提高到5000组样本 那么可能你会得到一条更平滑的曲线 通过在5000个样本中求平均值 你会得到比刚才1000组样本更平滑的曲线 这是你增大平均的训练样本数的情形 当然增大它的缺点就是 现在每5000个样本才能得到一个数据点 因此你所得到的关于学习算法表现的反馈 就显得有一些“延迟” 因为每5000个样本才能得到图上的一个数据点 而不是每1000个样本就能得到 沿着相似的脉络 有时候你运行梯度下降 可能也会得到这样的图像 如果出现这种情况 你要知道 可能你的代价函数就没有在减小了 也就是说 算法没有很好地学习 因为这看起来一直比较平坦 代价项并没有下降 但同样地 如果你对这种情况时 也用更大量的样本进行平均 你很可能会观察到红线所示的情况 能看得出 实际上代价函数是在下降的 只不过蓝线用来平均的样本数量太小了 并且蓝线太嘈杂 你看不出来代价函数的趋势确实是下降的 所以可能用5000组样本来平均 比用1000组样本来平均 更能看出趋势 当然 即使是使用一个较大的样本数量 比如我们用5000个样本来平均 我用另一种颜色来表示 即使如此 你还是可能会发现 这条学习曲线是这样的 它还是比较平坦 即使你用更多的训练样本 如果是这样的话 那可能就更肯定地说明 不知道出于什么原因 算法确实没怎么学习好 那么你就需要调整学习速率 或者改变特征变量 或者改变其他的什么 最后一种你可能会遇到的情况是 如果你画出曲线 你会发现曲线是这样的 实际上是在上升 这是一个很明显的信号 告诉你算法正在发散 那么你要做的事 就是用一个更小一点的学习速率α 好的 希望通过这几幅图 你能了解到 当你画出cost函数在某个范围的训练样本中求平均值时 各种可能出现的现象 也告诉你 在遇到不同的情况时 应该采取怎样的措施 所以如果曲线看起来噪声较大 或者老是上下振动 那就试试增大你要平均的样本数量 这样应该就能得到比较好的变化趋势 如果你发现代价值在上升 那么就换一个小一点的α值 最后还需要再说一下关于学习速率的问题 我们已经知道 当运行随机梯度下降时 算法会从某个点开始 然后曲折地逼近最小值 但它不会真的收敛 而是一直在最小值附近徘徊 因此你最终得到的参数 实际上只是接近全局最小值 而不是真正的全局最小值 在大多数随机梯度下降法的典型应用中 学习速率α一般是保持不变的 因此你最终得到的结果一般来说是这个样子的 如果你想让随机梯度下降确实收敛到全局最小值 你可以随时间的变化减小学习速率α的值 所以 一种典型的方法来设置α的值 是让α等于某个常数1 除以 迭代次数加某个常数2 迭代次数指的是你运行随机梯度下降的迭代次数 就是你算过的训练样本的数量 常数1和常数2是两个额外的参数 你需要选择一下 才能得到较好的表现 但很多人不愿意用这个办法的原因是 你最后会把问题落实到 把时间花在确定常数1和常数2上 这让算法显得更繁琐 也就是说 为了让算法更好 你要调整更多的参数 但如果你能调整得到比较好的参数的话 你会得到的图形是 你的算法会在最小值附近振荡 但当它越来越靠近最小值的时候 由于你减小了学习速率 因此这个振荡也会越来越小 直到落到几乎靠近全局最小的地方 我想这么说能听懂吧？ 这个公式起作用的原因是 随着算法的运行 迭代次数会越来越大 因此学习速率α会慢慢变小 因此你的每一步就会越来越小 直到最终收敛到全局最小值 所以 如果你慢慢减小α的值到0 你会最后得到一个更好一点的假设 但由于确定这两个常数需要更多的工作量 并且我们通常也对 能够很接近全局最小值的参数 已经很满意了 因此我们很少采用逐渐减小α的值的方法 在随机梯度下降中 你看到更多的还是让α的值为常数 虽然两种做法的人都有 总结一下 这段视频中 我们介绍了一种方法 近似地监测出随机梯度下降算法在最优化代价函数中的表现 这种方法不需要定时地扫描整个训练集 来算出整个样本集的代价函数 而是只需要每次对最后1000个 或者多少个样本 求一下平均值 应用这种方法 你既可以保证随机梯度下降法正在正常运转和收敛 也可以用它来调整学习速率α的大小【教育无边界字幕组】翻译: 所罗门捷列夫 校对: 竹二个 审核：Naplessss