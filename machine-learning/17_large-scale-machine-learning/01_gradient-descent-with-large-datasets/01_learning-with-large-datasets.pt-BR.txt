Nos próximos vídeos falaremos de aprendizado de máquina em larga escala. Ou seja, nossos algoritmos lidando com grandes quantidades de dados. Se olharmos para a história recente do campo, 5 ou 10 anos atrás, um dos motivos pelos quais os algoritmos de aprendizado funcionam bem melhor hoje do que, digamos, há 5 anos, é a quantidade enorme de dados que temos agora e que podemos usar para treinar os algoritmos. Neste próximos vídeos, falaremos sobre algoritmos para lidar com quantidades massivas de dados. Por que gostaríamos de usar esses conjuntos grandes de dados? Nós vimos que uma das melhores maneiras de conseguir um sistema de aprendizado de máquina de alta performance, caso o seu algoritmo tenha um baixo desvio, é treiná-lo com muitos dados. E um dos exemplos que já vimos antes é esse de distinguir entre palavras similares que geralmente confundem-se. Seja: Para o café da manhã eu comi (dois) ovos, e nós vimos que nesse tipo de aplicação, contanto que você providencie muitos dados para o algoritmo, ele se sairá muito bem. E são resultados como esses que nos levam a dizer que em aprendizado de máquina não é o melhor algoritmo que vence, mas aquele que dispõem de mais dados. Você quer então treinar o seu algoritmo com grandes conjuntos de dados, ao menos quando nós podemos ter esses grandes conjuntos. Mas o aprendizado com grandes conjuntos de dados também acompanha seus problemas típicos, especialmente, os computacionais. Digamos que o tamanho do seu conjunto de treinamento M seja 100.000.000. E esse é um número compatível com os conjuntos de dados atuais. Se você verificar os dados do senso dos EUA, uma vez que existe, aproximadamente, 300 milhões de pessoas nos EUA, você pode provavelmente chegar em centenas de milhões de registros. Se você olhar para a quantidade de tráfego que sites populares tem, você pode facilmente conseguir conjuntos de treinamento que são muito maiores que centenas de milhões. Digamos que você queira treinar uma regressão linear, ou talvez uma regressão logística, em ambos os casos usaria o gradiente descendente. E se você for ver o que precisará para calcular o gradiente, que é esse termo aqui, então quando M é cem milhões, você terá que lidar com uma somatória através de cem milhões de termos. Para que possa calcular essas derivadas e realizar um único passo em direção ao mínimo. Devido o custo computacional da somatória de centena de milhões de termos, para que se possa calcular apenas um passo do gradiente decrescente, nos próximos vídeos nós iremos falar sobre técnicas para tanto substituir isso por algo melhor ou encontrar maneiras mais eficientes de calcular essa derivada. Ao final dessa sequência e vídeos sobre aprendizado em larga escala, você saberá como ajustar modelos de regressão lineares, regressão logística, redes neurais e assim por diante mesmo com os conjuntos de dados modernos, que possuem, digamos, cem milhões de exemplos. Obviamente, antes de nos esforçarmos para treinar um modelo com cem milhões de dados, nós devemos nos perguntar o porquê de não usarmos apenas, digamos, mil exemplos. Talvez possamos pegar aleatoriamente subconjuntos de mil exemplos desse conjunto maior de cem milhões e treinar o nosso algoritmo apenas com mil dados. Portanto, antes de investir esforço em desenvolver algo capaz de lidar com essa quantidade massiva de dados, é sempre bom conferir se treinar o modelo com mil exemplos não é bom o suficiente. A maneira de verificar se usar um conjunto de treinamento bem menor pode ter um efeito tão bom quanto, ou seja, que usando um número muito menor como um conjunto de tamanho 1000, que teria performance tão boa quanto, é através tão usado método de colocar no gráfico as curvas de aprendizado, então se você montar as curvas de aprendizado e se a sua função objetivo de treino parece-se com algo assim, essa é a J de teta de treino. E se a sua função objetivo para a validação, Jcv de teta parecer-se com isso, então teremos um caso de algoritmo de aprendizado com alta variância, e estaremos mais confiantes na melhoria de sua performance ao adicionarmos mais exemplos de treinamento. Enquanto isso, se você desenhar suas curvas de aprendizado, se sua curva de treino paracer-se com isso, e a de validação cruzada com isso aqui, então parece que o algoritmo sofre de alto desvio. E neste último caso, como sabemos, se desenharmos, digamos, para m igual a 1000, com m indo de 500 até igual a 1000, então parece-nos improvável que aumentar m até cem milhões trará uma melhoria considerável, então é melhor ficarmos mesmo com n igual a 1000, ao invés de investir muito esforço em como escalaríamos esse algoritmo. É claro que se você estiver na situação mostrada na figura da direita, então algo natural a fazer seria adicionar novas variáveis, ou adicionar novas unidades ocultas na sua rede neural e assim por diante, e assim você terminaria em uma situação mais próxima da presenta à esquerda, onde estejamos em uma situação, talvez, de n igual a 1000, e então isso nos daria mais confiança em tentar modificar o algoritmo para usar muito mais do que mil exemplos e isso então valeria o tempo investido. Portanto, em aprendizado de máquina em larga escala, nós queremos propor maneiras computacionalmente razoáveis, ou maneiras computacionalmente eficientes, para lidar com conjuntos de dados muito grandes. Nos próximos vídeos, nós veremos dois dos principais conceitos. O primeiro é chamado de gradiente decrescente estocástico e o segundo é a divisão do cálculo do gradiente para possibilitar cálculos paralelos. Depois de aprender esses métodos, você estará apto para escalar seus algoritmos de aprendizado com grandes conjuntos de dados e terá uma performance melhor em diferente aplicações.