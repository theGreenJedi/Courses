1
00:00:00,251 --> 00:00:05,622
Para muitos problemas de aprendizado, entre os quais a regressão linear, regressão logística e redes neurais,

2
00:00:05,622 --> 00:00:11,955
a maneira pela qual derivamos o algoritmo foi através de uma função custo ou otimizando uma função objetivo.

3
00:00:11,955 --> 00:00:16,476
Então usávamos um algoritmo como o gradiente descendente para minimizar a função custo.

4
00:00:16,476 --> 00:00:22,461
Quando temos um conjunto de treinamento muito grande, o gradiente descendente torna-se computacionalmente muito custoso.

5
00:00:22,461 --> 00:00:29,300
Neste vídeo falaremos sobre uma modificação do algoritmo básico do gradiente descendente chamado de gradiente descendente estocástico,

6
00:00:29,300 --> 00:00:37,841
que nos permitirá escalar esses algoritmos usando conjuntos de dados muito maiores.

7
00:00:37,841 --> 00:00:41,928
Suponha que você está modelando com uma regressão linear e usando o gradiente descendente.

8
00:00:41,928 --> 00:00:48,055
Só pra recapitular, a hipótese será algo assim, e a função custo se parecerá com isto,

9
00:00:48,055 --> 00:00:54,459
a metade da média do erro quadrático da sua hipótese ao longo dos seus "m" exemplos de treinamento,

10
00:00:54,459 --> 00:00:59,705
e a função custo nós já vimos que tem esse formato de tigela.

11
00:00:59,705 --> 00:01:06,659
Traçando o gráfico em função dos parâmetros "θ₀" e "θ₁", a função custo "J" é uma função com esse tipo de curva.

12
00:01:06,659 --> 00:01:10,999
O gradiente descendente se parece com isso,

13
00:01:10,999 --> 00:01:15,594
onde a cada iteração atualizam-se os parâmetros "θ" usando essa expressão.

14
00:01:15,594 --> 00:01:22,574
No restante do vídeo, continuarei usando a regressão linear como exemplo.

15
00:01:22,574 --> 00:01:29,371
Mas o princípio do gradiente descendente estocástico é completamente aplicável para os outros algoritmos de aprendizagem,

16
00:01:29,371 --> 00:01:38,011
como regressão logística, redes neurais e outros algoritmos que são treinados com gradiente descendente usando um conjunto de dados.

17
00:01:38,011 --> 00:01:43,236
Aqui está uma figura que representa o funcionamento do gradiente descendente. Se os parâmetros são inicializados naquele ponto,

18
00:01:43,236 --> 00:01:50,072
ao longo das iterações do gradiente descendente os parâmetros serão conduzidos para o mínimo global.

19
00:01:50,072 --> 00:01:55,193
Então temos uma trajetória que parece perseguir o mínimo global de maneira bem direta.

20
00:01:55,193 --> 00:01:59,561
Mas o problema com o gradiente descendente ocorre quando "m" é grade.

21
00:01:59,561 --> 00:02:08,382
Nesse caso, calcular esse termo da derivada pode ser muito custoso, pois percorre todo os "m" exemplos.

22
00:02:08,382 --> 00:02:15,644
Então se "m" é 300 milhões, assim como nos EUA existem 300 milhões de pessoas,

23
00:02:15,644 --> 00:02:20,783
e então o censo dos EUA pode ter muitos dados nessa ordem de grandeza.

24
00:02:20,783 --> 00:02:26,715
Se você quer ajustar o seu modelo de regressão linear aos dados, precisará realizar a somatória por de 300 milhões de registros.

25
00:02:26,715 --> 00:02:36,385
E isso é bem dispendioso! Para darmos um nome para o algoritmo, essa versão particular do gradiente descendente também é chamada de "gradiente descendente em lote".

26
00:02:36,385 --> 00:02:41,352
O termo lote se refere a estarmos usando todos os exemplos de treinamento de uma só vez.

27
00:02:41,352 --> 00:02:44,303
Nós chamamos de lote o conjunto dos exemplos de treinamento.

28
00:02:44,303 --> 00:02:51,853
Esse pode não ser o melhor nome, mas é assim que as pessoas do campo chamam essa versão do gradiente descendente.

29
00:02:51,853 --> 00:02:57,157
Se imaginarmos que realmente temos 300 milhões de registros armazenados em disco,

30
00:02:57,157 --> 00:03:05,945
o algoritmo lerá no disco todos esses 300 milhões de registros para calcular a derivada.

31
00:03:05,945 --> 00:03:11,508
Você precisa transmitir todos esses registros para o computador pois você não consegue armazená-los todos na memória RAM.

32
00:03:11,508 --> 00:03:16,425
Então você precisa percorrê-los e ler todos e, lentamente, acumular a sua soma para calcualr a derivada.

33
00:03:16,425 --> 00:03:21,452
E depois de ter feito tudo isso, você terá terminado apenas um passo do algoritmo do gradiente descendente.

34
00:03:21,452 --> 00:03:24,749
Agora será preciso fazer tudo novamente,

35
00:03:24,749 --> 00:03:28,424
percorrer todos os 300 milhões de registros, acumular essas somas,

36
00:03:28,424 --> 00:03:32,578
e, tendo terminado tudo, será outro pequeno passo do gradiente descendente.

37
00:03:32,578 --> 00:03:36,959
Depois, novamente. E esse será apenas o terceiro passo, e assim por diante.

38
00:03:36,959 --> 00:03:40,819
Isso nos tomaria um tempo enorme para fazer com que o algoritmo convergisse.

39
00:03:40,819 --> 00:03:45,375
Em contraste com o gradiente descendente em lote, usaremos um algoritmo modificado,

40
00:03:45,375 --> 00:03:50,465
que não terá necessidade de verificar todos os exemplos de treinamento para cada uma das iterações,

41
00:03:50,465 --> 00:03:55,118
mas precisará verificar um único exemplo de treinamento para cada iteração.

42
00:03:55,118 --> 00:03:59,617
Antes de partirmos para o novo algoritmo, aqui está o gradiente descendente em lote,

43
00:03:59,617 --> 00:04:05,794
sendo esta a função custo, esta a atualização, e, claro,

44
00:04:05,794 --> 00:04:10,678
este termo que é usado na regra do gradiente descendente, a derivada parcial

45
00:04:10,678 --> 00:04:17,933
em relação aos parâmetros "θⱼ" do objetivo de otimização, "Jₜᵣₐᵢₙ(θ)".

46
00:04:17,933 --> 00:04:23,386
Agora, vejamos como é o algoritmo mais eficiente e que escala melhor com conjuntos grandes de dados.

47
00:04:23,386 --> 00:04:26,489
Para que o gradiente descendente estocástico funcione,

48
00:04:26,489 --> 00:04:32,657
definimos a função custo de uma maneira um pouco diferente. Definimos o custo em função de "θ"

49
00:04:32,657 --> 00:04:40,471
e em relação aos exemplos de treinamento "(x⁽ⁱ⁾, y⁽ⁱ⁾)",

50
00:04:40,471 --> 00:04:44,791
igual à metade do quadrado do erro da hipótese no exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)".

51
00:04:44,791 --> 00:04:53,386
Essa função custo quantifica o quão bem a minha hipótese está se saindo em um único exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)".

52
00:04:53,386 --> 00:05:01,010
Agora note que a função custo "Jₜᵣₐᵢₙ" pode ser escrita desta forma.

53
00:05:01,010 --> 00:05:09,606
"Jₜᵣₐᵢₙ(θ)" é a média ao longo dos "m" exemplos do custo da hipótese em cada exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)".

54
00:05:09,606 --> 00:05:13,522
Munidos com essa definição de função custo para a regressão linear,

55
00:05:13,522 --> 00:05:17,636
escreverei agora o que o gradiente descendente estocástico faz.

56
00:05:17,636 --> 00:05:26,940
O primeiro passo do gradiente descendente estocástico é embaralhar aleatoriamente o conjunto de dados.

57
00:05:26,940 --> 00:05:32,539
Ou seja, misturar, reorganizar aleatoriamente os "m" exemplos de treinamento,

58
00:05:32,539 --> 00:05:37,450
Este é um procedimento padrão de pré-processamento ao qual voltaremos em breve.

59
00:05:37,450 --> 00:05:42,997
Mas a parte principal do gradiente descendente estocástico se dá da seguinte forma.

60
00:05:42,997 --> 00:05:48,150
Repetimos, para "i = 1" a "m",

61
00:05:48,150 --> 00:05:53,067
Acessamos repetidamente os meus exemplos de treinamento e realizamos a seguinte atualização.

62
00:05:53,067 --> 00:06:06,523
"θⱼ = θⱼ - α · (h(x⁽ⁱ⁾) - y⁽ⁱ⁾) · x⁽ⁱ⁾ⱼ"

63
00:06:06,523 --> 00:06:12,961
Faremos essa atualização, como de costume, para todos os valores de "j".

64
00:06:12,961 --> 00:06:24,708
Note que este termo aqui é exatamente o que tínhamos dentro da somatória para o gradiente descendente em lote.

65
00:06:24,708 --> 00:06:31,256
Na verdade, para aqueles que são familiarizados com cálculo, é possível mostrar que esse termo aqui

66
00:06:31,256 --> 00:06:43,511
igual a derivada parcial em relação ao meu parâmetro "θⱼ" do custo em "(θ, (x⁽ⁱ⁾, y⁽ⁱ⁾))".

67
00:06:43,511 --> 00:06:47,383
O custo é essa expressão que definimos anteriormente.

68
00:06:47,383 --> 00:06:52,081
Concluindo o algoritmo, fecho a chave aqui.

69
00:06:52,081 --> 00:06:59,365
O que o gradiente descendente estocástico está fazendo é percorrer os exemplos de treinamento,

70
00:06:59,365 --> 00:07:04,349
começando pelo primeiro exemplo de treinamento, "(x⁽¹⁾, y⁽¹⁾)",

71
00:07:04,349 --> 00:07:09,399
e, então, olhando apenas para esse primeiro exemplo, ele irá realizar um pequeno passo do gradiente descendente,

72
00:07:09,399 --> 00:07:13,725
em relação apenas ao custo do primeiro exemplo de treinamento.

73
00:07:13,725 --> 00:07:15,717
Em outras palavras, olhamos para o primeiro exemplo

74
00:07:15,717 --> 00:07:21,214
e modificamos os parâmetros um pouquinho para ajustá-los ao primeiro exemplo um pouco melhor.

75
00:07:21,214 --> 00:07:29,244
Feito isso neste loop interior, o mesmo será feito com o segundo exemplo de treinamento

76
00:07:29,244 --> 00:07:33,848
e realiza outro pequeno passo no espaço dos parâmetros,

77
00:07:33,848 --> 00:07:39,682
modificando os parâmetros apenas um pouco para ajustá-los melhor ao segundo exemplo de treinamento.

78
00:07:39,682 --> 00:07:44,130
Tendo feito isso, iremos para o terceiro exemplo de treinamento.

79
00:07:44,130 --> 00:07:51,722
E modificaremos os parâmetros para se ajustarem melhor ao nosso terceiro exemplo,

80
00:07:51,722 --> 00:07:55,114
e assim por diante até termos percorrido todo o conjunto de treinamento.

81
00:07:55,114 --> 00:08:01,297
Esse loop de fora fará com que sejam tomados diversos passos ao longo de todo o conjunto de treinamento.

82
00:08:01,297 --> 00:08:07,346
Essa visão do gradiente descendente estocástico também nos indica por que queremos embaralhar o nosso conjunto de dados.

83
00:08:07,346 --> 00:08:10,772
Isso nos assegura que, quando percorremos os exemplos de treinamento,

84
00:08:10,772 --> 00:08:15,197
que nós acabamos por visitar os exemplos de treinamento de forma aleatória.

85
00:08:15,197 --> 00:08:21,229
Dependendo se os seus dados já vieram aleatoriamente ordenados ou se eles vieram organizados de alguma forma estranha,

86
00:08:21,229 --> 00:08:26,391
na prática, isso ajuda o algoritmo a convergir um pouco mais rápido.

87
00:08:26,391 --> 00:08:30,985
Portanto, por uma questão de segurança, é melhor embaralhar aleatoriamente o seu conjunto,

88
00:08:30,985 --> 00:08:34,056
se você não tiver certeza de que ele já está organizado de forma aleatória.

89
00:08:34,056 --> 00:08:37,240
Mas, mais importante, o gradiente descendente estocástico pode ser visto como

90
00:08:37,240 --> 00:08:45,504
algo bem parecido com a versão em lote, mas em vez de esperar toda a soma desses termos por todos os "m" exemplos,

91
00:08:45,504 --> 00:08:50,624
calculamos o termo do gradiente usando apenas um exemplo de treinamento,

92
00:08:50,624 --> 00:08:54,810
e já progredimos na melhoria dos parâmetros com apenas este exemplo.

93
00:08:54,810 --> 00:09:02,248
Fazemos isso em vez de esperar o algoritmo percorrer todos os 300 milhões de registros do censo dos EUA,

94
00:09:02,248 --> 00:09:05,632
ao invés de termos que passar por todos esses exemplos de treinamento,

95
00:09:05,632 --> 00:09:09,947
antes de podermos modificar os parâmetros e ir na direção do mínimo global.

96
00:09:09,947 --> 00:09:14,975
Com o gradiente descendente estocástico, precisamos olhar para um único exemplo de treinamento,

97
00:09:14,975 --> 00:09:22,188
e já começamos a progredir na direção dos parâmetros que nos levem ao mínimo global.

98
00:09:22,188 --> 00:09:27,558
Aqui temos o algoritmo reescrito, onde o primeiro passo é embaralhar aleatoriamente nossos dados,

99
00:09:27,558 --> 00:09:35,089
o segundo passo é onde o algoritmo realmente trabalha, onde é feita a atualização em relação a um único exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)".

100
00:09:35,089 --> 00:09:40,139
Vejamos o que esse algoritmo faz com os parâmetros.

101
00:09:40,139 --> 00:09:43,467
Antes, com o gradiente descendente em lote,

102
00:09:43,467 --> 00:09:46,331
o algoritmo que usa todos os exemplos de treinamento em um único passo,

103
00:09:46,331 --> 00:09:53,397
O gradiente descendente em lote tende a tomar uma trajetória relativamente reta até chegar no mínimo global, desta maneira.

104
00:09:53,397 --> 00:09:59,956
Já o gradiente descendente estocástico terá iterações muito mais rápidas,

105
00:09:59,956 --> 00:10:03,108
pois não precisaremos percorrer todos os exemplos de treinamento.

106
00:10:03,108 --> 00:10:07,259
Mas cada iteração tenta ajustar nossos parâmetros em relação a um único exemplo de treinamento.

107
00:10:07,259 --> 00:10:13,931
Então, se formos aplicar o gradiente descendente estocástico em um ponto, peguemos um como este para começar.

108
00:10:13,931 --> 00:10:19,556
A primeira iteração pode modificar os parâmetros nesse sentido,

109
00:10:19,556 --> 00:10:23,791
e talvez a segunda iteração, que olha somente o segundo exemplo de treinamento,

110
00:10:23,791 --> 00:10:28,278
tem a possibilidade de, com falta de sorte, nos levar, para uma direção ruim como esta.

111
00:10:28,278 --> 00:10:33,731
Na terceira iteração, quando modificamos os parâmetros para se ajustarem apenas em relação ao terceiro exemplo,

112
00:10:33,731 --> 00:10:36,418
talvez terminemos apontando para essa direção.

113
00:10:36,418 --> 00:10:42,717
Então iremos para a quarta iteração e teremos algo assim. A quinta, sexta, sétima e assim por diante.

114
00:10:42,717 --> 00:10:46,725
Quando você executa o gradiente descendente estocástico,

115
00:10:46,725 --> 00:10:52,923
você perceberá que ele geralmente modifica os parâmetros na direção do mínimo global, mas não sempre.

116
00:10:52,923 --> 00:11:00,117
E isso fará com que tenhamos uma aparência mais aleatória em direção ao mínimo global.

117
00:11:00,117 --> 00:11:07,630
Na verdade, gradiente descendente estocástico não converge como em lote.

118
00:11:07,630 --> 00:11:15,196
O que o estocástico faz é rondar em uma região perto do mínimo global,

119
00:11:15,196 --> 00:11:18,740
mas ele nunca chega e permanece lá.

120
00:11:18,740 --> 00:11:21,676
Mas na prática isso não é um problema,

121
00:11:21,676 --> 00:11:26,788
desde que os parâmetros terminem em uma região perto o suficiente do mínimo global.

122
00:11:26,788 --> 00:11:32,164
Portanto, se os parâmetros terminam bem perto do mínimo global, a hipótese será boa.

123
00:11:32,164 --> 00:11:36,340
Portanto, quando executamos o gradiente descendente estocástico,

124
00:11:36,340 --> 00:11:43,658
obtemos parâmetros perto do mínimo global e isso é bom o suficiente na prática.

125
00:11:43,658 --> 00:11:47,121
Um último detalhe: no gradiente descendente estocástico,

126
00:11:47,121 --> 00:11:51,099
nós temos que repetir esse loop externo, o que significa ter esse loop interno repetido múltiplas vezes.

127
00:11:51,099 --> 00:11:53,892
Mas quantas vezes repetimos esse loop externo?

128
00:11:53,892 --> 00:11:59,336
Dependendo do tamanho do conjunto de treinamento, realizar esse loop uma única vez pode ser suficiente.

129
00:11:59,336 --> 00:12:02,064
É comum até, digamos, umas 10 vezes.

130
00:12:02,064 --> 00:12:05,852
Então podemos acabar repetindo esse loop interno de uma até dez vezes.

131
00:12:05,852 --> 00:12:12,309
Se tivermos um conjunto de dados enorme como o censo dos EUA,

132
00:12:12,309 --> 00:12:15,260
com 300 milhões de exemplos,

133
00:12:15,260 --> 00:12:19,609
é possível que, ao realizar apenas uma vez o processo pelo conjunto de treinamento,

134
00:12:19,609 --> 00:12:23,073
ou seja, de "i = 1" até 300 milhões, é

135
00:12:23,073 --> 00:12:25,720
possível que um único loop pelo conjunto de dados

136
00:12:25,720 --> 00:12:29,872
seja o suficiente para gerar uma hipótese muito boa.

137
00:12:29,872 --> 00:12:36,613
Nesses casos em que "m" é muto, muito grande, é provável que em uma única iteração do loop interno você consiga bons resultados.

138
00:12:36,613 --> 00:12:43,071
Mas, geralmente, realizar de 1 até 10 repetições ao longo do seu conjunto de treinamento será o mais comum.

139
00:12:43,071 --> 00:12:45,439
Mas isso realmente vai depender do tamanho do seu conjunto de treinamento.

140
00:12:45,439 --> 00:12:49,413
Se você comparar com o gradiente descendente em lote,

141
00:12:49,413 --> 00:12:53,905
no qual, após realizar um passo utilizando todo o conjunto de treinamento,

142
00:12:53,905 --> 00:12:57,034
você terá realizado apenas um passo do gradiente descendente.

143
00:12:57,034 --> 00:13:01,983
Em desses pequenos passinhos do gradiente descendente onde você se aproximou um pouquinho,

144
00:13:01,983 --> 00:13:05,776
e é por isso que o gradiente descendente estocástico pode ser bem mais rápido.

145
00:13:05,776 --> 00:13:10,880
Esse foi o algoritmo do gradiente descendente estocástico.

146
00:13:10,880 --> 00:13:15,594
Se você implementá-lo, você poderá utilizar muitos de seus algoritmos de aprendizagem

147
00:13:15,594 --> 99:59:59,000
em conjuntos de dados de escala muito maior e com eficiência.