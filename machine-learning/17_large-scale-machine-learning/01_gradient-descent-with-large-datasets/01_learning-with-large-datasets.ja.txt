続く幾つかのビデオで、大規模スケールの機械学習について話す。 つまり、アルゴリズムなんだけれど、とてもビッグなデータセットを見る物だ。 ここ最近5年とか10年の機械学習の歴史を振り返ると、 5年前と比べても凄く学習アルゴリズムがうまく動作するようになった理由の一つには、 アルゴリズムのトレーニングに使えるデータの量が単純に増えた、というのがある。 ここからの幾つかのビデオでは、そんな巨大なデータセットがあった時にそれを扱うアルゴリズムについて議論していきたい。 さて、そもそも何故そんな大きなデータセットを使いたいと思うのか？ 我らは既に、機械学習のシステムで高いパフォーマンスを得たい時には、ベストな方法の一つに 低いバイアスの学習アルゴリズムを使い、大量のデータでトレーニングさせる、というのがあった。 既に見た前出の例としては、このややこしい単語の分類の例がある。 For breakfastには、I ateといえばtwoのeggsとなる。
この例の問題を解くと、こんな結果となる。 見た所、データを大量に突っ込めば突っ込む程良く振舞うように見える。 だから、これらのような結果から、以下のような事が良く言われる。 機械学習においては、勝者はもっとも良いアルゴリズムを持つ物では無く、もっともデータを持っている人だ、と。 だから大規模なデータセットから学習したい、少なくともそんなデータセットが入手可能ならば。 しかし大規模なデータセットからの学習は、特有の問題もつきまとう。具体的には、計算的な問題だ。 トレーニングセットのサイズmが100,000,000だとしよう。 これはこんにち的なデータセットしては、普通に現実的な範囲だ。 USの国勢調査のデータを見ると、そこには、 3億の人がUSには居るから、普通に何億ってデータを扱う事になる。 人気のあるwebサイトのトラフィックの量を見ると、 簡単に億より多くの手本を得る事になる。 線形回帰のモデルをトレーニングしたいとしよう。またはロジスティック回帰でも良い。 その場合、これが最急降下法のルールとなる。 そして勾配を計算する為に必要な事を見ると、 それはここの項だ。そしてmが一億の時は、 1億に渡る和を取る必要がある、 これらの微分項を計算する為には、そして最急降下法の1ステップを実行する為には。 1億に渡る和を取るという計算量的なコストの為に、 最急降下法のたった1ステップを計算する為だけに。 続く一連のビデオで、これを別の何かに置き換えるテクニックや、 この微分項を計算するより効率的な方法について議論する。 この大規模スケールの機械学習の一連のビデオを観終わった頃には、 線形回帰とかロジスティック回帰とかニューラルネットワークのモデルのフィッティングを こんにち的なデータセット、つまり一億の手本とかに行う方法を理解する事になるだろう。 もちろん、一億の手本でモデルをトレーニングするという努力を払う前に、 単に1000の手本を使うだけでダメなのか？という事も自らに問うてみなくてはならない。 時には1億の手本からランダムに1000の手本を ピックアップして、その1000の手本でトレーニングするだけ、でも良いかもしれない。 だからこれらの巨大なモデルをトレーニングするのに必要なソフトウェアの開発などに投資する前に、 1000の手本でトレーニングしてみる事は、良いサニティチェックとなる。 より少ないトレーニングセットでサニティチェックを行う方法は、 つまりより少ないm=1000のサイズのトレーニングセットを用いる方法は、 通常の学習曲線をプロットする、という方法で良いだろう。 もし学習曲線をプロットしてみて、トレーニングの目的関数がこんな感じなら、 この目的関数はJ trainのシータだが、 そしてクロスバリデーションセットの目的関数、Jcvのシータがこんな感じだったとする。 その場合、高バリアンスな学習アルゴリズムのようなので、 さらに追加でトレーニング手本を足す事は、パフォーマンスを改善すると確信が持てる。 一方、対照的に、学習曲線をプロットしたら、 トレーニングの目的関数がこんな感じで、クロスバリデーションの目的関数がこんな感じだと、 これは高バイアスな学習アルゴリズムに見える。 この後者の場合には、例えばこれがm=1000までのプロットだとして、 この辺がm=500で、1000までとして、 するとたぶん、データを1億に増やしてもたぶんあまり良くはならないだろう。 それならばアルゴリズムのスケールを増やす為に努力を費やすよりは、 m=1000のままにしておく方が良かろう。 もちろん、この右側の図のような状況だったら、 次に行うべき自然なステップとしては、追加のフィーチャーを足すとか、 隠れユニットをニューラルネットワークに足すとか、そういう事で、 そういう事を通してm=1000のままだと左側の状態に近づいていったら その時は初めて1億以上の手本を使うように、 インフラを追加したり、アルゴリズムを変更したりする事に、より確信を持てるようになり、それは実際に良い自分の時間の使い方だと思えるだろう。 さて、大規模スケールの機械学習においては、とてもビッグなデータを扱うのに、 計算量的にリーズナブルな、または効率的な方法を知りたい。 続く幾つかのビデオでは、二つの主なアイデアを見ていく。 最初の物は確率的な最急降下法、そして二番目はMap Reduceと呼ばれる物。とてもビッグなデータを見るのに。 そしてこれらの手法を学んだ後には、あなたの学習アルゴリズムをビッグにスケールアップ出来るようになり、 様々な応用に対して、もっと良いパフォーマンスが得られるようになる事を祈る。