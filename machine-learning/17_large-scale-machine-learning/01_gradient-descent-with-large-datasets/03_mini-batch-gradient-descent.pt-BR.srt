1
00:00:00,000 --> 00:00:07,306
No vídeo anterior, nós falamos sobre o gradiente decrescente estocástico, e como ele poderia ser bem mais veloz que o gradiente decrescente de lote completo.

2
00:00:07,306 --> 00:00:12,866
Neste vídeo, vamos falar sobre outra variação e que chama-se gradiente decrescente com lote parcial.

3
00:00:12,866 --> 00:00:16,906
E ele pode rodar às vezes até mais rápido que o gradiente descendente estocástico.

4
00:00:16,906 --> 00:00:22,046
Para resumir o algoritmo que fizemos uso até agora,

5
00:00:22,046 --> 00:00:26,619
no gradiente descendente de lote completo nós usamos todos os exemplos em cada passo.

6
00:00:26,619 --> 00:00:31,792
Enquanto na versão estocástica nós usamos um único exemplo em cada passo.

7
00:00:31,792 --> 00:00:36,120
O que o gradiente descendente de lote parcial faz é um meio termo entre esses dois.

8
00:00:36,120 --> 00:00:46,559
Especificamente, com esse algoritmo nós iremos usar "b" exemplos em cada iteração, onde b é um parâmetro chamado como "tamanho do lote".

9
00:00:46,559 --> 00:00:52,688
Então a idéia é que ele resulte em algo que fique entre os dois casos, onde usamos apenas um exemplo e onde usamos todos.

10
00:00:52,688 --> 00:00:57,488
E ele é exatamente igual o gradiente descendente completo, exceto pelo fato que usarei apenas uma parte dos exemplos.

11
00:00:57,488 --> 00:01:08,672
Uma escolha típica para o valor de b poderia ser algo como 10. E um típico intervalo para b seria algo entre 2 até 100.

12
00:01:08,672 --> 00:01:13,668
Estes seriam um intervalo bem comum para a escolha do gradiente descendente lote parcial.

13
00:01:13,668 --> 00:01:21,153
E o intuito é que ao invés de usarmos um exemplo por vez ou todos de uma só vez, que usemos b exemplos por vez.

14
00:01:21,153 --> 00:01:28,833
Vou escrever isso de maneira informal. Peguemos então b, neste caso, digamos que b seja igual a 10.

15
00:01:28,833 --> 00:01:37,782
Então pegaremos os 10 próximos exemplos do meu conjunto de dados, e isso será um determinado subconjunto xi, yi.

16
00:01:37,782 --> 00:01:46,114
Se forem 10 exemplos, então os índices devem ir até x (i+9), y (i+9),

17
00:01:46,114 --> 00:01:57,794
e esses serão esses 10 exemplos juntos, e então nós, basicamente, realizaremos um passo do gradiente decrescente usando esses 10 exemplos.

18
00:01:57,794 --> 00:02:19,012
Sendo a taxa de aprendizado vezes um sobre dez, vezes a somatoria através de k igual a i até i+9 de h de x(k) menos y(k) e vezes x(k).

19
00:02:19,012 --> 00:02:27,213
E então nessa expressão, onde a somatória acontece entre os meus dez exemplos.

20
00:02:27,229 --> 00:02:32,370
Esse dez, como vocês devem ver, é o tamanho do meu gradiente com lote parcial e esse i+9,

21
00:02:32,370 --> 00:02:39,384
o 9 vem da escolha do parâmetro b, e isso aqui será incrementado,

22
00:02:39,384 --> 00:02:46,755
e i vai passar pelos próximos 10 exemplos e continuar evoluindo dessa maneira.

23
00:02:46,755 --> 00:02:50,584
Para finalizar, só para escrever o algoritmo todo.

24
00:02:50,584 --> 00:02:55,231
Para simplificarmos a parte dos índices,

25
00:02:55,231 --> 00:02:59,843
irei assumir que temos um gradiente descendente com lote parcial de tamanho dez e conjunto de treinamento de tamanho de 1000,

26
00:02:59,843 --> 00:03:05,045
o que faremos é ter esse tipo de "for" com i igual a um, onze, vinte e um,

27
00:03:05,045 --> 00:03:07,926
pulando de 10 em 10, pois escolhemos realizar com 10 exemplos de cada vez.

28
00:03:07,926 --> 00:03:13,648
E depois realizamos uma atualização no gradiente decrescente usando os dez exemplos por vez,

29
00:03:13,648 --> 00:03:21,566
então esse 10 e esse i+9 são consequências da escolha do parâmetro b como 10.

30
00:03:21,566 --> 00:03:27,435
E você pode perceber que esse loop do For terminará no 991, 

31
00:03:27,435 --> 00:03:34,457
pois se temos 1000 exemplos de treinamento, então eu preciso de 100 passos de tamanho 10 para percorre-los todos.

32
00:03:34,457 --> 00:03:37,729
E esse é o gradiente decrescente de lote parcial.

33
00:03:37,729 --> 00:03:43,219
Comparado com o completo, ele também possibilitará cálculos mais rápidos.

34
00:03:43,219 --> 00:03:49,487
E se pegarmos novamente o nosso exemplo do censo dos EUA com 300 milhões de exemplos de treinamento,

35
00:03:49,487 --> 00:03:55,621
então o que estou querendo mostrar aqui é que após utilizar os 10 primeiros exemplos, nós podemos começar a progredir 

36
00:03:55,621 --> 00:04:00,873
na questão da definição dos parâmetros teta, nós não precisamos utilizar todos os milhões de dados.

37
00:04:00,873 --> 00:04:05,377
Nós só precisamos pegar os primeiros 10 exemplos e isso já nos permitirá avançar 

38
00:04:05,377 --> 00:04:09,289
e então nós poderemos pegar os próximos dez e modificar novamente os parâmetros um pouquinho e assim por diante.

39
00:04:09,289 --> 00:04:14,186
E é por isso que o gradiente decrescente de lote parcial pode ser mais veloz que o completo.

40
00:04:14,186 --> 00:04:19,578
Basicamente, você pode começar a progredir na modificação dos parâmetros após verificar apenas dez exemplos

41
00:04:19,578 --> 00:04:24,836
ao invés de ter que esperar o algoritmo verificar todos os seus 300 milhões de dados.

42
00:04:24,836 --> 00:04:29,699
Agora a relação entre o gradiente descendente de lote parcial e o estocástico. 

43
00:04:29,699 --> 00:04:38,237
Por que nós queremos verificar b exemplos por vez ao invés de pegar apenas um único exemplo como o gradiente decrescente estocástico? 

44
00:04:38,237 --> 00:04:42,044
A resposta é vetorização.

45
00:04:42,044 --> 00:04:47,450
Particularmente, o gradiente decrescente de lote parcial tem uma grande chance de ter uma performance melhor que o estocástico

46
00:04:47,450 --> 00:04:50,817
apenas se for usada uma boa implementação vetorizada.

47
00:04:50,817 --> 00:04:58,571
Neste caso, a soma dos 10 exemplos pode ser realizada de forma vetorial,

48
00:04:58,571 --> 00:05:05,376
o que possibilitará o uso parcial de computação paralela para os dez exemplos.

49
00:05:05,376 --> 00:05:09,953
Em outras palavras, ao usar-se uma implementação vetorizada apropriada para calcular o restante dos termos,

50
00:05:09,953 --> 00:05:18,565
você pode usar bibliotecas de cálculo numérico e paralelizar o calculo do seu gradiente para os b exemplos,

51
00:05:18,565 --> 00:05:24,152
enquanto se você tivesse pegando apenas um único exemplo por vez com o caso estocástico,

52
00:05:24,152 --> 00:05:27,456
não haveria nada para botar em paralelo.

53
00:05:27,456 --> 00:05:29,824
Ou, ao menos, há menos para se colocar em paralelo.

54
00:05:29,824 --> 00:05:34,866
Uma desvantagem do gradiente decrescente de lote parcial é ter que lidar com um parâmetro extra b,

55
00:05:34,866 --> 00:05:39,006
o tamanho do lote parcial, com o qual você terá que lidar e isso talvez tome o seu tempo.

56
00:05:39,006 --> 00:05:45,611
Mas se você tiver uma implementação vetorizada boa, a velocidade do lote parcial pode ser ainda mais rápida que a da versão estocástica.

57
00:05:45,611 --> 00:05:52,937
E esse é o gradiente decrescente de lote parcial, que é um algoritmo que

58
00:05:52,937 --> 00:05:57,697
atua em um meio termo do que faz o gradiente descendente estocástico e o de lote completo.

59
00:05:57,697 --> 00:06:02,626
E se você escolher um bom valor para b, eu geralmente uso b igual a 10, 

60
00:06:02,626 --> 00:06:07,343
mas outro valores, qualquer coisa entre 2 e 100, pode ser razoavelmente comum. 

61
00:06:07,343 --> 00:06:11,917
Então escolhemos um valor para b e se usarmos uma boa implementação vetorial,

62
00:06:11,917 --> 00:06:15,917
teremos algumas vezes um resultado mais rápido que o gradiente descendente estocástico e que o gradiente descendente de lote completo.