आप अब जानते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट अल्गोरिद्म. लेकिन जब आप रन कर रहे हैं अल्गोरिद्म, आप कैसे सुनिश्चित करते हैं कि यह पूरी तरह डीबग हो गया है और यह सही कन्वर्ज हो रहा है? उतना ही महतवपूर्ण है, कैसे आप ट्यून करते हैं लर्निंग रेट अल्फ़ा स्टोकस्टिक ग्रेडीयंट डिसेंट के साथ. इस विडीओ में हम बात करेंगे कुछ टेक्नीक्स की इन कामों को करने के लिए, सुनिश्चित करने के लिए कि यह कन्वर्ज हो रहा है और चुनने के लिए लर्निंग रेट अल्फ़ा. पहले जब हम कर रहे थे बैच ग्रेडीयंट डिसेंट, हमारा स्टैंडर्ड तरीक़ा सुनिश्चित करने के लिए कि ग्रेडीयंट डिसेंट कन्वर्ज हो रहा है था कि हम प्लॉट कर सकते थे ऑप्टिमायज़ेशन कॉस्ट फ़ंक्शन को इटरेशन संख्या के फ़ंक्शन की तरह. तो वह था कॉस्ट फ़ंक्शन और हम तय कर पाए कि यह कॉस्ट फ़ंक्शन कम हो रहा है हर इटरेशन में. जब ट्रेनिंग सेट साइज़ छोटे थे, हम वह कर पाए क्योंकि हम कम्प्यूट कर पाए सम काफ़ी एफ़िशिएँट ढंग से. लेकिन जब आपके पास है एक वृहद / मैसिव ट्रेनिंग सेट साइज़ तब आप नहीं चाहते कि आपको रोकना पड़े आपका अल्गोरिद्म बीच बीच में॰ आप नहीं चाहते कि आपको रोकना पड़े स्टोकस्टिक ग्रेडीयंट डिसेंट कम्प्यूट करने के लिए यह कॉस्ट फ़ंक्शन क्योंकि इसे चाहिए एक सम आपके पूरे ट्रेनिंग सेट का. और पूरा उद्देश्य स्टोकस्टिक ग्रेडीयंट डिसेंट का था कि आप चाहते थे शुरू करना प्रोग्रस्स देखने के बाद केवल एक अकेला इग्ज़ाम्पल बिना आवश्यकता के देखने की आपका पूरा ट्रेनिंग सेट एकदम मध्य में अल्गोरिद्म के, सिर्फ़ कम्प्यूट करने के लिए चीज़ें जैसे कॉस्ट फ़ंक्शन पूरे ट्रेनिंग सेट का. अत: स्टोकस्टिक ग्रेडीयंट डिसेंट में, चेक करने के लिए कि अल्गोरिद्म कन्वर्ज हो रहा है, यहाँ है जो आप कर सकते हैं इसके स्थान पर. लेते हैं परिभाषा कॉस्ट की जो हमारे पास थी पहले. अत: कॉस्ट पेरमिटर्स थीटा की विद रिस्पेक्ट टु एक ट्रेनिंग इग्ज़ाम्पल के है सिर्फ़ वन हाफ़ स्क्वेर्ड एरर उस ट्रेनिंग इग्ज़ाम्पल पर. तब, जब स्टोकस्टिक ग्रेडीयंट डिसेंट लर्न कर रहा है, उसके ठीक पहले हम ट्रेन करते हैं एक ख़ास इग्ज़ाम्पल पर. तो, स्टोकस्टिक ग्रेडीयंट डिसेंट में हम देखेंगे इग्ज़ाम्पल्ज़ एक्स आइ, वाय आइ पर, क्रम में, और तब एक प्रकार से करते हैं एक छोटा अप्डेट इस इग्ज़ाम्पल के आधार पर. और हम जाते हैं अगले इग्ज़ाम्पल पर, एक्स आइ जमा 1, वाय आइ जमा 1, और इसी प्रकार आगे, सही? वह है जो स्टोकस्टिक ग्रेडीयंट डिसेंट करता है. अत:, जब अल्गोरिद्म देख रहा है इग्ज़ाम्पल एक्स आइ, वाय आइ पर, लेकिन इससे पहले कि यह करे अप्डेट पेरमिटर्स थीटा को उस इग्ज़ाम्पल का इस्तेमाल करके, आओ कम्प्यूट करते हैं कॉस्ट उस इग्ज़ाम्पल की. सिर्फ़ इसी चीज़ को कहने के लिए दोबारा, लेकिन दूसरे शब्दों में. एक स्टोकस्टिक ग्रेडीयंट डिसेंट स्कैन कर रहा है हमारे ट्रेनिंग सेट में से ठीक पहले उससे जब हम  थीटा को अप्डेट करते हैं एक ख़ास इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर. चलिए कम्प्यूट करते हैं कि हमारी हायपॉथिसस कितना अच्छा कर रही है उस ट्रेनिंग इग्ज़ाम्पल पर. और हम यह करना चाहते हैं थीटा को अप्डेट करने से पहले क्योंकि यदि हमने अभी अप्डेट किया थीटा को इग्ज़ाम्पल इस्तेमाल करके, आप जानते हैं, कि शायद यह बेहतर कर रहा हो उस इग्ज़ाम्पल पर बजाय उसके जो कि प्रतिनिधित्व करता हो॰ अंत में, चेक करने के लिए कन्वर्जेन्स स्टोकस्टिक ग्रेडीयंट डिसेंट की, हम क्या कर सकते हैं कि प्रत्येक, मान लो, प्रत्येक हज़ार इटरेशन्स पर, हम प्लॉट कर सकते हैं ये कॉस्ट्स जो हम कम्प्यूट कर रहे थे पिछले स्टेप में. हम कर सकते हैं प्लॉट ये कॉस्ट्स ऐव्रिज करके, मान लो, पिछले हज़ार इग्ज़ाम्पल्ज़ पर जो प्रॉसेस किए हैं अल्गोरिद्म ने. और यदि आप यह करते हैं, यह आपको देता है एक प्रकार से एक रनिंग अनुमान कि अल्गोरिद्म  कितने अच्छे से काम कर रहा है आप जानते हैं, पिछले 1000 ट्रेनिंग इग्ज़ाम्पल्ज़ पर जो आपके अल्गोरिद्म ने देखे हैं. अत:, तुलना में कम्प्यूट करने में जे<u > ट्रेन, समय-समय पर जिसे चाहिए स्कैन करना पूरे ट्रेनिंग सेट से.</u> इस दूसरी प्रक्रिया से, अच्छा, स्टोकस्टिक ग्रेडीयंट के हिस्से की तरह, इतना समय नहीं लगता इन कॉस्ट्स को कम्प्यूट करने के लिए ठीक पहले अप्डेट करने के पेरामिटर थीटा को. और हम यह सिर्फ़ प्रत्येक हजार इटरेशंन्स के बाद कर रहे हैं, हम सिर्फ़ ऐव्रिज करते हैं पिछली 1,000 कॉस्ट्स जो हमने कम्प्यूट की थी और उसे प्लॉट करते हैं. और उन प्लॉट्स को देखकर, यह चेक हो जाता है यदि स्टोकस्टिक ग्रेडीयंट डिसेंट कन्वर्ज हो रहा है. तो यहाँ हैं कुछ उदाहरण कि ये प्लॉट्स कैसे दिखते होंगे. मान लो आपने प्लॉट की है कोस्ट ऐव्रिज पिछले हज़ार इग्ज़ाम्पल्ज़ पर, क्योंकि ये हैं ऐव्रिज किए हुए सिर्फ़ एक हज़ार इग्ज़ाम्पल्ज़ पर, वे होंगे थोड़े नोयज़ी और इसलिए, यह शायद कम न हो प्रत्येक इटरेशन में. तब यदि आपको मिलता है एक चित्र जो ऐसा दिखता हैं, अत: प्लॉट नोयज़ी है क्योंकि यह ऐव्रिज है, आप जानते हैं, सिर्फ़ एक छोटे सब सेट पर, मान लो एक हज़ार ट्रेनिंग इग्ज़ाम्पल्ज़ पर. यदि आपको मिलता है एक चित्र जो ऐसा दिखता हैं, आप जानते हैं वह होगा एक अच्छा रन अल्गोरिद्म के साथ. शायद, जहाँ यह दिखता है कि कॉस्ट कम हुई है और तब यह प्लैटू जो दिखता है कि दबा दिया गया है, आप जानते हैं, शुरू करने पर उस पोईँट के आस पास से. लगता है, यह है आपकी कॉस्ट तब शायद आपके लर्निंग अल्गोरिद्म ने कन्वर्ज कर लिया है. यदि आप चाहते हैं इस्तेमाल करना एक छोटी लर्निंग रेट, कुछ जो शायद आप देखे ऐसा कि अल्गोरिद्म शुरू में शायद लेर्न करे धीरे अत: कॉस्ट कम होती जाती है ज्यादा धीरे॰ लेकिन तब आखिरकार आपके पास एक छोटी लर्निंग रेट है जो वास्तव में संभव है आपके अल्गोरिद्म को पहुँचने में, शायद एक थोड़े बेहतर हल पर. तो लाल लाइन शायद दर्शाती है आचरण स्टोकेस्टिक ग्रेडिएंट डिसेंट का एक छोटी लर्निंग रेट के साथ. और कारण कि यह ऐसा क्योंकि, आपको स्मरण होगा, स्टोकेस्टिक ग्रेडिएंट डिसेंट केवल कनवर्ज नहीं होता ग्लोबल मिनिमम पर. है कि यह क्या करता है कि पैरामीटर्स ओसीलेट / झूलते हैं थोड़ा बहुत ग्लोबल मिनिमम के इर्दगिर्द. और इसलिए लेने से एक छोटी लर्निंग रेट आपके पास होंगीं छोटी ऑसिलेशन्स॰ और कभी-कभी यह छोटा अंतर नगण्य होगा और कभी-कभी छोटी रेट से मिल सकती है आपको एक बेहतर वैल्यू पैरामीटर्स की. यहाँ है और कुछ चीजें जो हो सकती हैं. मान लो आप रन करते हो स्टोकेस्टिक ग्रेडिएंट डिसेंट और आप एवरेज करते हो एक हज़ार से अधिक एग्जामपल्स पर प्लॉट करते समय ये कॉस्ट्स. तो, आप जानते हैं, यहाँ हो सकता है परिणाम उनमें से किसी एक प्लॉट का. तब फिर से, यह एक प्रकार से कनवर्ज हो गया दिखता है. यदि आपको लेनी होती यह संख्या, एक हज़ार, और बढ़ानी होती एवरेज करने के लिए 5 हज़ार  एग्जामपल्स पर. तब यह संभव है शायद आपको मिले एक अधिक समतल कर्व जो दिखता है ऐसा. और एवरेज करने से, मान लो 5,000 एग्जामपल्स पर, बजाय 1,000 के, आपको शायद मिल सकता है एक समतल कर्व ऐसा. और इसलिए वह है प्रभाव बढ़ाने का संख्या एग्जामपल्स की जिस पर आप एवरेज करते हो. नुक्सान इसे ज्यादा बढ़ाने का निस्संदेह है कि अब आपको मिलता है केवल एक डेटा पॉइंट प्रत्येक 5,000 एग्जामपल्स पर. तो फीड बैक जो आपको मिलता है कैसा आपका लर्निंग अल्गोरिद्म कर रहा है, एक प्रकार से, शायद यह काफी विलंबित है क्योंकि आपको मिलता है एक डेटा पॉइंट आपके प्लॉट पर प्रत्येक 5,000 एग्जामपल्स पर बजाय 1,000 एग्जामपल्स पर. इसी प्रकार कभी-कभी जब रन करेंगे ग्रेडिएंट डिसेंट और आपको मिलेगा एक प्लॉट जो ऐसा दिखता है. और वैसे दिखने वाले एक प्लॉट से, आप जानते हैं, ऐसा लगता है कि कॉस्ट कम नहीं हो रही है बिल्कुल. ऐसा लगता है कि अल्गोरिद्म लर्न ही नहीं कर रहा है. यह है केवल, ऐसा दिखता है यहाँ एक दबा हुआ कर्व और कॉस्ट बिलकुल कम नहीं हो रही है. लेकिन फिर से यदि आपको इसे बढ़ाना होता ऐव्रिज करने के लिए इग्ज़ाम्पल्ज़ की एक बड़ी संख्या पर ऐसा सम्भव है कि आप देखें कुछ इस लाल लाइन जैसा ऐसा लगता है कॉस्ट वास्तव में कम हो रही है, यह है कि सिर्फ़ नीली लाइन ऐव्रिज हो रही है 2, 3 इग्ज़ाम्पल्ज़ पर, नीली लाइन इतनी नोयज़ी थी कि आप नहीं देख सकते थे असली ट्रेंड कॉस्ट में वास्तव में कम होते हुए और शायद एवरेज करने से 5,000 एग्जामपल्स पर बजाय 1,000 के सहायक हो सकता है. अवश्य ही हमने ऐव्रिज किया इग्ज़ाम्पल्ज़ की बड़ी संख्या पर जो यहाँ हमने ऐव्रिज किया 5,000 इग्ज़ाम्पल्ज़ पर, मैं सिर्फ़ ले रहा हूँ एक अलग रंग, यह सम्भव है कि आप देखें एक लर्निंग कर्व जो शायद ऐसा हो. यह अभी भी दबा हुआ है जब आपने ऐव्रिज भी कर लिया है इग्ज़ाम्पल्ज़ कि बड़ी संख्या पर. और जब आपको वह मिलता है, तब वह शायद होगा एक अधिक मज़बूत प्रमाण कि दुर्भाग्य से अल्गोरिद्म नहीं कुछ ज़्यादा लर्न कर पा रहा कारण जो भी हों. और आपको या तो बदलना चाहिए लर्निंग रेट या बदलने चाहिए फ़ीचर्ज़ या बदलना चाहिए कुछ और इस अल्गोरिद्म में. अंत में, एक आख़िरी चीज़ जो आप शायद देखें वह होगी यदि आपको प्लॉट करने हों ये कर्व्ज़ और आप देखें एक कर्व जो ऐसा दिखता हैं, जहाँ ऐसा लगता है कि यह बढ़ रहा है. और ऐसी स्थिति है तब यह एक संकेत है कि अल्गोरिद्म डाईवर्ज हो रहा है. और आपको क्या वाक़ई में क्या करना चाहिए कि ले एक छोटी वैल्यू लर्निंग रेट अल्फ़ा की. आशा है, इससे आपको समझ आ गया होगा एक रेंज कि क्या-क्या आप देख सकते हैं जब आप प्लॉट करते हैं ये कॉस्ट ऐव्रिज इग्ज़ाम्पल्ज़ की कुछ रेंज पर और बता सकते हैं कि क्या क्या आप कर सकते हैं प्रतिक्रिया में इन विभिन्न प्लॉट्स को देख कर. तो यदि प्लॉट्स दिखते हैं काफ़ी नोयज़ी, या यदि यह अधिक टेढ़ा मेढ़ा होता है, तब कोशिश करें बढ़ाने की इग्ज़ाम्पल्ज़ की संख्या जिस पर आप ऐव्रिज कर रहे हैं ताकि आप देख सके ओवरॉल ट्रेंड प्लॉट में बेहतर. और यदि आप देखते हैं एररज़ वास्तव में बढ़ रही है, कॉस्ट वास्तव में बढ़ रही है, कोशिश करें लेने की एक छोटी वैल्यू अल्फ़ा की. अंत में, लर्निंग रेट का पहलू थोड़ा और खोजने योग्य है. हमने देखा कि जब हम रन करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, अल्गोरिद्म शुरू होता है यहाँ और एक प्रकार से घूम-घूम कर पहुँचता है मिनिमम पर और तब यह असल में कन्वर्ज नहीं होता, और इसके बजाय घूमता रहता है मिनिमम के इर्दगिर्द हमेशा के लिए. और इसलिए अंत में आपको मिलती है एक पेरामिटर वैल्यू जो है आशापूर्वक नज़दीक ग्लोबल मिनिमम के जो वास्तव में ग्लोबल मिनिमम नहीं है. स्टोकस्टिक ग्रेडीयंट डिसेंट की अधिकांश इम्प्लमेंटेशन्स में, लर्निंग रेट अमूमन रखा जाता है कॉन्स्टंट. और इसलिए आपको मिलती है एक पिक्चर ऐसी. और यदि आप चाहते हैं स्टोकेस्टिक ग्रेडिएंट डिसेंट को कनवर्ज करवाना ग्लोबल मिनिमम पर, एक काम जो आप कर सकते हैं वह है कि आप धीरे धीरे कम करें लर्निंग रेट अल्फ़ा को. तो, एक आम तरीक़ा उसे करने का होगा कि सेट करें अल्फ़ा को किसी कॉन्स्टंट 1 विभाजित इटरेशन संख्या जमा कॉन्स्टंट 2 से. अत:, इटरेशन संख्या हैं संख्या इटरेशन्स की जो आपने की हैं स्टोकस्टिक ग्रेडीयंट डिसेंट की, तो यह वास्तव में हैं संख्या ट्रेनिंग इग्ज़ाम्पल्ज़ की जो आपने देखें हैं और कॉन्स्टंट 1 और कॉन्स्टंट 2 हैं अतिरिक्त पेरमिटर्स अल्गोरिद्म के जिनमें शायद आपको थोड़ी फेर-बदल करनी पड़े पाने के लिए एक बेहतर पर्फ़ॉर्मन्स. एक कारण कि लोग यह नहीं करना चाहते क्योंकि थोड़ा समय लग सकता है फेरबदल करने में इन 2 अतिरिक्त पेरमिटर्स, कोंस्टंट 1 और कोंस्टंट 2 में, और इसलिए यह करता है अल्गोरिद्म को अधिक तुनुकमिज़ाज. आप जानते हैं, यह है सिर्फ़ अतिरिक्त पेरमिटर्स जिनमें फेरबदल करके आप अल्गोरिद्म को बेहतर कर सकते हैं. लेकिन यदि आप कर पाते हैं ट्यून पेरमिटर्स को अच्छे से, तब पिक्चर जो आपको मिलती है कि अल्गोरिद्म वास्तव में घूम कर पहुँचता है मिनिमम की तरफ़, लेकिन जैसे यह नज़दीक पहुँचता है क्योंकि आप कम कर रहे हैं लर्निंग रेट घुमाव होते जाएँगे और छोटे जब तक यह ग्लोबल मिनिमम पर नहीं पहुँच जाता. उमीद है यह समझ आया होगा, सही? और कारण कि यह फ़ॉर्म्युला समझ आता है क्योंकि जैसे अल्गोरिद्म रन करता है, इटरेशन संख्या बड़ी होती जाती है और अल्फ़ा धीरे-धीरे कम होता जाता है, और इसलिए आप लेते हैं और छोटे स्टेप्स जब तक आशापूर्वक यह कन्वर्ज होता है ग्लोबल मिनिमम पर. तो यदि आप धीरे-धीरे कम करते हैं अल्फ़ा को ज़ीरो तक आपको मिलती है थोड़ी बेहतर हायपॉथिसस. लेकिन क्योंकि अतिरिक्त कार्य करना पड़ता है फेर-बदल करने में पेरमिटर्स को और क्योंकि स्पष्ट: हम काफ़ी ख़ुश है किसी भी पेरामिटर वैल्यू से जो नज़दीक है ग्लोबल मिनिमम के. अमूमन यह प्रक्रिया घटाने की अल्फ़ा को धीरे-धीरे नहीं की जाती और रखना लर्निंग रेट अल्फ़ा कोंस्टंट ज़्यादा आम ऐप्लिकेशन है स्टोकस्टिक ग्रेडीयंट डिसेंट की जबकि आप देखेंगे लोग इस्तेमाल करते हैं कोई भी वर्ज़न. सारांश में इस विडीओ में हमने बात की लगभग मॉनिटर करने की कि स्टोकस्टिक ग्रेडिएंट डिसेन्ट कैसे कर रहा है ऑप्टिमायज़ कॉस्ट फ़ंक्शन को. और यह है एक ढंग जिसमें आवश्यकता नहीं स्कैन करने की पूरे ट्रेनिंग सेट को समय-समय पर कम्प्यूट करने के लिए कॉस्ट फ़ंक्शन पूरे ट्रेनिंग सेट पर. लेकिन इसके स्थान पर यह देखता है मान लो सिर्फ़ आख़िर के हज़ार इग्ज़ाम्पल्ज़ या ऐसा कुछ. और आप इस्तेमाल कर सकते हैं इस विधि को दोनो तय करने के लिए स्टोकस्टिक ग्रेडीयंट डिसेंट सही जा रहा है और कन्वर्ज कर रहा है या प्रयोग कर सकते हैं इसे ट्यून करने के लिए लर्निंग रेट अल्फा को.