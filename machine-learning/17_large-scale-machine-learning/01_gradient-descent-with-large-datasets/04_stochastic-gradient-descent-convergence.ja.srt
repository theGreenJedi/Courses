1
00:00:00,493 --> 00:00:03,492
いまや、あなたは確率的最急降下法のアルゴリズムについて知った。

2
00:00:03,492 --> 00:00:09,907
だがアルゴリズムを実行している時、あなたはどうやってバグが無い、とかちゃんと収束している、という事を確認すれば良いだろうか？

3
00:00:09,907 --> 00:00:15,813
同じように重要な事として、どうやって確率的最急降下法においてどうやって学習率のアルファをチューンしたら良い？

4
00:00:15,813 --> 00:00:25,950
このビデオでは、これらを行う幾つかのテクニックを紹介する、収束を確認する方法と学習率アルファを選ぶ方法。

5
00:00:25,950 --> 00:00:30,600
バッチ最急降下法を使ってた頃を思い返すと、最急降下法が収束していたかを

6
00:00:30,600 --> 00:00:36,493
確認する標準的な方法は、最適化の目的関数の値を繰り返しの回数の関数としてプロットする事だった。

7
00:00:36,493 --> 00:00:44,366
これがコスト関数で、このコスト関数が各イテレーションで減少している事を確認したい。

8
00:00:44,366 --> 00:00:50,438
トレーニングサイズが小さい時はそれが出来た。
何故なら和の計算がとても早く行えたからだ。

9
00:00:50,438 --> 00:00:57,950
だが大量のトレーニングセットのサイズがあると、定期的にアルゴリズムを止めて、、、

10
00:00:57,950 --> 00:01:04,045
確率的最急降下法を定期的に止めてこのコスト関数を計算したくは無い、何故なら、

11
00:01:04,045 --> 00:01:07,442
このコスト関数の計算には、トレーニングセットサイズ全体に渡る和を必要とするから。

12
00:01:07,442 --> 00:01:12,466
そもそもに確率的最急降下法のポイントは、全て、アルゴリズムの途中でトレーニングセット全体を

13
00:01:12,466 --> 00:01:19,130
スキャンする必要無しに、一つの手本を見ただけで歩を進める事が出来る、

14
00:01:19,130 --> 00:01:25,583
という物だった。
コスト関数などを計算する為だけにトレーニングセット全体を見る、というような事無しに。

15
00:01:25,583 --> 00:01:32,472
だから確率的最急降下法においてアルゴリズムが収束しているのを確認する為にやる事としては、代わりにこんな事をやる。

16
00:01:32,472 --> 00:01:36,367
前に定義したコスト関数を使おう。

17
00:01:36,367 --> 00:01:42,647
単体のトレーニング手本に関するパラメータシータでのコストは、単にそのトレーニング手本における二乗誤差の半分だ。

18
00:01:42,647 --> 00:01:49,754
そして、確率的最急降下法を学習させている間、ある特定のサンプルを学習させる直前、

19
00:01:49,754 --> 00:01:54,601
つまり確率的最急降下法で順番に見ていって、あるサンプルxi, yiをこれから見よう、という時、

20
00:01:54,601 --> 00:01:57,329
この次にはこのサンプルによるちょっとの更新を行う訳だ。

21
00:01:57,329 --> 00:02:04,095
そして次の手本、x(i+1), y(i+1)に進む、などなど。

22
00:02:04,095 --> 00:02:05,880
以上が確率的最急降下法がやる事だが、

23
00:02:05,880 --> 00:02:15,024
つまりアルゴリズムが手本xi, yiを見ているが、まだパラメータシータをその手本を使ってアップデートしていない時の、

24
00:02:15,024 --> 00:02:20,255
その手本のコストを計算してみよう。

25
00:02:20,255 --> 00:02:23,577
同じ事をちょっとだけ異なる言葉で言い換えてみよう。

26
00:02:23,577 --> 00:02:33,294
確率的最急降下法がトレーニングセットをスキャンしていく訳だが、ある手本 x(i), y(i) を使ってシータをアップデートする直前で、

27
00:02:33,294 --> 00:02:38,198
そのトレーニング手本に対し仮説がどれだけ良いかを計算してみよう。

28
00:02:38,198 --> 00:02:43,852
これをシータをアップデートする前に行いたいのは、もしシータをその手本でアップデートした後では、

29
00:02:43,852 --> 00:02:49,061
その手本については代表的な値よりももっと良くなってしまうから。

30
00:02:49,061 --> 00:02:57,438
最後に、確率的最急降下法が収束しているかをチェックする為に出来る手段としては、各1000繰り返しごとに

31
00:02:57,438 --> 00:03:01,511
その前のステップで計算したこれらのコスト関数をプロットする、というのがある。

32
00:03:01,511 --> 00:03:07,450
アルゴリズムに処理された最後の1000手本に渡るコストの平均をプロット出来る。

33
00:03:07,450 --> 00:03:12,714
これをやると、アルゴリズムがどれくらいうまく行ってるかのランニングでの推計が得られる。

34
00:03:12,714 --> 00:03:17,049
アルゴリズムが見た最後の1000手本に関しての。

35
00:03:17,049 --> 00:03:23,974
J trainを定期的に計算するのと比べると、そちらはトレーニングセット全体をスキャンする必要があるが、

36
00:03:23,974 --> 00:03:27,973
この方法だと、確率的最急降下法の一部として、

37
00:03:27,973 --> 00:03:32,965
パラメータシータをアップデートする直前にこれらのコストを計算するのは、そんなに高くはつかない。

38
00:03:32,965 --> 00:03:40,276
我らがやる事は、各1000イテレーションごととかに、そこまでに計算した最後の1000コストを平均して、それをプロットする。

39
00:03:40,276 --> 00:03:47,537
そのプロットを見る事で、確率的最急降下法が収束しているかをチェックする事が出来る。

40
00:03:47,537 --> 00:03:51,708
ここに、そのプロットがどんな風になりうるかの例がある。

41
00:03:51,708 --> 00:03:55,519
最後1000手本に渡るコストの平均をプロットしたとしよう、

42
00:03:55,519 --> 00:04:01,073
これは1000個だけの手本に渡る平均なので、ちょっとノイジーになるだろう、そして

43
00:04:01,073 --> 00:04:03,873
各イテレーションで必ず減少する、という訳でも無かろう。

44
00:04:03,873 --> 00:04:07,828
そしてこんな感じの図が得られたとすると、プロットはノイジーでしょう、

45
00:04:07,828 --> 00:04:11,721
何故なら小さなサブセット、1000個のトレーニング手本に渡ってだけの平均だから。

46
00:04:11,721 --> 00:04:17,283
で、もしこんな感じの図を得られたなら、これは結構良くアルゴリズムは実行されている、

47
00:04:17,283 --> 00:04:24,195
コストが下がっていって、その先である点から台地のように平坦になってる、こんな場合は。

48
00:04:24,195 --> 00:04:29,603
コストがこんな感じの時は、学習アルゴリズムはきっと収束している。

49
00:04:29,603 --> 00:04:34,252
もしもっと小さい学習率を用いて試したければ、その結果はこんな見た目で、

50
00:04:34,252 --> 00:04:39,229
アルゴリズムは最初はゆっくりと学習していく。だからコストはもっとゆっくりと下がっていく。

51
00:04:39,229 --> 00:04:47,585
だがやがてより小さい学習率だと、アルゴリズムは、たぶんちょっとだけ良い解となる。

52
00:04:47,585 --> 00:04:53,426
赤い線で、よりゆっくりな、より小さい学習率を用いた時の確率的最急降下法の場合を表すとする。

53
00:04:53,426 --> 00:05:00,594
この場合により良い解となる理由は、確率的最急降下法はグローバル最小に収束するのでは無く、

54
00:05:00,594 --> 00:05:05,068
グローバル最小の回りをちょっとだけ振動するのだった。

55
00:05:05,068 --> 00:05:09,231
だからより小さい学習率を使う事で、最終的にはより小さな振幅にする事が出来る。

56
00:05:09,231 --> 00:05:12,896
時にはこの小さな違いは無視出来る物だろう、

57
00:05:12,896 --> 00:05:19,686
時にはより小さい方がわずかに良いパラメータの値を得られるだろう。

58
00:05:19,686 --> 00:05:22,269
こんな場合もありうる。

59
00:05:22,269 --> 00:05:27,986
確率的最急降下法を走らせて、これらの1000個の手本に渡ってコストを平均してプロットしたとして、

60
00:05:27,986 --> 00:05:32,369
こんな結果が得られる場合もある。

61
00:05:32,369 --> 00:05:34,353
この場合も、一種の収束しているように見える。

62
00:05:34,353 --> 00:05:42,119
もしこの数字、1000を増やして5000手本に渡って平均をとれば、

63
00:05:42,119 --> 00:05:47,913
もっとスムースなカーブ、もっとこんな感じのが得られたと思われる。

64
00:05:47,913 --> 00:05:56,547
1000手本の代わりに5000手本に渡って平均を取るとすると、もっとスムースなカーブ、こんな感じの物が得られるだろう。

65
00:05:56,547 --> 00:06:00,248
それが平均を取る対象の手本の数を増やす効果だ。

66
00:06:00,248 --> 00:06:06,229
この値を大きくし過ぎた場合の欠点はもちろん、5000手本につき、たった一つの点しか得られないという事。

67
00:06:06,229 --> 00:06:12,001
だからアルゴリズムがどの位良く動いているかのフィードバックを得るのが、より遅れる事になる。

68
00:06:12,001 --> 00:06:17,681
何故ならプロット上の1点を得る為に1000手本じゃなくて5000手本ごとになるからだ。

69
00:06:17,681 --> 00:06:23,911
同様に最急降下法を走らせると、こんなプロットが得られる事もある。

70
00:06:23,911 --> 00:06:32,079
このプロットでは、コストは全く減少してないように見える。

71
00:06:32,079 --> 00:06:34,023
アルゴリズムは全く学習していないように見える。

72
00:06:34,023 --> 00:06:39,261
ここはフラットなカーブで、コストは低下してないように見える。

73
00:06:39,261 --> 00:06:46,260
だがこの場合も、平均を取る範囲を大きくすると、

74
00:06:46,260 --> 00:06:49,729
この赤い線のような物が得られる可能性がある。

75
00:06:49,729 --> 00:06:55,127
コストは実際に減少しているように見える。
青い線は2, 3の手本に渡ってだけの平均。

76
00:06:55,127 --> 00:07:01,374
青い線はあまりにもノイジーなので、実際のトレンド、コストが実際に低下しているというトレンドが見えない。

77
00:07:01,374 --> 00:07:06,688
そして1000の代わりに5000手本に渡って平均を取るという事が、助けになるかもしれない。

78
00:07:06,688 --> 00:07:12,358
もちろん、大きな数の手本に渡って平均をとっても、ここでは5000手本に渡って平均をとってみたが、

79
00:07:12,358 --> 00:07:16,998
ここでは別の色を使ったが、その時に、こんな風に学習曲線がなる場合もありうる。

80
00:07:16,998 --> 00:07:21,197
大きな数の手本に渡って平均しても、フラットなままだ。

81
00:07:21,197 --> 00:07:25,908
そしてそれが得られたら、それは不運にも何らかの理由で、

82
00:07:25,908 --> 00:07:29,287
アルゴリズムがあまり学習出来ていない、という事に、より固く確信を持てる。

83
00:07:29,287 --> 00:07:34,969
そして学習率を変えるなりフィーチャーを変えるなり、またはアルゴリズムに関しての何かを変えるなりをしなくてはならない。

84
00:07:34,969 --> 00:07:39,235
最後にもう一つ、これらの曲線をプロットしてみたら、

85
00:07:39,235 --> 00:07:43,273
そうしたらこんな曲線を得たとすると、つまり実際に増加しているようにみえたら、

86
00:07:43,273 --> 00:07:48,066
その時はそれはアルゴリズムが発散しているサインだ。

87
00:07:48,066 --> 00:07:53,965
その場合にすべき事は、より小さい値の学習率アルファを使う事だ。

88
00:07:53,965 --> 00:07:58,143
以上で、ある範囲の手本に渡るコスト関数の平均をプロットした時に、

89
00:07:58,143 --> 00:08:02,946
どんな事が起こりうるのか、そのそれぞれのプロットごとのオススメの対応について、

90
00:08:02,946 --> 00:08:07,765
感じがつかめたかな。

91
00:08:07,765 --> 00:08:15,070
もしプロットがあまりにもノイジーに見えたら、またはくねくねあがったり下がったりしすぎているようなら、平均を取る手本の範囲を増やしてみてくれ、

92
00:08:15,070 --> 00:08:18,734
するとプロットの全体的なトレンドをより良く分かるようになるだろう。

93
00:08:18,734 --> 00:08:25,836
そして誤差が実際に増加していたら、コストが実際に増加していたら、より小さい値のアルファを試してみてくれ。

94
00:08:25,836 --> 00:08:31,649
最後に、学習率の問題についてもうちょっと良く見てみよう。

95
00:08:31,649 --> 00:08:38,922
確率的最急降下法を走らせると、アルゴリズムはここから始まって、最小に向かってくねくねと歩くのを見た。

96
00:08:38,922 --> 00:08:43,494
そしてそれは実際には収束せずに、そのかわりに最小の付近を永遠にうろちょろし続ける。

97
00:08:43,494 --> 00:08:50,225
つまり、最終的にはグローバル最小に近いパラメータが得られる事が期待出来るが、完全にグローバル最小に一致する訳では無い。

98
00:08:50,225 --> 00:08:57,991
もっとも典型的な確率的最急降下法の実装では、学習率アルファは定数のまま据え置くのが普通だ。

99
00:08:57,991 --> 00:09:02,022
つまり最終的に得られるのはまさにこんな図となる。

100
00:09:02,022 --> 00:09:06,523
もし確率的最急降下法に実際にグローバル最小に収束してほしい、と思うなら、

101
00:09:06,523 --> 00:09:11,825
一つ考えられる手としては、学習率アルファを時間がたつにつれて徐々に下げていく、という物がある。

102
00:09:11,825 --> 00:09:22,240
割と良くやるのは、アルファをイコール、constant1割る事のイテレーション数+constant2、とかそんな数字にする。ここでconstant1とconstant2は何らかの定数。

103
00:09:22,240 --> 00:09:28,169
イテレーション数というのは確率的最急降下法の何回目のイテレーションか、を表す物で、

104
00:09:28,169 --> 00:09:29,519
ようするにそこまで見たトレーニング手本の数だ。

105
00:09:29,519 --> 00:09:34,103
そしてconst1とconst2はアルゴリズムの追加のパラメータで、

106
00:09:34,103 --> 00:09:38,160
良いパフォーマンスを得る為にいじらなくてはいけないかもしれない物だ。

107
00:09:38,160 --> 00:09:43,004
この方法を人々があんまり取らない理由としては、これら二つの追加のパラメータ、constant1とconstant2を

108
00:09:43,004 --> 00:09:48,122
調整するのに時間を食われるからだ。
そのせいでアルゴリズムが気難しくなる。

109
00:09:48,122 --> 00:09:52,113
つまりアルゴリズムがうまく行くように時間を浪費するハメになるパラメータが増えるのだ。

110
00:09:52,113 --> 00:09:57,246
だがもしパラメータをいい感じにチューン出来たら、得られる図は、

111
00:09:57,246 --> 00:10:02,834
アルゴリズムが最初はふらつきつつ、最小に向かっていくが、だが近づくと、

112
00:10:02,834 --> 00:10:07,024
学習率もそれにつれてどんどん下がっていくので、ふらつきは小さくなり、

113
00:10:07,024 --> 00:10:12,729
グローバル最小に至るまで小さくなり続ける。
これは納得出来るだろう。

114
00:10:12,729 --> 00:10:21,608
そしてこの式が納得出来る理由は、アルゴリズムが走るにつれて、イテレーション回数も大きくなっていくので、アルファはゆっくりと小さくなっていき、

115
00:10:21,608 --> 00:10:27,506
すると一歩一歩がどんどん小さくなっていき、それはグローバル最小に収束するまで小さくなり続ける。

116
00:10:27,506 --> 00:10:33,484
つまり、アルファをゆっくりと0へと減少させていくと、最終的にはちょっとだけ良い仮説が得られる。

117
00:10:33,484 --> 00:10:40,078
だが、定数をいじるのにかかる余計な仕事と、さらにざっくばらんに言ってしまえばグローバル最小に近いんなら、

118
00:10:40,078 --> 00:10:43,892
どんなパラメータの値でもまったく幸せなので、

119
00:10:43,892 --> 00:10:50,863
典型的には、このアルファをゆっくり減少させる、という手続きは、普通はやらん。で、確率的最急降下法を適用する時には、

120
00:10:50,863 --> 00:10:56,983
アルファは定数のままにしておく方がもっと普通だ。
どちらのバージョンを使う人も見かけはするけど。

121
00:10:56,983 --> 00:11:03,595
まとめると、このビデオでは、確率的最急降下法がどうなってるかを

122
00:11:03,595 --> 00:11:08,256
コスト関数の観点から近似的にモニタリングする方法を議論した。

123
00:11:08,256 --> 00:11:17,043
これはコスト関数を計算する為に定期的にトレーニングセット全体をスキャンする必要が無くて、

124
00:11:17,043 --> 00:11:20,693
代わりに例えば最後の1000手本とかを見る手法だ。

125
00:11:20,693 --> 00:11:27,592
そしてこの手法は確率的最急降下法がうまく機能していて、収束している、という事を確認するのにも、

126
00:11:27,592 --> 00:11:31,468
学習率アルファをチューンするのにも用いる事が出来る。