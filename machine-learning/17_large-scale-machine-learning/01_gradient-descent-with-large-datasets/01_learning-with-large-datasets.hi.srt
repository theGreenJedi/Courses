1
00:00:00,332 --> 00:00:04,284
अगले कुछ वीडियोज़ में हम बात करेंगे लार्ज स्केल मशीन लर्निंग की.

2
00:00:04,284 --> 00:00:08,316
वह कि, अल्गोरिद्म्स लेकिन दृष्टिकोण बड़े / बिग डाटा सेट्स का रखते हुए.

3
00:00:08,316 --> 00:00:12,839
यदि आप पीछे देखो हाल ही का 5 या 10 साल का इतिहास मशीन लर्निंग का.

4
00:00:12,839 --> 00:00:17,853
एक कारण कि लर्निंग अल्गोरिद्म्स इतना ज्यादा अच्छे से काम करते हैं अब 5 साल पहले के मुक़ाबले,

5
00:00:17,853 --> 00:00:22,657
है केवल बहुत मात्रा में डेटा जो अब हमारे पास है और जिन पर हम अपने अल्गोरिद्म्स को ट्रेन कर सकते हैं.

6
00:00:22,657 --> 00:00:29,741
इन अगले कुछ वीडियोज़ में, हम बात करेंगे अल्गोरिद्म्स की जो डील करते हैं इन मेस्सिव डेटा सेट्स को.

7
00:00:32,926 --> 00:00:35,527
तो क्यों हम चाहते है उपयोग करना इतने बड़े डेटा सेट्स को?

8
00:00:35,527 --> 00:00:40,564
हमने पहले देखा है कि श्रेष्ठतम उपाय एक अच्छी पर्फ़ॉर्मन्स वाला मशीन लर्निंग सिस्टम पाने का 

9
00:00:40,564 --> 00:00:46,168
है अगर आप लेते हैं एक लो-बायस लर्निंग अल्गोरिद्म, और उसे ट्रेन करते हैं बहुत से डेटा पर.

10
00:00:46,168 --> 00:00:53,561
और इसलिए, एक शुरू का उदाहरण हमने पहले देखा वह था यह उदाहरण क्लैसिफ़ाई करने का कोनफ़्यूसेबल वर्ड्ज़ / शब्द॰
.

11
00:00:53,561 --> 00:01:00,726
अत:, फ़ोर ब्रेक्फ़स्ट आई ऐट टू (टीडबल्यूओ) एग्ज़ और हमने देखा इस उदाहरण में, इस प्रकार के परिणाम,

12
00:01:00,726 --> 00:01:06,436
जहाँ, आप जानते हैं, जब तक फ़ीड करते रहते हैं अल्गोरिद्म को बहुत सा डेटा, वह अच्छा करता प्रतीत होता है.

13
00:01:06,436 --> 00:01:10,419
और इसलिए इस तरह के परिणामों से, प्रचलित हुआ मशीन लर्निंग में कि 

14
00:01:10,419 --> 00:01:15,151
अक्सर यह नहीं कि किसके पास श्रेष्ठतम अल्गोरिद्म है वही जीतता है. यह कि जिसके पास अधिकतम डेटा है.

15
00:01:15,151 --> 00:01:19,568
अत: आप लर्न करना चाहते हैं लार्ज डेटा सेट्स से, कम से कम तब जब हमें मिल सकते हैं इस प्रकार के लार्ज डेटा सेट्स.

16
00:01:19,568 --> 00:01:27,027
लेकिन लर्निंग लार्ज डेटा सेट्स के साथ आतीं हैं उससे ही सम्बंधित कुछ अलग कठिनाइयाँ, विशेषत: कॉम्प्यूटेशनल कठिनाइयाँ.

17
00:01:27,027 --> 00:01:33,870
मान लीजिए कि आपका ट्रेनिंग सेट साइज़ एम है 100,000,000.

18
00:01:33,870 --> 00:01:37,934
और यह वास्तव में सम्भव है आजकल के बहुत से डेटा सेट्स के लिए.

19
00:01:37,934 --> 00:01:40,518
अगर आप देखें द यूएस के सेंसस डेटा सेट को, अगर वहाँ है, आप जानते हैं,

20
00:01:40,518 --> 00:01:44,663
300 मिल्यन लोग यूएस में, आपको प्रायः मिल सकते हैं कई सौ मिल्यन्स रेकोडर्स.

21
00:01:44,663 --> 00:01:47,856
अगर आप देखे ट्रैफ़िक संख्या जो लोकप्रिय / पॉप्युलर वेब साइट्स को मिलती है,  

22
00:01:47,856 --> 00:01:52,509
आपको आसानी से ट्रेनिंग सेट मिल जाते है जो बहुत बड़े हैं सौ मिल्यंज़ के इग्ज़ैम्पल्ज़ से भी.

23
00:01:52,509 --> 00:01:57,407
और मान लीजिए आप ट्रेन करना चाहते हैं एक लिनीअर रेग्रेशन मॉडल, या शायद एक लजिस्टिक रेग्रेशन मॉडल, 

24
00:01:57,407 --> 00:02:01,692
जिस केस में यह ग्रेडीयंट डिसेंट रूल है.

25
00:02:01,692 --> 00:02:05,372
और अगर आप देखें कि आपको क्या चाहिए ग्रेडीयंट कम्प्यूट करने के लिए, 

26
00:02:05,372 --> 00:02:09,992
जो यह टर्म है यहाँ पर, तब जब एम एक सौ मिल्यन होता है, 

27
00:02:09,992 --> 00:02:13,976
आपको करना पड़ता है समेशन एक सौ मिल्यन टर्म्ज़ से अधिक का,

28
00:02:13,976 --> 00:02:18,977
कम्प्यूट करने के लिए ये डेरिवेटिवज़ टर्म्ज़ और करने के एक अकेला स्टेप डिसेंट का.

29
00:02:18,977 --> 00:02:25,627
कॉम्प्यूटेशनल इक्स्पेन्स के कारण जो एक सौ मिल्यन से ज़्यादा टर्म्ज़ के समेशन से आता है

30
00:02:25,627 --> 00:02:28,628
कम्प्यूट करने के लिए एक अकेला स्टेप ग्रेडीयंट डिसेंट का,

31
00:02:28,628 --> 00:02:31,530
अगले कुछ विडीओज़ में मैंने बात की है तकनीकों की

32
00:02:31,530 --> 00:02:38,413
या तो हम इसे बदल दें किसी और चीज़ से या ढूँढें कार्यक्षम / इफ़िशंट तरीक़े कम्प्यूट करने के लिए यह डेरिवेटिव.

33
00:02:38,413 --> 00:02:41,709
लार्ज स्केल मशीन लर्निंग विडीओज़ के इस क्रम के अंत तक,

34
00:02:41,709 --> 00:02:47,045
आप जान गए है मॉडल कैसे फ़िट करते हैं, लिनियर रिग्रेशन, लोजिस्टिक रिग्रेशन, न्यूरल नेटवर्क्स इत्यादि  

35
00:02:47,045 --> 00:02:50,990
यहाँ तक की आजकल के डेटा सेट्स से भी जिनमे हो सकते हैं शायद एक सौ मिल्यन इग्ज़ैम्पल्ज़.

36
00:02:50,990 --> 00:02:56,035
निस्संदेह, इससे पहले कि हम चेष्टा करें ट्रेन करने की एक मोडल एक सौ मिल्यन इग्ज़ैम्पल्ज़ के साथ,

37
00:02:56,035 --> 00:03:01,276
हमें अपने आप से यह भी पूछना चाहिए, क्यों न प्रयोग करें सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़.

38
00:03:01,276 --> 00:03:04,923
शायद हम रैंडम्ली / बिना किसी क्रम के ले सकते हैं सबसेट्स एक हज़ार इग्ज़ैम्पल्ज़ के

39
00:03:04,923 --> 00:03:10,254
एक सौ मिल्यन इग्ज़ैम्पल्ज़ में से और ट्रेन कर सकते हैं हमारा अल्गोरिद्म सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़ पर.

40
00:03:10,254 --> 00:03:16,076
अत: इससे पहले कि इतनी मेहनत करें वास्तव में सॉफ़्टवेयर डिवेलप करने की ट्रेन करने के लिए इन बड़े माडल्ज़ को 

41
00:03:16,076 --> 00:03:22,461
अच्छा रहता है कि आप एक सैनिटी चेक कर ले, कि अगर ट्रेनिंग सिर्फ़ एक हज़ार इग्ज़ैम्पल्ज़ पर भी उतना अच्छा काम कर रही है.

42
00:03:22,461 --> 00:03:29,731
तरीक़ा सैनिटी चेक का कि एक छोटा ट्रेनिंग सेट भी उतना अच्छा काम कर रहा है

43
00:03:29,731 --> 00:03:33,958
कि अगर एक छोटा ट्रेनिंग सेट जहाँ एम है 1000 साइज़ का ट्रेनिंग सेट 

44
00:03:33,958 --> 00:03:37,797
वह भी उतना अच्छा काम कर रहा है, वह है प्रचलित तरीक़ा प्लॉट करके देखने का लर्निंग कर्व्ज़,

45
00:03:37,797 --> 00:03:46,872
तो अगर आपको प्लॉट करने होंगें लर्निंग कर्व्ज़ और अगर आपका ट्रेनिंग अब्जेक्टिव ऐसा होगा,

46
00:03:46,872 --> 00:03:49,553
वह जे ट्रेन थीटा है.

47
00:03:49,553 --> 00:03:56,422
और अगर आपका क्रॉस वैलिडेशन सेट अब्जेक्टिव, जेसीवी ऑफ थीटा ऐसा होगा,

48
00:03:56,422 --> 00:04:00,310
तब यह लगता है एक हाई-वेरीयन्स लर्निंग अल्गोरिद्म,

49
00:04:00,310 --> 00:04:05,913
और हम होंगे अधिक विश्वस्त / कॉन्फ़िडेंट कि अतिरिक्त ट्रेनिंग इग्ज़ैम्पल्ज़ लेने से पर्फ़ॉर्मन्स बेहतर हो जाएगी.

50
00:04:05,913 --> 00:04:10,462
जबकि इसके विपरीत, यदि आपको प्लॉट करने होगे लर्निंग कर्व्ज़,

51
00:04:10,462 --> 00:04:20,339
अगर आपका ट्रेनिंग अब्जेक्टिव ऐसा होगा, और आपका क्रॉस वैलिडेशन सेट का अब्जेक्टिव ऐसा होगा, 

52
00:04:20,339 --> 00:04:24,292
तब यह हो लगता है एक श्रेष्ठ हाई-बायस लर्निंग अल्गोरिद्म,

53
00:04:24,292 --> 00:04:28,084
और इस दूसरे केस में, आप जानते हैं, अगर आप प्लॉट करें, 

54
00:04:28,084 --> 00:04:33,437
मान लीजिए, एम बराबर है 1000 तक और इसलिए वह है एम बराबर है 500 से एम बराबर है 1000 तक,

55
00:04:33,437 --> 00:04:39,400
तब ऐसा लगता है कि शायद सम्भव न है एम को सौ मिल्यन तक बढ़ाने से बेहतर कार्य होगा 

56
00:04:39,400 --> 00:04:42,736
और तब अच्छा रहेगा आपका एम बराबर है 1000 तक ही बने रहना,

57
00:04:42,736 --> 00:04:47,000
इसके बजाय कि आप इतनी मेहनत करें यह जानने के लिए कि कैसे स्केल करना है अल्गोरिद्म को.

58
00:04:47,000 --> 00:04:51,029
निस्संदेह, अगर आप होते इस स्थिति में जो दिखाई गई है दाईं तरफ़ के चित्र में, 

59
00:04:51,029 --> 00:04:53,885
तब यह स्वभाविक हो जाता और फ़ीचर्ज़ जोड़ लेना,

60
00:04:53,885 --> 00:04:58,484
या अतिरिक्त हिडन यूनिट्स जोड़ना आपके न्यूरल नेटवर्क में इत्यादि,

61
00:04:58,484 --> 00:05:04,627
ताकि आप उस स्थिति में आ पाएँ जो पास हो बाईं तरफ़ की तरह, जहाँ यह शायद है एम बराबर है 1000,  

62
00:05:04,627 --> 00:05:09,553
और यह तब आपको अधिक आश्वासन देगा कि कोशिश और इंफ़्रा स्ट्रक्चर जोड़ने की अल्गोरिद्म को बदलने के लिए 

63
00:05:09,553 --> 00:05:14,735
प्रयोग करने के लिए एक हज़ार से कहीं ज़्यादा इग्ज़ैम्पल्ज़ शायद आपके समय का अच्छा उपयोग होगा.

64
00:05:14,735 --> 00:05:19,642
तो लार्ज-स्केल मशीन लर्निंग में, हम चाहते है निकालना कॉम्प्यूटेशनली उचित रास्ते,

65
00:05:19,642 --> 00:05:24,026
या कॉम्प्यूटेशनली कार्यक्षम / इफ़िशंट रास्ते, डील करने के लिए बड़े /बिग डेटा सेट्स से.  

66
00:05:24,026 --> 00:05:26,826
अगले कुछ विडीओज़ में हम देखेंगे दो प्रमुख सिद्धांतों को.

67
00:05:26,826 --> 00:05:33,464
पहले को कहते हैं स्टोकेस्टिक ग्रेडीयंट डिसेंट और दूसरे को कहते हैं मैप रिडयूस
 डील करने के लिए बड़े डेटा सेट्स से.

68
00:05:33,464 --> 00:05:39,986
और जब आप लर्न कर लेंगे इन तरीकों को, आशा है वह आपको करवा पाएँगे स्केल अप आपके लर्निंग अल्गोरिद्म्स को बिग डेटा के लिए 

69
00:05:39,986 --> 00:05:43,986
और दिलवा पाएँगे अधिक बेहतर पर्फ़ॉर्मन्स विभिन्न ऐप्लिकेशन्स में.