बहुत से लर्निंग अल्गोरिद्म्स में, जैसे लिनियर रिग्रेशन, लोजिस्टिक रिग्रेशन और न्यूरल नेटवर्क्स, जिस तरह से हम डिराइव करते हैं अल्गोरिद्म वह था बनाना एक कोस्ट फ़ंक्शन या बनाना एक ऑप्टिमायज़ेशन अब्जेक्टिव. और तब इस्तेमाल करते हुए एक अल्गोरिद्म ग्रेडीयंट डिसेंट जैसा मिनमायज़ करने के लिए उस कॉस्ट फ़ंक्शन को. हमारे पास है एक बहुत बड़ा ट्रेनिंग सेट, ग्रेडीयंट डिसेंट हो जाता है एक कॉम्प्यूटेशनली खर्चीली  प्रक्रिया. इस विडीओ में, हम बात करेंगे एक बदलाव की मौलिक ग्रेडीयंट डिसेंट अल्गोरिद्म में जिसे कहते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, जो हमें स्केल करने देता है इन अल्गोरिद्म्स को अधिक बड़े ट्रेनिंग सेट्स पर. मान लो आप ट्रेन कर रहे हैं एक लिनीअर रेग्रेशन मॉडल ग्रेडीयंट डिसेंट इस्तेमाल करके. एक पुनरावृति करते हुए, हायपॉथिसस दिखेगी ऐसी, और कॉस्ट फ़ंक्शन दिखेगा ऐसा, जो है सम आधा ऐव्रिज स्क्वेर्ड एरर्स का आपकी हायपॉथिसस का आपके एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर, और कॉस्ट फ़ंक्शन जो हमने पहले देखा है दिखता है इस प्रकार का धनुष के आकार का फ़ंक्शन. अत:, प्लॉट करने पर इसे पेरमिटर्स थीटा 0 और थीटा 1 के फ़ंक्शन की तरह, कोस्ट फ़ंक्शन जे है एक प्रकार का धनुष के आकार का फ़ंक्शन. और ग्रेडीयंट डिसेंट दिखता है ऐसा, जहाँ ग्रेडीयंट डिसेंट के अंदर का लूप आप बार बार अप्डेट करते हो पेरमिटर्स थीटा को उस इक्स्प्रेशन से. अब बाक़ी के इस विडीओ में, मैं इस्तेमाल करता रहूँगा लिनीअर रेग्रेशन को मेरे उदाहरण के लिए. लेकिन विचार यहाँ हैं, विचार स्टोकस्टिक ग्रेडीयंट डिसेंट का है पूर्णतया सार्वजनिक और अप्लाई करता है दूसरे लर्निंग अल्गोरिद्म्स को भी जैसे लजिस्टिक रेग्रेशन, न्यूरल नेटवर्क्स और दूसरे अल्गोरिद्म्स जो आधारित हैं ट्रेन करने में ग्रेडीयंट डिसेंट को एक विशेष ट्रेनिंग सेट पर. तो यहाँ है एक चित्र कि ग्रेडीयंट डिसेंट क्या करता है, यदि पेरमिटर्स इनिशलाइज्ड किए हैं पोईँट को वहाँ पर तब जैसे आप रन करते हैं ग्रेडीयंट डिसेंट, ग्रेडीयंट डिसेंट की अलग इटरेशन्स ले जायेंगी पेरमिटर्स को ग्लोबल मिनिमम पर. अत: लो एक ट्रेजेक्टरी / प्रक्षेप पथ जो वैसा दिखता है और जाता है सीधा ग्लोबल मिनिमम पर. अब, समस्या ग्रेडीयंट डिसेंट के साथ है कि यदि एम बड़ा है. तब कम्प्यूट करना इस डेरिवेटिव टर्म को काफ़ी महँगा हो सकता है क्योंकि क़ीमत है सम करना सारे एम इग्ज़ाम्पल्ज़ पर. अत: यदि एम है 300 मिल्यन / 30 करोड़, सही. अत: द यूनाइटेड स्टेट्स में, क़रीब 300 मिल्यन लोग हैं. और इसलिए द यूएस या यूनाइटेड स्टेट्स का सेन्सस / जनगणना का डेटा शायद होगा स्तर का उतने रेकर्ड्ज़. तो आप फ़िट करना चाहते हैं लिनीअर रेग्रेशन मॉडल उसे तब आपको करना पड़ेगा सम 300 मिल्यन रेकर्ड्ज़ पर. और वह बहुत महँगा है. अल्गोरिद्म को एक नाम देने के लिए, यह विशेष वर्ज़न ग्रेडीयंट डिसेंट का बैच ग्रेडीयंट डिसेंट भी कहलाता है. और टर्म बैच बताती है कि हम देख रहे हैं सारे ट्रेनिंग इग्ज़ाम्पल्ज़ को एक साथ. हम इसे कहते हैं एक प्रकार से बैच सारे ट्रेनिंग इग्ज़ाम्पल्ज़ का. और यह वास्तव में नहीं, शायद श्रेष्ठतम नाम लेकिन यह है वह जो मशीन लर्निंग के लोग ग्रेडीयंट डिसेंट के इस ख़ास वर्ज़न को कहते हैं. और यदि आप कल्पना करें कि आपके पास हैं 300 मिल्यन जनगणना के रेकर्ड्ज़ डिस्क पर स्टोर किये हुए. जिस तरह से यह अल्गोरिद्म काम करता है आपको चाहिए रीड करना आपकी कम्प्यूटर की मेमरी में सारे 300 मिल्यन रेकर्ड्ज़ कम्प्यूट करने के लिए यह डेरिवेटिव टर्म. आपको करने पड़ते हैं स्ट्रीम सारे ये रेकर्ड्ज़ कम्प्यूटर से क्योंकि कम्प्यूटर मेमरी सारे रेकर्ड्ज़ स्टोर नहीं कर सकती. तो आपको उनमें से रीड करना पड़ता है और धीरे-धीरे, आप जानते हैं इकट्ठा करते हैं सम कम्प्यूट करने के लिए डेरिवेटिव. और तब यह सब काम कर लेने के बाद, उससे आपका एक स्टेप होता है ग्रेडीयंट डिसेंट का. और तब आपको यह पूरा काम दोबारा करना पड़ता है. आप जानते हैं, स्कैन करना 300 मिल्यन रेकर्ड्ज़ से, इकट्ठा करना ये सम. और तब यह सब काम कर लेने के बाद, आप लेते हैं एक और स्टेप ग्रेडीयंट डिसेंट का. और तब दोबारा उसे करो. और तब आप लेते हैं एक तीसरा स्टेप. और इसी प्रकार आगे. और इसलिए यह लेगा बहुत अधिक समय अल्गोरिद्म को कन्वर्ज करने में. बैच ग्रेडीयंट डिसेंट के विपरीत, हम बनाएँगे एक भिन्न अल्गोरिद्म जिसे आवश्यकता नहीं है देखने की सारे ट्रेनिंग इग्ज़ाम्पल्ज़ प्रत्येक इटरेशन में, लेकिन वह देखता हैं केवल एक अकेला ट्रेनिंग इग्ज़ाम्पल एक इटरेशन में, नए अल्गोरिद्म पर जाने से पहले, यहाँ है सिर्फ़ एक बैच ग्रेडीयंट डिसेंट अल्गोरिद्म लिखा हुआ फिर से वह है कॉस्ट फ़ंक्शन और वह है अप्डेट और निश्चय ही यह टर्म यहाँ, जो इस्तेमाल की जाती है ग्रेडीयंट डिसेंट रूल मैं, यह है पर्शियल डेरिवेटिव विद रिस्पेक्ट टु पेरामिटरज़ थीटा जे हमारे ऑप्टिमायज़ेशन अब्जेक्टिव का, जे ट्रेन थीटा का. अब, चलिए देखते हैं एक अधिक एफ़िशिएँट अल्गोरिद्म जो स्केल बेहतर करता है बड़े डेटा सेट्स को. बनाने के लिए अल्गोरिद्म जिसे कहते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, यह संचालित करता है कॉस्ट फ़ंक्शन को एक थोड़े अलग ढंग से तब वे परिभाषित करते हैं कॉस्ट पेरामिटर थीटा की विद रिस्पेक्ट टु एक ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ) बराबर वन हाफ़ टाइम्ज़ स्क्वेर्ड एरर जो मेरी हायपॉथिसस उठाती है उस ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर. अत: यह कॉस्ट फ़ंक्शन टर्म का वास्तव में मतलब है कि कितना सही है मेरी हायपॉथिसस एक अकेले इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर. अब आप ध्यान करें कि पूरा कॉस्ट फ़ंक्शन जे ट्रेन अब लिखा जा सकता है इसी समान. अत: जे ट्रेन हैं केवल ऐव्रिज मेरे एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर मेरी हायपॉथिसस की कॉस्ट का उस इग्ज़ाम्पल एक्स(आइ), वाय(आइ) पर. इस दृष्टिकोण के साथ कॉस्ट फ़ंक्शन पर लिनीअर रेग्रेशन के, अब मैं लिखता हूँ क्या स्टोकस्टिक ग्रेडीयंट डिसेंट करता है. पहला स्टेप स्टोकस्टिक ग्रेडीयंट डिसेंट का है कि क्रमरहित करना डेटा सेट को. अत: उससे मेरा मतलब है कि रैंडम ढंग से फेर-बदल करना या रैंडम ढंग से क्रम बदलना आपके एम ट्रेनिंग इग्ज़ाम्पल्ज़ का. यह एक प्रकार से स्टैंडर्ड प्री-प्रासेसिंग स्टेप हैं मैं थोड़ी देर में इस पर वापिस आऊँगा. लेकिन मुख्य काम होता है ग्रेडीयंट डिसेंट का निम्न प्रकार से. हम दोहराएँगे आइ बराबर 1 से एम तक. अत: हम बार बार जाएँगे होते हुए मेरे ट्रेनिंग इग्ज़ाम्पल्ज़ से और करेंगे निम्न अप्डेट. अप्डेट करना है पेरामिटर थीटा जे को थीटा जे माइनस अल्फ़ा टाइम्ज़ एच ऑफ़ एक्स(आइ) माइनस वाय(आइ) टाइम्ज़ एक्स(आइ)जे. और हम करेंगे यह अप्डेट हमेशा की तरह जे की सारी वैल्यूज़ के लिए. अब, आप ध्यान दें कि यह टर्म यहाँ पर है बिल्कुल वैसी जो हमारे पास थी समेशन के अंदर बैच ग्रेडीयंट डिसेंट में. वैसे तो, आप में से वे जो जानते हैं कैल्क्युलुस यह दिखाना सम्भव है कि वह टर्म, जो है यह टर्म यहाँ, बराबर है पर्शियल डेरिवेटिव के विद रिस्पेक्ट टु मेरे पेरामिटर थीटा जे के कॉस्ट पेरामिटरज़ थीटा की एक्स(आइ), वाय (आइ) पर. जहाँ कॉस्ट है निस्संदेह यह चीज़ जो परिभाषित की गई थी पहले. और सिर्फ़ रैप अप करने के लिए इस अल्गोरिद्म को, मैं बंद करता हूँ मेरी कर्ली ब्रेसिज़ यहाँ पर. तो स्टोकस्टिक ग्रेडीयंट डिसेंट क्या करता है कि यह वास्तव में स्कैन करता है ट्रेनिंग इग्ज़ाम्पल्ज़ में से. और पहले यह देखता है मेरा पहला ट्रेनिंग इग्ज़ाम्पल एक्स(1), वाय(1). और तब देखते हुए सिर्फ़ इस पहले ट्रेनिंग इग्ज़ाम्पल को, यह लेता है जैसे एक मूलत: एक छोटा ग्रेडीयंट डिसेंट स्टेप विद रिस्पेक्ट टु कॉस्ट सिर्फ़ पहले ट्रेनिंग इग्ज़ाम्पल की. अत: दूसरे शब्दों में, हम देखते हैं पहले इग्ज़ाम्पल पर और बदलते हैं पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ पहला ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर. ऐसा कर चुकने के बाद अंदर यह इनर फ़ॉर-लूप तब जाएगा दूसरे ट्रेनिंग इग्ज़ाम्पल पर. और यह क्या करेगा कि वहाँ लेगा एक और छोटा स्टेप पेरामिटर स्पेस में, अत: बदलेगा पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ एक दूसरा ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर. ऐसा कर लेने के बाद यह जाएगा तीसरे ट्रेनिंग इग्ज़ाम्पल पर. और बदलेगा पेरमिटर्स थोड़े से फ़िट करने के लिए सिर्फ़ एक तीसरा ट्रेनिंग इग्ज़ाम्पल थोड़ा बेहतर, और इसी प्रकार आगे जब तक आप जानते हैं, आप पूरे ट्रेनिंग सेट से गुज़र नहीं लेते. और तब यह बाहरी रिपीट लूप शायद इसे करवाएगा बहुत से पास्सेस पूरे ट्रेनिंग सेट के ऊपर. यह दृष्टिकोण स्टोकस्टिक ग्रेडीयंट डिसेंट का प्रेरित भी करता है कि क्यों हम चाहते थे शुरू करना क्रम रहित फेर-बदल करना अपने डेटा सेट को. यह हमें नहीं दिखाता कि जब हम स्कैन करते हैं डेटा सेट में यहाँ, कि हम जाते हैं ट्रेनिंग इग्ज़ाम्पल्ज़ पर एक प्रकार के क्रम रहित विधि से. निर्भर करते हुए कि क्या आपका डेटा पहले से ही क्रम रहित है या पहले से ही क्रम बद्ध है किसी भिन्न क्रम में, व्यावहारिक रूप में यह गति बढ़ा देता है स्टोकस्टिक ग्रेडीयंट डिसेंट की थोड़ी सी. अत: ध्यान रखते हुए सेफ़्टी का, यह अक्सर बेहतर होता है क्रम रहित फेर-बदल करना डेटा में यदि आप विश्वस्त नहीं हैं कि यह आपको मिला कर्म रहित तरीक़े से. परंतु, महत्वपूर्ण है एक और दृष्टिकोण स्टोकस्टिक ग्रेडीयंट डिसेंट का है कि काफ़ी कुछ ग्रेडीयंट डिसेंट जैसा है लेकिन इंतज़ार करने के स्थान पर सम अप करने के सारी ग्रेडीयंट टर्म्ज़ का पूरे एम ट्रेनिंग इग्ज़ाम्पल्ज़ पर, हम क्या करते हैं कि हम लेते हैं यह ग्रेडीयंट टर्म इस्तेमाल करके सिर्फ़ एक अकेला ट्रेनिंग इग्ज़ाम्पल और हमने शुरू कर दिया है सुधार पेरमिटर्स में अभी से ही. अत: बजाय, आप जानते हैं, इंतज़ार करने के जाने का सारे 300,000 ट्रेनिंग इग्ज़ाम्पल्ज़ में से यूनाइटेड स्टेट्स के जनगणना रेकर्ड्ज़ में से, मतलब, बजाय इस आवश्यकता के कि स्कैन करें सारे ट्रेनिंग इग्ज़ाम्पल्ज़ इससे पहले कि बदल सकते हैं पेरमिटर्स थोड़े थोड़े और जा सकते हैं ग्लोबल मिनिमम की तरफ़. स्टोकस्टिक ग्रेडीयंट डिसेंट में इसके स्थान पर हम सिर्फ़ देखते हैं एक अकेले ट्रेनिंग इग्ज़ाम्पल पर और हम शुरू कर सकते हैं बदलाव पेरमिटर्स में इस केस में खिसकाते हुए पेरमिटर्स को ग्लोबल मिनिमम की तरफ़. तो, यहाँ है अल्गोरिद्म लिखा हुआ फिर से जहाँ पहला स्टेप है क्रमरहित करना डेटा को और दूसरा स्टेप हैं जहाँ असल में काम होता है जहाँ वह अप्डेट है विद रिस्पेक्ट टु एक अकेला ट्रेनिंग इग्ज़ाम्पल एक्स(आइ), वाय(आइ). तो देखते है क्या करता है यह अल्गोरिद्म पेरमिटर्स को. पहले, हमने देखा था जब हम कर रहे थे बैच ग्रेडीयंट डिसेंट, कि वह अल्गोरिद्म देखता है सारे ट्रेनिंग इग्ज़ाम्पल्ज़ को एक समय में, बैच ग्रेडीयंट डिसेंट लेगा, आप जानते हैं, लेता है एक उचित रूप से सीधी रेखा का पथ ग्लोबल मिनिमम पर जाने के लिए इस प्रकार. इसके विपरीत स्टोकस्टिक ग्रेडीयंट डिसेंट में प्रत्येक इटरेशन होगी अधिक शीघ्र क्योंकि हमें आवश्यकता नहीं है सम करने की पूरे ट्रेनिंग इग्ज़ाम्पल्ज़ पर. लेकिन प्रत्येक इटरेशन कोशिश कर रही है फ़िट करने के लिए एक ट्रेनिंग इग्ज़ाम्पल को बेहतर. अत:, यदि हमें शुरू करना होता स्टोकस्टिक ग्रेडीयंट डिसेंट, ओह, चलो शुरू करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट उस पोईँट पर ऐसे ही. पहली इटरेशन, आप जानते हैं, शायद ले जाएगी पेरमिटर्स को उस दिशा में और शायद दूसरी इटरेशन दख़ते हुए सिर्फ़ दूसरे इग्ज़ाम्पल को शायद सिर्फ़ संयोग से, हम होते हैं अधिक बदकिस्मत और असल में जाते हैं एक ग़लत दिशा में पेरमिटर्स की उस प्रकार. तीसरी इटरेशन में जहाँ हम कोशिश करते हैं बदलने की पेरमिटर्स को फ़िट करने के लिए सिर्फ़ तीसरा ट्रेनिंग इग्ज़ाम्पल बेहतर, शायद हम जाते हैं उस दिशा में. और तब हम देखते हैं चौथा ट्रेनिंग इग्ज़ाम्पल और हम वह करेंगे. पाँचवा, छठा, 7वाँ और आगे. और जैसे आप रन करते हैं स्टोकस्टिक ग्रेडीयंट डिसेंट, आपको क्या मिलता है कि यह अक्सर खिसकाएगा पेरमिटर्स को ग्लोबल मिनिमम की दिशा में, लेकिन हमेशा नहीं. और इसलिए लेगा कुछ अधिक अनियमित सा दिखता हुआ घुमावदार रास्ता ग्लोबल मिनिमम तक पहुँचने का. और असल में जैसे आप रन करते हो स्टोकस्टिक ग्रेडीयंट डिसेंट यह वास्तव में कन्वर्ज नहीं करता उसी मायने में जैसे बैच ग्रेडीयंट डिसेंट करता है और यह इधर उधर होता है लगातार किसी क्षेत्र में जो किसी क्षेत्र में नज़दीक ग्लोबल मिनिमम के, लेकिन यह पहुँचता नहीं है ग्लोबल मिनिमम पर और रूकता नहीं है वहाँ. लेकिन व्यावहारिक रुप में, यह समस्या नहीं है क्योंकि, आप जानते हैं, जब तक पेरमिटर्स पहुँचते हैं किसी क्षेत्र में जो शायद काफ़ी नज़दीक ग्लोबल मिनिमम के. अत:, जब पेरमिटर्स पहुँचते हैं काफी नज़दीक ग्लोबल मिनिमम के, वह होगी एक काफ़ी अच्छी हायपॉथिसस और इसलिए रन करके स्टोकस्टिक ग्रेडीयंट डिसेंट हमें मिलता हैं एक पेरामिटर नज़दीक ग्लोबल मिनिमम के और वह पर्याप्त है, आप जानते हैं, वास्तव में काम के उद्देश्य से. सिर्फ़ एक अंतिम गौण बात. स्टोकस्टिक ग्रेडीयंट डिसेंट में, हमारे पास था यह बाहरी लूप रिपीट जो कहता है करने को इस आंतरिक लूप को बहुत बार. तो, कितनी बार हम करते हैं रिपीट इस बाहरी लूप को? निर्भर करते हुए ट्रेनिंग सेट के साइज़ पर, इस लूप को एक बार करना भी शायद पर्याप्त होगा. और, आप जानते है, शायद 10 बार काफ़ी आम है तो हम शायद करें रिपीट इस आंतरिक लूप को कोई एक से दस बार तक. अत: यदि हमारे पास है एक आप जानते है, वास्तव में मैसिव डेटा सेट जैसे यूएस जनगणना का उस उदाहरण में जिसकी मैं बात करता रहा हूँ लगभग 300 मिल्यन इग्ज़ाम्पल्ज़ वाला, यह सम्भव है कि जब तक आप लेते हैं एक पास आपके ट्रेनिंग सेट से. अत: यह है आइ बराबर 1 से 300 मिल्यन तक. यह सम्भव है कि जब तक आप लेते हैं एक पास आपके ट्रेनिंग सेट से आपके पास हो एक सर्वथा अच्छी हायपॉथिसस. जिस स्थिति में, आप जानते हैं, यह आंतरिक लूप आपको शायद करना पड़े सिर्फ़ एक बार यदि एम है बहुत अधिक बड़ा. लेकिन सामान्य तौर पर लेना कोई 1 से 10 पास्सेस आपके डेटा से, आप जानते है, शायद काफ़ी आम है. परंतु वास्तव में यह निर्भर करता है साइज़ पर आपके डेटा सेट पर. और यदि आप तुलना करते हैं इसकी ग्रेडीयंट डिसेंट के साथ. बैच ग्रेडीयंट डिसेंट में, एक पास लेने के बाद आपके पूरे डेटा सेट से, आपने लिया होगा केवल एक अकेला ग्रेडीयंट डिसेंट स्टेप. अत: एक इनमें से छोटा स्टेप ग्रेडीयंट डिसेंट का होगा जहाँ आप लेते हैं एक छोटा ग्रेडीयंट डिसेंट स्टेप और इसलिए स्टोकस्टिक ग्रेडीयंट डिसेंट हो सकता है काफ़ी तीव्र. तो वह था स्टोकस्टिक ग्रेडीयंट डिसेंट अल्गोरिद्म. और यदि आप इसे इम्प्लमेंट करते है, आशा है वह आपको स्केल अप करने देगा बहुत से आपके लर्निंग अल्गोरिद्म्स को एक काफ़ी बड़े सेट्स पर और मिल सकती है बेहतर पर्फ़ॉर्मन्स उस तरह से.