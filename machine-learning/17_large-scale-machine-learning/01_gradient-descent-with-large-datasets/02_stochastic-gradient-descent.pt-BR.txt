Para muitos problemas de aprendizado, entre os quais a regressão linear, regressão logística e redes neurais, a maneira pela qual derivamos o algoritmo foi através de uma função custo ou otimizando uma função objetivo. Então usávamos um algoritmo como o gradiente descendente para minimizar a função custo. Quando temos um conjunto de treinamento muito grande, o gradiente descendente torna-se computacionalmente muito custoso. Neste vídeo falaremos sobre uma modificação do algoritmo básico do gradiente descendente chamado de gradiente descendente estocástico, que nos permitirá escalar esses algoritmos usando conjuntos de dados muito maiores. Suponha que você está modelando com uma regressão linear e usando o gradiente descendente. Só pra recapitular, a hipótese será algo assim, e a função custo se parecerá com isto, a metade da média do erro quadrático da sua hipótese ao longo dos seus "m" exemplos de treinamento, e a função custo nós já vimos que tem esse formato de tigela. Traçando o gráfico em função dos parâmetros "θ₀" e "θ₁", a função custo "J" é uma função com esse tipo de curva. O gradiente descendente se parece com isso, onde a cada iteração atualizam-se os parâmetros "θ" usando essa expressão. No restante do vídeo, continuarei usando a regressão linear como exemplo. Mas o princípio do gradiente descendente estocástico é completamente aplicável para os outros algoritmos de aprendizagem, como regressão logística, redes neurais e outros algoritmos que são treinados com gradiente descendente usando um conjunto de dados. Aqui está uma figura que representa o funcionamento do gradiente descendente. Se os parâmetros são inicializados naquele ponto, ao longo das iterações do gradiente descendente os parâmetros serão conduzidos para o mínimo global. Então temos uma trajetória que parece perseguir o mínimo global de maneira bem direta. Mas o problema com o gradiente descendente ocorre quando "m" é grade. Nesse caso, calcular esse termo da derivada pode ser muito custoso, pois percorre todo os "m" exemplos. Então se "m" é 300 milhões, assim como nos EUA existem 300 milhões de pessoas, e então o censo dos EUA pode ter muitos dados nessa ordem de grandeza. Se você quer ajustar o seu modelo de regressão linear aos dados, precisará realizar a somatória por de 300 milhões de registros. E isso é bem dispendioso! Para darmos um nome para o algoritmo, essa versão particular do gradiente descendente também é chamada de "gradiente descendente em lote". O termo lote se refere a estarmos usando todos os exemplos de treinamento de uma só vez. Nós chamamos de lote o conjunto dos exemplos de treinamento. Esse pode não ser o melhor nome, mas é assim que as pessoas do campo chamam essa versão do gradiente descendente. Se imaginarmos que realmente temos 300 milhões de registros armazenados em disco, o algoritmo lerá no disco todos esses 300 milhões de registros para calcular a derivada. Você precisa transmitir todos esses registros para o computador pois você não consegue armazená-los todos na memória RAM. Então você precisa percorrê-los e ler todos e, lentamente, acumular a sua soma para calcualr a derivada. E depois de ter feito tudo isso, você terá terminado apenas um passo do algoritmo do gradiente descendente. Agora será preciso fazer tudo novamente, percorrer todos os 300 milhões de registros, acumular essas somas, e, tendo terminado tudo, será outro pequeno passo do gradiente descendente. Depois, novamente. E esse será apenas o terceiro passo, e assim por diante. Isso nos tomaria um tempo enorme para fazer com que o algoritmo convergisse. Em contraste com o gradiente descendente em lote, usaremos um algoritmo modificado, que não terá necessidade de verificar todos os exemplos de treinamento para cada uma das iterações, mas precisará verificar um único exemplo de treinamento para cada iteração. Antes de partirmos para o novo algoritmo, aqui está o gradiente descendente em lote, sendo esta a função custo, esta a atualização, e, claro, este termo que é usado na regra do gradiente descendente, a derivada parcial em relação aos parâmetros "θⱼ" do objetivo de otimização, "Jₜᵣₐᵢₙ(θ)". Agora, vejamos como é o algoritmo mais eficiente e que escala melhor com conjuntos grandes de dados. Para que o gradiente descendente estocástico funcione, definimos a função custo de uma maneira um pouco diferente. Definimos o custo em função de "θ" e em relação aos exemplos de treinamento "(x⁽ⁱ⁾, y⁽ⁱ⁾)", igual à metade do quadrado do erro da hipótese no exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)". Essa função custo quantifica o quão bem a minha hipótese está se saindo em um único exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)". Agora note que a função custo "Jₜᵣₐᵢₙ" pode ser escrita desta forma. "Jₜᵣₐᵢₙ(θ)" é a média ao longo dos "m" exemplos do custo da hipótese em cada exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)". Munidos com essa definição de função custo para a regressão linear, escreverei agora o que o gradiente descendente estocástico faz. O primeiro passo do gradiente descendente estocástico é embaralhar aleatoriamente o conjunto de dados. Ou seja, misturar, reorganizar aleatoriamente os "m" exemplos de treinamento, Este é um procedimento padrão de pré-processamento ao qual voltaremos em breve. Mas a parte principal do gradiente descendente estocástico se dá da seguinte forma. Repetimos, para "i = 1" a "m", Acessamos repetidamente os meus exemplos de treinamento e realizamos a seguinte atualização. "θⱼ = θⱼ - α · (h(x⁽ⁱ⁾) - y⁽ⁱ⁾) · x⁽ⁱ⁾ⱼ" Faremos essa atualização, como de costume, para todos os valores de "j". Note que este termo aqui é exatamente o que tínhamos dentro da somatória para o gradiente descendente em lote. Na verdade, para aqueles que são familiarizados com cálculo, é possível mostrar que esse termo aqui igual a derivada parcial em relação ao meu parâmetro "θⱼ" do custo em "(θ, (x⁽ⁱ⁾, y⁽ⁱ⁾))". O custo é essa expressão que definimos anteriormente. Concluindo o algoritmo, fecho a chave aqui. O que o gradiente descendente estocástico está fazendo é percorrer os exemplos de treinamento, começando pelo primeiro exemplo de treinamento, "(x⁽¹⁾, y⁽¹⁾)", e, então, olhando apenas para esse primeiro exemplo, ele irá realizar um pequeno passo do gradiente descendente, em relação apenas ao custo do primeiro exemplo de treinamento. Em outras palavras, olhamos para o primeiro exemplo e modificamos os parâmetros um pouquinho para ajustá-los ao primeiro exemplo um pouco melhor. Feito isso neste loop interior, o mesmo será feito com o segundo exemplo de treinamento e realiza outro pequeno passo no espaço dos parâmetros, modificando os parâmetros apenas um pouco para ajustá-los melhor ao segundo exemplo de treinamento. Tendo feito isso, iremos para o terceiro exemplo de treinamento. E modificaremos os parâmetros para se ajustarem melhor ao nosso terceiro exemplo, e assim por diante até termos percorrido todo o conjunto de treinamento. Esse loop de fora fará com que sejam tomados diversos passos ao longo de todo o conjunto de treinamento. Essa visão do gradiente descendente estocástico também nos indica por que queremos embaralhar o nosso conjunto de dados. Isso nos assegura que, quando percorremos os exemplos de treinamento, que nós acabamos por visitar os exemplos de treinamento de forma aleatória. Dependendo se os seus dados já vieram aleatoriamente ordenados ou se eles vieram organizados de alguma forma estranha, na prática, isso ajuda o algoritmo a convergir um pouco mais rápido. Portanto, por uma questão de segurança, é melhor embaralhar aleatoriamente o seu conjunto, se você não tiver certeza de que ele já está organizado de forma aleatória. Mas, mais importante, o gradiente descendente estocástico pode ser visto como algo bem parecido com a versão em lote, mas em vez de esperar toda a soma desses termos por todos os "m" exemplos, calculamos o termo do gradiente usando apenas um exemplo de treinamento, e já progredimos na melhoria dos parâmetros com apenas este exemplo. Fazemos isso em vez de esperar o algoritmo percorrer todos os 300 milhões de registros do censo dos EUA, ao invés de termos que passar por todos esses exemplos de treinamento, antes de podermos modificar os parâmetros e ir na direção do mínimo global. Com o gradiente descendente estocástico, precisamos olhar para um único exemplo de treinamento, e já começamos a progredir na direção dos parâmetros que nos levem ao mínimo global. Aqui temos o algoritmo reescrito, onde o primeiro passo é embaralhar aleatoriamente nossos dados, o segundo passo é onde o algoritmo realmente trabalha, onde é feita a atualização em relação a um único exemplo "(x⁽ⁱ⁾, y⁽ⁱ⁾)". Vejamos o que esse algoritmo faz com os parâmetros. Antes, com o gradiente descendente em lote, o algoritmo que usa todos os exemplos de treinamento em um único passo, O gradiente descendente em lote tende a tomar uma trajetória relativamente reta até chegar no mínimo global, desta maneira. Já o gradiente descendente estocástico terá iterações muito mais rápidas, pois não precisaremos percorrer todos os exemplos de treinamento. Mas cada iteração tenta ajustar nossos parâmetros em relação a um único exemplo de treinamento. Então, se formos aplicar o gradiente descendente estocástico em um ponto, peguemos um como este para começar. A primeira iteração pode modificar os parâmetros nesse sentido, e talvez a segunda iteração, que olha somente o segundo exemplo de treinamento, tem a possibilidade de, com falta de sorte, nos levar, para uma direção ruim como esta. Na terceira iteração, quando modificamos os parâmetros para se ajustarem apenas em relação ao terceiro exemplo, talvez terminemos apontando para essa direção. Então iremos para a quarta iteração e teremos algo assim. A quinta, sexta, sétima e assim por diante. Quando você executa o gradiente descendente estocástico, você perceberá que ele geralmente modifica os parâmetros na direção do mínimo global, mas não sempre. E isso fará com que tenhamos uma aparência mais aleatória em direção ao mínimo global. Na verdade, gradiente descendente estocástico não converge como em lote. O que o estocástico faz é rondar em uma região perto do mínimo global, mas ele nunca chega e permanece lá. Mas na prática isso não é um problema, desde que os parâmetros terminem em uma região perto o suficiente do mínimo global. Portanto, se os parâmetros terminam bem perto do mínimo global, a hipótese será boa. Portanto, quando executamos o gradiente descendente estocástico, obtemos parâmetros perto do mínimo global e isso é bom o suficiente na prática. Um último detalhe: no gradiente descendente estocástico, nós temos que repetir esse loop externo, o que significa ter esse loop interno repetido múltiplas vezes. Mas quantas vezes repetimos esse loop externo? Dependendo do tamanho do conjunto de treinamento, realizar esse loop uma única vez pode ser suficiente. É comum até, digamos, umas 10 vezes. Então podemos acabar repetindo esse loop interno de uma até dez vezes. Se tivermos um conjunto de dados enorme como o censo dos EUA, com 300 milhões de exemplos, é possível que, ao realizar apenas uma vez o processo pelo conjunto de treinamento, ou seja, de "i = 1" até 300 milhões, é possível que um único loop pelo conjunto de dados seja o suficiente para gerar uma hipótese muito boa. Nesses casos em que "m" é muto, muito grande, é provável que em uma única iteração do loop interno você consiga bons resultados. Mas, geralmente, realizar de 1 até 10 repetições ao longo do seu conjunto de treinamento será o mais comum. Mas isso realmente vai depender do tamanho do seu conjunto de treinamento. Se você comparar com o gradiente descendente em lote, no qual, após realizar um passo utilizando todo o conjunto de treinamento, você terá realizado apenas um passo do gradiente descendente. Em desses pequenos passinhos do gradiente descendente onde você se aproximou um pouquinho, e é por isso que o gradiente descendente estocástico pode ser bem mais rápido. Esse foi o algoritmo do gradiente descendente estocástico. Se você implementá-lo, você poderá utilizar muitos de seus algoritmos de aprendizagem em conjuntos de dados de escala muito maior e com eficiência.