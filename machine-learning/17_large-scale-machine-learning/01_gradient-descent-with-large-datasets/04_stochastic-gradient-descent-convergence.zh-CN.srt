1
00:00:00,493 --> 00:00:03,492
现在你已经知道了随机梯度下降算法

2
00:00:03,492 --> 00:00:09,907
但是当你运行这个算法时 你如何确保调试过程已经完成 并且能正常收敛呢？

3
00:00:09,907 --> 00:00:15,813
还有 同样重要的是 你怎样调整随机梯度下降中学习速率α的值

4
00:00:15,813 --> 00:00:25,950
在这段视频中 我们会谈到一些方法来处理这些问题 确保它能收敛 以及选择合适的学习速率α

5
00:00:25,950 --> 00:00:30,600
回到我们之前批量梯度下降的算法 我们确定梯度下降已经收敛的一个标准方法

6
00:00:30,600 --> 00:00:36,493
是画出最优化的代价函数 关于迭代次数的变化

7
00:00:36,493 --> 00:00:44,366
这就是代价函数 我们要保证这个代价函数在每一次迭代中 都是下降的

8
00:00:44,366 --> 00:00:50,438
当训练集比较小的时候 我们不难完成 因为要计算这个求和是比较方便的

9
00:00:50,438 --> 00:00:57,950
但当你的训练集非常大的时候 你不希望老是定时地暂停算法

10
00:00:57,950 --> 00:01:04,045
来计算一遍这个求和

11
00:01:04,045 --> 00:01:07,442
因为这个求和计算需要考虑整个的训练集

12
00:01:07,442 --> 00:01:12,466
而随机梯度下降的算法是 你每次只考虑一个样本

13
00:01:12,466 --> 00:01:19,130
然后就立刻进步一点点 不需要在算法当中 时不时地扫描一遍全部的训练集

14
00:01:19,130 --> 00:01:25,583
来计算整个训练集的代价函数

15
00:01:25,583 --> 00:01:32,472
因此 对于随机梯度下降算法 为了检查算法是否收敛 我们可以进行下面的工作

16
00:01:32,472 --> 00:01:36,367
让我们沿用之前定义的cost函数

17
00:01:36,367 --> 00:01:42,647
关于θ的cost函数 等于二分之一倍的训练误差的平方和

18
00:01:42,647 --> 00:01:49,754
然后 在随机梯度下降法学习时 在我们对某一个样本进行训练前

19
00:01:49,754 --> 00:01:54,601
在随机梯度下降中 我们要关注样本(x(i),y(i))

20
00:01:54,601 --> 00:01:57,329
然后关于这个样本更新一小步 进步一点点

21
00:01:57,329 --> 00:02:04,095
然后再转向下一个样本 (x(i+1),y(i+1))

22
00:02:04,095 --> 00:02:05,880
随机梯度下降就是这样进行的

23
00:02:05,880 --> 00:02:15,024
在算法扫描到样本(x(i),y(i)) 但在更新参数θ之前

24
00:02:15,024 --> 00:02:20,255
使用这个样本 我们可以算出这个样本对应的cost函数

25
00:02:20,255 --> 00:02:23,577
我再换一种方式表达一遍

26
00:02:23,577 --> 00:02:33,294
当随机梯度下降法对训练集进行扫描时 在我们使用某个样本(x(i),y(i))来更新θ前

27
00:02:33,294 --> 00:02:38,198
让我们来计算出 这个假设对这个训练样本的表现

28
00:02:38,198 --> 00:02:43,852
我要在更新θ前来完成这一步 原因是如果我们用这个样本更新θ以后

29
00:02:43,852 --> 00:02:49,061
再让它在这个训练样本上预测 其表现就比实际上要更好了

30
00:02:49,061 --> 00:02:57,438
最后 为了检查随机梯度下降的收敛性 我们要做的是 每1000次迭代

31
00:02:57,438 --> 00:03:01,511
我们可以画出前一步中计算出的cost函数

32
00:03:01,511 --> 00:03:07,450
我们把这些cost函数画出来 并对算法处理的最后1000个样本的cost值求平均值

33
00:03:07,450 --> 00:03:12,714
如果你这样做的话 它会很有效地帮你估计出

34
00:03:12,714 --> 00:03:17,049
你的算法在最后1000个样本上的表现

35
00:03:17,049 --> 00:03:23,974
所以 我们不需要时不时地计算Jtrain 那样的话需要所有的训练样本

36
00:03:23,974 --> 00:03:27,973
随机梯度下降法的这个步骤

37
00:03:27,973 --> 00:03:32,965
只需要在每次更新θ之前进行 也并不需要太大的计算量

38
00:03:32,965 --> 00:03:40,276
要做的就是 每1000次迭代运算中 我们对最后1000个样本的cost值求平均然后画出来

39
00:03:40,276 --> 00:03:47,537
通过观察这些画出来的图 我们就能检查出随机梯度下降是否在收敛

40
00:03:47,537 --> 00:03:51,708
这是几幅画出来的图的例子

41
00:03:51,708 --> 00:03:55,519
假如你已经画出了最后1000组样本的cost函数的平均值

42
00:03:55,519 --> 00:04:01,073
由于它们都只是1000组样本的平均值 因此它们看起来有一点嘈杂

43
00:04:01,073 --> 00:04:03,873
因此cost的值不会在每一个迭代中都下降

44
00:04:03,873 --> 00:04:07,828
假如你得到一种这样的图像 看起来是有噪声的

45
00:04:07,828 --> 00:04:11,721
因为它是在一小部分样本 比如1000组样本中求的平均值

46
00:04:11,721 --> 00:04:17,283
如果你得到像这样的图 那么你应该判断这个算法是在下降的

47
00:04:17,283 --> 00:04:24,195
看起来代价值在下降 然后从大概这个点开始变得平缓

48
00:04:24,195 --> 00:04:29,603
这就是代价函数的大致走向 这基本说明你的学习算法已经收敛了

49
00:04:29,603 --> 00:04:34,252
如果你想试试更小的学习速率 那么你很有可能看到的是

50
00:04:34,252 --> 00:04:39,229
算法的学习变得更慢了 因此代价函数的下降也变慢了

51
00:04:39,229 --> 00:04:47,585
但由于你使用了更小的学习速率 你很有可能会让算法收敛到一个好一点的解

52
00:04:47,585 --> 00:04:53,426
红色的曲线代表随机梯度下降使用一个更小的学习速率

53
00:04:53,426 --> 00:05:00,594
出现这种情况是因为 别忘了 随机梯度下降不是直接收敛到全局最小值

54
00:05:00,594 --> 00:05:05,068
而是在局部最小附近反复振荡

55
00:05:05,068 --> 00:05:09,231
所以使用一个更小的学习速率 最终的振荡就会更小

56
00:05:09,231 --> 00:05:12,896
有时候这一点小的区别可以忽略

57
00:05:12,896 --> 00:05:19,686
但有时候一点小的区别 你就会得到更好一点的参数

58
00:05:19,686 --> 00:05:22,269
接下来再看几种其他的情况

59
00:05:22,269 --> 00:05:27,986
假如你还是运行随机梯度下降 然后对1000组样本取cost函数的平均值 并且画出图像

60
00:05:27,986 --> 00:05:32,369
那么这是另一种可能的图形

61
00:05:32,369 --> 00:05:34,353
看起来这样还是已经收敛了

62
00:05:34,353 --> 00:05:42,119
如果你把这个数 1000 提高到5000组样本

63
00:05:42,119 --> 00:05:47,913
那么可能你会得到一条更平滑的曲线

64
00:05:47,913 --> 00:05:56,547
通过在5000个样本中求平均值 你会得到比刚才1000组样本更平滑的曲线

65
00:05:56,547 --> 00:06:00,248
这是你增大平均的训练样本数的情形

66
00:06:00,248 --> 00:06:06,229
当然增大它的缺点就是 现在每5000个样本才能得到一个数据点

67
00:06:06,229 --> 00:06:12,001
因此你所得到的关于学习算法表现的反馈 就显得有一些“延迟”

68
00:06:12,001 --> 00:06:17,681
因为每5000个样本才能得到图上的一个数据点 而不是每1000个样本就能得到

69
00:06:17,681 --> 00:06:23,911
沿着相似的脉络 有时候你运行梯度下降 可能也会得到这样的图像

70
00:06:23,911 --> 00:06:32,079
如果出现这种情况 你要知道 可能你的代价函数就没有在减小了

71
00:06:32,079 --> 00:06:34,023
也就是说 算法没有很好地学习

72
00:06:34,023 --> 00:06:39,261
因为这看起来一直比较平坦 代价项并没有下降

73
00:06:39,261 --> 00:06:46,260
但同样地 如果你对这种情况时 也用更大量的样本进行平均

74
00:06:46,260 --> 00:06:49,729
你很可能会观察到红线所示的情况

75
00:06:49,729 --> 00:06:55,127
能看得出 实际上代价函数是在下降的 只不过蓝线用来平均的样本数量太小了

76
00:06:55,127 --> 00:07:01,374
并且蓝线太嘈杂 你看不出来代价函数的趋势确实是下降的

77
00:07:01,374 --> 00:07:06,688
所以可能用5000组样本来平均 比用1000组样本来平均 更能看出趋势

78
00:07:06,688 --> 00:07:12,358
当然 即使是使用一个较大的样本数量 比如我们用5000个样本来平均

79
00:07:12,358 --> 00:07:16,998
我用另一种颜色来表示 即使如此 你还是可能会发现 这条学习曲线是这样的

80
00:07:16,998 --> 00:07:21,197
它还是比较平坦 即使你用更多的训练样本

81
00:07:21,197 --> 00:07:25,908
如果是这样的话 那可能就更肯定地说明

82
00:07:25,908 --> 00:07:29,287
不知道出于什么原因 算法确实没怎么学习好 

83
00:07:29,287 --> 00:07:34,969
那么你就需要调整学习速率 或者改变特征变量 或者改变其他的什么

84
00:07:34,969 --> 00:07:39,235
最后一种你可能会遇到的情况是 如果你画出曲线

85
00:07:39,235 --> 00:07:43,273
你会发现曲线是这样的 实际上是在上升

86
00:07:43,273 --> 00:07:48,066
这是一个很明显的信号 告诉你算法正在发散

87
00:07:48,066 --> 00:07:53,965
那么你要做的事 就是用一个更小一点的学习速率α

88
00:07:53,965 --> 00:07:58,143
好的 希望通过这几幅图 你能了解到 

89
00:07:58,143 --> 00:08:02,946
当你画出cost函数在某个范围的训练样本中求平均值时 各种可能出现的现象

90
00:08:02,946 --> 00:08:07,765
也告诉你 在遇到不同的情况时 应该采取怎样的措施

91
00:08:07,765 --> 00:08:15,070
所以如果曲线看起来噪声较大 或者老是上下振动 

92
00:08:15,070 --> 00:08:18,734
那就试试增大你要平均的样本数量 这样应该就能得到比较好的变化趋势

93
00:08:18,734 --> 00:08:25,836
如果你发现代价值在上升 那么就换一个小一点的α值

94
00:08:25,836 --> 00:08:31,649
最后还需要再说一下关于学习速率的问题

95
00:08:31,649 --> 00:08:38,922
我们已经知道 当运行随机梯度下降时 算法会从某个点开始 然后曲折地逼近最小值

96
00:08:38,922 --> 00:08:43,494
但它不会真的收敛 而是一直在最小值附近徘徊

97
00:08:43,494 --> 00:08:50,225
因此你最终得到的参数 实际上只是接近全局最小值 而不是真正的全局最小值

98
00:08:50,225 --> 00:08:57,991
在大多数随机梯度下降法的典型应用中 学习速率α一般是保持不变的

99
00:08:57,991 --> 00:09:02,022
因此你最终得到的结果一般来说是这个样子的

100
00:09:02,022 --> 00:09:06,523
如果你想让随机梯度下降确实收敛到全局最小值

101
00:09:06,523 --> 00:09:11,825
你可以随时间的变化减小学习速率α的值

102
00:09:11,825 --> 00:09:22,240
所以 一种典型的方法来设置α的值 是让α等于某个常数1 除以 迭代次数加某个常数2

103
00:09:22,240 --> 00:09:28,169
迭代次数指的是你运行随机梯度下降的迭代次数

104
00:09:28,169 --> 00:09:29,519
就是你算过的训练样本的数量

105
00:09:29,519 --> 00:09:34,103
常数1和常数2是两个额外的参数

106
00:09:34,103 --> 00:09:38,160
你需要选择一下 才能得到较好的表现

107
00:09:38,160 --> 00:09:43,004
但很多人不愿意用这个办法的原因是 你最后会把问题落实到

108
00:09:43,004 --> 00:09:48,122
把时间花在确定常数1和常数2上 这让算法显得更繁琐

109
00:09:48,122 --> 00:09:52,113
也就是说 为了让算法更好 你要调整更多的参数

110
00:09:52,113 --> 00:09:57,246
但如果你能调整得到比较好的参数的话 你会得到的图形是

111
00:09:57,246 --> 00:10:02,834
你的算法会在最小值附近振荡 但当它越来越靠近最小值的时候

112
00:10:02,834 --> 00:10:07,024
由于你减小了学习速率 因此这个振荡也会越来越小

113
00:10:07,024 --> 00:10:12,729
直到落到几乎靠近全局最小的地方 我想这么说能听懂吧？

114
00:10:12,729 --> 00:10:21,608
这个公式起作用的原因是 随着算法的运行 迭代次数会越来越大 因此学习速率α会慢慢变小

115
00:10:21,608 --> 00:10:27,506
因此你的每一步就会越来越小 直到最终收敛到全局最小值

116
00:10:27,506 --> 00:10:33,484
所以 如果你慢慢减小α的值到0 你会最后得到一个更好一点的假设

117
00:10:33,484 --> 00:10:40,078
但由于确定这两个常数需要更多的工作量 并且我们通常也对

118
00:10:40,078 --> 00:10:43,892
能够很接近全局最小值的参数 已经很满意了

119
00:10:43,892 --> 00:10:50,863
因此我们很少采用逐渐减小α的值的方法 在随机梯度下降中

120
00:10:50,863 --> 00:10:56,983
你看到更多的还是让α的值为常数 虽然两种做法的人都有

121
00:10:56,983 --> 00:11:03,595
总结一下 这段视频中 我们介绍了一种方法

122
00:11:03,595 --> 00:11:08,256
 近似地监测出随机梯度下降算法在最优化代价函数中的表现

123
00:11:08,256 --> 00:11:17,043
这种方法不需要定时地扫描整个训练集 来算出整个样本集的代价函数

124
00:11:17,043 --> 00:11:20,693
而是只需要每次对最后1000个 或者多少个样本 求一下平均值

125
00:11:20,693 --> 00:11:27,592
应用这种方法 你既可以保证随机梯度下降法正在正常运转和收敛

126
00:11:27,592 --> 00:11:31,468
也可以用它来调整学习速率α的大小【教育无边界字幕组】翻译: 所罗门捷列夫 校对: 竹二个 审核：Naplessss