In this and the next video I want to
work through a detailed example showing how a neural network can compute a complex
non linear function of the input. And hopefully this will give you a good
sense of why neural networks can be used to learn complex
non linear hypotheses. Consider the following problem
where we have features X1 and X2 that are binary values. So, either 0 or 1. So, X1 and X2 can each take on
only one of two possible values. In this example,
I've drawn only two positive examples and two negative examples. That you can think of this as a simplified
version of a more complex learning problem where we may have a bunch of positive
examples in the upper right and lower left and a bunch of negative
examples denoted by the circles. And what we'd like to do is learn
a non-linear division of boundary that may need to separate the positive and
negative examples. So, how can a neural network do this and
rather than using the example and the variable to use this maybe easier
to examine example on the left. Concretely what this is, is really computing the type of
label y equals x 1 x or x 2. Or actually this is actually the x 1 x nor
x 2 function where x nor is the alternative notation for
not x 1 or x 2. So, x 1 x or x 2 that's true only if
exactly 1 of x 1 or x 2 is equal to 1. It turns out that these specific
examples in the works out a little bit better if we use the XNOR example instead. These two are the same of course. This means not x1 or x2 and so, we're
going to have positive examples of either both are true or both are false and
what have as y equals 1, y equals 1. And we're going to have y equals 0 if only
one of them is true and we're going to figure out if we can get a neural network
to fit to this sort of training set. In order to build up to a network that
fits the XNOR example we're going to start with a slightly simpler one and
show a network that fits the AND function. Concretely, let's say we have input x1 and x2 that are again binaries so,
it's either 0 or 1 and
let's say our target labels y = x1 AND x2. This is a logical AND. So, can we get a one-unit network to
compute this logical AND function? In order to do so, I'm going to actually draw in the bias
unit as well the plus one unit. Now let me just assign some values to
the weights or parameters of this network. I'm gonna write down the parameters
on this diagram here, -30 here. +20 and + 20. And what this mean is just that
I'm assigning a value of -30 to the value associated with X0
this +1 going into this unit and a parameter value of +20 that
multiplies to X1 a value of +20 for the parameter that multiplies into x 2. So, concretely it's the same that the hypothesis h(x)=g(-30+20
X1 plus 20 X2. So, sometimes it's just
convenient to draw these weights. Draw these parameters up here in
the diagram within and of course this- 30. This is actually theta 1 of 1 0. This is theta 1 of 1 1 and
that's theta 1 of 1 2 but it's just easier to think
about it as associating these parameters with
the edges of the network. Let's look at what this little
single neuron network will compute. Just to remind you the sigmoid activation
function g(z) looks like this. It starts from 0 rises
smoothly crosses 0.5 and then it asymptotic as 1 and
to give you some landmarks, if the horizontal axis value
z is equal to 4.6 then the sigmoid function is equal to 0.99. This is very close to 1 and
kind of symmetrically, if it's -4.6 then the sigmoid function
there is 0.01 which is very close to 0. Let's look at the four possible
input values for x1 and x2 and look at what the hypotheses
will output in that case. If x1 and x2 are both equal to 0. If you look at this, if x1 x2 are both equal to 0
then the hypothesis of g of -30. So, this is a very far to the left of this
diagram so it will be very close to 0. If x 1 equals 0 and x equals 1,
then this formula here evaluates the g that is the sigma function applied to -10,
and again that's you know to the far left of this plot and
so, that's again very close to 0. This is also g of minus 10 that
is f x 1 is equal to 1 and x 2 0, this minus 30 plus
20 which is minus 10 and finally if x 1 equals 1 x 2 equals 1 then
you have g of minus 30 plus 20 plus 20. So, that's g of positive 10 which
is there for very close to 1. And if you look in this column this
is exactly the logical and function. So, this is computing h of x is approximately x 1 and x 2. In other words it outputs one If and
only if x2, x1 and x2, are both equal to 1. So, by writing out our
little truth table like this we manage to figure what's
the logical function that our neural network computes. This network showed here computes the OR
function. Just to show you how I worked that out. If you are write out the hypothesis
that this confusing g of -10 + 20 x 1 + 20 x 2 and so
you fill in these values. You find that's g of minus
10 which is approximately 0. g of 10 which is approximately 1 and so
on and these are approximately 1 and approximately 1 and these numbers
are essentially the logical OR function. So, hopefully with this you now understand
how single neurons in a neural network can be used to compute logical
functions like AND and OR and so on. In the next video we'll continue
building on these examples and work through a more complex example. We'll get to show you how a neural network
now with multiple layers of units can be used to compute more complex
functions like the XOR function or the XNOR function.