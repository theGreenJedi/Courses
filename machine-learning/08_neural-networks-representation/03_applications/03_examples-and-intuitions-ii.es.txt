En este video quiero trabajar en nuestro ejemplo para mostrar cómo una red neuronal puede calcular hipótesis complejas no lineales. En el último video vimos cómo una red neuronal puede usarse para calcular las funciones x1 y x2 y la función x1 o x2 cuando x1 y x2 son binarias. Es decir, cuando toman los valores 0 y 1. También podemos hacer que una red calcule la negación, lo que significa calcular la función "not x1". Voy a escribir las formas asociadas con esta red. Sólo tenemos una variable de entrada, x1 En este caso, y la unidad de oscilación más 1 y, si asocio esto con los pesos más 10 y menos 20, entonces mi hipótesis está calculando esto. H de x es igual a sigmoide de 10 menos 20 veces x1. Entonces, cuando x1 es igual a 0, mi hipótesis estará calculando g de 10 menos 20 veces 0 que es exactamente 10. Y eso es aproximadamente 1, y cuando x es igual a 1, esto será g de menos 10, que es aproximadamente igual a 0. Y, si observas lo que son estos valores, esta es esencialmente la función "not x1". Así que, para incluir negaciones, la idea general es poner un gran peso negativo delante de la variable que desea anular. Así que, si es -20, multiplicado por x1 y, ya sabes, esa es la idea general de como se llega a la negación de x1. Entonces, en un ejemplo que espero que puedas resolver por ti mismo, si quieres calcular una función como esta: "not x1 and not x2", ya sabes, mientras que parte de eso probablemente sería poner grandes pesos negativos delante de x1 y x2, pero debería ser factible para hacer que una red neuronal trabaje con sólo una unidad de salida para calcular esto también. ¿De acuerdo? Entonces, esta gran función "not x1 and not x2" va a ser igual a 1 si y sólo si, x1 es igual a x2 y es igual a cero, ¿no? Entonces, esta es una función lógica, es not x1, lo que significa que x1 debe ser igual a cero y no x2. Lo que significa que x2 debe ser igual a cero también. Entonces, esta función lógica es igual a 1, si y sólo si x1 y x2 son iguales a cero. Y, con suerte, podrás encontrar cómo hacer que una pequeña red neuronal calcule esta función lógica también. Ahora, tomando las tres piezas que hemos colocado juntas, la red para calcular x1 y x2 y la red para calcular not x1 y not x2 y una última red para calcular x1 or x2, deberíamos ser capaces de reunir estas tres piezas para calcular esta función x1, XNOR x2. Y, como recordatorio, si fuera x1, x2, la función que queremos calcular tendría ejemplos negativos aquí y aquí, y tendríamos ejemplos positivos allí y allí. Por lo que, claramente, necesitaríamos un límite de decisión no lineal para separar los ejemplos positivos y negativos. Dibujemos la red. Voy a tomar mi entrada más 1, x1, x2, y crear mi primera unidad oculta aquí. Voy a llamarla a(2)1 porque es mi primera unidad oculta. Y voy a copiar los pesos de la red roja; las redes x1 y x2. Entonces, ahora menos 30, 20, 20. A continuación, voy a crear una segunda unidad oculta, a la que voy a llamar a(2)2, y que es la segunda unidad oculta de la capa dos. Y voy a copiarla desde la red azul en el medio, así que voy a tener los pesos 10, menos 20, menos 20. Entonces, tomemos algunos valores de la tabla de verdad. para la red roja, sabemos que estaba calculando x1 y x2. Y entonces esto es aproximadamente 0, 0, 0, 1, dependiendo de los valores de x1 y x2. Y para a(2)2, que es la red azul, bien, conocemos la función not x1 y not x2, entonces la salida es 1, 0, 0, 0 para los 4 los valores de x 1 y x2. Finalmente, voy a crear mi nota de salida, mi unidad de salida es a(3)1. Esta es una salida h de x más y voy a copiarla a la red OR y, para eso, voy a necesitar una unidad de oscilación más uno aquí. Entonces, colocamos eso y voy a copiar desde los pesos de las redes verdes. Entonces, es menos 10, 20, 20 y sabemos desde antes que esto calcula la función OR. Entonces, pasemos a las entradas de la tabla de verdad. Para la primera entrada es 0 o 1, que va a ser 1, después el siguiente 0, o 0, que es 0, 0, o 0, que es 0, 1 o 0, y todo esto es a 1 y, por lo tanto, h de x es igual a 1 cuando x1 y x2 son 0 o cuando tanto x1 como x2 son 1. Y concretamente, h de x muestra 1 exactamente en estas dos ubicaciones y se muestra 0 de lo contrario y, por lo tanto, con esta red neuronal, que tiene una capa de entrada, una capa oculta y una capa de salida, terminamos con un límite de decisión no lineal que calcula esta función XNOR. Y la intuición más general es que en la capa de entrada, sólo tuvimos nuestras entradas en bruto y después tuvimos una capa oculta, que calculó algunas funciones un poco más complejas de las entradas que se muestran aquí; estas son funciones un poco más complejas, y después, añadiendo aún otra capa, terminamos con una función no lineal aún más compleja. Y este es el tipo de intuición acerca de por qué las redes neuronales pueden calcular funciones bastante complicadas que, cuando se tienen varias capas, se tiene, ya sabes, una función relativamente simple de las entradas y la segunda capa, Pero la tercera capa puede trabajar a partir de eso para calcular funciones aún más complejas y, después, La capa después de esta puede calcular funciones todavía más complejas. Para terminar este video, quiero mostrarte un ejemplo divertido de la aplicación de una red neuronal que captura esta intuición de las capas más profundas calculando variables más complejas. Quiero enseñarte un video que me dio un buen amigo mío, Yon Khun. Yon es profesor en New York University, en NYU, y él fue uno de los primeros pioneros en la investigación de las redes neuronales y una especie de leyenda en el campo. Ahora sus ideas se utilizan en todo tipo de productos y aplicaciones en todo el mundo. Así que quiero mostrarte un video de algunos de sus primeros trabajos, en los cuales él utilizó una red neuronal para reconocer la escritura a mano - para hacer un reconocimiento de los dígitos escritos a mano. Quizás recuerdes que al inicio de esta clase, al principio de esta clase, dije que uno de los primeros éxitos de las redes neuronales fue tratar de usarlas para leer códigos postales, para ayudarnos a, ya sabes, enviar correos. Entonces, para leer códigos postales. Entonces, este es uno de los intentos. Entonces, este es uno de los algoritmos que se usaron para tratar de solucionar ese problema. En el video te mostraré esta área aquí es el área de entrada que muestra un caracter escrito a mano que se muestra a la red. Esta columna aquí muestra una visualización de las variables calculadas, de forma que la primera capa oculta de la red, y la primera capa oculta, ya sabes, esta visualización muestra variables diferentes, diferentes bordes y líneas y así sucesivamente detectados. Esta es una visualización de la siguiente capa oculta. Es un poco más difícil ver cómo entender las capas ocultas más profundamente, y esa es la visualización de lo que la siguiente capa oculta está calculando. Probablemente te costará trabajo ver lo que está sucediendo, ya sabes, mucho más allá de la primera capa oculta, Pero después, finalmente, todas estas variables aprendidas son introducidas en la capa de salida y aquí se muestran las respuestas finales, el valor final predictivo para lo que la red neuronal considera que es el dígito escrito a mano que se está mostrando. Así que echemos un vistazo al video. Y, espero que hayas disfrutado el video y espero que te haya dado alguna intuición acerca de las clases de funciones bastantes complicadas que las redes neuronales pueden aprender, en donde se toma esta imagen como entrada, sólo se toman los pixeles en bruto de esta entrada y el primer extremo de la capa calcula un conjunto de variables, el siguiente extremo de la capa calcula variables aún más complejas y variables todavía más complejas y estas variables entonces pueden ser utilizadas por, esencialmente, la capa final de los clasificadores de regresión logística para hacer predicciones precisas sobre lo que son los números que ve a la red.