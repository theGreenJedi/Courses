1
00:00:00,420 --> 00:00:01,540
En este video

2
00:00:01,700 --> 00:00:02,680
quiero trabajar en nuestro ejemplo

3
00:00:03,390 --> 00:00:04,480
para mostrar cómo una red

4
00:00:04,730 --> 00:00:07,140
neuronal puede calcular hipótesis complejas no lineales.

5
00:00:10,110 --> 00:00:11,240
En el último video vimos

6
00:00:11,490 --> 00:00:12,790
cómo una red neuronal puede

7
00:00:13,020 --> 00:00:13,900
usarse para calcular las funciones

8
00:00:14,420 --> 00:00:16,120
x1 y x2 y la

9
00:00:16,230 --> 00:00:18,410
función x1 o x2 cuando

10
00:00:18,750 --> 00:00:20,250
x1 y x2 son binarias.

11
00:00:20,870 --> 00:00:23,080
Es decir, cuando toman los valores 0 y 1.

12
00:00:23,230 --> 00:00:24,580
También podemos hacer que

13
00:00:24,620 --> 00:00:27,130
una red calcule la negación, lo que significa

14
00:00:27,330 --> 00:00:30,040
calcular la función "not x1".

15
00:00:30,280 --> 00:00:31,670
Voy a escribir

16
00:00:31,890 --> 00:00:33,670
las formas asociadas con esta red.

17
00:00:33,970 --> 00:00:35,350
Sólo tenemos una variable de entrada, x1

18
00:00:35,450 --> 00:00:36,550
En este caso, y la

19
00:00:36,620 --> 00:00:38,210
unidad de oscilación más 1 y,

20
00:00:38,680 --> 00:00:40,130
si asocio esto con

21
00:00:41,070 --> 00:00:42,610
los pesos más 10 y

22
00:00:43,120 --> 00:00:45,700
menos 20, entonces mi hipótesis está calculando esto.

23
00:00:46,080 --> 00:00:47,740
H de x es igual a sigmoide de

24
00:00:47,880 --> 00:00:49,600
10 menos 20 veces x1.

25
00:00:50,390 --> 00:00:51,710
Entonces, cuando x1 es

26
00:00:51,940 --> 00:00:52,880
igual a 0, mi

27
00:00:52,960 --> 00:00:54,060
hipótesis estará calculando

28
00:00:55,160 --> 00:00:57,340
g de 10 menos

29
00:00:57,970 --> 00:00:59,910
20 veces 0 que es exactamente 10.

30
00:01:00,090 --> 00:01:01,600
Y eso es aproximadamente

31
00:01:02,440 --> 00:01:03,390
1, y cuando x es

32
00:01:03,500 --> 00:01:04,300
igual a 1, esto será

33
00:01:04,380 --> 00:01:05,740
g de menos 10, que es

34
00:01:06,210 --> 00:01:09,380
aproximadamente igual a 0.

35
00:01:09,550 --> 00:01:10,320
Y, si observas lo que

36
00:01:10,450 --> 00:01:11,720
son estos valores, esta es esencialmente

37
00:01:12,230 --> 00:01:13,470
la función "not x1".

38
00:01:14,560 --> 00:01:16,410
Así que, para incluir negaciones, la

39
00:01:16,700 --> 00:01:18,640
idea general es poner

40
00:01:19,080 --> 00:01:20,460
un gran peso negativo delante

41
00:01:20,650 --> 00:01:22,870
de la variable que desea anular.

42
00:01:23,100 --> 00:01:24,710
Así que, si es -20, multiplicado por

43
00:01:25,590 --> 00:01:26,780
x1 y, ya sabes, esa es la idea

44
00:01:27,230 --> 00:01:28,110
general de como se llega

45
00:01:28,320 --> 00:01:30,500
a la negación de x1.

46
00:01:30,700 --> 00:01:32,210
Entonces, en un ejemplo que

47
00:01:32,580 --> 00:01:33,410
espero que puedas resolver

48
00:01:33,480 --> 00:01:35,090
por ti mismo, si quieres

49
00:01:35,280 --> 00:01:36,410
calcular una función como esta:

50
00:01:36,580 --> 00:01:38,870
"not x1 and not x2",

51
00:01:39,090 --> 00:01:40,100
ya sabes, mientras que parte de eso

52
00:01:40,390 --> 00:01:41,860
probablemente sería poner grandes pesos

53
00:01:42,290 --> 00:01:44,150
negativos delante de x1

54
00:01:44,500 --> 00:01:45,330
y x2, pero debería ser

55
00:01:45,580 --> 00:01:47,320
factible para hacer que

56
00:01:47,490 --> 00:01:49,910
una red neuronal trabaje con sólo

57
00:01:50,420 --> 00:01:52,810
una unidad de salida para calcular esto también.

58
00:01:52,990 --> 00:01:53,460
¿De acuerdo?

59
00:01:53,680 --> 00:01:55,130
Entonces, esta gran

60
00:01:55,300 --> 00:01:56,290
función "not x1 and not

61
00:01:56,590 --> 00:01:57,990
x2" va a ser

62
00:01:58,210 --> 00:02:00,450
igual a 1 si y sólo

63
00:02:00,780 --> 00:02:06,960
si, x1 es

64
00:02:07,350 --> 00:02:09,860
igual a x2 y es igual a cero, ¿no?

65
00:02:10,420 --> 00:02:11,480
Entonces, esta es una función lógica, es

66
00:02:11,680 --> 00:02:14,290
not x1, lo que significa que x1 debe ser igual a cero y no x2.

67
00:02:14,530 --> 00:02:17,130
Lo que significa que x2 debe ser igual a cero también.

68
00:02:17,800 --> 00:02:19,210
Entonces, esta función lógica es

69
00:02:19,450 --> 00:02:20,210
igual a 1, si y sólo

70
00:02:20,540 --> 00:02:22,900
si x1 y x2 son iguales a cero.

71
00:02:23,910 --> 00:02:25,600
Y, con suerte, podrás

72
00:02:25,710 --> 00:02:26,630
encontrar cómo hacer que una

73
00:02:26,950 --> 00:02:28,240
pequeña red neuronal calcule

74
00:02:28,640 --> 00:02:29,830
esta función lógica también.

75
00:02:33,430 --> 00:02:34,350
Ahora, tomando las tres piezas

76
00:02:34,820 --> 00:02:36,720
que hemos colocado juntas, la red

77
00:02:37,400 --> 00:02:38,710
para calcular x1 y

78
00:02:38,910 --> 00:02:40,620
x2 y la red para

79
00:02:40,960 --> 00:02:42,070
calcular not x1 y

80
00:02:42,340 --> 00:02:44,170
not x2 y una última

81
00:02:44,620 --> 00:02:45,910
red para calcular x1 or

82
00:02:46,570 --> 00:02:47,700
x2, deberíamos ser

83
00:02:47,760 --> 00:02:49,420
capaces de reunir estas tres piezas

84
00:02:49,840 --> 00:02:51,270
para calcular esta función x1, XNOR

85
00:02:51,470 --> 00:02:52,810
x2.

86
00:02:53,860 --> 00:02:54,930
Y, como recordatorio, si

87
00:02:55,100 --> 00:02:57,130
fuera x1, x2, la

88
00:02:58,080 --> 00:02:58,830
función que queremos

89
00:02:59,090 --> 00:03:00,900
calcular tendría ejemplos negativos

90
00:03:01,520 --> 00:03:02,690
aquí y aquí, y tendríamos

91
00:03:02,830 --> 00:03:04,370
ejemplos positivos allí y allí.

92
00:03:04,730 --> 00:03:06,270
Por lo que, claramente, necesitaríamos

93
00:03:06,570 --> 00:03:08,400
un límite de decisión no lineal

94
00:03:08,940 --> 00:03:10,540
para separar los ejemplos positivos y negativos.

95
00:03:12,950 --> 00:03:13,460
Dibujemos la red.

96
00:03:14,260 --> 00:03:15,820
Voy a tomar mi entrada

97
00:03:16,570 --> 00:03:18,610
más 1, x1, x2, y crear

98
00:03:19,150 --> 00:03:20,390
mi primera unidad oculta aquí.

99
00:03:20,660 --> 00:03:22,010
Voy a llamarla a(2)1

100
00:03:22,770 --> 00:03:24,060
porque es mi primera unidad oculta.

101
00:03:24,510 --> 00:03:25,660
Y voy a copiar

102
00:03:25,920 --> 00:03:27,410
los pesos de la red

103
00:03:27,740 --> 00:03:30,020
roja; las redes x1 y x2.

104
00:03:30,820 --> 00:03:32,410
Entonces, ahora menos 30, 20, 20.

105
00:03:32,650 --> 00:03:36,060
A continuación, voy a crear

106
00:03:36,420 --> 00:03:37,700
una segunda unidad oculta, a la que

107
00:03:37,930 --> 00:03:39,960
voy a llamar a(2)2, y que

108
00:03:40,350 --> 00:03:42,610
es la segunda unidad oculta de la capa dos.

109
00:03:43,550 --> 00:03:44,590
Y voy a copiarla desde la

110
00:03:44,740 --> 00:03:45,940
red azul en el

111
00:03:46,170 --> 00:03:47,080
medio, así que voy a

112
00:03:47,130 --> 00:03:49,230
tener los pesos 10, menos 20,

113
00:03:50,150 --> 00:03:51,060
menos 20.

114
00:03:52,150 --> 00:03:55,570
Entonces, tomemos algunos valores de la tabla de verdad.

115
00:03:56,170 --> 00:03:57,350
para la red roja, sabemos que

116
00:03:57,590 --> 00:03:59,340
estaba calculando x1 y x2.

117
00:03:59,690 --> 00:04:00,940
Y entonces esto es

118
00:04:01,040 --> 00:04:02,460
aproximadamente 0, 0,

119
00:04:02,540 --> 00:04:05,030
0, 1, dependiendo de los valores de x1 y x2.

120
00:04:07,040 --> 00:04:09,560
Y para a(2)2, que es la red azul,

121
00:04:10,590 --> 00:04:11,750
bien, conocemos la función not x1

122
00:04:12,240 --> 00:04:13,640
y not x2, entonces la salida es 1,

123
00:04:13,640 --> 00:04:15,610
0, 0, 0 para los

124
00:04:15,700 --> 00:04:17,830
4 los valores de x 1 y x2.

125
00:04:18,480 --> 00:04:19,560
Finalmente, voy a

126
00:04:19,810 --> 00:04:21,300
crear mi nota de salida, mi

127
00:04:21,490 --> 00:04:23,950
unidad de salida es a(3)1.

128
00:04:24,860 --> 00:04:26,230
Esta es una salida h

129
00:04:26,590 --> 00:04:28,270
de x más y voy a

130
00:04:28,390 --> 00:04:30,030
copiarla a la red

131
00:04:30,320 --> 00:04:32,470
OR y, para eso, voy a

132
00:04:32,860 --> 00:04:34,330
necesitar una unidad de oscilación más uno aquí.

133
00:04:34,810 --> 00:04:36,010
Entonces, colocamos eso y voy a

134
00:04:36,320 --> 00:04:38,360
copiar desde los pesos de las redes verdes.

135
00:04:38,950 --> 00:04:39,750
Entonces, es menos 10, 20, 20

136
00:04:42,370 --> 00:04:44,460
y sabemos desde antes que esto calcula la función OR.

137
00:04:46,660 --> 00:04:48,200
Entonces, pasemos a las entradas de la tabla de verdad.

138
00:04:50,300 --> 00:04:51,660
Para la primera entrada es 0

139
00:04:51,720 --> 00:04:53,930
o 1, que va a ser

140
00:04:54,140 --> 00:04:55,710
1, después el siguiente 0, o

141
00:04:55,800 --> 00:04:57,280
0, que es 0,

142
00:04:57,350 --> 00:04:58,920
0, o 0, que es 0,

143
00:04:58,960 --> 00:05:00,420
1 o 0, y todo esto

144
00:05:00,600 --> 00:05:02,450
es a 1 y, por lo tanto, h

145
00:05:02,640 --> 00:05:04,820
de x es igual a 1

146
00:05:04,980 --> 00:05:06,270
cuando x1 y

147
00:05:06,780 --> 00:05:08,360
x2 son 0 o cuando tanto x1 como

148
00:05:08,590 --> 00:05:10,160
x2 son 1. Y

149
00:05:10,900 --> 00:05:12,170
concretamente, h de x

150
00:05:12,680 --> 00:05:15,340
muestra 1 exactamente en estas

151
00:05:15,560 --> 00:05:16,850
dos ubicaciones y se muestra

152
00:05:17,230 --> 00:05:19,270
0 de lo contrario y, por lo tanto,

153
00:05:19,570 --> 00:05:20,970
con esta red neuronal, que

154
00:05:21,210 --> 00:05:23,030
tiene una capa de entrada, una

155
00:05:23,200 --> 00:05:24,560
capa oculta y una capa de

156
00:05:24,880 --> 00:05:25,920
salida, terminamos

157
00:05:26,100 --> 00:05:28,450
con un límite de decisión no lineal que

158
00:05:29,120 --> 00:05:30,520
calcula esta función XNOR.

159
00:05:31,640 --> 00:05:33,390
Y la intuición más general es

160
00:05:33,710 --> 00:05:34,870
que en la capa de

161
00:05:34,990 --> 00:05:35,780
entrada, sólo tuvimos nuestras

162
00:05:36,060 --> 00:05:37,400
entradas en bruto y después tuvimos

163
00:05:37,610 --> 00:05:39,510
una capa oculta, que calculó algunas

164
00:05:39,680 --> 00:05:41,140
funciones un poco más complejas de

165
00:05:41,250 --> 00:05:42,080
las entradas que se muestran

166
00:05:42,430 --> 00:05:43,410
aquí; estas son funciones un poco

167
00:05:43,550 --> 00:05:44,960
más complejas, y después,

168
00:05:45,250 --> 00:05:46,510
añadiendo aún otra capa, terminamos

169
00:05:46,640 --> 00:05:49,030
con una función no lineal aún más compleja.

170
00:05:50,550 --> 00:05:51,340
Y este es el tipo de

171
00:05:51,450 --> 00:05:53,810
intuición acerca de por qué las redes

172
00:05:54,100 --> 00:05:55,270
neuronales pueden calcular funciones

173
00:05:55,840 --> 00:05:57,270
bastante complicadas que, cuando se tienen

174
00:05:57,340 --> 00:05:58,550
varias capas, se tiene, ya sabes,

175
00:05:58,910 --> 00:06:00,300
una función relativamente simple de

176
00:06:00,390 --> 00:06:01,500
las entradas y la segunda capa,

177
00:06:02,160 --> 00:06:03,110
Pero la tercera capa puede trabajar

178
00:06:03,340 --> 00:06:04,590
a partir de eso para calcular funciones

179
00:06:04,820 --> 00:06:06,330
aún más complejas y, después,

180
00:06:06,790 --> 00:06:08,730
La capa después de esta puede calcular funciones todavía más complejas.

181
00:06:10,340 --> 00:06:11,740
Para terminar este video,

182
00:06:11,800 --> 00:06:13,330
quiero mostrarte un ejemplo divertido de

183
00:06:13,480 --> 00:06:14,650
la aplicación de una red

184
00:06:14,880 --> 00:06:16,400
neuronal que captura esta intuición

185
00:06:17,260 --> 00:06:19,440
de las capas más profundas calculando variables más complejas.

186
00:06:19,900 --> 00:06:21,040
Quiero enseñarte

187
00:06:21,200 --> 00:06:22,480
un video que me dio

188
00:06:22,930 --> 00:06:24,170
un buen amigo mío, Yon Khun.

189
00:06:24,850 --> 00:06:26,240
Yon es profesor en

190
00:06:26,610 --> 00:06:27,680
New York University, en NYU,

191
00:06:28,230 --> 00:06:29,400
y él fue uno de

192
00:06:29,470 --> 00:06:30,910
los primeros pioneros en la investigación

193
00:06:31,130 --> 00:06:32,590
de las redes neuronales y una especie

194
00:06:32,930 --> 00:06:34,610
de leyenda en el campo.

195
00:06:34,930 --> 00:06:36,520
Ahora sus ideas se

196
00:06:36,560 --> 00:06:38,340
utilizan en todo tipo de productos

197
00:06:38,980 --> 00:06:40,490
y aplicaciones en todo el mundo.

198
00:06:41,470 --> 00:06:42,230
Así que quiero mostrarte

199
00:06:42,380 --> 00:06:43,410
un video de algunos de sus

200
00:06:43,740 --> 00:06:44,890
primeros trabajos, en los cuales él

201
00:06:44,980 --> 00:06:46,110
utilizó una red neuronal

202
00:06:47,000 --> 00:06:50,300
para reconocer la escritura a mano - para hacer un reconocimiento de los dígitos escritos a mano.

203
00:06:51,370 --> 00:06:52,510
Quizás recuerdes que al inicio de esta

204
00:06:52,720 --> 00:06:53,630
clase, al principio de esta

205
00:06:53,730 --> 00:06:55,180
clase, dije que uno de

206
00:06:55,460 --> 00:06:56,720
los primeros éxitos de las redes neuronales

207
00:06:57,140 --> 00:06:58,170
fue tratar de usarlas

208
00:06:58,320 --> 00:07:00,580
para leer códigos postales, para ayudarnos

209
00:07:00,850 --> 00:07:02,940
a, ya sabes, enviar correos. Entonces, para leer códigos postales.

210
00:07:03,880 --> 00:07:04,910
Entonces, este es uno de los intentos.

211
00:07:05,250 --> 00:07:06,220
Entonces, este es uno de los

212
00:07:06,650 --> 00:07:08,370
algoritmos que se usaron para tratar de solucionar ese problema.

213
00:07:09,320 --> 00:07:10,420
En el video te mostraré

214
00:07:11,060 --> 00:07:12,640
esta área aquí es el

215
00:07:12,910 --> 00:07:14,420
área de entrada que muestra un

216
00:07:14,980 --> 00:07:16,460
caracter escrito a mano que se muestra a la

217
00:07:16,560 --> 00:07:18,610
red. Esta columna aquí

218
00:07:19,490 --> 00:07:21,350
muestra una visualización de

219
00:07:21,460 --> 00:07:23,550
las variables calculadas, de forma que la

220
00:07:23,900 --> 00:07:24,760
primera capa oculta de la

221
00:07:24,830 --> 00:07:26,090
red, y la primera

222
00:07:26,400 --> 00:07:28,420
capa oculta, ya sabes, esta visualización muestra

223
00:07:28,720 --> 00:07:31,190
variables diferentes, diferentes bordes y líneas y así sucesivamente detectados.

224
00:07:32,360 --> 00:07:35,260
Esta es una visualización de la siguiente capa oculta.

225
00:07:35,530 --> 00:07:36,390
Es un poco más difícil ver

226
00:07:36,770 --> 00:07:38,170
cómo entender las capas ocultas más

227
00:07:38,730 --> 00:07:39,680
profundamente, y esa es la visualización

228
00:07:40,460 --> 00:07:41,830
de lo que la siguiente capa oculta está calculando.

229
00:07:42,140 --> 00:07:43,530
Probablemente te costará trabajo

230
00:07:44,180 --> 00:07:45,550
ver lo que está sucediendo, ya sabes,

231
00:07:45,700 --> 00:07:46,800
mucho más allá de la primera capa oculta,

232
00:07:47,640 --> 00:07:49,160
Pero después, finalmente, todas estas

233
00:07:49,260 --> 00:07:51,110
variables aprendidas son

234
00:07:51,430 --> 00:07:52,590
introducidas en la capa de salida

235
00:07:53,260 --> 00:07:54,830
y aquí se muestran

236
00:07:55,030 --> 00:07:56,370
las respuestas finales, el valor

237
00:07:56,800 --> 00:07:58,850
final predictivo para lo que

238
00:07:59,390 --> 00:08:02,150
la red neuronal considera que es el dígito escrito a mano que se está mostrando.

239
00:08:03,130 --> 00:08:04,270
Así que echemos un vistazo al video.

240
00:09:42,060 --> 00:09:44,370
Y, espero que

241
00:09:50,610 --> 00:09:52,010
hayas disfrutado el video y

242
00:09:52,260 --> 00:09:53,480
espero que te haya dado alguna

243
00:09:53,670 --> 00:09:55,240
intuición acerca de las clases

244
00:09:55,450 --> 00:09:57,120
de funciones bastantes complicadas que las redes

245
00:09:57,320 --> 00:09:58,420
neuronales pueden aprender, en donde

246
00:09:58,740 --> 00:10:00,250
se toma esta imagen como entrada,

247
00:10:00,670 --> 00:10:01,510
sólo se toman los pixeles en bruto de

248
00:10:01,620 --> 00:10:03,140
esta entrada y el primer extremo

249
00:10:03,310 --> 00:10:04,640
de la capa calcula un conjunto

250
00:10:04,770 --> 00:10:05,680
de variables, el siguiente extremo

251
00:10:05,740 --> 00:10:06,900
de la capa calcula variables

252
00:10:07,330 --> 00:10:08,620
aún más complejas y variables todavía más complejas

253
00:10:09,560 --> 00:10:10,640
y estas variables entonces pueden

254
00:10:10,780 --> 00:10:12,030
ser utilizadas por, esencialmente, la capa

255
00:10:12,940 --> 00:10:14,700
final de los clasificadores de regresión logística

256
00:10:15,810 --> 00:10:17,550
para hacer predicciones precisas sobre lo que

257
00:10:17,880 --> 00:10:19,190
son los números que ve a la red.