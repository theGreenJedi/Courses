1
00:00:00,800 --> 00:00:05,590
इस वीडियो में मैं बताना चाहता हूँ आपको कि कैसे हम दर्शाते हैं न्यूरल नेटवर्क्स को.

2
00:00:05,590 --> 00:00:08,140
दूसरे शब्दों में, कैसे हम दर्शाते हैं हमारी हायपॉथिसस या

3
00:00:08,140 --> 00:00:12,060
कैसे हम दर्शाते हैं हमारे मॉडल को जब इस्तेमाल कर रहे हैं न्यूरल नेटवर्क्स.

4
00:00:12,060 --> 00:00:16,110
न्यूरल नेटवर्क विकसित किए गए थे अनुकरण करते हुए न्यूरॉन्स का या

5
00:00:16,110 --> 00:00:18,560
दिमाग में न्यूरॉन्स के नेटवर्क का.

6
00:00:18,560 --> 00:00:21,430
अत:, समझाने के लिए हायपॉथिसस रेप्रेज़ेंटेशन

7
00:00:21,430 --> 00:00:26,420
चलो शुरू करते हैं देखते हुए कि मस्तिष्क में एक न्यूरॉन कैसा दिखता है.

8
00:00:26,420 --> 00:00:27,110
आपका मस्तिष्क और

9
00:00:27,110 --> 00:00:32,410
मेरा इन न्यूरॉन्स से पूरा पैक है इस तरह के और न्यूरॉन्स कोशिकाएँ मस्तिष्क में.

10
00:00:32,410 --> 00:00:36,260
और दो बातों पर ध्यान आकर्षित करने के लिए पहली बात है कि

11
00:00:36,260 --> 00:00:40,320
न्यूरॉन की एक सेल बॉडी है, इस तरह, और इसके अलावा

12
00:00:40,320 --> 00:00:44,330
न्यूरॉन में बहुत सी इनपुट तारों की संख्या, और इन्हे डेंड्रायट्स कहा जाता है.

13
00:00:44,330 --> 00:00:50,220
आप सोचें उन्हें इनपुट तारों की तरह, वे लेती हैं इनपुट अन्य स्थानों से.

14
00:00:50,220 --> 00:00:55,347
और न्यूरॉन में एक आउट्पुट तार भी होता है जिसे कहते हैं ऐक्सॉन, और

15
00:00:55,347 --> 00:01:01,478
यह आउट्पुट तार है जो यह इस्तेमाल करता है भेजने के लिए संकेत दूसरे न्यूरॉन्स को,

16
00:01:01,478 --> 00:01:05,051
इसलिए अन्य न्यूरॉन्स को संदेश भेजने के लिए.

17
00:01:05,051 --> 00:01:09,760
तो, एक साधारण स्तर पर क्या एक न्यूरॉन है, है एक कम्प्यूटेशनल इकाई जो

18
00:01:09,760 --> 00:01:14,623
लेती है इन्पुट्स, इनपुट तारों के माध्यम से और कुछ गणना करता है और

19
00:01:14,623 --> 00:01:20,490
फिर यह भेजता है आउट्पुट्स ऐक्सॉन्स के माध्यम से अन्य नोड्स को या अन्य न्यूरांस को मस्तिष्क में.

20
00:01:20,490 --> 00:01:24,290
यहाँ न्यूरॉन्स के एक समूह का एक उदाहरण है.

21
00:01:24,290 --> 00:01:28,420
जिस तरह से न्यूरॉन्स एक दूसरे के साथ संवाद करते हैं वे हैं छोटी पल्सेज़

22
00:01:28,420 --> 00:01:33,140
बिजली की, जिन्हें स्पाइक्स भी कहते हैं लेकिन उसका सिर्फ़ मतलब है बिजली की पल्सेज़.

23
00:01:33,140 --> 00:01:37,921
तो यहाँ एक न्यूरॉन है और यह क्या करता है, यदि यह एक संदेश भेजना चाहता है यह क्या

24
00:01:37,921 --> 00:01:40,977
करता है बिजली का एक छोटा पल्स भेजता है.

25
00:01:40,977 --> 00:01:47,446
ऐक्सॉन के माध्यम से किसी अन्य न्यूरान को और यहाँ, यह ऐक्सॉन वह है यह खुली तार,

26
00:01:47,446 --> 00:01:52,276
जुड़ती है दूसरे न्यूरॉन के डेन्ड्राइट से यहाँ पर,

27
00:01:52,276 --> 00:01:57,714
जो तब लेता है यह आने वाला संदेश, करता है कुछ गणना.

28
00:01:57,714 --> 00:02:04,033
और वे, निर्णय लेते है कि भेजना है यह संदेश इस ऐक्सॉन पर अन्य न्यूरॉन्स को,

29
00:02:04,033 --> 00:02:08,630
और यह प्रक्रिया है जिसके द्वारा मानव की सोच होती है.

30
00:02:08,630 --> 00:02:10,800
यह ये न्यूरॉन्स हैं गणना करते हुए और

31
00:02:10,800 --> 00:02:16,540
भेजते हुए संदेश अन्य न्यूरॉन्स, जो इन्पुट्स उन्हें मिली हैं उसके परिणाम स्वरूप.

32
00:02:16,540 --> 00:02:21,073
और, वैसे, ऐसे ही हमारी इंद्रियाँ और हमारी मांसपेशियों भी काम करते हैं.

33
00:02:21,073 --> 00:02:26,550
यदि आप अपनी मांसपेशियों को हिलाना चाहते हैं जिस ढंग से वह होता है कि आपका न्यूरॉन शायद

34
00:02:26,550 --> 00:02:32,282
भेजे यह बिजली आपकी मांसपेशियों को और वह सिकोड़े आपकी मांसपेशियों को और

35
00:02:32,282 --> 00:02:37,759
आपकी आँखों को, कुछ इंद्रियाँ जैसे आँखे अवश्य भेजती हैं एक संदेश आपके मस्तिष्क को तब यह

36
00:02:37,759 --> 00:02:43,020
क्या करती है कि यह भेजती है बिजली का पल्स एक न्यूरान को आपके मस्तिष्क में इस तरह.

37
00:02:43,020 --> 00:02:48,116
एक न्यूरल नेटवर्क में, या यूँ कहें, एक कृत्रिम न्यूरल नेटवर्क में जो हमने

38
00:02:48,116 --> 00:02:52,893
इम्प्लमेंट किया है कम्प्यूटर पर, हम इस्तेमाल करेंगे एक बहुत सरल मॉडल

39
00:02:52,893 --> 00:02:58,540
जो एक न्यूरॉन करता है हम मॉडल करेंगे एक न्यूरॉन को सिर्फ़ एक लॉजिस्टिक यूनिट की तरह.

40
00:02:58,540 --> 00:03:03,170
तो जब मैं बनाता हूँ एक पीला वृत्त उस तरह, आप सोचें उसे एक करते हुए

41
00:03:03,170 --> 00:03:07,260
एक भूमिका, जो है बॉडी एक नयूरोन की, और

42
00:03:07,260 --> 00:03:12,910
हम तब देते हैं न्यूरॉन को कुछ इन्पुट्स विभिन्न डेंड्रायट्स पर या इनपुट तारों पर.

43
00:03:14,850 --> 00:03:17,380
और न्यूरॉन करता है कुछ गणना.

44
00:03:17,380 --> 00:03:21,560
और आउट्पुट करता है कुछ वैल्यू इस आउट्पुट तार पर, या

45
00:03:21,560 --> 00:03:24,480
जैविक न्यूरॉन में, यह एक ऐक्सॉन है.

46
00:03:24,480 --> 00:03:26,710
और जब भी मैं एक चित्र खींचता हूँ इस तरह से,

47
00:03:26,710 --> 00:03:31,650
इसका मतलब क्या है कि यह दर्शाता है एक कॉम्प्यूटेशन h ऑफ़

48
00:03:31,650 --> 00:03:37,690
x बराबर है एक बटा एक जमा e की पॉवर नेगेटिव थीटा ट्रान्स्पोज़ x.

49
00:03:37,690 --> 00:03:41,530
जहाँ हमेशा की तरह, x और थीटा हैं हमारे पेरामिटर वेक्टर्स, इस तरह.

50
00:03:42,630 --> 00:03:46,365
तो यह एक बहुत ही सरल, शायद एक बेहद सरलीकृत मॉडल है,

51
00:03:46,365 --> 00:03:51,136
कॉम्प्यूटेशन्स का जो न्यूरॉन करता है, जहां इसे मिलती हैं कुछ इन्पुट्स, x1,

52
00:03:51,136 --> 00:03:54,267
x2, x3 और यह आउट्पुट करता है कुछ वैल्यू जो कम्प्यूट की जाती है इस तरह.

53
00:03:59,308 --> 00:04:06,320
जब मैं बनाता हूँ एक न्यूरल नेटवर्क, अक्सर मैं सिर्फ़ बनाता हूँ केवल इनपुट नोड्स x1, x2, x3.

54
00:04:06,320 --> 00:04:09,860
कभी कभी जब ऐसा करना उपयोगी है, मैं बनाऊँगा एक अतिरिक्त नोड x0 के लिए.

55
00:04:11,070 --> 00:04:16,644
यह x0 अब उसे कभी कभी कहते हैं बाइयस यूनिट या बाइयस न्यूरान, लेकिन

56
00:04:16,644 --> 00:04:22,044
क्योंकि पहले से ही x0 है बराबर 1, कभी मैं यह बनाता हूँ, कभी

57
00:04:22,044 --> 00:04:28,620
नहीं निर्भर करते हुए जो भी ज़्यादा सुविधा जनक है नोटेशन के हिसाब से उस उदाहरण के लिए.

58
00:04:31,302 --> 00:04:36,537
अंत में, एक आख़िरी जानकारी टर्मिनॉलॉजी की जब हम बात करते हैं न्यूरल नेटवर्क्स की,

59
00:04:36,537 --> 00:04:39,612
कभी कभी हम कहेंगे कि यह है एक न्यूरॉन या

60
00:04:39,612 --> 00:04:44,710
एक कृत्रिम न्यूरॉन एक सिग्मोईड लॉजिस्टिक ऐक्टिवेशन फ़ंक्शन के साथ.

61
00:04:44,710 --> 00:04:48,920
तो यह ऐक्टिवेशन फ़ंक्शन न्यूरल नेटवर्क टर्मिनॉलॉजी में.

62
00:04:48,920 --> 00:04:52,722
यह है सिर्फ़ एक अन्य टर्म उस फ़ंक्शन के लिए

63
00:04:52,722 --> 00:04:57,320
उस नॉन-लिनीअर g(z)= 1 ओवर 1+e की पॉवर -z के लिए.

64
00:04:57,320 --> 00:05:01,410
और जबकि अभी तक मैं कहता रहा हूँ थीटा को पेरमिटर्स मॉडल के

65
00:05:01,410 --> 00:05:04,070
मैं ज्यादातर जारी रखूँगा प्रयोग करना उस टर्मिनॉलॉजी का.

66
00:05:04,070 --> 00:05:08,430
यहाँ, यह है एक कॉपी पेरमिटर्स की, लेकिन न्यूरल नेटवर्क्स में, न्यूरल नेटवर्क के

67
00:05:08,430 --> 00:05:13,030
साहित्य में कभी कभी आप सुनेगें लोगों को बात करते हुए वेटस की एक मॉडल के और

68
00:05:13,030 --> 00:05:17,190
वेटस का सिर्फ़ मतलब है वही जो मॉडल के पेरमिटर्स का है.

69
00:05:17,190 --> 00:05:21,540
लेकिन मैं ज्यादातर जारी रखूँगा प्रयोग करना पेरामिटर टर्मिनॉलॉजी का इन वीडीयोज़ में,

70
00:05:21,540 --> 00:05:24,860
लेकिन कभी कभी, आप सुन सकते हैं दूसरों को वेटस टर्मिनॉलॉजी का उपयोग करते हुए.

71
00:05:27,880 --> 00:05:31,660
तो, यह छोटा चित्र दर्शाता है एक अकेला न्यूरॉन.

72
00:05:34,500 --> 00:05:41,690
क्या है एक न्यूरल नेटवर्क, है सिर्फ़ एक समूह इन विभिन्न न्यूरॉन्स का जोड़े हुए एक साथ.

73
00:05:41,690 --> 00:05:46,747
पूरी तरह से, यहाँ हमारे पास हैं इनपुट यूनिट्स x1, x2, x3 और एक बार फिर,

74
00:05:46,747 --> 00:05:52,940
कभी कभी आप इस अतिरिक्त नोड x0 को बना सकते हैं और कभी-कभी नहीं, बस इसे यहाँ से बना दें.

75
00:05:52,940 --> 00:05:59,111
और यहाँ हमारे पास हैं तीन न्यूरॉन्स जिन्हें मैंने लिखा है b1, b2, b3.

76
00:05:59,111 --> 00:06:00,992
मैं बात करूँगा उन इंडिसीज़ की बाद में.

77
00:06:00,992 --> 00:06:06,569
और एक बार फिर हम जोड़ सकते हैं यदि हम चाहते हैं a0 और

78
00:06:06,569 --> 00:06:10,480
जोड़ दें मिश्रित बाइयस यूनिट वहाँ.

79
00:06:10,480 --> 00:06:11,970
जिसकी वैल्यू है हमेशा 1.

80
00:06:11,970 --> 00:06:16,520
और फिर अंत में हमारे पास है यह तीसरा नोड और अंतिम लेयर, और

81
00:06:16,520 --> 00:06:22,370
यहाँ है यह तीसरा नोड जो आउट्पुट करता है वैल्यू जो हायपॉथिसस h(x) कम्प्यूट करती है.

82
00:06:22,370 --> 00:06:26,253
थोड़ी और जानकारी देने के लिए टर्मिनॉलॉजी की, एक न्यूरल नेटवर्क में,

83
00:06:26,253 --> 00:06:31,222
पहली लेयर, इसे इनपुट लेयर भी कहते हैं क्योंकि यह है जहाँ हम

84
00:06:31,222 --> 00:06:33,805
इनपुट करते हैं हमारे फ़ीचर्ज़ x1, x2, x3.

85
00:06:33,805 --> 00:06:39,283
अंतिम लेयर को आउट्पुट लेयर भी कहते हैं क्योंकि उस लेयर में है एक न्यूरॉन,

86
00:06:39,283 --> 00:06:44,544
यह यहाँ, जो आउट्पुट करता हैं अंतिम वैल्यू हायपॉथिसस द्वारा कम्प्यूट की हुई.

87
00:06:44,544 --> 00:06:49,180
और फिर, बीच में लेयर 2, इसे हिडन लेयर कहा जाता है.

88
00:06:49,180 --> 00:06:53,701
टर्म हिडन लेयर एक बहुत बढ़िया टर्मिनॉलॉजी नहीं है, लेकिन विचार है कि,

89
00:06:53,701 --> 00:06:55,688
आप जानते हैं, आपने सूपर्वायज़ किया पहले,

90
00:06:55,688 --> 00:06:59,796
जहाँ आप देख सकते हैं इन्पुट्स, आपको दिखती है सही आउट्पुट्स, जहाँ

91
00:06:59,796 --> 00:07:04,550
इस हिडन लेयर की वैल्यूज़ आप नहीं देख पाते हैं ट्रेनिंग सेट अप में.

92
00:07:04,550 --> 00:07:08,490
यह नहीं है x, और यह नहीं है y, इसलिए हम इसे हिडन कहते हैं.

93
00:07:08,490 --> 00:07:12,730
और वे प्रयास करते हैं देखने का न्यूरल नेट्स को एक से अधिक लेयर के साथ लेकिन

94
00:07:12,730 --> 00:07:17,280
इस उदाहरण में, हमारे पास है एक इनपुट लेयर, लेयर 1, एक हिडन लेयर, लेयर 2,

95
00:07:17,280 --> 00:07:19,260
एक आउट्पुट लेयर, लेयर 3.

96
00:07:19,260 --> 00:07:22,054
लेकिन असल में, जो कुछ भी नहीं है एक इनपुट लेयर और

97
00:07:22,054 --> 00:07:24,498
नहीं है एक आउट्पुट लेयर कहलाता है एक हिडन लेयर.

98
00:07:29,200 --> 00:07:34,020
तो मैं काफ़ी स्पष्ट होना चाहता हूँ कि यह न्यूरल नेटवर्क क्या कर रहा है.

99
00:07:34,020 --> 00:07:37,570
चलो करते हैं कॉम्प्यूटेशनल सटेप्स जो है और

100
00:07:37,570 --> 00:07:41,580
दर्शाए गए हैं इस चित्र में.

101
00:07:41,580 --> 00:07:45,600
इन विशिष्ट कॉम्प्यूटेशन्स को समझाने के लिए जो दर्शाई गई हैं एक न्यूरल नेटवर्क से,

102
00:07:45,600 --> 00:07:47,350
यहाँ है थोड़ी और नोटेशन.

103
00:07:47,350 --> 00:07:52,300
मैं इस्तेमाल करूँगा एक सूपरस्क्रिप्ट j सबस्क्रिप्ट i का डिनोट करने के लिए ऐक्टिवेशन को

104
00:07:52,300 --> 00:07:56,810
न्यूरॉन i का या यूनिट i का लेयर जे में.

105
00:07:56,810 --> 00:08:01,530
तो पूरी तरह से यह देता है सूपरस्क्रिप्ट एक सब ग्रूप एक को,

106
00:08:01,530 --> 00:08:07,180
वह है ऐक्टिवेशन पहली यूनिट का लेयर दो में, हमारी हिडन लेयर में.

107
00:08:07,180 --> 00:08:11,280
और ऐक्टिवेशन से मेरा सिर्फ़ मतलब है वैल्यू जो कम्प्यूट कि जाती है और

108
00:08:11,280 --> 00:08:13,260
आउट्पुट की जाती है एक विशेष यूनिट से.

109
00:08:13,260 --> 00:08:18,010
इसके अलावा, न्यूरल नेटवर्क को पेरामिटर दिए गए हैं इन मेट्रिसीज़ से, थीटा

110
00:08:18,010 --> 00:08:22,720
सूपरस्क्रिप्ट j, जहाँ थीटा j होगा एक मेट्रिक्स वेट्स की जो नियंत्रित करता है

111
00:08:22,720 --> 00:08:27,070
फ़ंक्शन मैपिंग एक लेयर से, मान लो पहली लेयर से दूसरी लेयर तक,

112
00:08:27,070 --> 00:08:28,920
या दूसरी लेयर से तीसरी लेयर तक.

113
00:08:30,020 --> 00:08:33,360
तो यहाँ हैं कॉम्प्यूटेशन्स जो दर्शाई गई इस चित्र के द्वारा.

114
00:08:34,540 --> 00:08:39,442
इस पहली हिडन यूनिट की वैल्यू कम्प्यूट की जाती है निम्न तरह से,

115
00:08:39,442 --> 00:08:44,940
यहाँ है एक a 21 जो बराबर है सिग्मोईड फ़ंक्शन के, सिग्मोईड ऐक्टिवेशन फ़ंक्शन के,

116
00:08:44,940 --> 00:08:47,800
जिसे लॉजिस्टिक ऐक्टिवेशन फ़ंक्शन भी कहते हैं,

117
00:08:47,800 --> 00:08:53,780
अप्लाई किया हुआ इस तरह के लिनीअर संयोजन को इन इन्पुट्स के.

118
00:08:53,780 --> 00:08:58,540
और फिर इस दूसरी हिडन यूनिट का है यह ऐक्टिवेशन

119
00:08:58,540 --> 00:09:01,793
वैल्यू कम्प्यूट की गई है इसके सिग्मोईड की तरह.

120
00:09:01,793 --> 00:09:08,037
और इसी तरह इस तीसरी हिडन यूनिट कम्प्यूट की जाती है उस फ़ॉर्म्युला से.

121
00:09:08,037 --> 00:09:13,857
तो यहाँ हमारे पास है 3 थीटा 1 जो है एक मेट्रिक्स

122
00:09:13,857 --> 00:09:19,167
पेरमिटर्स की संचालित करते हुए हमारी मैपिंग

123
00:09:19,167 --> 00:09:26,719
हमारी तीन विभिन्न यूनिट्स से, हमारी हिडन यूनिट्स तक.

124
00:09:26,719 --> 00:09:29,958
थीटा 1 होगी एक 3.

125
00:09:35,213 --> 00:09:41,761
थीटा 1 होगी एक 3x4-डिमेन्शनल मेट्रिक्स.

126
00:09:41,761 --> 00:09:46,777
और अधिक सामान्य रूप में, यदि एक नेटवर्क में हैं sj यूनिट्स लेयर j में

127
00:09:46,777 --> 00:09:50,686
s(j + 1) यूनिट्स लेयर j + 1 में, तब मेट्रिक्स थीटा j

128
00:09:50,686 --> 00:09:55,428
जो संचालित करती हैं फ़ंक्शन मैपिंग लेयर j से j +1 तक.

129
00:09:55,428 --> 00:10:00,105
जो मुझे बताना पड़ेगा sj + 1, sj +1 से मैं सिर्फ़ स्पष्ट करूँगा

130
00:10:00,105 --> 00:10:02,318
यह नोटेशन सही करने के लिए.

131
00:10:02,318 --> 00:10:06,649
यह है सबस्क्रिप्ट j + 1 और वह है सबस्क्रिप्ट j, और

132
00:10:06,649 --> 00:10:12,315
फिर यह पूरी चीज़, प्लस 1, यह पूरी चीज़ (sj +1), ठीक है?

133
00:10:12,315 --> 00:10:16,784
तो वह है सबस्क्रिप्ट j + 1,

134
00:10:21,847 --> 00:10:26,851
तो वह है सबस्क्रिप्ट j + 1, sj +1 के साथ जहाँ

135
00:10:26,851 --> 00:10:31,736
यह प्लस नहीं है हिस्सा सबस्क्रिप्ट का.

136
00:10:31,736 --> 00:10:36,508
ठीक है, तो हमने बात की कि ये तीन हिडन यूनिट्स क्या करती हैं कम्प्यूट करने के लिए उनकी वैल्यूज़.

137
00:10:36,508 --> 00:10:40,252
अंत में, मैपिंग है इस अंतिम लेयर को और

138
00:10:40,252 --> 00:10:45,036
उसके बाद हमारे पास है एक और यूनिट जो कम्प्यूट करती हैं h ऑफ़ x और

139
00:10:45,036 --> 00:10:51,488
वह है बराबर लिखी जा सकती है a(3)1 जैसे भी और वह है बराबर इसके.

140
00:10:51,488 --> 00:10:56,105
और आप ध्यान दें कि मैंने लिखी है सूपरस्क्रिप्ट दो यहाँ,

141
00:10:56,105 --> 00:11:00,560
क्योंक थीटा सूपरस्क्रिप्ट दो का है मेट्रिक्स पेरमिटर्स की, या

142
00:11:00,560 --> 00:11:06,149
मेट्रिक्स वेट्स की जो नियंत्रित करती है फ़ंक्शन जो मैप करता है हिडन यूनिट्स से,

143
00:11:06,149 --> 00:11:12,330
जो है लेयर दो की यूनिट्स से लेयर तीन की एक यूनिट को, जो है आउट्पुट यूनिट.

144
00:11:12,330 --> 00:11:17,340
सारांश में, हमने क्या किया है कि दिखाया है कैसे एक चित्र इस तरह का यहाँ परिभाषित करता हैं

145
00:11:17,340 --> 00:11:22,020
एक कृत्रिम न्यूरल नेटवर्क को जो परिभाषित करता है एक फ़ंक्शन h

146
00:11:22,020 --> 00:11:27,520
मैप करता है x की इनपुट वैल्यूज़ उस स्पेस जो देती है y.

147
00:11:27,520 --> 00:11:31,480
और ये हायपॉथिसस पेरामिटर की गई हैं पेरमिटर्स से

148
00:11:31,480 --> 00:11:36,240
जो डिनोट किए गए है एक कैपिटल थीटा से ताकि जैसे हम बदलाव करते हैं थीटा में,

149
00:11:36,240 --> 00:11:38,805
हमें मिलती हैं विभिन्न हायपॉथिसस और हमें मिलते हैं विभिन्न हायपॉथिसस फ़ंक्शंज़.

150
00:11:38,805 --> 00:11:40,630
मैप करते हुए मान लो x से y तक.

151
00:11:42,590 --> 00:11:47,418
तो यह देता है हमें गणितीय परिभाषा कि कैसे दर्शाते हैं हायपॉथिसस

152
00:11:47,418 --> 00:11:48,722
न्यूरल नेटवर्क में.

153
00:11:48,722 --> 00:11:53,178
अगले कुछ वीडियो में, मैं आप को एक बेहतर अनुभव देने की कोशिश करूँगा

154
00:11:53,178 --> 00:11:58,132
क्या ये ह्यपोथेसीस रेप्रेज़ेंटेशन करती हैं, तथा करेंगे कुछ उदाहरण और

155
00:11:58,132 --> 00:12:00,850
बात करेंगे कैसे कम्प्यूट करना है उन्हें कुशलता से.