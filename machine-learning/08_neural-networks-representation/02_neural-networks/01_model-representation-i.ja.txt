このビデオでは、 ニューラルネットワークを どう表現するかについて話す。 言い換えると、我らの仮説を どう表現するか、または、 ニューラルネットワークを使う時はどうモデルを表現するか、という事。 ニューラルネットワークは脳内のニューロン、または ニューロンのネットワークをシミュレートする事で発展した。 だから仮説の表現を 説明するために、 脳の中のニューロンを一つ取り出すと どんな感じか見てみよう。 あなたや私の脳は、 こんなニューロンがたくさんごちゃごちゃ詰まっている。 ニューロンとは脳の細胞で 特徴的なのは 2つある。 一つ目は ニューロンには細胞体があり、 ーこんな感じのー で、さらに、 ニューロンは幾つかの 入力のワイヤがある事で、 これらはdendriteと呼ばれていて、 それは入力のワイヤと考えられる。 そしてこれらが他の場所からの 入力を受け取る。 そしてニューロンは出力ワイヤも持っていて、それはaxonと呼ばれる。 そしてこの出力ワイヤは 他のニューロンに シグナルを送るのに、 言い換えると他のニューロンにメッセージを送るのに使われる。 つまり一番単純なレベルでは、 ニューロンとは何かというと、 たくさんの入力を入力ワイヤから受け取り なんらかの計算を行い、その出力を axonを通して脳内の他のニューロンに送る 計算ユニットと 考えることが出来る。 これはニューロンのグループのイラストだ。 ニューロンがお互いにコミュニケートする方法は 僅かな電気信号によってだ。 それらはスパイクとも呼ばれている。
だけどそれらは単なる小さな電気を意味するに過ぎない。 ここに一つニューロンがあり、 それがする事といえば、 もしメッセージを送りたい時は axonを通して、別のニューロンに わずかな電気のパルスを 送る。 そしてこれがaxonだ。 この出力ワイヤがあって、 それがこうして、二番目のニューロンの 入力ワイヤ、またの名をdendriteに つながっている。 そしてそこからこの信号に入力のメッセージとして受け取って、 なんらかの計算を行い、 またさらに他のニューロンへ 出力メッセージを axonを通して送るかもしれない。 以上が、全ての人類が 考えた時に起こるプロセスで、 これらのニューロンが計算をして、 メッセージを他のニューロンに 送る。 与えられた入力に対する結果として。 ところで、これはまた、 我らの感覚や筋肉が機能する方法でもある。 もし筋肉の一つを 動かそうとすれば、 それが実現されるのは、 ニューロンが筋肉に 電気のパルスを送り、 それが筋肉を収縮させたり、 目の場合、 何らかのセンサー、例えば目とかの場合、 脳にメッセージを送るには 電気のパルスを 脳にあるニューロンに 送るという手段を通してだ。 ニューラルネットワークにおいては、 いや我らがコンピュータで実装する 人工的なニューラルネットワークにおいては、と言うべきか、 その場合、ニューロンがやってる事の とてもシンプルなモデルを 使う事になる。 ニューロンを単なるロジスティックの単位としてモデル化する。 だから黄色で円を こんな感じで描いたら、 これはニューロンの本体みたいな役割をしている、と 考えてくれ。 そしてそこに、 いくつかの入力を dendritesまたの名を入力ワイヤを 通して食わす。
するとニューロンはなんらかの計算行い、 この出力ワイヤから なんらかの値を出力する。 生物的なニューロンなら それはaxonに相当する。 そしてこんなダイアグラムを描いた時はいつでも、 これの意味する所は、 h(x)の計算であり、それは 1足すeのマイナス シータ転置x 分の1 で、 xとシータはいつも通り、 パラメータベクトルとかを 表している。 つまりこれはとても単純化した、 ちょっとあまりにも単純化しすぎた感じの ニューロンが行なっている事のモデルで いくつかの入力、 x1、x2、x3を受け取り そんな風に計算された何らか値を出力する、という。 ニューラルネットワークを書く時は 普通は入力のノードだけを x1、x2、x3と描くのだが、 たまに、そちらの方が便利な時に限り 追加のノード、x0を描く事もある。 このx0ノードは バイアスユニットとかバイアスニューロンと 呼ばれることがある。 だがx0は1と決まっているので、 このノードは描いたり描かなかったりする。 それは単に、その例にとって 記しておいた方が便利かどうかで決めてる。 最後に、もう一つだけ 用語を導入しておく。 ニューラルネットワークについて話してる時は、 このニューロン、 人工的なニューロンの sigmoid関数またはロジスティック関数を、 アクティベーション関数と呼ぶ事がある。 このアクティベーション関数というのはニューラルネットワークの用語で これはこの非線形の g(z)イコール 1足すe の-z 分の1の、 もう一つの呼び名に 過ぎない。 一方でここまでは シータをモデルのパラメータと 呼んできたし、今後もだいたいは その用語を使い続けるが、 ニューラルネットワークでは ニューラルネットワークの文献では 人々はたまに モデルのウェイトと呼んでいるのを 見かけるかもしれない。
このウェイトというのは このモデルのパラメータと 完全に同じ意味だ。 このクラスのビデオではだいたい パラメータという用語を使うが、 たまに他の人がウェイトって用語を使ってるのを聞くことがあるかもしれない。 このちっぽけなダイアグラムは 単体のニューロンを表している。 ニューラルネットワークとは 単にこれらのニューロンが幾つか集まった グループの事だ。 具体的には、ここに我らの 入力単位、x1、x2、x3があり、 繰り返しになるが たまにこの追加の x0のノードを描いたり描かなかったりする。 今回はここにそれを描いとく。 そしてここに、我らは3つのニューロンを 持っている。 それをa(2)1、a(2)2、a(2)3と書いた。 上と下の添字は後ほど。 そして繰り返すが、 もし必要なら このa0という追加の バイアスユニットをここに足す事もある。 それはいつでも値1を出力する。 そして最後に、この三番目の ノードが最後のレイヤーにある。 そしてこの 三番目のノードが仮説である h(x)の計算結果を出力する。 ニューラルネットワークの用語を もうちょっと導入しておく。 この最初のレイヤーは 入力レイヤーとも呼ばれる。 何故ならこれが 我らのフィーチャーであるx1、x2、x3をインプットする所だから。 最後のレイヤーは 出力レイヤーとも呼ばれる。 何故ならこのレイヤーが ここにあるニューロンこそが、 仮説による最終的な計算結果を 出力するから。 そして2つの間にあるレイヤーを 隠れたレイヤー（hidden layer）と呼ぶ。 隠れたレイヤーという用語は そんな良い用語とは思わないが、 直感的には 教師あり学習では、 入力と正解の出力は見れる訳だが 隠れたレイヤーはトレーニングセットでは 観測出来ない値だ。 だからxでもyでも無い物は、隠れたレイヤーと呼んでいる。 そしてのちほど、一つよりも多い隠れたレイヤーの 例を見ていく。 だがこの例では 入力レイヤーであるレイヤー1が一つに、 一つの隠れたレイヤーのレイヤー2に、 出力レイヤーのレイヤー3がある。 だが基本的には入力レイヤーでなく、 そして出力レイヤーでも無い物は なんでも隠れたレイヤーと呼ぶ。 さて、私は このニューラルネットワークが 何をするのかをとっても分かりやすくしたい。 これに埋め込まれた計算過程を 順番に見ていこう。 このダイアグラムで表現されている物の。 ニューラルネットワークで表現されている 特定の計算を説明する為に、 もうちょっと記法を追加しておく。 上付き添字のjと 下付き添字のiを レイヤーjにある ニューロンiまたはユニットiのアクティベーションを 示すのに使う。
具体的には、 これは上付き添字2の下付き添字1で これは二番目のレイヤーの つまり隠れたレイヤーの 最初のユニットだ。 そしてアクティベーションという言葉で、 特定のニューロンからの 計算結果の値、つまり出力の値を指す。 さらに、我らのニューラルネットワークは これらの行列、シータの上付き添字jで パラメータ化される。ここで シータjは一つのレイヤーから、、、 例えば最初のレイヤーから二番目のレイヤーへと、とか、 二番目のレイヤーから三番目のレイヤーへと、などを マッピングする関数を制御する ウェイトとなる行列だ。 つまり、これがこのダイアグラムで表現される計算だ。 ここにある、最初の隠れたユニットは 以下のように計算された値だ: a(2)1 イコール sigmoid関数、または sigmoidアクティベーション関数または ロジスティックアクティベーション関数と呼ばれるが、 それがこんな形の 入力の線形の組み合わせに適用される。 そしてこの二番目の隠れたユニットは このsigmoid関数で 計算される、このアクティベーションの値だ。 以下同様に、この三番目の隠れたユニットは この式で計算される。 つまりここでは、3つの入力ユニットと 3つの隠れユニットがある。 つまりシータ1の次元は それはパラメータの行列で 3つの入力ユニットと 3つの隠れユニットを合わせた物だから、 つまりシータ1は 3、、、 シータ1は つまり 3x4次元の行列だ。 より一般的には ネットワークがレイヤーjに sのjだけのユニットを、 j+1番目のレイヤーに sのj+1個のユニットがあるとすると、 行列であるところのシータjは レイヤーjからレイヤーj+1の マッピングの関数を決定する訳だが、 その次元は、 sのj+1 掛ける sのj足す1となる。 ノーテーションを明確にしておこう。 これはsの下付き添字でj+1。 そっちはsの下付き添字でj、 そしてこの全体に、 1を足している。 これ全体に対して。それがs jに1を足す、という事。オーケー？ つまりsの下付き添字j+1に 足す事の、、、 じゃなかった、掛けるだ。えーと、 sのj+1に、 掛ける事のsのjに 足すことの1、 最後の足す1は添字じゃないよ。 さて、3つの隠れユニットがどんな計算をするかを 話してきた訳だが、 最後にこの最後の、出力レイヤー、 我らはもう一つレイヤーを持ってた訳だが、 このレイヤーはh(x)を計算する。 それはイコール、、、 ところでそれは、a(3)の1と書けて、 それはイコール、これとなる。 そして気づいたかもしれないが 私は上付き添字で2とここでは書いた。 その理由は、シータの上付き添字の2は パラメータの行列、 またの名をウェイトの行列で それは隠れユニット、 それはレイヤー2のユニットの事だが、 それとレイヤー3のユニットを それは出力ユニットだが、 それをマップする関数を制御する。 。 まとめると、我らがやった事は ここにあるような絵が どう人工的なニューラルネットワークを 定義するかを見た。 それは入力値のxを 願わくばあるyの予測値に マップする関数を定義する。 そしてこれらの仮説はパラメータで パラメトライズされていて、 そのパラメータは大文字のシータで示している。 こうする事で シータを変える事で異なる仮説が得られる訳だ。 つまりxからyへとマップする、 別の関数が得られるって事だ。 そして、これは仮説をニューラルネットワークで 表す方法の 数学による定義を与えてくれる。 以後の一連のビデオでは これらの仮説の表現が 何をするかの 直感を伝えたいと思う。 それと同時に、 例を幾つか見て、それらをどう効率的に計算するかもお話する。