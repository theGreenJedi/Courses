पिछले विडीओ में, हमने दी एक गणितीय परिभाषा कि कैसे रेप्रेज़ेंट करते हैं या कैसे कम्प्यूट करते हैं हायपाथिसस न्यूरल नेटवर्क की. इस विडीओ में, मैं चाहता हूँ दिखाना आपको कि कैसे वास्तव में करते हैं कम्प्यूट कुशलता से, और बताना आपको कि कैसे लिखते हैं एक वेक्टराइज्ड इम्प्लमेंटेशन, और दूसरा, और अधिक महत्वपूर्ण बात, मैं चाहता हूँ देना शुरू करना आपको अनुभव कि क्यों ये न्यूरल नेटवर्क रेप्रेज़ेंटेशन्स एक अच्छा विचार हो सकता है और कैसे वे हमारी मदद कर सकते हैं लर्न करने में जटिल नॉन-लिनीअर हायपॉथिसस. लें ये न्यूरल नेटवर्क. इससे पहले हमने कहा था कि क्रम सटेप्स का जो हमें चाहिए करने के लिए कम्प्यूट एक हायपॉथिसस की आउट्पुट हैं ये इक्वेज़न दी हुईं बाईं तरफ़ जहाँ हम कम्प्यूट करते हैं ऐक्टिवेशन वैल्यूज़ तीन हिडन नोड्ज़ की और फिर हम इस्तेमाल करते हैं उन्हें कम्प्यूट करने के लिए आउट्पुट हमारी हायपॉथिसस h ऑफ़ x की. अब मैं करूँगा परिभाषित कुछ अतिरिक्त टर्म्ज़. तो, यह टर्म यहाँ जिसे मैं रेखांकित कर रहा हूँ, मैं करूँगा परिभाषित उसे z सूपरस्क्रिप्ट 2 सबस्क्रिप्ट 1. ताकि हमारे पास है वह a(2)1, जो है यह टर्म है बराबर g ऑफ़ z की पॉवर 1. और वैसे तो, ये सूपरस्क्रिप्ट 2, आप जानते हैं, उसका क्या मतलब है कि z2 और यह a2 भी, सूपरस्क्रिप्ट 2 कोष्ठकों में का मतलब है कि ये हैं वैल्यूज़ संलग्न लेयर 2 से, जो है हिडन लेयर न्यूरल नेटवर्क में. अब यह टर्म यहाँ मैं इसी प्रकार परिभाषित करूँगा z(2)2. और अंत में, यह आख़िरी टर्म जो मैं रेखांकित कर रहा हूँ, मैं उसे परिभाषित करता हूँ z(2)3 की तरह. तो इसी तरह से हमारे पास है a(2)3 बराबर g ऑफ़ z(2)3. तो ये z वैल्यूज़ हैं सिर्फ़ एक लिनीअर संयोजन, एक वेटेड एक लिनीअर संयोजन, इनपुट वैल्यूज़ x0, x1, x2, x3 का, जो जाती हैं एक विशेष न्यूरॉन में. अब, अगर आप देखें नम्बर्ज़ के इस ब्लॉक को, आप शायद ध्यान करेंगे कि वह ब्लॉक नम्बर्ज़ का कॉरेस्पॉंड करता है समान रूप से मेट्रिक्स वेक्टर ऑपरेशन से, मेट्रिक्स वेक्टर गुणन से जो है x1 गुणा वेक्टर x. इस को इस्तेमाल करते हुए इस बात का हम कर पाएँगे वेक्टराइज़ इस कॉम्प्यूटेशन को न्यूरल नेटवर्क की. वास्तव में चलो करते हैं परिभाषित फ़ीचर वेक्टर x हमेशा की तरह वेक्टर x0, x1, x2, x3 का जहाँ x0 है हमेशा की तरह हमेशा बराबर है 1 और वह परिभाषित करता है z2 को वेक्टर इन z वैल्यूज़ का, आप जानते हैं z(2)1, z(2)2, z(2)3 ka. और ध्यान दें कि, वहाँ z2, यह एक तीन-डिमेन्शनल वेक्टर. अब हम कर सकते हैं वेक्टराइज़ इस कॉम्प्यूटेशन को a(2)1, a(2)2, a(2)3 निम्न तरह से. हम लिख सकते हैं इसे सिर्फ़ दो सटेप्स में. हम कर सकते हैं कम्प्यूट z2 को थीटा 1 गुणा x की तरह जो देगा हमें यह वेक्टर z2; और फिर a2 है g z2 का और सिर्फ़ स्पष्ट करने के लिए z2 यहाँ, यह एक तीन-डिमेन्शनल वेक्टर और a2 भी है एक तीन-डिमेन्शनल वेक्टर और इसलिए यह ऐक्टिवेशन g. यह अप्लाई करता है सिग्मोईड फ़ंक्शन एलिमेंट-वाइज़ प्रत्येक z2 के एलिमेंट्स को. और वैसे तो, बनाने के हमारी नोटेशन थोड़ी और समान उससे जो हम बाद में करेंगे, इस इनपुट लेयर में हमारे पास हैं इन्पुट्स x, लेकिन हम सोच सकते हैं उसे ऐक्टिवेशन पहली लेयर में की तरह भी. तो, यदि मैं परिभाषित करता हूँ a1 को बराबर x के. तो, a1 है वेक्टर, मैं अब ले सकता हूँ यह x यहाँ और इसके स्थान पर रख सकता हूँ z2 बराबर थीटा1 गुणा a1 सिर्फ़ परिभाषित करके a1 को ऐक्टिवेशन मेरी इनपुट लेयर में. अब, मैंने जो लिखा है अभी तक मुझे मिली हैं वैल्यूज़ a1, a2, a3 और वास्तव मैं मुझे लगाना चाहिए सूपरस्क्रिप्ट वहाँ भी. लेकिन मुझे चाहिए एक और वैल्यू, जो है कि मुझे चाहिए यह a(0)2 भी. और वह कॉरेस्पॉंड करता है एक बाइयस यूनिट को हिडन लेयर में जो जाता है आउट्पुट में वहाँ. ज़ाहिर है, था एक बाइयस यूनिट यहाँ भी जो, आप जानते हैं, मैंने केवल बनाया नहीं था यहाँ पर लेकिन ध्यान रखनाए के लिए इस अतिरिक्त बाइयस यूनिट का, हम क्या करंगे कि जोड़ेंगे एक अतिरिक्त a0 सूपरस्क्रिप्ट 2, जो है बराबर एक, और बाद में इस सटेप के हमारे पास अब है वह a2 होगा एक चार डिमेन्शनल / आयामों वाला फ़ीचर वेक्टर क्योंकि हमने अभी जोड़ा है यह अतिरिक्त, आप जानते हैं, a0 जो है बराबर 1 कॉरेस्पॉंड करता है बाइयस यूनिट को हिडन लेयर में. और अंत में, कम्प्यूट करने के लिए वास्तविक वैल्यू आउट्पुट हमारी हायपॉथिसस की, हमें तब बस ज़रूरत है कम्प्यूट करने की z3. तो z3 है बराबर इस टर्म के यहाँ जो मैं रेखांकित कर रहा हूँ, यह टर्म यहाँ है z3. और z3 को कहा गया है 2 गुणा a2 और अंत में मेरी हायपॉथिसस आउट्पुट h ऑफ़ x जो है a3 मतलब ऐक्टिवेशन मेरी एक और सिर्फ़ एक यूनिट का आउट्पुट लेयर में. तो, वह है केवल एक रियल नम्बर. आप लिख सकते हैं इसे a3 की तरह यह a(3)1 और वह है g z3 का. यह प्रक्रिया कम्प्यूट करने की h ऑफ़ x फ़ॉर्वर्ड प्रॉपगेशन भी कहलाती है और इसे वह कहते हैं क्योंकि हम शुरू करते हैं ऐक्टिवेशन्स से इनपुट यूनिट्स के और फिर एक तरह से फ़ॉर्वर्ड प्रापगेट करते हैं उसे हिडन लेयर तक और कम्प्यूट करते हैं ऐक्टिवेशन्स हिडन लेयर के और फिर हम एक तरह से फ़ॉर्वर्ड प्रापगेट करते हैं उसे और कम्प्यूट करते हैं ऐक्टिवेशन्स आउट्पुट लेयर के, लेकिन यह प्रक्रिया कम्प्यूट करने की ऐक्टिवेशन्स इनपुट से फिर हिडन फिर आउट्पुट लेयर, और वह भी फ़ॉर्वर्ड प्रॉपगेशन भी कहलाती है और हमने जो अभी किया है हमने अभी बनाया है एक वेक्टर वाइज़ इम्प्लमेंटेशन इस प्रक्रिया का. तो, यदि आप इम्प्लमेंट करते हैं इसे इस्तेमाल करके इन इक्वेज़ंज़ को जो हमारे पास हैं दाईं तरफ़, ये देंगी आपको एक कुशल ढंग या दोनो एक कुशल ढंग कम्प्यूट करने का h ऑफ़ x. यह फ़ॉर्वर्ड प्रॉपगेशन दृष्टि कोण सहायता करता है हमें समझने में कि क्या न्यूरल नेटवर्क कर रहा हो सकता हैं और क्यों वे हमारी मदद कर सकते हैं लर्न करने में दिलचस्प नॉन-लिनीअर हायपॉथिसस. लें ये निम्न न्यूरल नेटवर्क और मान लो ढक देता हूँ बाईं तरफ़ का हिस्सा इस चित्र का अभी के लिए. यदि आप देखते हैं क्या बचा है इस चित्र में. और यह दिखता है बहुत कुछ जैसे लॉजिस्टिक रिग्रेशन जहाँ क्या कर रहे हैं हम कि हम इस्तेमाल कर रहे हैं वह नोड, वह है केवल लॉजिस्टिक रिग्रेशन यूनिट और हम कर रहे हैं इस्तेमाल उसका करने के लिए एक प्रिडिक्शन h ऑफ़ x. और वस्तुत:, हायपॉथिसस क्या आउट्पुट कर रही है कि h ऑफ़ x होगा बराबर g को जो है मेरा सिग्मोईड ऐक्टिवेशन फ़ंक्शन गुणा थीटा 0 गुणा a0 है बराबर 1 जमा थीटा 1 जमा थीटा 2 गुणा a2 जमा थीटा 3 गुणा a3 जहाँ वैल्यूज़ a1, a2, a3 हैं वे जो दी गई हैं इन तीन दी हुई यूनिट्स से. अब, वास्तव में समान करने के लिए हमारी पहले की नोटेशन से. असल में हमें चाहिए, आप जानते हैं, भरनी ये सूपरस्क्रिप्ट्स 2 यहाँ हर जगह और मेरे पास हैं ये इंडिसीज़ 1 भी वहाँ क्योंकि मेरे पास है केवल एक आउट्पुट यूनिट, लेकिन यदि आप ध्यान करें नीले हिस्सों पर नोटेशन के. यह है, आप जानते हैं, यह दिखता है काफ़ी कुछ जैसे स्टैंडर्ड लॉजिस्टिक रेग्रेशन मॉडल है, सिवाय कि अब मेरे पास है एक कैपिटल थीटा बजाय लोअर केस थीटा के. और यह क्या कर रहा है सिर्फ़ लॉजिस्टिक रिग्रेशन. लेकिन जहाँ फ़ीचर्ज़ फ़ीड किए जाते हैं लॉजिस्टिक रिग्रेशन में हैं ये वैल्यूज़ कम्प्यूट की गई हिडन लेयर द्वारा. सिर्फ़ फिर से बताने के लिएउसे फिर से, क्या न्यूरल नेटवर्क कर रहा है कि लॉजिस्टिक रिग्रेशन के समान, सिवाय कि बजाय इस्तेमाल करने के प्रारम्भिक फ़ीचर्ज़ x1, x2, x3, यह इस्तेमाल कर रहा है नए फ़ीचर्ज़ a1, a2, a3. फिर से, हम डालेंगे सूपरस्क्रिप्ट्स वहाँ, आप जानते हैं, नोटेशन के हिसाब से. और अच्छी बात इस बारे में, है कि फ़ीचर्ज़ a1, a2, a3, वे ख़ुद भी लर्न किए हुए हैं इनपुट के फ़ंक्शन से. वास्तव में, फ़ंक्शन मैपिंग लेयर 1 से लेयर 2 तक, वह निर्धारित की जाती है किसी सेट से पेरमिटर्स थीटा 1 के. तो, यह है जैसे कि न्यूरल नेटवर्क, बजाय होने के विवश फ़ीड करने के लिए फ़ीचर्ज़ x1, x2, x3 लॉजिस्टिक रेग्रेशन को. यह कर पाता है लर्न इसके अपने फ़ीचर्ज़, a1, a2, a3 फ़ीड करने के लिए लॉजिस्टिक रिग्रेशन में और जैसे कि आप कल्पना कर सकते हैं निर्भर करते हुए क्या पेरमिटर्स यह लेता है थीटा 1 के लिए. आप लर्न कर सकते हैं कुछ बहुत दिलचस्प जटिल फ़ीचर्ज़ और इसलिए आपको मिल सकती है एक बेहतर हायपॉथिसस तुलना में यदि आप मजबूर होते इस्तेमाल करने के लिए मूल फ़ीचर्ज़ x1, x2 या x3 या यदि आप मजबूर होते चुनने के लिए पालिनोमीयल टर्म्ज़, आप जानते हैं, x1, x2, x3 और इसी प्रकार आगे. लेकिन इसके बजाय, इस अल्गोरिद्म में है सुविधा प्रयास करने की लर्न करने के लिए जो भी फ़ीचर्ज़ एक बार में, इस्तेमाल करके ये a1, a2, a3 फ़ीड करने के लिए इस अंतिम यूनिट में जो है अनिवार्यत: लॉजिस्टिक रिग्रेशन यहाँ. मुझे समझ में आया कि यह उदाहरण यहाँ वर्णित किया है एक कुछ ऊँचे स्तर पर और इसलिए मुझे निश्चित नहीं है यदि यह अनुभव न्यूरल नेटवर्क का, आप जानते हैं, होना अधिक जटिल फ़ीचर्ज़ का कुछ अभी समझ आया होगा, लेकिन यदि अभी नहीं आया है अगले दो वीडियो में मैं करूँगा विस्तार से एक विशेष उदाहरण कि कैसे एक न्यूरल नेटवर्क हो सकता है इस्तेमाल इस हिडन लेयर का वहाँ कम्प्यूट करने के लिए अधिक जटिल फ़ीचर्ज़ फ़ीड करने के लिए इस अंतिम आउट्पुट लेयर में और कैसे वह लर्न कर सकता है अधिक जटिल हायपॉथिसस. तो, यदि जो मैं कह रहा हूँ यहाँ नहीं कुछ समझ आ रहा, बने रहें मेरे साथ अगले दो वीडियो तक और उम्मीद है वहाँ कर लेने से वे उदाहरण यह विवरण थोड़ा और समझ आएगा. लेकिन सिर्फ़ इशारा करने के लिए, आपको मिल सकते हैं न्यूरल नेटवर्क्स दूसरे तरह के चित्रों से भी, और जिस तरह से न्यूरल नेटवर्क जोड़े जाते हैं, उसे कहते हैं आर्किटेक्चर. तो टर्म आर्किटेक्चर बताती है कि कैसे विभिन्न न्यूरांस एक-दूसरे से जोड़े जाते हैं. यह है एक उदाहरण एक अलग न्यूरल नेटवर्क आर्किटेक्चर का और एक बार फिर आपको शायद मिल सकता है यह अनुभव कि कैसे दूसरी लेयर, यहाँ हमारे पास है तीन हिडन यूनिट्स जो कम्प्यूट कर रही हैं कुछ जटिल फ़ंक्शन शायद इनपुट लेयर का और फ़िर तीसरी लेयर ले सकती है दूसरी लेयर के फ़ीचर्ज़ और कम्प्यूट कर सकती है और अधिक जटिल फ़ीचर्ज़ लेयर तीन में ताकि जब तक हम पहुँचते हैं आउट्पुट लेयर तक, लेयर चार, आप के पास हैं और भी अधिक जटिल फ़ीचर्ज़ जो कम्प्यूट कर सकते हैं लेयर तीन में और पा सकते हैं बहुत दिलचस्प नॉन-लिनीअर हायपॉथिसस. वैसे तो, एक नेटवर्क में इस तरह के, लेयर एक, इसे कहते हैं इनपुट लेयर, लेयर चार अभी भी है आउट्पुट लेयर, और इस नेटवर्क में हैं दो हिडन लेयर्स. तो वह सब कुछ जो नहीं है एक इनपुट लेयर या एक आउट्पुट लेयर कहलाता है एक हिडन लेयर. तो, उम्मीद है इस वीडियो से आपको समझ आया होगा कि कैसे फ़ीड फ़ॉर्वर्ड प्रॉपगेशन स्टेप एक न्यूरल नेटवर्क में काम करता है आप शुरू करते हैं ऐक्टिवेशन्स से इन्पुट लेयर के और फ़ॉर्वर्ड प्रापगेट करते हैं उसे पहली हिडन लेयर को, और फिर दूसरी हिडन लेयर को, और फिर अंत में आउट्पुट लेयर को. और आपने देखा कैसे हम कर सकते हैं वेक्टराइज़ उस कॉम्प्यूटेशन को. अगले में, मैंने महसूस किया कि कुछ अनुभव इस वीडियो में कैसे, आप जानते हैं, अन्य कुछ लेयर्स कम्प्यूट कर रही हैं जटिल फ़ीचर्ज़ पिछली लेयर्स के. मुझे लगता है कि कुछ हिस्सा उस अनुभव का शायद अभी भी थोड़ा काल्पनिक और ऊँचे स्तर का हो सकता है. और इसलिए मैं क्या चाहता हूँ करना दो वीडियो में कि हल करूँ विस्तार से एक उदाहरण कि कैसे एक न्यूरल नेटवर्क हो सकता है इस्तेमाल कम्प्यूट करने के लिए नॉन-लिनीअर फ़ंक्शन इनपुट का और मैं आशा करता हूँ कि वह देगा आपको एक बेहतर समझ इस तरह के जटिल नॉनलिनीअर हायपाथिसस की जो हमें मिल सकती है न्यूरल नेटवर्क से.