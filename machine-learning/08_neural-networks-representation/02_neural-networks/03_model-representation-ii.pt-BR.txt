No último vídeo, nós vimos uma definição matemática de como representar, ou de como computar a hipótese usada
pela Rede Neural. Neste vídeo, gostaria de mostrar exatamente como computar de maneira eficiente, usando uma implementação vetorizada. Mas, ainda mais importante, eu gostaria de te dar uma
intuição sobre porque representações de Redes Neurais podem ser uma boa ideia, e como elas podem nos ajudar a aprender
hipóteses não-lineares. Considere esta Rede Neural. Anteriormente foi dito que a sequência de passos necessários para computar a saída de uma hipótese são estas equações mostradas à esquerda, onde computamos os valores de ativação dos três neurônios da camada oculta, e então usamos estes para computar a saída para nossa hipótese h(x). Agora, eu vou definir alguns outros termos. O termo que estou sublinhando aqui, eu irei definir como z⁽²⁾₁. Então nós temos que? a⁽²⁾₁, que é este termo, é igual a g(z⁽²⁾₁). E aliás, o que estes sobrescritos (⁽²⁾), os sobrescritos em z⁽²⁾ e em a⁽²⁾, 2 entre parênteses sobrescrito significam que estes são valores associados à camada 2, ou seja, à camada oculta da Rede Neural. Já este termo aqui, eu irei definir como z⁽²⁾₂ . Por fim, este últmo termo, que estou sublinhando, será definido como z⁽²⁾₃ . Similarmente, temos " a⁽²⁾₃ =  g(z⁽²⁾₃ ) ". z(2)3. Assim, estes valores "z" são apenas uma combinação linear combinação linear ponderada, dos valores de entrada x₀ , x₁ , x₂, x₃ , que vão
em um neurônio, em particular. Agora se você olhar para este bloco de números, verá que este bloco é estranhamente similar à operação matriz vetor, à multiplicação matriz vetor de " x₁ " vezes o vetor "x". Usando esta observação seremos capazes de vetorizar o cálulo da Rede Neural. Na verdade, vamos definir o vetor de atributos "x" como sendo o vetor de x₀, x₁, x₂, x₃ , onde x₀  é igual a 1, e vamos definir z₂ como sendo o vetor de valores de "z", ou seja,
z⁽²⁾₁ , z⁽²⁾₂ , z⁽²⁾₃ . Note que z⁽²⁾  é um vetor tridimensional. Agora podemos vetorizar a
computação de a⁽²⁾₁ , a⁽²⁾₂ , a⁽²⁾₃ da seguinte forma: Escrevendo isso em 2 passos: Podemos computar z⁽²⁾ como "θ⁽¹⁾ · x", e isso nos dará o vetor z⁽²⁾; e então a⁽²⁾  é "g(z⁽²⁾)",  e para ficar mais claro, este z⁽²⁾ é um vetor tridimensional, assim como a⁽²⁾. E então, temos este "g" que aplica a função sigmoide para cada um dos elementos de z⁽²⁾. Aliás, para deixar a notação mais consistente com o que faremos mais tarde, na camada de entrada temos as entradas "x". Mas também podemos pensar nisto como as
ativações da primeira camada. Assim, se definirmos "a⁽¹⁾ = x", onde a⁽¹⁾ é um vetor, posso tomar este "x" e substituí-lo com "z⁽²⁾ = θ⁽¹⁾ · a⁽¹⁾ ", apenas definindo a⁽¹⁾ como ativações na camada de entrada. Assim, com o que escrevi até agora, eu tenho os valores de a⁽²⁾₁ , a⁽²⁾₂ , a⁽²⁾₃ . E, na verdade, eu deveria colocar os sobrescritos neles também. Mas preciso de mais um valor, que é este valor a⁽⁰⁾₂ que corresponde à unidade de bias na camada oculta que vai para a saída. É claro, existe uma unidade de bias aqui, ela só não foi desenhada. Mas para lidar com esta unidade de bias extra, o que faremos é adicionar um  a⁽²⁾₀  extra, que é igual a "1", e após este passo, temos que a⁽²⁾ será um vetor de atributos de quatro dimensões, porque nós adicionamos este a₀ extra que é igual a "1", e corresponde à unidade de bias na camada oculta. Finalmente, para se computar o valor real da saída de nossa hipótese, devemos simplesmente computar z⁽³⁾. Onde z⁽³⁾ é igual a este termo que estou sublinhando. Este termo interno é z⁽³⁾. E z⁽³⁾ é definido como "θ⁽²⁾ · a⁽²⁾", e finalmente a saída da minha hipótese h(x) é a⁽³⁾ , que é a ativação da minha única unidade na camada de saída.
Isso é apenas número real, você pode escrevê-lo como a⁽³⁾ ou a⁽³⁾₁ , que é g(z⁽³⁾). Este processo de computar h(x) é chamado de "propagação adiante"
(forward propagation), e é chamado assim porque começamos com as ativações das unidades de entrada, e então propagamos para a camada oculta.
Computamos as ativações da camada oculta, e então propagamos iss, e computamos as ativações da camada
de saída. Esse processo de computar as ativações da entrada, e da oculta, e então da saída, é chamado de propagação adiante, e o que fizemos foi apresentar a implementação vetorizada deste procedimento. Logo, se você implementar isso usando estas equações mostradas aqui à direita, isso te dará uma maneira eficiente de computar h(x). Essa visão de propagação adiante também nos ajudar a entender o que Redes Neurais podem fazer, e como podem nos ajudar a aprender hipóteses
não-lineares interessantes. Considere esta Rede Neural, e digamos que eu cubra a parte esquerda da figura, por enquanto. Se você olhar para o que restou da figura. Isso se parece com a Regressão Logística. Onde, o que estamos fazendo é usar este nó, que é apenas a unidade de Regressão Logística, e estamos usando ela para fazer uma predição de h(x). E, o que a hipótese está retornando é h(x), que será igual a "g", que é a minha função de
ativação sigmoide, vezes Θ₀ , vezes a₀, que é igual a "1", mais Θ₁ , mais Θ₂ vezes a₂ , mais Θ₃ vezes a₃ , onde os valores a₁, a₃ e a₃ são aqueles dados por estas três
unidades ocultas. Agora, para ser consistente com a notação anterior, na verdade, precisamos preencher com os sobrescritos 2 (⁽²⁾) aqui,
na verdade em todo lugar. E também tem esses indices 1 (₁) aqui, porque eu tenho apenas uma unidade de saída,
mas se você focar na parte azul na notação, isso é muito parecido com o modelo padrão de Regressão Logística, exceto que agora eu tenho Teta maiúsculo (Θ),
e não Teta minúsculo (θ). Mas o que isto está fazendo é apenas Regressão Logística. No entanto, os atributos que alimentão a Regressão Logística são estes valores computados pela camada oculta. E só para reforçar, o que esta Rede Neural está fazendo é muito parecido com
Regressão Logística, exceto que ao invés de usar os atributos originais x₁ , x₂ , x₃ , ela usa estes novos atributos a⁽²⁾₁ , a⁽²⁾₂  e a⁽²⁾₃. Novamente, vamos colocar os sobrescritos, para ser consistente com a notação. E a parte legal disso é que os atributos a⁽²⁾₁ , a⁽²⁾₂ a⁽²⁾₃  são, eles mesmos, aprendidos como funções da entrada. Na verdade, a função que mapeia a camada 1 para a camada 2, é determinada por um outro conjunto de parâmetros, Θ⁽¹⁾. Então, é como se a Rede Neural, ao invés de ser forçada a alimentar a Regressão Logística com os atributos x₁ , x₂ , x₃ , ela pode aprender seus próprios atributos, a⁽²⁾₁ , a⁽²⁾₂  e a⁽²⁾₃ , para alimentar a Regressão Logística. E, como você pode imaginar, dependendo de quais parâmetros ela escolhe para Θ₁ , você pode aprender alguns atributos bem interessantes e complexos, e assim você pode terminar com uma hipótese melhor do que se você estivesse restrito aos atributos brutos x₁ , x₂ , x₃ . Mesmo se você usasse os termos polinomiais x₁x₂, x₂x₃ , e assim por diante. Ao invés disso, este algoritmo tem a flexibilidade para tentar aprender quaisquer atributos, usando a⁽²⁾₁ , a⁽²⁾₂  e a⁽²⁾₃ para alimentar esta última unidade, que é essencialmente uma Regressão Logística. Eu sei que este exemplo foi descrito em alto nível, então, não sei se esta intuição de Redes Neurais, tendo atributos mais complexos faz muito sentido agora, mas caso não faça, nos próximos 2 vídeos eu darei um exemplo específico de como uma Rede Neural pode usar a camada oculta para computar atributos mais complexos, para alimentar a camada de saída. E, como isso pode aprender
hipóteses mais complexas. Então, caso o que eu disse não faça muito sentido, continue assistindo os próximos dois vídeos, e espero que então com estes exemplos, esta explicação fará mais sentido. E, só para mencionar. Podem existir Redes Neurais com outros tipos de diagrama, e a forma que as Redes Neurais são conectadas.
Isso é chamado de "arquitetura". Assim, o termo arquitetura se refere a como diferentes neurônios são
conectados entre si. É um exemplo de estudo criativo. de uma arquitetura diferente. E, mais uma vez, você pode usar a intuição de como a segunda camada, aqui nós temos três unidades que computam algumas funções, possivelmente complexas, da camada de entrada, e então a camada 3 pode tomar os atributos da 2, e computar atributos ainda mais complexos, tal que, quando se chega à camada de saída, camada 4, você pode ter atributos ainda mais complexos do que você é capaz de computar na camada 3. E assim, ter hipóteses
não-lineares bem interessantes. Aliás, em uma rede como esta, a camada 1, é chamada camada de entrada.
A camada 4 ainda é a de camada de saída, e esta rede tem duas camadas ocultas. Então, qualquer coisa que não é uma camada de entrada ou saída, é chamada de camada oculta. Espero que desse vídeo você tenha adquirido uma noção de como a etapa
de "feed forward propagation" de uma Rede Neural funciona. E de como ela começa com a ativação da camada de entrada, e propaga para a primeira camada oculta, para a segunda camada oculta,
e finalmente para a camada de saída. E você também viu como podemos vetorizar essa computação. Eu sei que parte da intuição deste vídeo, de como uma determinada camada computa atributos complexos
da camada anterior. Eu sei que parte dessa intuição ainda pode estar
um pouco abstrata e em alto nível. E, o que vou fazer nos próximos 2 vídeos, é dar um exemplo detalhado de como uma Rede Neural pode ser usada para computar
funções não-lineares a partir das entradas, e espero te dar uma boa noção do tipo de hipótese não-linear complexa que
podemos obter das Redes Neurais.
Tradução: Henrique de Assis L Ribeiro | Revisão: Pablo de Morais Andrade