1
00:00:00,780 --> 00:00:01,870
このビデオでは、

2
00:00:02,070 --> 00:00:03,210
ニューラルネットワークを

3
00:00:03,470 --> 00:00:04,970
どう表現するかについて話す。

4
00:00:05,520 --> 00:00:06,690
言い換えると、我らの仮説を

5
00:00:07,050 --> 00:00:08,130
どう表現するか、または、

6
00:00:08,350 --> 00:00:11,270
ニューラルネットワークを使う時はどうモデルを表現するか、という事。

7
00:00:12,050 --> 00:00:13,750
ニューラルネットワークは脳内のニューロン、または

8
00:00:14,320 --> 00:00:17,650
ニューロンのネットワークをシミュレートする事で発展した。

9
00:00:18,540 --> 00:00:19,830
だから仮説の表現を

10
00:00:20,400 --> 00:00:22,330
説明するために、

11
00:00:22,580 --> 00:00:23,590
脳の中のニューロンを一つ取り出すと

12
00:00:24,050 --> 00:00:25,250
どんな感じか見てみよう。

13
00:00:26,390 --> 00:00:27,630
あなたや私の脳は、

14
00:00:28,160 --> 00:00:29,610
こんなニューロンがたくさんごちゃごちゃ詰まっている。

15
00:00:30,170 --> 00:00:31,300
ニューロンとは脳の細胞で

16
00:00:31,380 --> 00:00:32,740
特徴的なのは

17
00:00:33,000 --> 00:00:34,740
2つある。

18
00:00:34,970 --> 00:00:36,590
一つ目は

19
00:00:36,780 --> 00:00:37,820
ニューロンには細胞体があり、

20
00:00:38,360 --> 00:00:40,320
ーこんな感じのー で、さらに、

21
00:00:40,500 --> 00:00:41,480
ニューロンは幾つかの

22
00:00:41,680 --> 00:00:43,060
入力のワイヤがある事で、

23
00:00:43,260 --> 00:00:44,360
これらはdendriteと呼ばれていて、

24
00:00:44,670 --> 00:00:47,370
それは入力のワイヤと考えられる。

25
00:00:48,180 --> 00:00:49,500
そしてこれらが他の場所からの

26
00:00:49,660 --> 00:00:51,330
入力を受け取る。

27
00:00:51,600 --> 00:00:54,270
そしてニューロンは出力ワイヤも持っていて、それはaxonと呼ばれる。

28
00:00:55,140 --> 00:00:56,710
そしてこの出力ワイヤは

29
00:00:57,290 --> 00:00:58,910
他のニューロンに

30
00:00:59,140 --> 00:01:00,690
シグナルを送るのに、

31
00:01:01,290 --> 00:01:04,130
言い換えると他のニューロンにメッセージを送るのに使われる。

32
00:01:05,280 --> 00:01:07,220
つまり一番単純なレベルでは、

33
00:01:07,410 --> 00:01:08,740
ニューロンとは何かというと、

34
00:01:09,430 --> 00:01:10,470
たくさんの入力を入力ワイヤから受け取り

35
00:01:10,650 --> 00:01:13,220
なんらかの計算を行い、その出力を

36
00:01:14,430 --> 00:01:15,700
axonを通して脳内の他のニューロンに送る

37
00:01:15,830 --> 00:01:17,640
計算ユニットと

38
00:01:18,150 --> 00:01:19,540
考えることが出来る。

39
00:01:20,460 --> 00:01:23,370
これはニューロンのグループのイラストだ。

40
00:01:24,260 --> 00:01:25,350
ニューロンがお互いにコミュニケートする方法は

41
00:01:26,120 --> 00:01:28,410
僅かな電気信号によってだ。

42
00:01:29,230 --> 00:01:31,820
それらはスパイクとも呼ばれている。
だけどそれらは単なる小さな電気を意味するに過ぎない。

43
00:01:33,140 --> 00:01:35,000
ここに一つニューロンがあり、

44
00:01:35,680 --> 00:01:37,060
それがする事といえば、

45
00:01:37,190 --> 00:01:38,260
もしメッセージを送りたい時は

46
00:01:38,500 --> 00:01:39,280
axonを通して、別のニューロンに

47
00:01:39,710 --> 00:01:41,190
わずかな電気のパルスを

48
00:01:41,820 --> 00:01:44,110
送る。

49
00:01:44,970 --> 00:01:46,610
そしてこれがaxonだ。

50
00:01:47,250 --> 00:01:48,310
この出力ワイヤがあって、

51
00:01:49,190 --> 00:01:50,840
それがこうして、二番目のニューロンの

52
00:01:51,030 --> 00:01:52,270
入力ワイヤ、またの名をdendriteに

53
00:01:52,550 --> 00:01:54,300
つながっている。

54
00:01:54,560 --> 00:01:55,860
そしてそこからこの信号に入力のメッセージとして受け取って、

55
00:01:56,830 --> 00:01:58,510
なんらかの計算を行い、

56
00:01:58,720 --> 00:01:59,710
またさらに他のニューロンへ

57
00:02:00,030 --> 00:02:01,450
出力メッセージを

58
00:02:02,020 --> 00:02:04,090
axonを通して送るかもしれない。

59
00:02:04,400 --> 00:02:05,740
以上が、全ての人類が

60
00:02:05,940 --> 00:02:07,570
考えた時に起こるプロセスで、

61
00:02:08,060 --> 00:02:09,540
これらのニューロンが計算をして、

62
00:02:09,730 --> 00:02:11,150
メッセージを他のニューロンに

63
00:02:11,630 --> 00:02:13,120
送る。

64
00:02:13,380 --> 00:02:15,560
与えられた入力に対する結果として。

65
00:02:16,530 --> 00:02:17,560
ところで、これはまた、

66
00:02:18,340 --> 00:02:21,030
我らの感覚や筋肉が機能する方法でもある。

67
00:02:21,680 --> 00:02:23,340
もし筋肉の一つを

68
00:02:23,500 --> 00:02:24,460
動かそうとすれば、

69
00:02:24,760 --> 00:02:26,110
それが実現されるのは、

70
00:02:26,240 --> 00:02:27,370
ニューロンが筋肉に

71
00:02:28,470 --> 00:02:29,590
電気のパルスを送り、

72
00:02:30,160 --> 00:02:32,440
それが筋肉を収縮させたり、

73
00:02:32,710 --> 00:02:34,030
目の場合、

74
00:02:34,330 --> 00:02:35,510
何らかのセンサー、例えば目とかの場合、

75
00:02:35,650 --> 00:02:36,710
脳にメッセージを送るには

76
00:02:36,950 --> 00:02:37,810
電気のパルスを

77
00:02:38,360 --> 00:02:39,900
脳にあるニューロンに

78
00:02:40,670 --> 00:02:42,670
送るという手段を通してだ。

79
00:02:43,460 --> 00:02:45,490
ニューラルネットワークにおいては、

80
00:02:46,040 --> 00:02:47,700
いや我らがコンピュータで実装する

81
00:02:48,040 --> 00:02:49,250
人工的なニューラルネットワークにおいては、と言うべきか、

82
00:02:49,290 --> 00:02:50,980
その場合、ニューロンがやってる事の

83
00:02:51,200 --> 00:02:52,560
とてもシンプルなモデルを

84
00:02:53,160 --> 00:02:54,380
使う事になる。

85
00:02:54,510 --> 00:02:57,720
ニューロンを単なるロジスティックの単位としてモデル化する。

86
00:02:58,590 --> 00:02:59,480
だから黄色で円を

87
00:02:59,770 --> 00:03:01,130
こんな感じで描いたら、

88
00:03:01,240 --> 00:03:03,130
これはニューロンの本体みたいな役割をしている、と

89
00:03:03,280 --> 00:03:04,710
考えてくれ。

90
00:03:04,870 --> 00:03:06,480
そしてそこに、

91
00:03:07,210 --> 00:03:08,840
いくつかの入力を

92
00:03:09,670 --> 00:03:11,670
dendritesまたの名を入力ワイヤを

93
00:03:11,910 --> 00:03:16,150
通して食わす。
するとニューロンはなんらかの計算行い、

94
00:03:17,390 --> 00:03:19,050
この出力ワイヤから

95
00:03:19,200 --> 00:03:21,260
なんらかの値を出力する。

96
00:03:21,820 --> 00:03:23,400
生物的なニューロンなら

97
00:03:23,530 --> 00:03:25,160
それはaxonに相当する。

98
00:03:25,310 --> 00:03:26,660
そしてこんなダイアグラムを描いた時はいつでも、

99
00:03:26,830 --> 00:03:28,020
これの意味する所は、

100
00:03:28,550 --> 00:03:30,040
h(x)の計算であり、それは

101
00:03:32,780 --> 00:03:34,290
1足すeのマイナス シータ転置x 分の1 で、

102
00:03:35,290 --> 00:03:37,590
xとシータはいつも通り、

103
00:03:37,930 --> 00:03:39,330
パラメータベクトルとかを

104
00:03:39,650 --> 00:03:42,610
表している。

105
00:03:42,920 --> 00:03:44,410
つまりこれはとても単純化した、

106
00:03:44,780 --> 00:03:46,490
ちょっとあまりにも単純化しすぎた感じの

107
00:03:46,670 --> 00:03:48,050
ニューロンが行なっている事のモデルで

108
00:03:48,320 --> 00:03:49,200
いくつかの入力、

109
00:03:49,260 --> 00:03:50,790
x1、x2、x3を受け取り

110
00:03:51,650 --> 00:03:54,150
そんな風に計算された何らか値を出力する、という。

111
00:03:59,960 --> 00:04:01,250
ニューラルネットワークを書く時は

112
00:04:01,900 --> 00:04:03,430
普通は入力のノードだけを

113
00:04:03,720 --> 00:04:04,770
x1、x2、x3と描くのだが、

114
00:04:06,330 --> 00:04:07,740
たまに、そちらの方が便利な時に限り

115
00:04:08,170 --> 00:04:09,780
追加のノード、x0を描く事もある。

116
00:04:11,050 --> 00:04:12,200
このx0ノードは

117
00:04:12,370 --> 00:04:13,960
バイアスユニットとかバイアスニューロンと

118
00:04:14,960 --> 00:04:17,970
呼ばれることがある。

119
00:04:18,500 --> 00:04:21,350
だがx0は1と決まっているので、

120
00:04:21,530 --> 00:04:22,320
このノードは描いたり描かなかったりする。

121
00:04:22,820 --> 00:04:24,280
それは単に、その例にとって

122
00:04:24,800 --> 00:04:27,560
記しておいた方が便利かどうかで決めてる。

123
00:04:28,080 --> 00:04:32,810
最後に、もう一つだけ

124
00:04:33,270 --> 00:04:34,800
用語を導入しておく。

125
00:04:34,900 --> 00:04:36,690
ニューラルネットワークについて話してる時は、

126
00:04:36,810 --> 00:04:38,500
このニューロン、

127
00:04:38,790 --> 00:04:40,330
人工的なニューロンの

128
00:04:40,440 --> 00:04:42,720
sigmoid関数またはロジスティック関数を、

129
00:04:43,090 --> 00:04:44,250
アクティベーション関数と呼ぶ事がある。

130
00:04:44,760 --> 00:04:48,030
このアクティベーション関数というのはニューラルネットワークの用語で

131
00:04:48,140 --> 00:04:49,200
これはこの非線形の

132
00:04:49,540 --> 00:04:51,210
g(z)イコール

133
00:04:51,560 --> 00:04:53,190
1足すe の-z 分の1の、

134
00:04:53,430 --> 00:04:55,170
もう一つの呼び名に

135
00:04:55,260 --> 00:04:56,020
過ぎない。

136
00:04:56,660 --> 00:04:58,410
一方でここまでは

137
00:04:58,930 --> 00:05:00,090
シータをモデルのパラメータと

138
00:05:00,600 --> 00:05:02,500
呼んできたし、今後もだいたいは

139
00:05:02,940 --> 00:05:04,790
その用語を使い続けるが、

140
00:05:05,480 --> 00:05:06,480
ニューラルネットワークでは

141
00:05:07,680 --> 00:05:08,960
ニューラルネットワークの文献では

142
00:05:09,400 --> 00:05:10,290
人々はたまに

143
00:05:10,620 --> 00:05:12,160
モデルのウェイトと呼んでいるのを

144
00:05:12,400 --> 00:05:13,760
見かけるかもしれない。
このウェイトというのは

145
00:05:13,950 --> 00:05:15,490
このモデルのパラメータと

146
00:05:15,750 --> 00:05:17,470
完全に同じ意味だ。

147
00:05:17,830 --> 00:05:18,890
このクラスのビデオではだいたい

148
00:05:19,900 --> 00:05:21,010
パラメータという用語を使うが、

149
00:05:21,620 --> 00:05:24,180
たまに他の人がウェイトって用語を使ってるのを聞くことがあるかもしれない。

150
00:05:27,890 --> 00:05:29,290
このちっぽけなダイアグラムは

151
00:05:29,430 --> 00:05:31,340
単体のニューロンを表している。

152
00:05:34,470 --> 00:05:35,790
ニューラルネットワークとは

153
00:05:36,560 --> 00:05:38,590
単にこれらのニューロンが幾つか集まった

154
00:05:38,780 --> 00:05:40,500
グループの事だ。

155
00:05:41,630 --> 00:05:42,770
具体的には、ここに我らの

156
00:05:43,530 --> 00:05:45,070
入力単位、x1、x2、x3があり、

157
00:05:45,410 --> 00:05:47,170
繰り返しになるが

158
00:05:47,540 --> 00:05:49,070
たまにこの追加の

159
00:05:49,370 --> 00:05:50,760
x0のノードを描いたり描かなかったりする。

160
00:05:51,340 --> 00:05:52,490
今回はここにそれを描いとく。

161
00:05:53,620 --> 00:05:54,950
そしてここに、我らは3つのニューロンを

162
00:05:55,300 --> 00:05:56,800
持っている。

163
00:05:56,930 --> 00:05:58,890
それをa(2)1、a(2)2、a(2)3と書いた。

164
00:05:59,060 --> 00:06:00,250
上と下の添字は後ほど。

165
00:06:00,700 --> 00:06:02,140
そして繰り返すが、

166
00:06:02,730 --> 00:06:03,790
もし必要なら

167
00:06:04,500 --> 00:06:05,440
このa0という追加の

168
00:06:05,620 --> 00:06:08,840
バイアスユニットをここに足す事もある。

169
00:06:10,240 --> 00:06:12,030
それはいつでも値1を出力する。

170
00:06:12,390 --> 00:06:13,680
そして最後に、この三番目の

171
00:06:13,880 --> 00:06:15,450
ノードが最後のレイヤーにある。

172
00:06:15,710 --> 00:06:16,800
そしてこの

173
00:06:16,990 --> 00:06:18,600
三番目のノードが仮説である

174
00:06:19,210 --> 00:06:21,020
h(x)の計算結果を出力する。

175
00:06:22,330 --> 00:06:23,480
ニューラルネットワークの用語を

176
00:06:23,610 --> 00:06:25,250
もうちょっと導入しておく。

177
00:06:25,530 --> 00:06:27,340
この最初のレイヤーは

178
00:06:27,480 --> 00:06:28,610
入力レイヤーとも呼ばれる。

179
00:06:29,040 --> 00:06:30,160
何故ならこれが

180
00:06:30,400 --> 00:06:33,510
我らのフィーチャーであるx1、x2、x3をインプットする所だから。

181
00:06:33,770 --> 00:06:35,560
最後のレイヤーは

182
00:06:35,850 --> 00:06:37,190
出力レイヤーとも呼ばれる。

183
00:06:37,640 --> 00:06:39,550
何故ならこのレイヤーが

184
00:06:39,840 --> 00:06:41,010
ここにあるニューロンこそが、

185
00:06:41,150 --> 00:06:42,340
仮説による最終的な計算結果を

186
00:06:42,400 --> 00:06:43,980
出力するから。

187
00:06:44,370 --> 00:06:46,180
そして2つの間にあるレイヤーを

188
00:06:46,420 --> 00:06:48,900
隠れたレイヤー（hidden layer）と呼ぶ。

189
00:06:49,830 --> 00:06:51,300
隠れたレイヤーという用語は

190
00:06:51,450 --> 00:06:53,290
そんな良い用語とは思わないが、

191
00:06:54,160 --> 00:06:55,680
直感的には

192
00:06:56,020 --> 00:06:57,450
教師あり学習では、

193
00:06:57,530 --> 00:06:59,820
入力と正解の出力は見れる訳だが

194
00:07:00,640 --> 00:07:02,530
隠れたレイヤーはトレーニングセットでは

195
00:07:02,660 --> 00:07:04,260
観測出来ない値だ。

196
00:07:04,520 --> 00:07:07,280
だからxでもyでも無い物は、隠れたレイヤーと呼んでいる。

197
00:07:08,170 --> 00:07:09,860
そしてのちほど、一つよりも多い隠れたレイヤーの

198
00:07:10,050 --> 00:07:11,260
例を見ていく。

199
00:07:11,370 --> 00:07:12,690
だがこの例では

200
00:07:13,020 --> 00:07:14,290
入力レイヤーであるレイヤー1が一つに、

201
00:07:14,480 --> 00:07:16,010
一つの隠れたレイヤーのレイヤー2に、

202
00:07:16,260 --> 00:07:18,900
出力レイヤーのレイヤー3がある。

203
00:07:19,390 --> 00:07:20,530
だが基本的には入力レイヤーでなく、

204
00:07:20,990 --> 00:07:22,260
そして出力レイヤーでも無い物は

205
00:07:22,410 --> 00:07:24,480
なんでも隠れたレイヤーと呼ぶ。

206
00:07:26,710 --> 00:07:29,620
さて、私は

207
00:07:29,710 --> 00:07:30,610
このニューラルネットワークが

208
00:07:31,090 --> 00:07:33,130
何をするのかをとっても分かりやすくしたい。

209
00:07:33,970 --> 00:07:34,840
これに埋め込まれた計算過程を

210
00:07:35,760 --> 00:07:37,600
順番に見ていこう。

211
00:07:38,050 --> 00:07:40,410
このダイアグラムで表現されている物の。

212
00:07:41,560 --> 00:07:42,800
ニューラルネットワークで表現されている

213
00:07:43,660 --> 00:07:44,960
特定の計算を説明する為に、

214
00:07:45,580 --> 00:07:46,910
もうちょっと記法を追加しておく。

215
00:07:47,270 --> 00:07:48,400
上付き添字のjと

216
00:07:48,950 --> 00:07:50,520
下付き添字のiを

217
00:07:51,090 --> 00:07:53,640
レイヤーjにある

218
00:07:54,060 --> 00:07:55,390
ニューロンiまたはユニットiのアクティベーションを

219
00:07:55,720 --> 00:07:58,290
示すのに使う。
具体的には、

220
00:07:59,390 --> 00:08:01,280
これは上付き添字2の下付き添字1で

221
00:08:01,380 --> 00:08:03,930
これは二番目のレイヤーの

222
00:08:04,010 --> 00:08:05,320
つまり隠れたレイヤーの

223
00:08:05,450 --> 00:08:07,140
最初のユニットだ。

224
00:08:07,280 --> 00:08:08,640
そしてアクティベーションという言葉で、

225
00:08:08,970 --> 00:08:10,360
特定のニューロンからの

226
00:08:10,710 --> 00:08:12,530
計算結果の値、つまり出力の値を指す。

227
00:08:13,200 --> 00:08:14,320
さらに、我らのニューラルネットワークは

228
00:08:14,850 --> 00:08:17,050
これらの行列、シータの上付き添字jで

229
00:08:17,470 --> 00:08:19,520
パラメータ化される。ここで

230
00:08:19,690 --> 00:08:20,600
シータjは一つのレイヤーから、、、

231
00:08:20,820 --> 00:08:21,820
例えば最初のレイヤーから二番目のレイヤーへと、とか、

232
00:08:22,140 --> 00:08:23,770
二番目のレイヤーから三番目のレイヤーへと、などを

233
00:08:24,130 --> 00:08:25,780
マッピングする関数を制御する

234
00:08:25,990 --> 00:08:28,360
ウェイトとなる行列だ。

235
00:08:29,580 --> 00:08:32,990
つまり、これがこのダイアグラムで表現される計算だ。

236
00:08:34,520 --> 00:08:35,770
ここにある、最初の隠れたユニットは

237
00:08:37,060 --> 00:08:38,600
以下のように計算された値だ:

238
00:08:38,840 --> 00:08:40,020
a(2)1 イコール

239
00:08:40,260 --> 00:08:41,980
sigmoid関数、または

240
00:08:42,400 --> 00:08:44,240
sigmoidアクティベーション関数または

241
00:08:45,210 --> 00:08:46,550
ロジスティックアクティベーション関数と呼ばれるが、

242
00:08:47,760 --> 00:08:49,730
それがこんな形の

243
00:08:49,990 --> 00:08:52,360
入力の線形の組み合わせに適用される。

244
00:08:53,840 --> 00:08:56,560
そしてこの二番目の隠れたユニットは

245
00:08:56,820 --> 00:08:58,330
このsigmoid関数で

246
00:08:59,010 --> 00:09:01,400
計算される、このアクティベーションの値だ。

247
00:09:02,470 --> 00:09:04,110
以下同様に、この三番目の隠れたユニットは

248
00:09:04,260 --> 00:09:07,010
この式で計算される。

249
00:09:08,330 --> 00:09:10,060
つまりここでは、3つの入力ユニットと

250
00:09:10,780 --> 00:09:13,960
3つの隠れユニットがある。

251
00:09:16,830 --> 00:09:18,840
つまりシータ1の次元は

252
00:09:19,590 --> 00:09:21,530
それはパラメータの行列で

253
00:09:22,060 --> 00:09:23,590
3つの入力ユニットと

254
00:09:23,740 --> 00:09:24,870
3つの隠れユニットを合わせた物だから、

255
00:09:25,170 --> 00:09:26,530
つまりシータ1は

256
00:09:27,080 --> 00:09:28,210
3、、、

257
00:09:29,880 --> 00:09:35,390
シータ1は

258
00:09:35,640 --> 00:09:36,870
つまり

259
00:09:38,130 --> 00:09:39,640
3x4次元の行列だ。

260
00:09:40,650 --> 00:09:42,620
より一般的には

261
00:09:43,870 --> 00:09:45,440
ネットワークがレイヤーjに

262
00:09:45,710 --> 00:09:46,710
sのjだけのユニットを、

263
00:09:47,210 --> 00:09:48,440
j+1番目のレイヤーに

264
00:09:48,650 --> 00:09:49,980
sのj+1個のユニットがあるとすると、

265
00:09:50,310 --> 00:09:51,700
行列であるところのシータjは

266
00:09:52,010 --> 00:09:53,560
レイヤーjからレイヤーj+1の

267
00:09:53,780 --> 00:09:55,390
マッピングの関数を決定する訳だが、

268
00:09:55,640 --> 00:09:56,660
その次元は、

269
00:09:57,280 --> 00:10:00,160
sのj+1 掛ける sのj足す1となる。

270
00:10:00,580 --> 00:10:02,390
ノーテーションを明確にしておこう。

271
00:10:02,580 --> 00:10:04,440
これはsの下付き添字でj+1。

272
00:10:04,440 --> 00:10:05,810
そっちはsの下付き添字でj、

273
00:10:06,100 --> 00:10:07,260
そしてこの全体に、

274
00:10:07,380 --> 00:10:09,060
1を足している。

275
00:10:09,430 --> 00:10:11,860
これ全体に対して。それがs jに1を足す、という事。オーケー？

276
00:10:12,260 --> 00:10:13,730
つまりsの下付き添字j+1に 足す事の、、、

277
00:10:14,080 --> 00:10:22,400
じゃなかった、掛けるだ。えーと、

278
00:10:22,560 --> 00:10:24,090
sのj+1に、

279
00:10:24,400 --> 00:10:26,230
掛ける事のsのjに 足すことの1、

280
00:10:27,220 --> 00:10:30,460
最後の足す1は添字じゃないよ。

281
00:10:32,400 --> 00:10:33,520
さて、3つの隠れユニットがどんな計算をするかを

282
00:10:33,690 --> 00:10:36,120
話してきた訳だが、

283
00:10:37,180 --> 00:10:41,240
最後にこの最後の、出力レイヤー、

284
00:10:41,370 --> 00:10:42,280
我らはもう一つレイヤーを持ってた訳だが、

285
00:10:42,540 --> 00:10:44,270
このレイヤーはh(x)を計算する。

286
00:10:44,350 --> 00:10:46,090
それはイコール、、、

287
00:10:46,230 --> 00:10:47,210
ところでそれは、a(3)の1と書けて、

288
00:10:47,270 --> 00:10:50,830
それはイコール、これとなる。

289
00:10:52,030 --> 00:10:53,110
そして気づいたかもしれないが

290
00:10:53,290 --> 00:10:54,480
私は上付き添字で2とここでは書いた。

291
00:10:54,670 --> 00:10:56,380
その理由は、シータの上付き添字の2は

292
00:10:57,130 --> 00:10:58,340
パラメータの行列、

293
00:10:59,080 --> 00:11:01,170
またの名をウェイトの行列で

294
00:11:01,380 --> 00:11:02,830
それは隠れユニット、

295
00:11:03,240 --> 00:11:05,090
それはレイヤー2のユニットの事だが、

296
00:11:05,600 --> 00:11:06,850
それとレイヤー3のユニットを

297
00:11:07,720 --> 00:11:09,230
それは出力ユニットだが、

298
00:11:09,590 --> 00:11:10,840
それをマップする関数を制御する。

299
00:11:12,360 --> 00:11:12,360
。

300
00:11:12,550 --> 00:11:13,460
まとめると、我らがやった事は

301
00:11:13,830 --> 00:11:14,900
ここにあるような絵が

302
00:11:15,230 --> 00:11:16,670
どう人工的なニューラルネットワークを

303
00:11:17,350 --> 00:11:20,280
定義するかを見た。

304
00:11:20,920 --> 00:11:22,160
それは入力値のxを

305
00:11:23,090 --> 00:11:24,880
願わくばあるyの予測値に

306
00:11:25,140 --> 00:11:26,650
マップする関数を定義する。

307
00:11:27,500 --> 00:11:29,430
そしてこれらの仮説はパラメータで

308
00:11:30,190 --> 00:11:31,600
パラメトライズされていて、

309
00:11:31,690 --> 00:11:33,070
そのパラメータは大文字のシータで示している。

310
00:11:33,460 --> 00:11:35,020
こうする事で

311
00:11:35,170 --> 00:11:36,920
シータを変える事で異なる仮説が得られる訳だ。

312
00:11:37,650 --> 00:11:38,930
つまりxからyへとマップする、

313
00:11:39,490 --> 00:11:42,490
別の関数が得られるって事だ。

314
00:11:42,940 --> 00:11:44,000
そして、これは仮説をニューラルネットワークで

315
00:11:44,790 --> 00:11:45,980
表す方法の

316
00:11:46,140 --> 00:11:48,400
数学による定義を与えてくれる。

317
00:11:49,430 --> 00:11:50,750
以後の一連のビデオでは

318
00:11:50,780 --> 00:11:51,930
これらの仮説の表現が

319
00:11:52,090 --> 00:11:53,580
何をするかの

320
00:11:53,760 --> 00:11:56,280
直感を伝えたいと思う。

321
00:11:56,410 --> 00:11:57,290
それと同時に、

322
00:11:57,370 --> 00:12:00,280
例を幾つか見て、それらをどう効率的に計算するかもお話する。