1
00:00:00,780 --> 00:00:01,870
在这个视频中 我想

2
00:00:02,070 --> 00:00:03,210
开始向你介绍

3
00:00:03,470 --> 00:00:04,970
我们该如何表示神经网络

4
00:00:05,520 --> 00:00:06,690
换句话说 当我们在

5
00:00:07,050 --> 00:00:08,130
运用神经网络时

6
00:00:08,350 --> 00:00:11,270
我们该如何表示我们的假设或模型

7
00:00:12,050 --> 00:00:13,750
神经网络是在模仿

8
00:00:14,320 --> 00:00:17,650
大脑中的神经元或者神经网络时发明的

9
00:00:18,540 --> 00:00:19,830
因此 要解释如何表示

10
00:00:20,400 --> 00:00:22,330
模型假设 我们先来看单个

11
00:00:22,580 --> 00:00:23,590
神经元在大脑中

12
00:00:24,050 --> 00:00:25,250
是什么样的

13
00:00:26,390 --> 00:00:27,630
我们的大脑中充满了

14
00:00:28,160 --> 00:00:29,610
这样的神经元

15
00:00:30,170 --> 00:00:31,300
神经元是大脑中的细胞

16
00:00:31,380 --> 00:00:32,740
其中有两点

17
00:00:33,000 --> 00:00:34,740
值得我们注意

18
00:00:34,970 --> 00:00:36,590
一是神经元有

19
00:00:36,780 --> 00:00:37,820
像这样的细胞主体

20
00:00:38,360 --> 00:00:40,320
二是神经元有

21
00:00:40,500 --> 00:00:41,480
一定数量的

22
00:00:41,680 --> 00:00:43,060
输入神经

23
00:00:43,260 --> 00:00:44,360
这些输入神经叫做树突

24
00:00:44,670 --> 00:00:47,370
可以把它们想象成输入电线

25
00:00:48,180 --> 00:00:49,500
它们接收来自其他

26
00:00:49,660 --> 00:00:51,330
神经元的信息

27
00:00:51,600 --> 00:00:54,270
神经元的输出神经叫做轴突

28
00:00:55,140 --> 00:00:56,710
这些输出神经

29
00:00:57,290 --> 00:00:58,910
是用来

30
00:00:59,140 --> 00:01:00,690
给其他神经元传递信号

31
00:01:01,290 --> 00:01:04,130
或者传送信息的

32
00:01:05,280 --> 00:01:07,220
简而言之

33
00:01:07,410 --> 00:01:08,740
神经元是一个计算单元

34
00:01:09,430 --> 00:01:10,470
它从输入神经接受一定数目的信息

35
00:01:10,650 --> 00:01:13,220
并做一些计算

36
00:01:14,430 --> 00:01:15,700
然后将结果通过它的

37
00:01:15,830 --> 00:01:17,640
轴突传送到其他节点

38
00:01:18,150 --> 00:01:19,540
或者大脑中的其他神经元

39
00:01:20,460 --> 00:01:23,370
下面是一组神经元的示意图

40
00:01:24,260 --> 00:01:25,350
神经元利用微弱的电流

41
00:01:26,120 --> 00:01:28,410
进行沟通

42
00:01:29,230 --> 00:01:31,820
这些弱电流也称作动作电位 其实就是一些微弱的电流

43
00:01:33,140 --> 00:01:35,000
所以如果

44
00:01:35,680 --> 00:01:37,060
神经元想要

45
00:01:37,190 --> 00:01:38,260
传递一个消息

46
00:01:38,500 --> 00:01:39,280
它就会就通过它的轴突

47
00:01:39,710 --> 00:01:41,190
发送一段微弱电流

48
00:01:41,820 --> 00:01:44,110
给其他神经元

49
00:01:44,970 --> 00:01:46,610
这就是轴突

50
00:01:47,250 --> 00:01:48,310
这里是一条

51
00:01:49,190 --> 00:01:50,840
连接到输入神经

52
00:01:51,030 --> 00:01:52,270
或者连接另一个神经元

53
00:01:52,550 --> 00:01:54,300
树突的神经

54
00:01:54,560 --> 00:01:55,860
接下来这个神经元接收这条消息

55
00:01:56,830 --> 00:01:58,510
做一些计算

56
00:01:58,720 --> 00:01:59,710
它有可能会反过来将

57
00:02:00,030 --> 00:02:01,450
在轴突上的

58
00:02:02,020 --> 00:02:04,090
自己的消息传给其他神经元

59
00:02:04,400 --> 00:02:05,740
这就是所有

60
00:02:05,940 --> 00:02:07,570
人类思考的模型：

61
00:02:08,060 --> 00:02:09,540
我们的神经元把

62
00:02:09,730 --> 00:02:11,150
自己的收到的消息进行计算

63
00:02:11,630 --> 00:02:13,120
并向其他神经元

64
00:02:13,380 --> 00:02:15,560
传递消息

65
00:02:16,530 --> 00:02:17,560
顺便说一下 这也是

66
00:02:18,340 --> 00:02:21,030
我们的感觉和肌肉运转的原理

67
00:02:21,680 --> 00:02:23,340
如果你想活动一块肌肉

68
00:02:23,500 --> 00:02:24,460
就会触发一个神经元

69
00:02:24,760 --> 00:02:26,110
给你的肌肉

70
00:02:26,240 --> 00:02:27,370
发送脉冲

71
00:02:28,470 --> 00:02:29,590
并引起

72
00:02:30,160 --> 00:02:32,440
你的肌肉收缩

73
00:02:32,710 --> 00:02:34,030
如果一些感官

74
00:02:34,330 --> 00:02:35,510
比如说眼睛

75
00:02:35,650 --> 00:02:36,710
想要给大脑传递

76
00:02:36,950 --> 00:02:37,810
一个消息

77
00:02:38,360 --> 00:02:39,900
那么它就像这样发送

78
00:02:40,670 --> 00:02:42,670
电脉冲给大脑的

79
00:02:43,460 --> 00:02:45,490
在一个神经网络里

80
00:02:46,040 --> 00:02:47,700
或者说在我们在电脑上

81
00:02:48,040 --> 00:02:49,250
实现的人工神经网络里

82
00:02:49,290 --> 00:02:50,980
我们将使用

83
00:02:51,200 --> 00:02:52,560
一个非常简单的模型

84
00:02:53,160 --> 00:02:54,380
来模拟神经元的工作

85
00:02:54,510 --> 00:02:57,720
我们将神经元模拟成一个逻辑单元

86
00:02:58,590 --> 00:02:59,480
当我画一个这样的

87
00:02:59,770 --> 00:03:01,130
黄色圆圈时 你应该

88
00:03:01,240 --> 00:03:03,130
把它想象成

89
00:03:03,280 --> 00:03:04,710
作用类似于

90
00:03:04,870 --> 00:03:06,480
神经元的东西

91
00:03:07,210 --> 00:03:08,840
然后我们通过

92
00:03:09,670 --> 00:03:11,670
它的树突或者说它的输入神经

93
00:03:11,910 --> 00:03:16,150
传递给它一些信息 然后神经元做一些计算

94
00:03:17,390 --> 00:03:19,050
并通过它的输出神经

95
00:03:19,200 --> 00:03:21,260
即它的轴突

96
00:03:21,820 --> 00:03:23,400
输出计算结果

97
00:03:23,530 --> 00:03:25,160
当我画一个

98
00:03:25,310 --> 00:03:26,660
像这样的图表时

99
00:03:26,830 --> 00:03:28,020
就表示对h(x)的计算

100
00:03:28,550 --> 00:03:30,040
h(x)等于1除以

101
00:03:32,780 --> 00:03:34,290
1加e的

102
00:03:35,290 --> 00:03:37,590
负θ转置乘以 x

103
00:03:37,930 --> 00:03:39,330
通常  x和θ

104
00:03:39,650 --> 00:03:42,610
是我们的参数向量

105
00:03:42,920 --> 00:03:44,410
这是一个简单的模型

106
00:03:44,780 --> 00:03:46,490
甚至说是一个过于简单的

107
00:03:46,670 --> 00:03:48,050
模拟神经元的模型

108
00:03:48,320 --> 00:03:49,200
它被输入 x1  x2和 x3

109
00:03:49,260 --> 00:03:50,790
然后输出一些

110
00:03:51,650 --> 00:03:54,150
类似这样的结果

111
00:03:59,960 --> 00:04:01,250
当我绘制一个神经网络时

112
00:04:01,900 --> 00:04:03,430
通常我只绘制

113
00:04:03,720 --> 00:04:04,770
输入节点 x1  x2  x3

114
00:04:06,330 --> 00:04:07,740
但有时也可以这样做：

115
00:04:08,170 --> 00:04:09,780
我增加一个额外的节点 x0

116
00:04:11,050 --> 00:04:12,200
这个 x0 节点

117
00:04:12,370 --> 00:04:13,960
有时也被称作偏置单位

118
00:04:14,960 --> 00:04:17,970
或偏置神经元 但因为

119
00:04:18,500 --> 00:04:21,350
x0 总是等于1

120
00:04:21,530 --> 00:04:22,320
所以有时候 我会画出它

121
00:04:22,820 --> 00:04:24,280
有时我不会画出

122
00:04:24,800 --> 00:04:27,560
这取决于它是否对例子有利

123
00:04:28,080 --> 00:04:32,810
现在来讨论

124
00:04:33,270 --> 00:04:34,800
最后一个关于

125
00:04:34,900 --> 00:04:36,690
神经网络的术语

126
00:04:36,810 --> 00:04:38,500
有时我们会说

127
00:04:38,790 --> 00:04:40,330
这是一个神经元

128
00:04:40,440 --> 00:04:42,720
一个有s型函数或者逻辑函数作为激励函数的

129
00:04:43,090 --> 00:04:44,250
人工神经元

130
00:04:44,760 --> 00:04:48,030
在神经网络术语中

131
00:04:48,140 --> 00:04:49,200
激励函数只是对类似非线性

132
00:04:49,540 --> 00:04:51,210
函数g(z)的另一个术语称呼

133
00:04:51,560 --> 00:04:53,190
g(z)等于

134
00:04:53,430 --> 00:04:55,170
1除以1

135
00:04:55,260 --> 00:04:56,020
加e的-z次方

136
00:04:56,660 --> 00:04:58,410
到目前为止

137
00:04:58,930 --> 00:05:00,090
我一直称θ为

138
00:05:00,600 --> 00:05:02,500
模型的参数

139
00:05:02,940 --> 00:05:04,790
以后大概会继续将这个术语与

140
00:05:05,480 --> 00:05:06,480
“参数”相对应 而不是与神经网络

141
00:05:07,680 --> 00:05:08,960
在关于神经网络的文献里

142
00:05:09,400 --> 00:05:10,290
有时你可能会看到人们

143
00:05:10,620 --> 00:05:12,160
谈论一个模型的权重

144
00:05:12,400 --> 00:05:13,760
权重其实和

145
00:05:13,950 --> 00:05:15,490
模型的参数

146
00:05:15,750 --> 00:05:17,470
是一样的东西

147
00:05:17,830 --> 00:05:18,890
在视频中

148
00:05:19,900 --> 00:05:21,010
我会继续使用“参数”这个术语

149
00:05:21,620 --> 00:05:24,180
但有时你可能听到别人用“权重”这个术语

150
00:05:27,890 --> 00:05:29,290
这个小圈

151
00:05:29,430 --> 00:05:31,340
代表一个单一的神经元

152
00:05:34,470 --> 00:05:35,790
神经网络其实就是

153
00:05:36,560 --> 00:05:38,590
这些不同的神经元

154
00:05:38,780 --> 00:05:40,500
组合在一起的集合

155
00:05:41,630 --> 00:05:42,770
具体来说 这里是我们的

156
00:05:43,530 --> 00:05:45,070
输入单元 x1  x2和 x3

157
00:05:45,410 --> 00:05:47,170
再说一次

158
00:05:47,540 --> 00:05:49,070
有时也可以画上

159
00:05:49,370 --> 00:05:50,760
额外的节点 x0

160
00:05:51,340 --> 00:05:52,490
我把 x0 画在这了

161
00:05:53,620 --> 00:05:54,950
这里有

162
00:05:55,300 --> 00:05:56,800
3个神经元

163
00:05:56,930 --> 00:05:58,890
我在里面写了a(2)1 a(2)2

164
00:05:59,060 --> 00:06:00,250
和a(2)3

165
00:06:00,700 --> 00:06:02,140
然后再次说明

166
00:06:02,730 --> 00:06:03,790
我们可以在这里

167
00:06:04,500 --> 00:06:05,440
添加一个a0

168
00:06:05,620 --> 00:06:08,840
和一个额外的偏度单元

169
00:06:10,240 --> 00:06:12,030
它的值永远是1

170
00:06:12,390 --> 00:06:13,680
最后 我们在

171
00:06:13,880 --> 00:06:15,450
最后一层有第三个节点

172
00:06:15,710 --> 00:06:16,800
正是这第三个节点

173
00:06:16,990 --> 00:06:18,600
输出

174
00:06:19,210 --> 00:06:21,020
假设函数h(x)计算的结果

175
00:06:22,330 --> 00:06:23,480
再多说一点关于

176
00:06:23,610 --> 00:06:25,250
神经网络的术语

177
00:06:25,530 --> 00:06:27,340
网络中的第一层

178
00:06:27,480 --> 00:06:28,610
也被称为输入层

179
00:06:29,040 --> 00:06:30,160
因为我们在这一层

180
00:06:30,400 --> 00:06:33,510
输入我们的特征项 x1 x2 x3

181
00:06:33,770 --> 00:06:35,560
最后一层

182
00:06:35,850 --> 00:06:37,190
也称为输出层

183
00:06:37,640 --> 00:06:39,550
因为这一层的

184
00:06:39,840 --> 00:06:41,010
神经元—我指的这个

185
00:06:41,150 --> 00:06:42,340
输出

186
00:06:42,400 --> 00:06:43,980
假设的最终计算结果

187
00:06:44,370 --> 00:06:46,180
中间的两层

188
00:06:46,420 --> 00:06:48,900
也被称作隐藏层

189
00:06:49,830 --> 00:06:51,300
隐藏层不是一个

190
00:06:51,450 --> 00:06:53,290
很合适的术语 但是

191
00:06:54,160 --> 00:06:55,680
直觉上我们知道

192
00:06:56,020 --> 00:06:57,450
在监督学习中

193
00:06:57,530 --> 00:06:59,820
你能看到输入 也能看到正确的输出

194
00:07:00,640 --> 00:07:02,530
而隐藏层的值

195
00:07:02,660 --> 00:07:04,260
你在训练集里是看不到的

196
00:07:04,520 --> 00:07:07,280
它的值不是 x 也不是y 所以我们叫它隐藏层

197
00:07:08,170 --> 00:07:09,860
稍后我们会看到神经网络

198
00:07:10,050 --> 00:07:11,260
可以有不止一个的

199
00:07:11,370 --> 00:07:12,690
隐藏层 但在

200
00:07:13,020 --> 00:07:14,290
这个例子中 我们有一个

201
00:07:14,480 --> 00:07:16,010
输入层—第1层 一个隐藏层—

202
00:07:16,260 --> 00:07:18,900
第2层 和一个输出层—第3层

203
00:07:19,390 --> 00:07:20,530
但实际上任何

204
00:07:20,990 --> 00:07:22,260
非输入层或非输出层的层

205
00:07:22,410 --> 00:07:24,480
就被称为隐藏层

206
00:07:26,710 --> 00:07:29,620
接下来

207
00:07:29,710 --> 00:07:30,610
我希望你们明白神经网络

208
00:07:31,090 --> 00:07:33,130
究竟在做什么

209
00:07:33,970 --> 00:07:34,840
让我们逐步分析

210
00:07:35,760 --> 00:07:37,600
这个图表所呈现的

211
00:07:38,050 --> 00:07:40,410
计算步骤

212
00:07:41,560 --> 00:07:42,800
为了解释这个神经网络

213
00:07:43,660 --> 00:07:44,960
具体的计算步骤

214
00:07:45,580 --> 00:07:46,910
这里还有些记号要解释

215
00:07:47,270 --> 00:07:48,400
我要使用a上标(j)

216
00:07:48,950 --> 00:07:50,520
下标i表示

217
00:07:51,090 --> 00:07:53,640
第j层的

218
00:07:54,060 --> 00:07:55,390
第i个神经元或单元

219
00:07:55,720 --> 00:07:58,290
具体来说 这里

220
00:07:59,390 --> 00:08:01,280
a上标(2) 下标1

221
00:08:01,380 --> 00:08:03,930
表示第2层的

222
00:08:04,010 --> 00:08:05,320
第一个激励

223
00:08:05,450 --> 00:08:07,140
即隐藏层的第一个激励

224
00:08:07,280 --> 00:08:08,640
所谓激励(activation) 是指

225
00:08:08,970 --> 00:08:10,360
由一个具体神经元读入

226
00:08:10,710 --> 00:08:12,530
计算并输出的值

227
00:08:13,200 --> 00:08:14,320
此外 我们的神经网络

228
00:08:14,850 --> 00:08:17,050
被这些矩阵参数化

229
00:08:17,470 --> 00:08:19,520
θ上标(j)

230
00:08:19,690 --> 00:08:20,600
它将成为

231
00:08:20,820 --> 00:08:21,820
一个波矩阵

232
00:08:22,140 --> 00:08:23,770
控制着

233
00:08:24,130 --> 00:08:25,780
从一层 比如说从第一层

234
00:08:25,990 --> 00:08:28,360
到第二层或者第二层到第三层的作用

235
00:08:29,580 --> 00:08:32,990
所以 这就是这张图所表示的计算

236
00:08:34,520 --> 00:08:35,770
这里的第一个隐藏单元

237
00:08:37,060 --> 00:08:38,600
是这样计算它的值的：

238
00:08:38,840 --> 00:08:40,020
a(2)1等于

239
00:08:40,260 --> 00:08:41,980
s函数

240
00:08:42,400 --> 00:08:44,240
或者说s激励函数

241
00:08:45,210 --> 00:08:46,550
也叫做逻辑激励函数

242
00:08:47,760 --> 00:08:49,730
作用在这种

243
00:08:49,990 --> 00:08:52,360
输入的线性组合上的结果

244
00:08:53,840 --> 00:08:56,560
第二个隐藏单元

245
00:08:56,820 --> 00:08:58,330
等于s函数作用在这个

246
00:08:59,010 --> 00:09:01,400
线性组合上的值

247
00:09:02,470 --> 00:09:04,110
同样 对于第三个

248
00:09:04,260 --> 00:09:07,010
隐藏的单元 它是通过这个公式计算的

249
00:09:08,330 --> 00:09:10,060
在这里 我们有三个

250
00:09:10,780 --> 00:09:13,960
输入单元和三个隐藏单元

251
00:09:16,830 --> 00:09:18,840
这样一来

252
00:09:19,590 --> 00:09:21,530
参数矩阵控制了

253
00:09:22,060 --> 00:09:23,590
我们来自

254
00:09:23,740 --> 00:09:24,870
三个输入单元

255
00:09:25,170 --> 00:09:26,530
三个隐藏单元的映射

256
00:09:27,080 --> 00:09:28,210
因此θ1的维数

257
00:09:29,880 --> 00:09:35,390
将变成3

258
00:09:35,640 --> 00:09:36,870
θ1将变成一个

259
00:09:38,130 --> 00:09:39,640
3乘4维的

260
00:09:40,650 --> 00:09:42,620
矩阵 更一般的

261
00:09:43,870 --> 00:09:45,440
如果一个网络在第j

262
00:09:45,710 --> 00:09:46,710
层有sj个单元

263
00:09:47,210 --> 00:09:48,440
在j+1层有

264
00:09:48,650 --> 00:09:49,980
sj+1个单元

265
00:09:50,310 --> 00:09:51,700
那么矩阵θ(j)

266
00:09:52,010 --> 00:09:53,560
即控制第j层到

267
00:09:53,780 --> 00:09:55,390
第j+1层映射

268
00:09:55,640 --> 00:09:56,660
的矩阵的

269
00:09:57,280 --> 00:10:00,160
维度为s(j+1) * (sj+1)

270
00:10:00,580 --> 00:10:02,390
这里要搞清楚

271
00:10:02,580 --> 00:10:04,440
这个是s下标j+1

272
00:10:04,440 --> 00:10:05,810
而这个是

273
00:10:06,100 --> 00:10:07,260
s下标j 然后

274
00:10:07,380 --> 00:10:09,060
整体加上1

275
00:10:09,430 --> 00:10:11,860
整体加1 明白了吗

276
00:10:12,260 --> 00:10:13,730
所以θ(j)的维度是

277
00:10:14,080 --> 00:10:22,400
s(j+1)行 sj+1列

278
00:10:22,560 --> 00:10:24,090
这里sj+1

279
00:10:24,400 --> 00:10:26,230
当中的1

280
00:10:27,220 --> 00:10:30,460
不是下标的一部分

281
00:10:32,400 --> 00:10:33,520
以上我们讨论了

282
00:10:33,690 --> 00:10:36,120
三个隐藏单位是怎么计算它们的值

283
00:10:37,180 --> 00:10:41,240
最后 在输出层

284
00:10:41,370 --> 00:10:42,280
我们还有一个

285
00:10:42,540 --> 00:10:44,270
单元 它计算

286
00:10:44,350 --> 00:10:46,090
h(x)  这个也可以

287
00:10:46,230 --> 00:10:47,210
写成a(3)1

288
00:10:47,270 --> 00:10:50,830
就等于后面这块

289
00:10:52,030 --> 00:10:53,110
注意到我这里

290
00:10:53,290 --> 00:10:54,480
写了个上标2

291
00:10:54,670 --> 00:10:56,380
因为θ上标2

292
00:10:57,130 --> 00:10:58,340
是参数矩阵

293
00:10:59,080 --> 00:11:01,170
或着说是权重矩阵 该矩阵

294
00:11:01,380 --> 00:11:02,830
控制从第二层

295
00:11:03,240 --> 00:11:05,090
即隐藏层的3个单位

296
00:11:05,600 --> 00:11:06,850
到第三层

297
00:11:07,720 --> 00:11:09,230
的一个单元

298
00:11:09,590 --> 00:11:10,840
即输出单元

299
00:11:12,360 --> 00:11:12,360
的映射

300
00:11:12,550 --> 00:11:13,460
总之 以上我们

301
00:11:13,830 --> 00:11:14,900
展示了像这样一张图是

302
00:11:15,230 --> 00:11:16,670
怎样定义

303
00:11:17,350 --> 00:11:20,280
一个人工神经网络的

304
00:11:20,920 --> 00:11:22,160
这个神经网络定义了函数h：

305
00:11:23,090 --> 00:11:24,880
从输入 x

306
00:11:25,140 --> 00:11:26,650
到输出y的映射

307
00:11:27,500 --> 00:11:29,430
我将这些假设的参数

308
00:11:30,190 --> 00:11:31,600
记为大写的θ

309
00:11:31,690 --> 00:11:33,070
这样一来

310
00:11:33,460 --> 00:11:35,020
不同的θ

311
00:11:35,170 --> 00:11:36,920
对应了不同的假设

312
00:11:37,650 --> 00:11:38,930
所以我们有不同的函数

313
00:11:39,490 --> 00:11:42,490
比如说从 x到y的映射

314
00:11:42,940 --> 00:11:44,000
以上就是我们怎么

315
00:11:44,790 --> 00:11:45,980
从数学上定义

316
00:11:46,140 --> 00:11:48,400
神经网络的假设

317
00:11:49,430 --> 00:11:50,750
在接下来的视频中

318
00:11:50,780 --> 00:11:51,930
我想要做的就是

319
00:11:52,090 --> 00:11:53,580
让你对这些假设的作用

320
00:11:53,760 --> 00:11:56,280
有更深入的理解

321
00:11:56,410 --> 00:11:57,290
并且讲解几个例子

322
00:11:57,370 --> 00:12:00,280
然后谈谈如何有效的计算它们 【教育无边界字幕组】翻译人员不详 校对: sherry 审核:所羅門捷列夫