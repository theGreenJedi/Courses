1
00:00:00,280 --> 00:00:01,330
在前面的视频里 我们

2
00:00:01,570 --> 00:00:03,540
解释了怎样用数学来

3
00:00:03,700 --> 00:00:04,990
定义或者计算

4
00:00:05,090 --> 00:00:07,160
神经网络算法的假设

5
00:00:08,420 --> 00:00:09,620
在这段视频中 我想

6
00:00:09,730 --> 00:00:11,280
告诉你如何

7
00:00:11,450 --> 00:00:14,040
高效地进行计算

8
00:00:14,710 --> 00:00:16,050
并展示一个向量化的实现方法

9
00:00:17,660 --> 00:00:18,930
更重要的是 我想

10
00:00:19,100 --> 00:00:21,110
让你们明白为什么

11
00:00:21,390 --> 00:00:22,590
这样表示神经网络

12
00:00:23,360 --> 00:00:24,640
是一个好的方法 并且明白

13
00:00:25,010 --> 00:00:27,290
它们怎样帮助我们学习复杂的非线性假设

14
00:00:28,970 --> 00:00:29,880
以这个神经网络为例

15
00:00:30,520 --> 00:00:31,720
以前我们说

16
00:00:32,010 --> 00:00:33,070
计算出假设输出

17
00:00:33,170 --> 00:00:34,090
的步骤

18
00:00:34,650 --> 00:00:35,850
是左边的这些

19
00:00:36,320 --> 00:00:37,780
方程 通过这些方程

20
00:00:37,950 --> 00:00:38,770
我们计算出

21
00:00:39,540 --> 00:00:41,330
三个隐藏单元的激励值

22
00:00:41,450 --> 00:00:43,220
然后利用

23
00:00:43,420 --> 00:00:44,580
这些值来计算

24
00:00:44,650 --> 00:00:45,710
假设h(x)的最终输出

25
00:00:46,680 --> 00:00:48,410
接下来 我要

26
00:00:48,480 --> 00:00:50,200
定义一些额外的项

27
00:00:50,570 --> 00:00:52,210
因此 这里

28
00:00:52,410 --> 00:00:54,090
我画线的项

29
00:00:54,180 --> 00:00:55,560
把它定义为

30
00:00:56,230 --> 00:00:58,410
z上标(2) 下标1

31
00:00:58,790 --> 00:00:59,830
这样一来 就有了

32
00:01:00,650 --> 00:01:02,310
a(2)1 这个项

33
00:01:02,470 --> 00:01:03,930
等于

34
00:01:04,170 --> 00:01:06,020
g(z(2)1)

35
00:01:06,130 --> 00:01:08,100
另外顺便提一下

36
00:01:08,180 --> 00:01:09,750
这些上标2

37
00:01:10,570 --> 00:01:11,580
的意思是

38
00:01:11,870 --> 00:01:12,960
在z(2)和a(2)中

39
00:01:13,080 --> 00:01:14,140
括号中的

40
00:01:14,840 --> 00:01:16,450
2表示这些值

41
00:01:16,740 --> 00:01:18,330
与第二层相关

42
00:01:18,570 --> 00:01:19,810
即与神经网络中的

43
00:01:20,100 --> 00:01:21,390
隐藏层有关

44
00:01:22,820 --> 00:01:25,200
接下来 这里的项

45
00:01:25,990 --> 00:01:27,640
我将同样定义为

46
00:01:29,530 --> 00:01:30,140
z(2)2

47
00:01:30,490 --> 00:01:31,860
最后这个

48
00:01:32,170 --> 00:01:33,100
我画线的项

49
00:01:34,160 --> 00:01:37,040
我把它定义为z(2)3

50
00:01:37,090 --> 00:01:38,710
这样 我们有a(2)3

51
00:01:38,850 --> 00:01:43,200
等于

52
00:01:44,990 --> 00:01:45,360
g(z(2)3)

53
00:01:45,480 --> 00:01:46,760
所以这些z值都是

54
00:01:47,290 --> 00:01:48,940
一个线性组合

55
00:01:49,360 --> 00:01:51,200
是输入值x0 x1 x2 x3的

56
00:01:51,490 --> 00:01:52,800
加权线性组合

57
00:01:53,060 --> 00:01:55,350
它将会进入一个特定的神经元

58
00:01:57,090 --> 00:01:58,260
现在 看一下

59
00:01:58,900 --> 00:02:00,470
这一堆数字

60
00:02:01,990 --> 00:02:03,310
你可能会注意到这块

61
00:02:03,490 --> 00:02:05,880
对应了

62
00:02:06,950 --> 00:02:08,330
矩阵向量运算

63
00:02:08,800 --> 00:02:10,260
类似于矩阵向量乘法

64
00:02:11,070 --> 00:02:12,710
x1乘以向量x

65
00:02:12,790 --> 00:02:14,840
观察到一点

66
00:02:15,580 --> 00:02:18,730
我们就能将

67
00:02:19,700 --> 00:02:20,280
神经网络的计算向量化了

68
00:02:21,470 --> 00:02:23,510
具体而言 我们定义

69
00:02:23,680 --> 00:02:24,810
特征向量x

70
00:02:25,290 --> 00:02:27,020
为x0 x1

71
00:02:27,260 --> 00:02:28,550
x2 x3组成的向量 其中x0

72
00:02:29,010 --> 00:02:30,280
仍然等于1

73
00:02:30,610 --> 00:02:31,860
并定义

74
00:02:32,390 --> 00:02:33,420
z(2)为

75
00:02:34,360 --> 00:02:37,250
这些z值组成的向量 即z(2)1 z(2)2 z(2)3

76
00:02:38,560 --> 00:02:40,210
注意 在这里 z(2)

77
00:02:40,440 --> 00:02:42,500
是一个三维向量

78
00:02:43,910 --> 00:02:47,200
下面 我们可以这样

79
00:02:48,270 --> 00:02:48,860
向量化a(2)1 a(2)2 a(2)3的计算

80
00:02:49,490 --> 00:02:50,690
我们只用两个步骤

81
00:02:51,500 --> 00:02:53,400
z(2)等于θ(1)

82
00:02:53,950 --> 00:02:55,490
乘以x

83
00:02:55,790 --> 00:02:57,020
这样就有了向量z(2)

84
00:02:57,400 --> 00:02:59,360
然后 a(2)等于

85
00:02:59,860 --> 00:03:02,180
g(z(2))

86
00:03:02,440 --> 00:03:03,860
需要明白 这里的z(2)是

87
00:03:04,200 --> 00:03:05,880
三维向量 并且

88
00:03:06,060 --> 00:03:08,150
a(2)也是一个三维

89
00:03:08,810 --> 00:03:10,410
向量 因此这

90
00:03:10,690 --> 00:03:12,680
里的激励g 将s函数

91
00:03:12,950 --> 00:03:15,290
逐元素作用于

92
00:03:15,550 --> 00:03:18,290
z(2)中的每个元素

93
00:03:18,380 --> 00:03:19,270
顺便说一下 为了让我们

94
00:03:19,950 --> 00:03:21,260
的符号和接下来的

95
00:03:21,440 --> 00:03:23,330
工作相一致

96
00:03:23,590 --> 00:03:24,600
在输入层 虽然我们有

97
00:03:24,670 --> 00:03:25,840
输入x 但我们

98
00:03:25,960 --> 00:03:26,950
还可以把这些想成

99
00:03:27,300 --> 00:03:29,270
是第一层的激励

100
00:03:29,680 --> 00:03:30,430
所以 我可以定义a(1)

101
00:03:30,470 --> 00:03:32,510
等于x 因此

102
00:03:32,660 --> 00:03:34,270
a(1)就是一个向量了

103
00:03:34,500 --> 00:03:35,520
我就可以把这里的x

104
00:03:36,230 --> 00:03:38,850
替换成a(1)

105
00:03:39,570 --> 00:03:40,680
z(2)就等于θ(1)乘以a(1)

106
00:03:41,410 --> 00:03:43,350
这都是通过在输入层定义a(1)做到的

107
00:03:44,990 --> 00:03:46,000
现在 就我目前所写的

108
00:03:46,280 --> 00:03:47,500
我得到了

109
00:03:47,900 --> 00:03:49,940
a1 a2 a3的值

110
00:03:50,820 --> 00:03:52,690
并且

111
00:03:52,780 --> 00:03:53,980
我应该把

112
00:03:54,290 --> 00:03:55,600
上标加上去

113
00:03:56,430 --> 00:03:57,530
但我还需要一个值

114
00:03:57,940 --> 00:03:59,810
我同样需要这个a(2)0

115
00:04:00,050 --> 00:04:02,050
它对应于

116
00:04:02,250 --> 00:04:04,350
隐藏层的

117
00:04:04,550 --> 00:04:06,420
得到这个输出的偏置单元

118
00:04:06,990 --> 00:04:07,780
当然 这里也有一个

119
00:04:07,810 --> 00:04:08,850
偏置单元

120
00:04:09,000 --> 00:04:10,060
我只是没有

121
00:04:10,270 --> 00:04:11,820
画出来 为了

122
00:04:11,970 --> 00:04:13,100
注意这额外的偏置单元

123
00:04:13,870 --> 00:04:15,650
接下来我们

124
00:04:16,320 --> 00:04:18,720
要额外加上一个a0 上标(2)

125
00:04:18,890 --> 00:04:20,870
它等于1 这样一来

126
00:04:21,010 --> 00:04:21,990
现在

127
00:04:22,290 --> 00:04:23,860
a(2)就是一个

128
00:04:24,010 --> 00:04:25,390
四维的特征向量

129
00:04:25,690 --> 00:04:26,820
因为我们刚添加了

130
00:04:27,300 --> 00:04:28,490
这个额外的

131
00:04:28,620 --> 00:04:30,260
a0 它等于

132
00:04:30,500 --> 00:04:31,700
1并且它是隐藏层的

133
00:04:32,080 --> 00:04:33,550
一个偏置单元 最后

134
00:04:35,080 --> 00:04:37,620
为了计算假设的

135
00:04:38,070 --> 00:04:40,100
实际输出值 我们

136
00:04:40,250 --> 00:04:41,190
只需要计算

137
00:04:42,470 --> 00:04:44,980
z(3)  z(3)等于

138
00:04:45,350 --> 00:04:47,940
这里我画线的项

139
00:04:48,800 --> 00:04:51,450
这个方框里的项就是z(3)

140
00:04:53,980 --> 00:04:55,160
z(3)等于θ(2)

141
00:04:55,500 --> 00:04:57,120
乘以a(2) 最后

142
00:04:57,810 --> 00:04:59,560
假设输出为h(x)

143
00:04:59,750 --> 00:05:01,210
它等于a(3)

144
00:05:01,360 --> 00:05:03,910
a(3)是输出层

145
00:05:04,750 --> 00:05:06,040
唯一的单元

146
00:05:06,290 --> 00:05:09,500
它是一个实数 你可以写成a(3)

147
00:05:10,050 --> 00:05:12,390
或a(3)1 这就是g(z(3))

148
00:05:13,240 --> 00:05:15,020
这个计算h(x)的过程

149
00:05:15,940 --> 00:05:18,110
也称为前向传播(forward propagation)

150
00:05:19,130 --> 00:05:20,440
这样命名是因为

151
00:05:20,550 --> 00:05:21,310
我们从

152
00:05:22,010 --> 00:05:24,400
输入层的激励开始

153
00:05:24,940 --> 00:05:26,770
然后进行前向传播给

154
00:05:26,860 --> 00:05:29,390
隐藏层并计算

155
00:05:29,580 --> 00:05:30,400
隐藏层的激励 然后

156
00:05:30,540 --> 00:05:32,040
我们继续前向传播

157
00:05:32,760 --> 00:05:36,270
并计算输出层的激励

158
00:05:37,480 --> 00:05:39,170
这个从输入层到

159
00:05:39,290 --> 00:05:40,400
隐藏层再到输出层依次计算激励的

160
00:05:40,940 --> 00:05:42,030
过程叫前向传播

161
00:05:43,320 --> 00:05:44,150
我们刚刚得到了

162
00:05:44,310 --> 00:05:45,370
这一过程的向量化

163
00:05:45,740 --> 00:05:47,140
实现方法

164
00:05:47,280 --> 00:05:48,890
如果你

165
00:05:48,970 --> 00:05:50,260
使用右边这些公式实现它

166
00:05:50,800 --> 00:05:51,740
就会得到

167
00:05:51,850 --> 00:05:53,280
一个有效的

168
00:05:53,460 --> 00:05:54,980
计算h(x)

169
00:05:55,120 --> 00:05:56,130
的方法

170
00:05:58,250 --> 00:05:59,860
这种前向传播的角度

171
00:06:00,860 --> 00:06:02,270
也可以帮助我们了解

172
00:06:02,550 --> 00:06:03,640
神经网络的原理

173
00:06:04,110 --> 00:06:05,290
和它为什么能够

174
00:06:05,510 --> 00:06:07,170
帮助我们学习非线性假设

175
00:06:08,670 --> 00:06:09,760
看一下这个神经网络

176
00:06:10,500 --> 00:06:11,820
我会暂时盖住

177
00:06:12,040 --> 00:06:13,810
图片的左边部分

178
00:06:14,650 --> 00:06:16,170
如果你观察图中剩下的部分

179
00:06:17,030 --> 00:06:18,020
这看起来很像

180
00:06:18,260 --> 00:06:19,520
逻辑回归

181
00:06:19,660 --> 00:06:20,570
在逻辑回归中 我们用

182
00:06:20,990 --> 00:06:22,000
这个节点 即

183
00:06:22,130 --> 00:06:23,770
这个逻辑回归单元

184
00:06:24,120 --> 00:06:26,060
来预测

185
00:06:26,380 --> 00:06:28,290
h(x)的值 具体来说

186
00:06:28,440 --> 00:06:30,340
假设输出的

187
00:06:30,710 --> 00:06:31,830
h(x)将

188
00:06:31,890 --> 00:06:33,760
等于s型激励函数

189
00:06:33,980 --> 00:06:38,110
g(θ0

190
00:06:38,560 --> 00:06:40,450
xa0

191
00:06:41,270 --> 00:06:43,380
+θ1xa1

192
00:06:45,220 --> 00:06:49,080
+θ2xa2

193
00:06:49,260 --> 00:06:52,090
+θ3xa3)

194
00:06:52,830 --> 00:06:55,180
其中

195
00:06:55,370 --> 00:06:56,910
a1 a2 a3

196
00:06:57,050 --> 00:06:59,860
由这三个单元给出

197
00:07:01,060 --> 00:07:02,790
为了和我之前的定义

198
00:07:03,490 --> 00:07:05,000
保持一致 需要

199
00:07:05,170 --> 00:07:06,360
在这里

200
00:07:06,470 --> 00:07:10,700
还有这些地方都填上上标(2)

201
00:07:12,260 --> 00:07:13,920
同样还要加上这些下标1

202
00:07:14,160 --> 00:07:16,800
因为我只有

203
00:07:16,930 --> 00:07:20,610
一个输出单元 但如果你只观察蓝色的部分

204
00:07:20,930 --> 00:07:21,900
这看起来

205
00:07:22,150 --> 00:07:23,680
非常像标准的

206
00:07:23,870 --> 00:07:25,530
逻辑回归模型 不同之处在于

207
00:07:25,600 --> 00:07:28,060
我现在用的是大写的θ 而不是小写的θ

208
00:07:29,170 --> 00:07:30,690
这样做完

209
00:07:30,850 --> 00:07:32,520
我们只得到了逻辑回归

210
00:07:33,660 --> 00:07:35,240
但是 逻辑回归的

211
00:07:35,590 --> 00:07:37,250
输入特征值

212
00:07:38,200 --> 00:07:40,170
是通过隐藏层计算的

213
00:07:41,340 --> 00:07:42,690
再说一遍

214
00:07:42,910 --> 00:07:44,420
神经网络所做的

215
00:07:45,130 --> 00:07:47,050
就像逻辑回归 但是它

216
00:07:47,440 --> 00:07:48,900
不是使用

217
00:07:49,110 --> 00:07:50,770
x1 x2 x3作为输入特征

218
00:07:52,400 --> 00:07:54,260
而是用a1 a2 a3作为新的输入特征

219
00:07:54,440 --> 00:07:56,810
同样 我们需要把

220
00:07:58,130 --> 00:08:00,380
上标加上来和之前的记号保持一致

221
00:08:02,820 --> 00:08:04,610
有趣的是

222
00:08:05,040 --> 00:08:06,220
特征项a1 a2

223
00:08:06,720 --> 00:08:08,310
a3它们是作为

224
00:08:08,760 --> 00:08:09,930
输入的函数来学习的

225
00:08:10,960 --> 00:08:12,640
具体来说 就是从第一层

226
00:08:13,320 --> 00:08:14,540
映射到第二层的函数

227
00:08:14,810 --> 00:08:16,390
这个函数由其他

228
00:08:16,750 --> 00:08:18,550
一组参数θ(1)决定

229
00:08:19,380 --> 00:08:20,210
所以 在神经网络中

230
00:08:20,270 --> 00:08:22,030
它没有用

231
00:08:22,240 --> 00:08:24,050
输入特征x1 x2 x3

232
00:08:24,120 --> 00:08:25,760
来训练逻辑回归

233
00:08:26,210 --> 00:08:27,440
而是自己

234
00:08:27,720 --> 00:08:29,320
训练逻辑回归

235
00:08:29,810 --> 00:08:32,010
的输入

236
00:08:32,130 --> 00:08:33,950
a1 a2 a3

237
00:08:34,650 --> 00:08:36,270
可以想象 如果

238
00:08:36,360 --> 00:08:37,690
在θ1中选择不同的参数

239
00:08:37,900 --> 00:08:39,880
有时可以学习到一些

240
00:08:40,390 --> 00:08:42,460
很有趣和复杂的特征 就可以

241
00:08:43,780 --> 00:08:44,830
得到一个

242
00:08:45,050 --> 00:08:46,650
更好的假设

243
00:08:46,840 --> 00:08:47,870
比使用原始输入

244
00:08:48,020 --> 00:08:50,520
x1 x2或x3时得到的假设更好

245
00:08:50,640 --> 00:08:52,530
你也可以

246
00:08:52,620 --> 00:08:53,730
选择多项式项

247
00:08:53,920 --> 00:08:55,550
x1 x2 x3等作为输入项

248
00:08:55,790 --> 00:08:57,250
但这个算法可以

249
00:08:57,530 --> 00:08:59,130
灵活地

250
00:08:59,420 --> 00:09:01,990
快速学习任意的特征项

251
00:09:02,680 --> 00:09:03,990
把这些a1 a2 a3

252
00:09:04,110 --> 00:09:05,190
输入这个

253
00:09:05,510 --> 00:09:07,830
最后的单元 实际上

254
00:09:09,240 --> 00:09:11,920
它是逻辑回归

255
00:09:12,550 --> 00:09:13,970
我觉得现在描述的这个例子

256
00:09:14,060 --> 00:09:15,500
有点高端 所以

257
00:09:15,750 --> 00:09:16,520
我不知道

258
00:09:17,440 --> 00:09:18,870
你是否能理解

259
00:09:19,720 --> 00:09:21,420
这个具有更复杂特征项的

260
00:09:21,630 --> 00:09:23,120
神经网络 但是

261
00:09:23,210 --> 00:09:24,440
如果你没理解

262
00:09:24,810 --> 00:09:25,860
在接下来的两个视频里

263
00:09:25,970 --> 00:09:27,300
我会讲解一个具体的例子

264
00:09:28,250 --> 00:09:29,590
它描述了怎样用神经网络

265
00:09:29,830 --> 00:09:30,860
如何利用这个隐藏层

266
00:09:31,250 --> 00:09:32,880
计算更复杂的特征

267
00:09:33,130 --> 00:09:34,520
并输入到最后的输出层

268
00:09:35,060 --> 00:09:37,100
以及为什么这样就可以学习更复杂的假设

269
00:09:37,920 --> 00:09:39,120
所以 如果我

270
00:09:39,180 --> 00:09:40,090
现在讲的

271
00:09:40,230 --> 00:09:41,650
你没理解 请继续

272
00:09:41,810 --> 00:09:42,960
观看接下来的两个视频

273
00:09:43,190 --> 00:09:44,370
希望它们

274
00:09:44,580 --> 00:09:46,690
提供的例子能够

275
00:09:47,030 --> 00:09:48,640
让你更加理解神经网络

276
00:09:49,020 --> 00:09:49,740
但有一点

277
00:09:49,820 --> 00:09:51,120
你还可以用其他类型的图来

278
00:09:51,470 --> 00:09:52,990
表示神经网络

279
00:09:53,080 --> 00:09:54,270
神经网络中神经元

280
00:09:54,450 --> 00:09:58,000
相连接的方式 称为神经网络的架构

281
00:09:58,390 --> 00:10:00,150
所以说 架构是指

282
00:10:00,490 --> 00:10:02,380
不同的神经元是如何相互连接的

283
00:10:03,220 --> 00:10:04,180
这里有一个不同的

284
00:10:04,840 --> 00:10:06,300
神经网络架构的例子

285
00:10:07,480 --> 00:10:08,750
你可以

286
00:10:09,260 --> 00:10:10,770
意识到这个第二层

287
00:10:10,940 --> 00:10:12,180
是如何工作的

288
00:10:12,900 --> 00:10:14,120
在这里 我们有三个隐藏单元

289
00:10:14,910 --> 00:10:16,200
它们根据输入层

290
00:10:16,660 --> 00:10:17,900
计算一个复杂的函数

291
00:10:17,990 --> 00:10:19,530
然后第三层

292
00:10:19,730 --> 00:10:20,750
可以将第二层

293
00:10:20,840 --> 00:10:22,260
训练出的特征项作为输入

294
00:10:22,550 --> 00:10:24,070
并在第三层计算一些更复杂的函数

295
00:10:24,980 --> 00:10:25,880
这样 在你到达

296
00:10:25,960 --> 00:10:27,160
输出层之前 即第四层

297
00:10:27,900 --> 00:10:29,130
就可以利用第三层

298
00:10:29,370 --> 00:10:30,690
训练出的更复杂的

299
00:10:30,860 --> 00:10:32,040
特征项作为输入

300
00:10:32,280 --> 00:10:34,710
以此得到非常有趣的非线性假设

301
00:10:36,730 --> 00:10:37,580
顺便说一下 在这样的

302
00:10:37,810 --> 00:10:38,980
网络里 第一层

303
00:10:39,130 --> 00:10:40,670
被称为输入层 第四层

304
00:10:41,360 --> 00:10:43,170
仍然是我们的输出层

305
00:10:43,340 --> 00:10:45,040
这个网络有两个隐藏层

306
00:10:46,000 --> 00:10:47,440
所以 任何一个不是

307
00:10:48,000 --> 00:10:49,020
输入层或输出层的

308
00:10:49,340 --> 00:10:50,590
都被称为隐藏层

309
00:10:53,390 --> 00:10:54,470
我希望从这个视频中

310
00:10:54,760 --> 00:10:55,840
你已经大致理解

311
00:10:56,140 --> 00:10:58,360
前向传播在

312
00:10:58,830 --> 00:11:00,230
神经网络里的工作原理：

313
00:11:00,390 --> 00:11:01,670
从输入层的激励

314
00:11:01,720 --> 00:11:03,150
开始 向前

315
00:11:03,450 --> 00:11:04,480
传播到

316
00:11:04,570 --> 00:11:05,560
第一隐藏层 然后传播到第二

317
00:11:06,070 --> 00:11:08,200
隐藏层 最终到达输出层

318
00:11:08,990 --> 00:11:10,250
并且你也知道了如何

319
00:11:10,560 --> 00:11:12,010
向量化这些计算

320
00:11:13,660 --> 00:11:14,830
我发现

321
00:11:15,240 --> 00:11:16,680
这个视频里我讲了

322
00:11:16,850 --> 00:11:19,220
某些层是如何

323
00:11:19,550 --> 00:11:22,570
计算前面层的复杂特征项

324
00:11:22,910 --> 00:11:23,540
我意识到这可能

325
00:11:24,190 --> 00:11:26,660
仍然有点抽象 显得比较高端

326
00:11:27,450 --> 00:11:28,240
所以 我将

327
00:11:28,350 --> 00:11:29,460
在接下来的两个视频中

328
00:11:30,210 --> 00:11:31,540
讨论具体的例子

329
00:11:32,510 --> 00:11:33,810
它描述了怎样用神经网络

330
00:11:33,960 --> 00:11:35,740
来计算

331
00:11:36,710 --> 00:11:38,030
输入的非线性函数

332
00:11:38,330 --> 00:11:39,450
希望能使你

333
00:11:39,540 --> 00:11:40,860
更好的理解

334
00:11:41,010 --> 00:11:44,630
从神经网络中得到的复杂非线性假设 【教育无边界字幕组】翻译:午后阳光 校对:sherry 审核:所羅門捷列夫