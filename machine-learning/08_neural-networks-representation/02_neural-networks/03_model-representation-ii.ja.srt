1
00:00:00,280 --> 00:00:01,330
前回のビデオでは

2
00:00:01,570 --> 00:00:03,540
ニューラルネットワークで使われる仮説を

3
00:00:03,700 --> 00:00:04,990
どう表現するか、または

4
00:00:05,090 --> 00:00:07,160
どう計算するかの数学的な定義を与えた。

5
00:00:08,420 --> 00:00:09,620
このビデオでは、

6
00:00:09,730 --> 00:00:11,280
その計算をどう実際に

7
00:00:11,450 --> 00:00:14,040
効率良く実行するかをお見せしたい、

8
00:00:14,710 --> 00:00:16,050
これはベクトル化した実装を見せるという事。

9
00:00:17,660 --> 00:00:18,930
次に、より重要な事として

10
00:00:19,100 --> 00:00:21,110
これらのニューラルネットワークの表現が

11
00:00:21,390 --> 00:00:22,590
なんで良さげなアイデアなのかを

12
00:00:23,360 --> 00:00:24,640
直感的に分かるような感覚を伝えたい。

13
00:00:25,010 --> 00:00:27,290
それらが非線形の複雑な仮説を学ぶ助けとなる。

14
00:00:28,970 --> 00:00:29,880
こんなニューラルネットワークを考えよう。

15
00:00:30,520 --> 00:00:31,720
以前、仮説の出力を得るのに

16
00:00:32,010 --> 00:00:33,070
必要な計算の

17
00:00:33,170 --> 00:00:34,090
ステップは

18
00:00:34,650 --> 00:00:35,850
これらの左にある

19
00:00:36,320 --> 00:00:37,780
式だと言った、

20
00:00:37,950 --> 00:00:38,770
アクティベーションの値があり

21
00:00:39,540 --> 00:00:41,330
3つの隠れユニットがあり

22
00:00:41,450 --> 00:00:43,220
それらを使って最終的な

23
00:00:43,420 --> 00:00:44,580
仮説の出力、h(x)を

24
00:00:44,650 --> 00:00:45,710
計算する。

25
00:00:46,680 --> 00:00:48,410
ここで幾つか

26
00:00:48,480 --> 00:00:50,200
追加の項を定義する。

27
00:00:50,570 --> 00:00:52,210
ここの下線を引いた

28
00:00:52,410 --> 00:00:54,090
この項は

29
00:00:54,180 --> 00:00:55,560
zの上付き添字2下付き添字1と

30
00:00:56,230 --> 00:00:58,410
定義する。

31
00:00:58,790 --> 00:00:59,830
つまりaの(2) 1は

32
00:01:00,650 --> 00:01:02,310
つまりこの項は

33
00:01:02,470 --> 00:01:03,930
gのz(2) 1と

34
00:01:04,170 --> 00:01:06,020
等しくなる。

35
00:01:06,130 --> 00:01:08,100
ところで

36
00:01:08,180 --> 00:01:09,750
これらの上付き添字2は

37
00:01:10,570 --> 00:01:11,580
見ての通り、その意味は

38
00:01:11,870 --> 00:01:12,960
このz(2)とa(2)はともに

39
00:01:13,080 --> 00:01:14,140
この上付きのカッコでくくられた2は

40
00:01:14,840 --> 00:01:16,450
これらの値が

41
00:01:16,740 --> 00:01:18,330
2番目のレイヤーと関連づけられている事を意味する、

42
00:01:18,570 --> 00:01:19,810
それはこの場合はニューラルネットワークの

43
00:01:20,100 --> 00:01:21,390
隠れレイヤに相当する。

44
00:01:22,820 --> 00:01:25,200
同様にこの項を

45
00:01:25,990 --> 00:01:27,640
z(2)　2と

46
00:01:29,530 --> 00:01:30,140
定義する。

47
00:01:30,490 --> 00:01:31,860
最後に、この最後の

48
00:01:32,170 --> 00:01:33,100
下線を引いたこの項は、

49
00:01:34,160 --> 00:01:37,040
z(2) 3と定義する。

50
00:01:37,090 --> 00:01:38,710
すると同様にa(2) 3は

51
00:01:38,850 --> 00:01:43,200
イコール

52
00:01:44,990 --> 00:01:45,360
gの z(2) 3となる。

53
00:01:45,480 --> 00:01:46,760
これらzの値は単なる

54
00:01:47,290 --> 00:01:48,940
個々のニューロンに入っていく

55
00:01:49,360 --> 00:01:51,200
入力の値であるx0、

56
00:01:51,490 --> 00:01:52,800
x1、x2、x3の

57
00:01:53,060 --> 00:01:55,350
重み付けした線形和に過ぎない。

58
00:01:57,090 --> 00:01:58,260
ここでここ数のかたまりを

59
00:01:58,900 --> 00:02:00,470
見てみると

60
00:02:01,990 --> 00:02:03,310
この数のかたまりは

61
00:02:03,490 --> 00:02:05,880
行列とベクトルの積に

62
00:02:06,950 --> 00:02:08,330
どうも似ている。

63
00:02:08,800 --> 00:02:10,260
行列掛けるベクトルの

64
00:02:11,070 --> 00:02:12,710
シータ1 掛ける xに。

65
00:02:12,790 --> 00:02:14,840
この観察に基づき、

66
00:02:15,580 --> 00:02:18,730
このニューラルネットワークの計算を

67
00:02:19,700 --> 00:02:20,280
ベクトル化出来る。

68
00:02:21,470 --> 00:02:23,510
具体的に言うと、フィーチャーベクトルxを

69
00:02:23,680 --> 00:02:24,810
いつも通り、

70
00:02:25,290 --> 00:02:27,020
x0、x1、x2、x3のベクトルとしよう。ここで

71
00:02:27,260 --> 00:02:28,550
x0はいつも通り

72
00:02:29,010 --> 00:02:30,280
いつでも1とする。

73
00:02:30,610 --> 00:02:31,860
そしてz2ベクトルを

74
00:02:32,390 --> 00:02:33,420
これらのzの値、

75
00:02:34,360 --> 00:02:37,250
z(2) 1、z(2) 2、z(2) 3のベクトルと定義する。

76
00:02:38,560 --> 00:02:40,210
そしてこの三次元ベクトル、

77
00:02:40,440 --> 00:02:42,500
z2を用いて、

78
00:02:43,910 --> 00:02:47,200
a(2) 1、 a(2) 2、 a(2) 3の計算を

79
00:02:48,270 --> 00:02:48,860
以下のようにベクトル化する事が出来る。

80
00:02:49,490 --> 00:02:50,690
これを2つのステップで書く。

81
00:02:51,500 --> 00:02:53,400
z2は、シータ1掛けるxで

82
00:02:53,950 --> 00:02:55,490
計算出来る。

83
00:02:55,790 --> 00:02:57,020
そしてそれでz2ベクトルが得られる。

84
00:02:57,400 --> 00:02:59,360
次にa2は

85
00:02:59,860 --> 00:03:02,180
gのz2で計算出来る、

86
00:03:02,440 --> 00:03:03,860
ここではっきりさせておきたいのは、

87
00:03:04,200 --> 00:03:05,880
このz2は三次元ベクトルで

88
00:03:06,060 --> 00:03:08,150
a2も三次元ベクトルという事。

89
00:03:08,810 --> 00:03:10,410
だからこのactivation g、

90
00:03:10,690 --> 00:03:12,680
これはz2の各要素に対し

91
00:03:12,950 --> 00:03:15,290
sigmoid関数を

92
00:03:15,550 --> 00:03:18,290
適用した物。

93
00:03:18,380 --> 00:03:19,270
ところで、のちほどのノーテーションと

94
00:03:19,950 --> 00:03:21,260
もう少し一貫させておくと

95
00:03:21,440 --> 00:03:23,330
この入力レイヤーは

96
00:03:23,590 --> 00:03:24,600
入力がxな訳だが

97
00:03:24,670 --> 00:03:25,840
それは最初のレイヤの

98
00:03:25,960 --> 00:03:26,950
アクティベーションとも

99
00:03:27,300 --> 00:03:29,270
考えられる。

100
00:03:29,680 --> 00:03:30,430
だからa1をxと

101
00:03:30,470 --> 00:03:32,510
定義する。

102
00:03:32,660 --> 00:03:34,270
つまりa1はベクトルで

103
00:03:34,500 --> 00:03:35,520
ここのxを

104
00:03:36,230 --> 00:03:38,850
置き換えて、z2イコール

105
00:03:39,570 --> 00:03:40,680
シータ1掛けるa1となる、

106
00:03:41,410 --> 00:03:43,350
a1を入力レイヤのアクティベーションとしただけ。

107
00:03:44,990 --> 00:03:46,000
さて、ここまで書いてきた事で

108
00:03:46,280 --> 00:03:47,500
いまやa1、a2、a3の値を

109
00:03:47,900 --> 00:03:49,940
得た訳だが

110
00:03:50,820 --> 00:03:52,690
ここでも

111
00:03:52,780 --> 00:03:53,980
上付き添字を

112
00:03:54,290 --> 00:03:55,600
つけておこう。

113
00:03:56,430 --> 00:03:57,530
だがもう一つ必要な値がある、

114
00:03:57,940 --> 00:03:59,810
それはa(0) 2だ、

115
00:04:00,050 --> 00:04:02,050
それはつまり

116
00:04:02,250 --> 00:04:04,350
隠れレイヤの

117
00:04:04,550 --> 00:04:06,420
バイアスユニットで、それは出力のここに行く。

118
00:04:06,990 --> 00:04:07,780
もちろん、ここにも

119
00:04:07,810 --> 00:04:08,850
バイアスユニットはある。

120
00:04:09,000 --> 00:04:10,060
ここの下には

121
00:04:10,270 --> 00:04:11,820
書かなかっただけで。

122
00:04:11,970 --> 00:04:13,100
だがこの追加のバイアスユニットを扱う為に

123
00:04:13,870 --> 00:04:15,650
ここに追加の

124
00:04:16,320 --> 00:04:18,720
a(0) 2を足す。

125
00:04:18,890 --> 00:04:20,870
それはイコール1だ。

126
00:04:21,010 --> 00:04:21,990
このステップを経る事で

127
00:04:22,290 --> 00:04:23,860
a2を4次元のフィーチャーベクトルとして

128
00:04:24,010 --> 00:04:25,390
得る事が出来た。

129
00:04:25,690 --> 00:04:26,820
何故ならたった今この

130
00:04:27,300 --> 00:04:28,490
追加のa0を

131
00:04:28,620 --> 00:04:30,260
それはイコール1で

132
00:04:30,500 --> 00:04:31,700
隠れレイヤーのバイアスユニットに対応するが

133
00:04:32,080 --> 00:04:33,550
それを足したからだ。
そして最後に

134
00:04:35,080 --> 00:04:37,620
実際に仮説の出力を

135
00:04:38,070 --> 00:04:40,100
計算するために、

136
00:04:40,250 --> 00:04:41,190
z3を計算する必要がある。

137
00:04:42,470 --> 00:04:44,980
z3はここの下線を引いた所に

138
00:04:45,350 --> 00:04:47,940
この項に等しい。

139
00:04:48,800 --> 00:04:51,450
この内側の項がz3だ。

140
00:04:53,980 --> 00:04:55,160
そしてz3はシータ2掛ける

141
00:04:55,500 --> 00:04:57,120
a2だ。

142
00:04:57,810 --> 00:04:59,560
そして最終的に、仮説の出力であるhのxは、

143
00:04:59,750 --> 00:05:01,210
a3で、

144
00:05:01,360 --> 00:05:03,910
それは出力レイヤの

145
00:05:04,750 --> 00:05:06,040
唯一のアクティベーションだ。

146
00:05:06,290 --> 00:05:09,500
つまり単なる実数で、a3は

147
00:05:10,050 --> 00:05:12,390
a(3) 1とも書けるが、それはgのz3だ。

148
00:05:13,240 --> 00:05:15,020
このhのxを計算するプロセスは

149
00:05:15,940 --> 00:05:18,110
フォワードプロパゲーションとも呼ばれる。

150
00:05:19,130 --> 00:05:20,440
その訳は入力ユニットの

151
00:05:20,550 --> 00:05:21,310
アクティベーションから始めて

152
00:05:22,010 --> 00:05:24,400
そしてある意味で

153
00:05:24,940 --> 00:05:26,770
隠れレイヤに対し、

154
00:05:26,860 --> 00:05:29,390
前方(フォワード)に伝播(プロパゲート)させて

155
00:05:29,580 --> 00:05:30,400
アクティベーションを計算し、

156
00:05:30,540 --> 00:05:32,040
それをさらに前方へ伝播させていって

157
00:05:32,760 --> 00:05:36,270
出力レイヤのアクティベーションを

158
00:05:37,480 --> 00:05:39,170
計算するからだ。入力から始めて

159
00:05:39,290 --> 00:05:40,400
隠れレイヤ、そして出力レイヤ。

160
00:05:40,940 --> 00:05:42,030
それをフォワードプロパゲーションと呼ぶ。

161
00:05:43,320 --> 00:05:44,150
そしてここまでやったのは

162
00:05:44,310 --> 00:05:45,370
この手順の

163
00:05:45,740 --> 00:05:47,140
ベクトル化した実装を遂げたという事。

164
00:05:47,280 --> 00:05:48,890
だからこの右側にある

165
00:05:48,970 --> 00:05:50,260
式でそれを実装すれば

166
00:05:50,800 --> 00:05:51,740
hのxを計算する

167
00:05:51,850 --> 00:05:53,280
効率的な方法と

168
00:05:53,460 --> 00:05:54,980
なってくれると

169
00:05:55,120 --> 00:05:56,130
言う訳だ。

170
00:05:58,250 --> 00:05:59,860
このフォワードプロパゲーションの見方は

171
00:06:00,860 --> 00:06:02,270
ニューラルネットワークが何を

172
00:06:02,550 --> 00:06:03,640
してくれるかを理解する助けともなってくれ、

173
00:06:04,110 --> 00:06:05,290
それは何故ニューラルネットワークが興味深い非線形の仮説を

174
00:06:05,510 --> 00:06:07,170
学習する助けとなるかもしれないかも教えてくれる。

175
00:06:08,670 --> 00:06:09,760
以下のようなニューラルネットワークを考えてみよう、

176
00:06:10,500 --> 00:06:11,820
そしてこの図の左側の部分を

177
00:06:12,040 --> 00:06:13,810
カバーで隠すとしよう。

178
00:06:14,650 --> 00:06:16,170
この図の残った所を見ると、

179
00:06:17,030 --> 00:06:18,020
これはまるで

180
00:06:18,260 --> 00:06:19,520
ロジスティック回帰のようだ、

181
00:06:19,660 --> 00:06:20,570
我らがやってる事は

182
00:06:20,990 --> 00:06:22,000
このノードを使って、

183
00:06:22,130 --> 00:06:23,770
このノードは単なるロジスティック回帰だ、

184
00:06:24,120 --> 00:06:26,060
それを使ってhのxという

185
00:06:26,380 --> 00:06:28,290
予言を行う。
具体的には

186
00:06:28,440 --> 00:06:30,340
仮説が出力するのは

187
00:06:30,710 --> 00:06:31,830
hのxは

188
00:06:31,890 --> 00:06:33,760
イコール、gの、、、

189
00:06:33,980 --> 00:06:38,110
これはsigmoid関数のアクティベーションだが、それに

190
00:06:38,560 --> 00:06:40,450
シータ0掛けるa0足すことの

191
00:06:41,270 --> 00:06:43,380
シータ1掛けるa1

192
00:06:45,220 --> 00:06:49,080
足すことのシータ2掛けるa2

193
00:06:49,260 --> 00:06:52,090
足すことのシータ3掛けるa3で、

194
00:06:52,830 --> 00:06:55,180
ここで値

195
00:06:55,370 --> 00:06:56,910
a1、a2、a3は

196
00:06:57,050 --> 00:06:59,860
これらのユニットから得られる値。

197
00:07:01,060 --> 00:07:02,790
これまでのノーテーションと

198
00:07:03,490 --> 00:07:05,000
一貫させる為には

199
00:07:05,170 --> 00:07:06,360
ここの全部に

200
00:07:06,470 --> 00:07:10,700
上付き添字の2 を追加する必要がある。

201
00:07:12,260 --> 00:07:13,920
さらにこれらの

202
00:07:14,160 --> 00:07:16,800
ここの所に1を追加する必要がある、何故なら

203
00:07:16,930 --> 00:07:20,610
出力ユニットは一つしか無いから。
でもこの青い部分に集中して見ると、

204
00:07:20,930 --> 00:07:21,900
これはいかにも普通の

205
00:07:22,150 --> 00:07:23,680
ロジスティック回帰のモデルで、

206
00:07:23,870 --> 00:07:25,530
小文字のシータの代わりに

207
00:07:25,600 --> 00:07:28,060
大文字のシータって所だけが違う。

208
00:07:29,170 --> 00:07:30,690
そしてこれがやってる事は

209
00:07:30,850 --> 00:07:32,520
単なるロジスティック回帰だ。

210
00:07:33,660 --> 00:07:35,240
だがロジスティック回帰に食わせる

211
00:07:35,590 --> 00:07:37,250
フィーチャーは、隠れレイヤによって

212
00:07:38,200 --> 00:07:40,170
計算されるこれらの値だ。

213
00:07:41,340 --> 00:07:42,690
もう一度言おう。

214
00:07:42,910 --> 00:07:44,420
このニューラルネットワークがやる事は

215
00:07:45,130 --> 00:07:47,050
ロジスティック回帰みたいなもんだが、

216
00:07:47,440 --> 00:07:48,900
もともとのフィーチャーである

217
00:07:49,110 --> 00:07:50,770
x1、x2、x3を使う代わりに

218
00:07:52,400 --> 00:07:54,260
これらの新しいフィーチャーa1、a2、a3を使うって所だけが違う。

219
00:07:54,440 --> 00:07:56,810
ここでも上付き添字をつけた、

220
00:07:58,130 --> 00:08:00,380
ノーテーションを揃える為だ。

221
00:08:02,820 --> 00:08:04,610
これのクールな所は

222
00:08:05,040 --> 00:08:06,220
フィーチャーa1，a2，a3，は

223
00:08:06,720 --> 00:08:08,310
それら自身が入力の関数として

224
00:08:08,760 --> 00:08:09,930
学習された物である、という事。

225
00:08:10,960 --> 00:08:12,640
具体的には、レイヤー1からレイヤー2へと

226
00:08:13,320 --> 00:08:14,540
マッピングする関数、

227
00:08:14,810 --> 00:08:16,390
それは別のパラメータの組である

228
00:08:16,750 --> 00:08:18,550
シータ1で定義される。

229
00:08:19,380 --> 00:08:20,210
つまりニューラルネットワークは

230
00:08:20,270 --> 00:08:22,030
ロジスティック回帰への

231
00:08:22,240 --> 00:08:24,050
入力のフィーチャーを

232
00:08:24,120 --> 00:08:25,760
x1、x2、x3、に制限する代わりに

233
00:08:26,210 --> 00:08:27,440
独自のフィーチャー

234
00:08:27,720 --> 00:08:29,320
a1，a2，a3を学習して

235
00:08:29,810 --> 00:08:32,010
それをロジスティック回帰に

236
00:08:32,130 --> 00:08:33,950
食わせられるような物だ。

237
00:08:34,650 --> 00:08:36,270
そして想像出来ると思うが、

238
00:08:36,360 --> 00:08:37,690
シータ1に何を選ぶかによって

239
00:08:37,900 --> 00:08:39,880
とても興味深く複雑な

240
00:08:40,390 --> 00:08:42,460
フィーチャーを学習出来て、

241
00:08:43,780 --> 00:08:44,830
それゆえに、結果として

242
00:08:45,050 --> 00:08:46,650
単に生のフィーチャーx1、x2、x3に

243
00:08:46,840 --> 00:08:47,870
食わせるフィーチャーを限定したり、

244
00:08:48,020 --> 00:08:50,520
その多項式、

245
00:08:50,640 --> 00:08:52,530
x1x2、x2x3などの多項式から選ぶより

246
00:08:52,620 --> 00:08:53,730
より良い仮説が

247
00:08:53,920 --> 00:08:55,550
得られる。

248
00:08:55,790 --> 00:08:57,250
このアルゴリズムはそれらの代わりに

249
00:08:57,530 --> 00:08:59,130
どんなフィーチャーでも一旦

250
00:08:59,420 --> 00:09:01,990
学習する柔軟性があり、

251
00:09:02,680 --> 00:09:03,990
そしてこれらa1，a2，a3を使って

252
00:09:04,110 --> 00:09:05,190
この最後のユニットに食わせる為に使う事が出来、

253
00:09:05,510 --> 00:09:07,830
この最後は本質的には

254
00:09:09,240 --> 00:09:11,920
ここのロジスティック回帰だ。

255
00:09:12,550 --> 00:09:13,970
ここに記述された例は

256
00:09:14,060 --> 00:09:15,500
いくらかハイレベルで、

257
00:09:15,750 --> 00:09:16,520
このニューラルネットワークの直感的な話が

258
00:09:17,440 --> 00:09:18,870
つまりより複雑なフィーチャーを

259
00:09:19,720 --> 00:09:21,420
使うというのが

260
00:09:21,630 --> 00:09:23,120
ピンと来てるか怪しい、というのは分かってる。

261
00:09:23,210 --> 00:09:24,440
でもまだピンと来ないなぁ、と思ってても、

262
00:09:24,810 --> 00:09:25,860
次の2つのビデオで、

263
00:09:25,970 --> 00:09:27,300
ニューラルネットワークが

264
00:09:28,250 --> 00:09:29,590
この隠れユニットを使って

265
00:09:29,830 --> 00:09:30,860
より複雑なフィーチャーを計算して

266
00:09:31,250 --> 00:09:32,880
それをこの最後のレイヤに食わせるか、という

267
00:09:33,130 --> 00:09:34,520
具体例を見ていく。

268
00:09:35,060 --> 00:09:37,100
そしてどうやってより複雑な仮説を学ぶ事が出来るのかということも見ていく。

269
00:09:37,920 --> 00:09:39,120
だからもし今回のビデオで

270
00:09:39,180 --> 00:09:40,090
私が言った事がいまいちピンと来なくても

271
00:09:40,230 --> 00:09:41,650
次の2つのビデオに

272
00:09:41,810 --> 00:09:42,960
頑張ってついてきてくれれば、

273
00:09:43,190 --> 00:09:44,370
それらの例を通して、

274
00:09:44,580 --> 00:09:46,690
きっとこの説明が

275
00:09:47,030 --> 00:09:48,640
よりはっきり分かると思う。

276
00:09:49,020 --> 00:09:49,740
だけど指摘しておくと、

277
00:09:49,820 --> 00:09:51,120
他の図式の形のニューラルネットワークも

278
00:09:51,470 --> 00:09:52,990
可能で、

279
00:09:53,080 --> 00:09:54,270
ニューラルネットワークがどう

280
00:09:54,450 --> 00:09:58,000
接続されるかはアーキテクチャと呼ばれる。

281
00:09:58,390 --> 00:10:00,150
つまりアーキテクチャという用語は

282
00:10:00,490 --> 00:10:02,380
異なるニューロンが互いにどう接続されるかを表す言葉。

283
00:10:03,220 --> 00:10:04,180
これは異なるニューラルネットの

284
00:10:04,840 --> 00:10:06,300
アーキテクチャの例だ。

285
00:10:07,480 --> 00:10:08,750
ここでもういちど

286
00:10:09,260 --> 00:10:10,770
以下のような直感を得ることが出来るかもしれない、

287
00:10:10,940 --> 00:10:12,180
それは二番目のレイヤ、

288
00:10:12,900 --> 00:10:14,120
ここの3つのユニットが

289
00:10:14,910 --> 00:10:16,200
何か複雑な関数、

290
00:10:16,660 --> 00:10:17,900
入力レイヤの複雑な関数を

291
00:10:17,990 --> 00:10:19,530
計算し、そして次に

292
00:10:19,730 --> 00:10:20,750
三番目のレイヤが二番目のレイヤから

293
00:10:20,840 --> 00:10:22,260
フィーチャーを受け取り、

294
00:10:22,550 --> 00:10:24,070
さらに複雑なフィーチャーを三番目のレイヤで行い、

295
00:10:24,980 --> 00:10:25,880
だから出力レイヤ、つまり四番目のレイヤに

296
00:10:25,960 --> 00:10:27,160
たどり着く頃には

297
00:10:27,900 --> 00:10:29,130
レイヤ3で計算した

298
00:10:29,370 --> 00:10:30,690
さらに複雑なフィーチャーを

299
00:10:30,860 --> 00:10:32,040
使う事が出来て、

300
00:10:32,280 --> 00:10:34,710
だからとても興味深い非線形の仮説を得る事が出来る、という事。

301
00:10:36,730 --> 00:10:37,580
ところで、こんなネットワークでも

302
00:10:37,810 --> 00:10:38,980
1つ目のレイヤ、これは

303
00:10:39,130 --> 00:10:40,670
入力レイヤと呼ばれ、

304
00:10:41,360 --> 00:10:43,170
レイヤ4も出力レイヤのままだ、

305
00:10:43,340 --> 00:10:45,040
そしてこのネットワークは2つの隠れレイヤを持っている。

306
00:10:46,000 --> 00:10:47,440
つまり入力レイヤでもなく

307
00:10:48,000 --> 00:10:49,020
出力レイヤでも無い物は

308
00:10:49,340 --> 00:10:50,590
全て隠れレイヤと呼ぶ。

309
00:10:53,390 --> 00:10:54,470
さて、このビデオで、

310
00:10:54,760 --> 00:10:55,840
ニューラルネットワークのフォワードプロパゲーションのステップが

311
00:10:56,140 --> 00:10:58,360
どういう感じで機能するか、

312
00:10:58,830 --> 00:11:00,230
分かっていただけただろうか。

313
00:11:00,390 --> 00:11:01,670
入力レイヤのアクティベーションから

314
00:11:01,720 --> 00:11:03,150
始めて、前方に伝播させていき、

315
00:11:03,450 --> 00:11:04,480
まず最初の隠れレイヤ、次に

316
00:11:04,570 --> 00:11:05,560
二番目の隠れレイヤ、と、

317
00:11:06,070 --> 00:11:08,200
そして最後に出力レイヤへと伝播させていく。

318
00:11:08,990 --> 00:11:10,250
そしてまた、それをどう

319
00:11:10,560 --> 00:11:12,010
ベクトル化して計算するかも見た。

320
00:11:13,660 --> 00:11:14,830
次回にやる事は、

321
00:11:15,240 --> 00:11:16,680
こごでの直感的な話、

322
00:11:16,850 --> 00:11:19,220
このビデオでやったあるレイヤが

323
00:11:19,550 --> 00:11:22,570
前のレイヤの複雑なフィーチャーを計算する、という話が

324
00:11:22,910 --> 00:11:23,540
いくらかそれらの直感的な話は、

325
00:11:24,190 --> 00:11:26,660
まだちょっと抽象的で高級だと思う。

326
00:11:27,450 --> 00:11:28,240
だから次の2つの動画で私は

327
00:11:28,350 --> 00:11:29,460
ニューラルネットワークが

328
00:11:30,210 --> 00:11:31,540
入力の非線形の関数として

329
00:11:32,510 --> 00:11:33,810
複雑な計算が行えるかの

330
00:11:33,960 --> 00:11:35,740
具体例をより詳細に

331
00:11:36,710 --> 00:11:38,030
見ていきたい。

332
00:11:38,330 --> 00:11:39,450
それを通して、ニューラルネットワークで出来る

333
00:11:39,540 --> 00:11:40,860
複雑な非線形の仮説についての

334
00:11:41,010 --> 00:11:44,630
感覚が良く身に着くことを狙って。