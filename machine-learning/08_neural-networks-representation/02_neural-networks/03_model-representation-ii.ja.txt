前回のビデオでは ニューラルネットワークで使われる仮説を どう表現するか、または どう計算するかの数学的な定義を与えた。 このビデオでは、 その計算をどう実際に 効率良く実行するかをお見せしたい、 これはベクトル化した実装を見せるという事。 次に、より重要な事として これらのニューラルネットワークの表現が なんで良さげなアイデアなのかを 直感的に分かるような感覚を伝えたい。 それらが非線形の複雑な仮説を学ぶ助けとなる。 こんなニューラルネットワークを考えよう。 以前、仮説の出力を得るのに 必要な計算の ステップは これらの左にある 式だと言った、 アクティベーションの値があり 3つの隠れユニットがあり それらを使って最終的な 仮説の出力、h(x)を 計算する。 ここで幾つか 追加の項を定義する。 ここの下線を引いた この項は zの上付き添字2下付き添字1と 定義する。 つまりaの(2) 1は つまりこの項は gのz(2) 1と 等しくなる。 ところで これらの上付き添字2は 見ての通り、その意味は このz(2)とa(2)はともに この上付きのカッコでくくられた2は これらの値が 2番目のレイヤーと関連づけられている事を意味する、 それはこの場合はニューラルネットワークの 隠れレイヤに相当する。 同様にこの項を z(2)　2と 定義する。 最後に、この最後の 下線を引いたこの項は、 z(2) 3と定義する。 すると同様にa(2) 3は イコール gの z(2) 3となる。 これらzの値は単なる 個々のニューロンに入っていく 入力の値であるx0、 x1、x2、x3の 重み付けした線形和に過ぎない。 ここでここ数のかたまりを 見てみると この数のかたまりは 行列とベクトルの積に どうも似ている。 行列掛けるベクトルの シータ1 掛ける xに。 この観察に基づき、 このニューラルネットワークの計算を ベクトル化出来る。 具体的に言うと、フィーチャーベクトルxを いつも通り、 x0、x1、x2、x3のベクトルとしよう。ここで x0はいつも通り いつでも1とする。 そしてz2ベクトルを これらのzの値、 z(2) 1、z(2) 2、z(2) 3のベクトルと定義する。 そしてこの三次元ベクトル、 z2を用いて、 a(2) 1、 a(2) 2、 a(2) 3の計算を 以下のようにベクトル化する事が出来る。 これを2つのステップで書く。 z2は、シータ1掛けるxで 計算出来る。 そしてそれでz2ベクトルが得られる。 次にa2は gのz2で計算出来る、 ここではっきりさせておきたいのは、 このz2は三次元ベクトルで a2も三次元ベクトルという事。 だからこのactivation g、 これはz2の各要素に対し sigmoid関数を 適用した物。 ところで、のちほどのノーテーションと もう少し一貫させておくと この入力レイヤーは 入力がxな訳だが それは最初のレイヤの アクティベーションとも 考えられる。 だからa1をxと 定義する。 つまりa1はベクトルで ここのxを 置き換えて、z2イコール シータ1掛けるa1となる、 a1を入力レイヤのアクティベーションとしただけ。 さて、ここまで書いてきた事で いまやa1、a2、a3の値を 得た訳だが ここでも 上付き添字を つけておこう。 だがもう一つ必要な値がある、 それはa(0) 2だ、 それはつまり 隠れレイヤの バイアスユニットで、それは出力のここに行く。 もちろん、ここにも バイアスユニットはある。 ここの下には 書かなかっただけで。 だがこの追加のバイアスユニットを扱う為に ここに追加の a(0) 2を足す。 それはイコール1だ。 このステップを経る事で a2を4次元のフィーチャーベクトルとして 得る事が出来た。 何故ならたった今この 追加のa0を それはイコール1で 隠れレイヤーのバイアスユニットに対応するが それを足したからだ。
そして最後に 実際に仮説の出力を 計算するために、 z3を計算する必要がある。 z3はここの下線を引いた所に この項に等しい。 この内側の項がz3だ。 そしてz3はシータ2掛ける a2だ。 そして最終的に、仮説の出力であるhのxは、 a3で、 それは出力レイヤの 唯一のアクティベーションだ。 つまり単なる実数で、a3は a(3) 1とも書けるが、それはgのz3だ。 このhのxを計算するプロセスは フォワードプロパゲーションとも呼ばれる。 その訳は入力ユニットの アクティベーションから始めて そしてある意味で 隠れレイヤに対し、 前方(フォワード)に伝播(プロパゲート)させて アクティベーションを計算し、 それをさらに前方へ伝播させていって 出力レイヤのアクティベーションを 計算するからだ。入力から始めて 隠れレイヤ、そして出力レイヤ。 それをフォワードプロパゲーションと呼ぶ。 そしてここまでやったのは この手順の ベクトル化した実装を遂げたという事。 だからこの右側にある 式でそれを実装すれば hのxを計算する 効率的な方法と なってくれると 言う訳だ。 このフォワードプロパゲーションの見方は ニューラルネットワークが何を してくれるかを理解する助けともなってくれ、 それは何故ニューラルネットワークが興味深い非線形の仮説を 学習する助けとなるかもしれないかも教えてくれる。 以下のようなニューラルネットワークを考えてみよう、 そしてこの図の左側の部分を カバーで隠すとしよう。 この図の残った所を見ると、 これはまるで ロジスティック回帰のようだ、 我らがやってる事は このノードを使って、 このノードは単なるロジスティック回帰だ、 それを使ってhのxという 予言を行う。
具体的には 仮説が出力するのは hのxは イコール、gの、、、 これはsigmoid関数のアクティベーションだが、それに シータ0掛けるa0足すことの シータ1掛けるa1 足すことのシータ2掛けるa2 足すことのシータ3掛けるa3で、 ここで値 a1、a2、a3は これらのユニットから得られる値。 これまでのノーテーションと 一貫させる為には ここの全部に 上付き添字の2 を追加する必要がある。 さらにこれらの ここの所に1を追加する必要がある、何故なら 出力ユニットは一つしか無いから。
でもこの青い部分に集中して見ると、 これはいかにも普通の ロジスティック回帰のモデルで、 小文字のシータの代わりに 大文字のシータって所だけが違う。 そしてこれがやってる事は 単なるロジスティック回帰だ。 だがロジスティック回帰に食わせる フィーチャーは、隠れレイヤによって 計算されるこれらの値だ。 もう一度言おう。 このニューラルネットワークがやる事は ロジスティック回帰みたいなもんだが、 もともとのフィーチャーである x1、x2、x3を使う代わりに これらの新しいフィーチャーa1、a2、a3を使うって所だけが違う。 ここでも上付き添字をつけた、 ノーテーションを揃える為だ。 これのクールな所は フィーチャーa1，a2，a3，は それら自身が入力の関数として 学習された物である、という事。 具体的には、レイヤー1からレイヤー2へと マッピングする関数、 それは別のパラメータの組である シータ1で定義される。 つまりニューラルネットワークは ロジスティック回帰への 入力のフィーチャーを x1、x2、x3、に制限する代わりに 独自のフィーチャー a1，a2，a3を学習して それをロジスティック回帰に 食わせられるような物だ。 そして想像出来ると思うが、 シータ1に何を選ぶかによって とても興味深く複雑な フィーチャーを学習出来て、 それゆえに、結果として 単に生のフィーチャーx1、x2、x3に 食わせるフィーチャーを限定したり、 その多項式、 x1x2、x2x3などの多項式から選ぶより より良い仮説が 得られる。 このアルゴリズムはそれらの代わりに どんなフィーチャーでも一旦 学習する柔軟性があり、 そしてこれらa1，a2，a3を使って この最後のユニットに食わせる為に使う事が出来、 この最後は本質的には ここのロジスティック回帰だ。 ここに記述された例は いくらかハイレベルで、 このニューラルネットワークの直感的な話が つまりより複雑なフィーチャーを 使うというのが ピンと来てるか怪しい、というのは分かってる。 でもまだピンと来ないなぁ、と思ってても、 次の2つのビデオで、 ニューラルネットワークが この隠れユニットを使って より複雑なフィーチャーを計算して それをこの最後のレイヤに食わせるか、という 具体例を見ていく。 そしてどうやってより複雑な仮説を学ぶ事が出来るのかということも見ていく。 だからもし今回のビデオで 私が言った事がいまいちピンと来なくても 次の2つのビデオに 頑張ってついてきてくれれば、 それらの例を通して、 きっとこの説明が よりはっきり分かると思う。 だけど指摘しておくと、 他の図式の形のニューラルネットワークも 可能で、 ニューラルネットワークがどう 接続されるかはアーキテクチャと呼ばれる。 つまりアーキテクチャという用語は 異なるニューロンが互いにどう接続されるかを表す言葉。 これは異なるニューラルネットの アーキテクチャの例だ。 ここでもういちど 以下のような直感を得ることが出来るかもしれない、 それは二番目のレイヤ、 ここの3つのユニットが 何か複雑な関数、 入力レイヤの複雑な関数を 計算し、そして次に 三番目のレイヤが二番目のレイヤから フィーチャーを受け取り、 さらに複雑なフィーチャーを三番目のレイヤで行い、 だから出力レイヤ、つまり四番目のレイヤに たどり着く頃には レイヤ3で計算した さらに複雑なフィーチャーを 使う事が出来て、 だからとても興味深い非線形の仮説を得る事が出来る、という事。 ところで、こんなネットワークでも 1つ目のレイヤ、これは 入力レイヤと呼ばれ、 レイヤ4も出力レイヤのままだ、 そしてこのネットワークは2つの隠れレイヤを持っている。 つまり入力レイヤでもなく 出力レイヤでも無い物は 全て隠れレイヤと呼ぶ。 さて、このビデオで、 ニューラルネットワークのフォワードプロパゲーションのステップが どういう感じで機能するか、 分かっていただけただろうか。 入力レイヤのアクティベーションから 始めて、前方に伝播させていき、 まず最初の隠れレイヤ、次に 二番目の隠れレイヤ、と、 そして最後に出力レイヤへと伝播させていく。 そしてまた、それをどう ベクトル化して計算するかも見た。 次回にやる事は、 こごでの直感的な話、 このビデオでやったあるレイヤが 前のレイヤの複雑なフィーチャーを計算する、という話が いくらかそれらの直感的な話は、 まだちょっと抽象的で高級だと思う。 だから次の2つの動画で私は ニューラルネットワークが 入力の非線形の関数として 複雑な計算が行えるかの 具体例をより詳細に 見ていきたい。 それを通して、ニューラルネットワークで出来る 複雑な非線形の仮説についての 感覚が良く身に着くことを狙って。