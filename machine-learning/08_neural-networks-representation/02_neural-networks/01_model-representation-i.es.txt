En este video quiero comenzar hablando sobre cómo representamos las redes neuronales, en otras palabras, cómo representamos nuestras hipótesis o cómo representamos nuestro modelo cuando utilizamos  redes neuronales. Las redes neuronales fueron desarrolladas como simulación de las neuronas o de las redes de neuronas en el cerebro. Así que, para explicar la representación de la hipótesis, vamos a empezar por observar cómo se ve una sola neurona en el cerebro. Tú cerebro y el mío están saturados de neuronas como estas y las neuronas son células en el cerebro, y las dos cosas que llaman la atención son primero que la neurona tiene un cuerpo celular como este y por otra parte, que la neurona tiene un número de cables de entrada llamados dendritas, que son como cabes de entrada y reciben entradas de otras ubicaciones, y la neurona también tiene un cable de salida llamado axón. Este cable de salida es lo que utiliza para enviar señales a otras neuronas o para enviar mensajes a otras neuronas. Así, en un nivel simple, una neurona es una unidad computacional que tiene un número de entradas a través de sus cables de entrada, y realiza algunos cálculos, y luego envía resultados, mediante su axón a otros nodos o a otras neuronas en el cerebro. Aquí hay una ilustración de un grupo de neuronas. La forma en que las neuronas se comunican unas con otras es con pequeñas pulsaciones eléctricas. También se llaman picos, pero se refieren a pequeños pulsos de electricidad. Así que, aquí está una neurona y lo que hace si quiere mandar un mensaje, es mandar un pequeño pulso de electricidad a través de su axón a diferentes neuronas y aquí a este axón. Tiene este cable abierto, que se conecta al cable de entrada o se conecta a la dendrita de esta segunda neurona por aquí, la cual entonces acepta este mensaje entrante, hace algunos cálculos y puede a su vez decidir enviar su mensajes o en su axón a otras neuronas. Y este es el proceso por el cual todo pensamiento humano ocurre mientras estas neuronas hacen cálculos y pasan mensajes a otras neuronas como resultado de lo que reciben en otras entradas. Y por cierto, así es cómo nuestros sentidos y nuestros músculos trabajan también. Si deseas mover uno de tus músculos, esto funciona gracias a que la neurona envía estas pulsaciones eléctricas a tus músculos y hace que tus músculos se contraigan y tus ojos, si algunos sensores como tus ojos quieren enviar un mensaje a tu cerebro, lo que hacen es mandar pulsaciones de electricidad a una neurona en tu cerebro. En una red neuronal, o en una red neuronal artificial que implementemos en una computadora, vamos a utiliza un modelo muy simple de lo que una neurona hace. Vamos a modelar una neurona como una unidad logística. Así que cuando yo dibujo un círculo amarillo como este, deberías pensar que es como jugar una función análoga, tal vez el cuerpo de una neurona, y cuando se alimenta la neurona con pocas entradas mediante sus dendritas o sus cables de entrada, la neurona hace algunos cálculos y da como resultado algún valor con su cable de salida o en una neurona biológica esa especie de axón y cada vez que dibujo un diagrama como este, lo que significa, lo que representa es un cálculo de, como sabes, h de x = 1 sobre 1 + e a la «theta» negativa transpuesta de x donde, como siempre, x y «theta» son nuestros vectores de parámetros. Así que esto es un muy simple, o un enormemente simplificado modelo de los cálculos que una neurona hace, donde obtiene el número de salidas, x1, x2, x3 y da como resultado algunos valores calculados así. Cuando dibujo una red neuronal, generalmente sólo dibujo los nodos de salida x1, x2, x3, a veces cuando es útil hacerlo. Dibujo un nodo extra para x0. Este nodo x0 es llamado en ocasiones unidad de oscilación o la neurona de oscilación porque x0 es igual a 1. A veces, dibujo el sesgo, a veces no, dependiendo de si es más conveniente en teoría para ese ejemplo Finalmente, un poco de terminología, cuando hablamos acerca de redes neuronales, a veces diremos que esta es una neurona, una neurona artificial con una función de activación sigmoidea o logística. Así que esta función de activación en la terminología de las redes neuronales es tan solo otro término para esa función, para esa no linealidad g de z = 1 sobre 1 + e elevado a z negativa. Y considerando que hasta ahora he estado llamando «theta» a los parámetros del modelo, continuaré usando esa terminología para conjugar a los parámetros, pero en las redes neuronales. En la literatura de las redes neuronales y a veces pueden escuchar a la gente hablar de los pesos de un modelo y pesos significa exactamente lo mismo que parámetros del modelo. Utilizamos el término parámetros en estos videos, pero a veces puedes oír a otros utilizan el término pesos. Entonces, este pequeño diagrama representa una sola neurona. Lo que una red neuronal, simplemente es, una prueba de estas diferentes neuronas unidas juntas. Específicamente, aquí tenemos unidades de entrada x1, x2 y x3, y una vez más, a veces puedo dibujar este nodo extra x0 y a veces no. Entonces, sólo lo dibujo aquí. Y aquí tenemos tres neuronas, que he escrito como, sabes, a(2)1, a(2)2 y a(2)3 alrededor de mejores índices una vez más y más tarde, podemos si lo deseamos, añadir a esto a0 y agregar una unidad adicional de oscilación. Siempre da como resultado el valor de 1. Entonces finalmente tenemos este tercer nodo en la capa final, y es este tercer nodo el que abre el valor que la hipótesis h de x calcula. Para introducir un poco más de terminología en una red neuronal, la primer capa, también es llamada capa de entrada porque aquí es donde introducimos nuestras variables x1, x2, x3. La capa final también es llamada capa de salida porque esa capa tiene las neuronas - esta de aquí - que dan como resultado el valor final calculado por una hipótesis y luego la capa dos en el medio, es llamada la capa oculta. El término capa oculta no es un término genial, pero la intuición indica que, como sabes, en el aprendizaje supervisado donde puedes ver las entradas, y podrás ver los resultados correctos. Considerando que la capa oculta tienen valores que no se logran observar en el conjunto de entrenamiento. No es X y no es Y, por lo que les llamamos ocultas. Y más adelante veremos redes neuronales con más de una capa oculta, pero en este ejemplo tenemos una capa de entrada, la capa 1; una capa oculta, la capa 2; y una capa de salida, la capa 3. Pero básicamente cualquier cosa que no sea una capa de entrada y que no sea una capa de salida será llamada capa oculta. Así es que quiero ser muy claro acerca de lo que esta red neuronal está haciendo. Retrocedamos a través de los pasos computacionales que están incorporados por esto, representados en este diagrama. Para explicar los cálculos específicos representados por una red neuronal, aquí está un poco más de notación. Voy a usar "a" superíndice "j" subíndice "i" para denotar la activación de la neurona "i" o de la unidad "i" en la capa "j".  Entonces, en concreto, esta "a" superíndice 2 subíndice 1 hace la activación de la primera unidad en la capa 2, en nuestra capa oculta. Y por activación, me refiero, como sabes, al valor que es calculado por y que es dado como resultado específicamente. Además, nuestra red neuronal es parametrizada por estas matrices, «theta» superíndice "j" donde nuestra «theta» "j" va a ser una matriz de onda controlando el mapeo de la función de una capa, tal vez de la primera capa a la segunda capa o de la segunda capa a la tercer capa. Así que, aquí están los cálculos que son representados por este diagrama. Esta primera unidad oculta aquí, tiene su valor calculado como sigue: es a(2)1 igual a la función sigmoidal o la función de activación sigmoidal también se llama función de activación logística, aplicada a esta clase de combinación lineal de sus entradas. Y después esta segunda unidad oculta tiene este valor de activación calculado como sigmoidal de este. Y del mismo modo, para esta tercera unidad oculta, está calculada mediante la fórmula. Así que aquí tenemos tres unidades de entrada y tres unidades ocultas. Y también la dimensión de «theta»1 la cual va a la matriz de parámetros que rigen nuestro mapeo desde las tres unidades de entrada, sobre tres unidades ocultas «theta»1 va a ser un 3, «theta»1 va a ser una matriz de 3x4 dimensiones y más en general, si una red tiene unidades Sj y su j y sus unidades Sj + 1 en su j + 1 entonces la matriz «theta» j que rige el mapeo de la función de la capa j a la capa j+1 que vamos a tener dimensión Sj + 1 por Sj + 1. Solo para aclarar esta notación, ¿correcto? esto es S subíndice j + 1 y eso es S subíndice j y luego todo esto, + 1. De todo esto, eso es j + 1, ¿de acuerdo? Así que eso es S subíndice j + 1, por tanto, eso es S subíndice j + 1 multiplicado por Sj + 1 donde + 1 no es parte del subíndice. Así es que, hemos hablado de lo que las tres unidades ocultas hacen para calcular sus valores. Finalmente, este último, el espinal en capa óptima, tenemos una unidad más que calcula h de x que es igual, y puede escribirse también como a(3)1 y es igual a esto. Y te das cuenta que tengo escrito esto con un superíndice 2 aquí porque «theta» superíndice 2 es la matriz de parámetros, o la matriz de pesos que controla la función que mapea las unidades ocultas, que son las unidades de la capa 2, a la unidad de la capa 3 que es la unidad de salida. Para resumir, lo que hemos hecho es mostrar como una imagen como esta de aquí define una red neuronal artificial que define una función h que mapea tus valores de entrada x que con suerte en algunos espacios y disposiciones y. Y estas hipótesis después parametrizadas por parámetros que estoy denotando con «theta» mayúscula así como vamos a variar «theta» para obtener diferentes hipótesis. Así es que llegamos a diferentes funciones de mapeo digamos de x a y.  
Entonces, esto nos da una definición matemática de cómo representar la hipótesis en la red neuronal. En algunos de los siguientes videos, lo que me gustaría hacer es darte un mejor entendimiento sobre lo que estas representaciones de hipótesis hacen, y también hablar de algunos ejemplos y de cómo calcularlos eficientemente.