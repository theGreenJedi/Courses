En el último video vimos una definición matemática sobre cómo representar, o cómo calcular las hipótesis utilizadas por las redes neuronales. En este video, me gustaría mostrarte cómo realizar ese cálculo de forma eficiente, y eso es, mostrarte la implementación de un aumento de vector. En segundo lugar, y más importante, quiero empezar a darte una intuición sobre por qué estas representaciones en redes neuronales pueden ser una buena idea y cómo pueden ayudarnos a aprender hipótesis complejas no lineales. Considera esta red neuronal. Anteriormente, dije que la secuencia de pasos que necesitamos para calcular la salida de una hipótesis en estas ecuaciones a la izquierda, en las que calculamos los valores de activación de las tres unidades ocultas y después los utilizamos para calcular la salida final de nuestras hipótesis h de x. 
Ahora, voy a definir algunos términos adicionales. Entonces, este término que estoy subrayando aquí, voy a definirlo como z superíndice 2 subíndice 1. De forma que tengamos que a(2)1, que es este término, sea igual a g de z a 1. Por cierto, estos superíndices 2, ya sabes, lo que significan es que la z2 y esta a2 también, el superíndice 2 entre paréntesis significa que éstos son valores asociados con la capa 2, esto es, con la capa oculta en la red neuronal. Ahora voy a definir este término de forma similar como z(2)2. Y, finalmente, este último término que estoy subrayando, voy a definirlo como z(2)3. Entonces, de forma similar, tenemos a(2)3 igual a g de z(2)3. Entonces, estos valores z sólo son una combinación lineal, una combinación lineal ponderada, de los valores de entrada x0, x1, 2x, 3x que entran en una neurona particular. Ahora, si observamos este bloque de números, puedes notar que ese bloque de números corresponden de forma sospechosamente similar al la operación de vector matriz, la multiplicación de vector matriz de x1 veces el vector x. Mediante esta observación, Vamos a ser capaces de vectorizar este cálculo de la red neuronal. Específicamente, vamos a definir el vector x de la variable de la forma usual, como el vector de x0, x1, x2, x3 donde x0, como siempre, es siempre igual a 1 y eso define z2 como el vector de estos valores z, sabes, de z(2)1 z(2)2, z(2)3. Y nota que, ahí, z2 es un vector de tres dimensiones. Ahora podemos vectorizar el cálculo de a(2)1, a(2)2, a(2)3 de la siguiente manera. Podemos escribir esto en sólo dos pasos. Podemos calcular z2 como «theta» 1 veces x y eso nos daría este vector z2; y entonces a2 es g de z2 y, sólo para ser claro, z2 es un vector de tres dimensiones y a2 también es un vector de tres dimensiones y, por lo tanto, la activación g. Esto aplica la función sigmoidea en torno al elemento a cada uno de los elementos de z2.
Y por cierto, para hacer nuestra notación un poco más coherente con lo que haremos más adelante, en esta capa de entrada tenemos las entradas x, pero también podemos pensar en éstas como en activaciones de las primeras capas. Entonces, si se define que a1 es igual a 0, Entonces, a1 es un vector, y ahora puedo tomar esta x de aquí y reemplazarla con z2 es igual a «theta» 1 veces a1 solo definiendo que a1 son activaciones en mi capa de entrada. Ahora, con lo que he escrito hasta ahora he obtenido yo mismo los valores de a1, a2, a3 y, realmente, debería poner los superíndices allí también. Pero necesito un valor más, lo que quiere decir que también quiero este a(0)2 y que corresponda con una unidad de oscilación en la capa oculta que va a la salida de allí. Por supuesto, había una unidad de oscilación aquí también que, ya sabes, no la coloqué aquí, pero, para resolver esta unidad de oscilación adicional, lo que vamos a hacer es sumar un a0 superíndice 2 adicional, que es igual a uno, y después de este paso, ahora tenemos que a2 va a ser un vector de variables de cuatro dimensiones porque acabamos de sumar este a0 adicional que es igual a 1 correspondiente a la unidad de oscilación en la capa oculta.
y, finalmente, para calcular el valor real de salida de nuestras hipótesis, simplemente tenemos que calcular z3. entonces, z3 es igual a este término que estoy subrayando. Este término interior es z3. Y z3 se indica como 2 veces a2 y, finalmente, la salida de mi hipótesis h de x, que es a3, es la activación de mi única unidad en la capa de salida. entonces, este sólo es el número real. Lo puedes escribir como a3 o como a(3)1, y eso es g de z3. A este proceso de calcular h de x también se llama propagación hacia adelante, y se llama así porque comenzamos con las activaciones de las unidades de entrada y luego es como si hubiera una propagación hacia adelante de esto hacia la capa oculta y se calcularan las activaciones de la capa oculta y luego se propagara eso y se calcularan las activaciones de la capa de salida, pero este proceso de calcular las activaciones desde la entrada, entonces la capa oculta y luego la capa de salida, y eso también se llama propagación hacia adelante. Y lo que acabamos de hacer fue trabajar en la implementación de este procedimiento en el sentido de vectores.
Entonces, si lo implementas usando estas ecuaciones que tenemos a la derecha, te darán una forma eficiente o ambas formas eficientes para calcular h de x. Esta vista de la propagación hacia adelante también nos ayuda a comprender lo que las redes neuronales podrían estar haciendo y por qué podrían ayudarnos a aprender interesantes hipótesis no lineales. Considera la siguiente red neuronal y supongamos que cubro la parte izquierda de esta imagen por ahora. Si miras lo que queda en esta imagen, se ve muy parecido a una regresión logística en donde lo que estamos haciendo es usar esta nota, esa es justo la unidad de la regresión logística y la estamos usando para hacer una predicción h de x. 
y, en concreto, lo que las hipótesis están mostrando es que h de x va a ser igual a g, lo que es mi función sigmoidea de activación multiplicada por «theta» 0 por a0, que es igual a 1 más «theta» 1 más «theta» 2 por a2 más «theta» 2 por a3, siendo que los valores a1, a2, a3 son aquellos dados por estas tres unidades. Ahora, para ser realmente consistente a mi notación previa, de hecho, necesitamos, ya sabes, completar estos superíndices 2 en todas partes, y también tengo estos índices 1 aquí porque sólo tengo una unidad de salida, pero si te enfocas en las partes azules de la notación. Esto es, ya sabes, esto se ve como el modelo de la regresión logística estándar, excepto que ahora tengo una «theta» mayúscula en lugar de una «theta» minúscula. Y lo que esto está haciendo sólo es la regresión lineal. Pero donde las variables se introducen en la regresión logística son estos valores calculados por la capa oculta. Sólo para repetirlo, lo que esta red neuronal está haciendo es como una regresión logística, excepto que en lugar de utilizar las variables originales x1, x2, x3, está utilizando estas nuevas variables a1, a2, a3. Una vez más, vamos a poner los superíndices allí, ya sabes, para ser consistentes con la notación. Y lo bueno de esto, es que las variables a1, a2, a3, fueron aprendidas como funciones de la entrada. Concretamente, el mapeo de la función desde la capa 1 a la capa 2, que está determinada por algún otro conjunto de parámetros, «theta» 1. Entonces, es como si la red neuronal, en lugar de estar obligada a alimentar las variables x1, x2, x3 a la regresión logística, llegar a aprender sus propias variables, a1, a2, a3, para introducirlas en la regresión logística y, como puedes imaginarte, dependiendo de los parámetros que elija para «theta» 1. 
Tú puedes aprender algunas variables bastante interesantes y complejas y, por lo tanto, puedes terminar con una mejores hipótesis que si estuvieras obligado a usar las variables en bruto x1, x2 o x3, o si te vas a limitar a, digamos, elegir los términos polinomiales, ya sabes, x1, x2, x3 y así sucesivamente. Pero, en cambio, este algoritmo tiene la flexibilidad para tratar de aprender cualquier número de variables a la vez, usando estos a1, a2, a3 para introducirlos en esta última unidad que, esencialmente, es la regresión logística de aquí. 
Me di cuenta de que este ejemplo se describe como con un nivel un poco elevado, por lo que no estoy seguro si esta intuición de la red neuronal, ya sabes, con variables más complejas ya tendrá sentido, pero si todavía no es así, en los siguientes dos videos, voy a explicar un ejemplo específico de cómo una red neuronal puede usar esta capa oculta para calcular variables más complejas para introducirlas en esta capa final de salida, y cómo eso puede aprender hipótesis más complejas. Entonces, si lo que estoy diciendo aquí no tiene mucho sentido, acompáñame en los siguientes dos videos y, con suerte, trabajando con esos ejemplos, esta explicación tendrá un poco más de sentido. Pero sólo el punto O. Tú puedes tener redes neuronales con otros tipos de diagramas también, y la forma en la que las redes neuronales están conectadas, a eso se le llama la arquitectura. Así que el término "arquitectura" se refiere a la forma en la que las diferentes neuronas están conectadas entre sí. Este es un ejemplo de una arquitectura diferente de una red neuronal y, nuevamente, puedes obtener esta intuición de cómo la segunda capa, aquí tenemos tres unidades de dirección que están calculando alguna función compleja, quizás de la capa de entrada, y luego la tercera capa puede tomar las variables de la segunda capa y calcular variables todavía más complejas en la capa tres, de forma que, para el momento en el que llegues a la capa de salida, la capa cuatro, puedes tener variables aún más complejas de lo que podrías calcular en la capa tres y así obtener hipótesis no lineales muy interesantes. Por cierto, en una red como esta, a la capa uno se le llama capa de entrada. La capa cuatro sigue siendo nuestra capa de salida, y esta red tiene dos capas ocultas. Así que, cualquier cosa que no sea una capa de entrada o una capa de salida, es llamado capa oculta. Entonces, espero que de este video hayan obtenido un sentido de cómo el paso de propagación hacia adelante en una red neuronal trabaja donde comienzas desde las activaciones de la capa de entrada y propagando eso hacia adelante hacia la primera capa oculta, después la segunda capa oculta y luego, finalmente, la capa de salida. Y también vimos cómo podemos vectorizar ese cálculo. En el siguiente, me di cuenta de que algunas de las intuiciones en este video sobre cómo, ya sabes, otras capas están calculando variables complejas de las capas anteriores. Me di cuenta de que esa intuición podría seguir siendo un poco abstracta y de un nivel un tanto elevado. Entonces, lo que me gustaría hacer en los dos videos es trabajar con un ejemplo detallado de cómo una red neuronal puede utilizarse para calcular funciones no lineales de la entrada, y espero que eso te de una buena idea del tipo de hipótesis complejas no lineales complejas que podemos obtener de las redes neuronales.