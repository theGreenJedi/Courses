이번과 다음 여러편의 비디오에서, 저는 뉴럴 네트워크라고 불리는 학습 알고리즘에 대해 설명하려고 합니다. 우리는 먼저 뉴럴 네트워크가 어떻게 작동하는지 설명하고 다음 여러편의 비디오에서 구체적인 학습 알고리즘을 설명하려고 합니다. 뉴럴 네트워크는 사실 꽤 오래전에 제시된 방법이지만, 한동안 사람들의 관심을 받지 못했습니다. 그러나 오늘날, 뉴럴 네트워크는 다양한 분야에 기계학습을 적용하기 위한 최첨단의 기법입니다. 그렇다면 왜 신경망 알고리즘이 필요할까요? 이미 선형 회귀와 로지스틱 회귀가 기계 학습에 사용되고 있습니다. 그런데 왜 우리는 신경망 알고리즘을 필요로 할까요? 신경망 이론에 대한 관심을 불러일으키기 위해, 먼저 몇가지 기계학습의 예를 보여드리겠습니다. 이와 같은 문제에서는 복잡한 비선형 가설들이 필요합니다. 지도 학습 분류 문제를 생각해봅시다. 이 그림과 같은 훈련 데이터가 주어진 문제에 로지스틱 회귀를 적용하려 한다면, 이 식과 같이 수많은 비선형적 요소들을 사용해 로직스틱 회귀를 적용해볼 수 밖에 없을것입니다. 여기 있는 g 함수는 주로 시그모이드 함수를 나타냅니다. 여기에서 시그모이드 함수를 사용하기 때문에, 수많은 다항식들을 포함할 수 있습니다. 만약 이 식에 충분히 많은 항들을 포함한다면, 아마도 양수값을 가지는 데이터와 음의 값을 가지는 데이터들을 구분할 수 있는 가설을 세울 수 있습니다. 여기에 제시된 특별한 방법은 오직 두개의 요소 - x1과 x2만으로 데이터가 주어졌을때에만 잘 동작합니다. 왜냐하면, x1과 x2로 이루어진 모든 항들을 포함할 수 있기 때문입니다. 그러나 많은 기계 학습 문제에서는 단지 두개가 아닌 훨씬 많은 요소들을 고려하게 될 것입니다. 이제 문제를 주택 수급 예측 문제로 돌려봅시다. 만약 당신이 회귀 문제가 아닌 주택 분류 문제를 가지고 있다면, 아마도 이런 상황일겁니다. 당신이 어떤 주택이 어떤 요소와 특징을 가졌는지 알고 있을 것입니다. 그 상태에서 당신이 앞으로 6개월 이내에 그 주택이 팔릴 가능성이 얼마나 될지 예측하려고 한다면, 그것은 분류 문제일 것입니다. 그리고 이와 같이 우리는 아주 많은 특징들을 떠올릴 수 있습니다. 주택마다 각각 다른 요소를 100가지 쯤은 떠올릴 수 있겠죠. 이렇게 수없이 다양한 항들이 있는 문제를 푸는 데에 모든 항과 이차항들을 포함하려고 한다면, 게다가 심지어 제곱항인 이차항에 다변수 이차항까지 포함한다면, 항이 엄청 많아질 겁니다. x1 제곱도 있을 거구요, x1x2, x1x3, 계속하자면 x1x4, 결국 x1x100까지 모두 포함되겠죠. 다음 x2 제곱, x2x3, 등등이 계속 이어집니다. 만약 당신이 단지 이차항들만을, 즉 두 항의 곱, x1 곱하기 x1 등등 만을 포함한다고 해도 항이 100개인 경우에 5000개가 넘는 요소를 포함하게 됩니다. 그리고, 점근적으로, 이차항의 수는 O(n*n)으로 증가하는데, 여기에서 n은 원래 요소의 갯수입니다. 주택 문제에서 떠올렸던 x1 부터 x100까지의 변수들 처럼요. 그리고, 그것은 사실 2분의 n 제곱에 가깝습니다. 그래서, 모든 이차 요소들을 포함하는 것은 좋은 생각같지는 않습니다. 왜냐하면 그렇게 되면 너무 많은 요소들을 포함하게 되어 훈련 데이터에 너무 딱 맞는 결과를 보게 될 것입니다. 그리고, 계산하는데에도 오래 걸리겠죠 그 수많은 요소들을 다루게 된다면요. 한가지 당신이 할수 있는것은 수많은 요소들 중 어떤 부분집합만을 포함하는 것입니다. 이런 경우를 생각해봅시다. 단지 x1 제곱, x2 제곱, x3 제곱, x100 제곱까지만 포함한다면, 요소의 수가 훨씬 적어집니다. 이렇게 오직 100개의 이차 요소들을 뽑아낼 수 있죠. 그러나 이것은 충분한 요소가 아니며 확실히 이 그림처럼 데이터 셋을 잘 가려내지는 못할 겁니다. 사실, 정말로 이들 이차 요소들과 원래 x1에서 x100까지의 요소들을 함께 포함한다면, 실제로는 매우 흥미로운 가설을 세울 수 있습니다. 그러니까, 이 그림처럼 타원 모양의 구분선을 그려낼 수 있습니다. 하지만 틀림없이 이 그림과 같은 복잡한 구분선은 그려낼 수 없을겁니다. 요소가 5000개나 된다고 생각할 수도 있습니다. 하지만 만약 세제곱이나 다른 3차항들, x1x2x3 라던가, x1제곱, x2x10, x11x17 등을 포함하기 시작하면 얼마나 많은 요소가 있을 지 상상해 보세요. 요소들은 n이 늘어날수록 세제곱에 비례해 수가 늘어날 것이고 만약 n 이 100이면, 계산해보세요. 대략 170,000개의 삼차 요소들을 포함하게 됩니다. 이렇게 고차 다항 요소들을 포함하는 것은 당신의 원래 요소의 수가 많을수록 더욱 급격하게 요소 공간을 확대하게 될 것입니다. 이 방식이 좋은 방식같진 않습니다. 즉 n이 클때, 비선형적인 분류를 위해서 요소를 추가하는 방식은 좋지 않습니다. 많은 기계학습 문제에서 n은 아주 큽니다. 예를 들어봅시다 컴퓨터 비전 문제를 고려해 봅시다. 그리고 당신이 기계학습을 이용하여 분류기를 트레이닝하여 이미지를 조사하고 이미지가 차인지 아닌지를 판단하게 하고 싶다고 가정해봅시다. 많은 사람들은 왜 컴퓨터 비전이 어려운지 궁금해합니다. 당신과 내가 이 그림을 바라볼때, 이것이 무엇인지는 매우 명백하죠. 당신은 어떻게 학습 알고리즘이 이 그림이 무엇인지 아는데 실패할 수 있는지 궁금할테죠. 왜 컴퓨터 비전이 어려운지를 이해하기 위해서, 작은 영역의 이미지를 확대해서 보도록 합시다. 여기 이 빨간 사각형을 확대해보죠. 우리는 자동차를 보고 있지만 컴퓨터는 다른 것을 봅니다. 컴퓨터가 보는 것은 행렬, 혹은 픽셀 값의 격자인데, 이 값은 이미지에서 각 픽셀의 밝기를 나타냅니다. 컴퓨터 비전 문제는 컴퓨터가 픽셀 밝기 값들로 이루어진 이런 행렬을 보고, 이 숫자들이 어떤 자동차의 문 손잡이를 나타낸다고 우리에게 말해줘야 하는 문제입니다. 구체적으로, 우리가 기계학습을 이용하여, 차량 식별기를 구축하기 위해 우리가 해야 하는 것은 라벨 트레이닝 셋을 마련하는 것입니다. 즉 "자동차"라고 표시된 예제들과 "자동차가 아님"이라고 표시된 다른 예제들을 준비해야 합니다. 그 후에 우리는 트레이닝 셋을 학습 알고리즘에 전달하여 분류기를 학습시키고 그 다음 학습된 알고리즘을 테스트할 것입니다. 이렇게, 새로운 이미지를 보여주고는 "이것은 무엇입니까?" 라고 질문하겠죠. 그리곤 분류기가 그것이 차라고 대답하기를 기대하겠죠. 왜 비선형적인 가설이 필요한지 이해하기 위해서 몇 개의 자동차 이미지와 자동차가 아닌 이미지를 봅시다. 우리의 학습 알고리즘에게 예제로 주었던 그 이미집니다. 이 이미지에서 몇개의 픽셀들을 골라봅시다. 그러면 픽셀 1 은 여기서, 픽셀 2 는 여기서 고르겠습니다. 그리고 이 차를 도면에 표시하겠습니다. 그러니까, 이 좌표값은 픽셀1 과 픽셀2 의 밝기를 값으로 가지는 좌표값입니다. 다른 이미지들도 같은 작업을 해 봅시다. 다른 "자동차" 예제를 고르고, 동일한 위치의 픽셀 두 개를 봅시다. 그러면 그 이미지의 픽셀 1 은 다른 밝기를, 픽셀 2도 역시 다른 밝기를 가지고 있겠죠. 따라서 다른 "자동차" 그림은 도표상에서 다른 위치에 놓이게 됩니다. "자동차"가 아닌 예제들도 도면에 표시해볼까요. 이 이미지는 "자동차가 아님" 예제구요, 이 이미지도 "자동차가 아님" 예제입니다. 그리고 만약 우리가 더욱더 많은 예제를 사용해 "자동차"들을 나타내기 위해 +를, "자동차가 아님"을 나타내기 위해 -를 사용해 표시하면, 우리는 "자동차"와 "자동차가 아님"들이 좌표 공간에서 서로 다른 영역에 위치하게 된다는 것을 발견할 겁니다. 그러므로 우리가 필요로 하는 것은 어떤 비선형적인 가설이며 이를 사용해 두가지 경우를 구분하도록 시도하는 것입니다. 특징 공간은 몇 차원일까요? 우리가 단지 50x50 픽셀짜리 이미지를 이용한다고 가정해 봅시다. 우리가 사용하는 이미지들은 단지 가로 길이가 50픽셀인 작은 이미지입니다. 그러나 결국 우리는 2500개의 픽셀을 가지게 됩니다. 따라서 특징 공간의 차원은 n 이 2500 이며, 우리의 특징 벡터 x는 모든 픽셀 값의 목록입니다. 픽셀 1의 밝기, 픽셀 2의 밝기, 등등 마지막 픽셀의 밝기까지 이어집니다. 전형적인 컴퓨터 표현에서 밝기 값은 0에서 255사이의 값입니다. 흑백 이미지를 사용할 경우에, 말이죠. 즉 n은 2500입니다. 만약 우리가 흑백 이미지들을 이용한다는 가정 하에서는요. 만약 우리가 분리된 RGB 이미지를 이용한다면, 이미지는 적색, 녹색, 청색 값을 각각 가지며 n이 7500이 됩니다. 우리가 모든 이차 요소들을 포함하는 비선형 가설을 학습하려고 하면 이차 요소라는게 Xi 곱하기 Xj 형태의 모든 항이니까 2500 픽셀이 결국엔 3백만개에 달하는 요소항으로 늘어납니다. 이성적으로, 그건 너무 많습니다. 계산이 무척 오래 걸릴겁니다. 매 예제마다 300만개의 특징을 찾고 표현하려면요. 단순 로지스틱 회귀를 이차, 또는 삼차 특징들과 함께하여, n 이 클때 복잡한 비선형 가설을 학습하는데 사용하는 것은 좋은 방법이 아니다. 왜냐하면, 너무 많은 특징들이 될 것이기 때문이다. 다음 몇편의 비디오에서 나는 뉴럴 네트워크에 대해서 설명할 것이다. 이는 복잡한 비선형 가설을 학습하는데 훨씬 좋은 방법이다. 심지어 당신의 입력 특징 공간 n 이 클때의 경우에도 말이다 중간에 나는 몇개의 재미있는 비디오를 보여줄 것이다. 이들은 뉴럴 네트워크의 역사적으로 중요한 응용이며, 나는 우리가 나중에 볼 이들 비디오가 당신이 보기에도 역시나 재미있기를 바란다.