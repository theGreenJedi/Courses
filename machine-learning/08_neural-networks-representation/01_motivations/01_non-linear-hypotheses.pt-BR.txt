Neste e nos próximos vídeos, eu gostaria de falar sobre um algoritmo de aprendizado chamado Rede Neural. Primeiramente vamos falar sobre representação, e então, nos próximos vídeos, falar sobre os algoritmos. Redes neurais são, na verdade, uma ideia bem antiga, mas que saiu da moda por um tempo. Mas atualmente, ela é a técnica no estado da arte para muitos problemas em Aprendizado de Máquina. Mas, por que precisamos de mais um algoritmo? Nós já temos Regressão Linear, Regressão Logística, por que precisamos de Redes Neurais? Para motivar nossa discussão em Redes Neurais, começarei apresentando exemplos de problemas, onde precisamos aprender hipóteses não-lineares complexas. Considere o problema de classificação,
com Aprendizado Supervisionado, onde você tem um conjunto
de treinamento como esse. Se você quiser aplicar uma Regressão Logística nesse problema, uma coisa que você pode fazer é aplicar a regressão logística com muitos termos não lineares como esses. Então aqui, o "g", como sempre, é a função "sigmoid", e podemos incluir vários termos polinomiais como esses. E, se você incluir termos polinomiais o suficiente, talvez, você possa conseguir uma hipótese que separa os exemplos negativos dos positivos. Esse método, em particular, funciona bem quando você tem apenas duas variáveis - x₁ e x₂ - porque você pode incluir todos esses termos polinomiais de x₁ e x₂. Mas em muitos outros casos de Aprendizado de Máquina, teremos muito mais variáveis que apenas duas. Nós já falamos há um tempo sobre previsão para casas. Suponha que você tenha um problema de classificação de casas, ao invés de um problema de Regressão. Você tem diferentes variáveis de uma casa, e você quer prever quais são qual a chance dessa casa ser vendida dentro dos próximos seis meses. Esse seria um problema de classificação. E, como vimos, podemos ter muitas variáveis, talvez uma centena de termos de diferentes casas. Para um problema assim, se fôssemos incluir todos os termos quadráticos - mesmo apenas os termos quadráticos - já seriam muitos termos. Teríamos termos como: x₁², x₁x₂, x₁x₃, x₁x₄, ... até  x₁x₁₀₀ , e então x₂², x₂x₃, e assim por diante. E se você incluir apenas os termos de segunda ordem, ou seja, os termos que são produto, de dois desses termos, x₁ · x₁ , e assim por diante, e então, para o caso de "n=100", teríamos aproximadamente 5,000 variáveis. E, assintoticamente, o número de termos quadráticos cresce aproximadamente na ordem de n², onde "n" é o número de variáveis originais, como de x₁ até x₁₀₀ , que tínhamos. Isso é, na verdade, mais próximo de n²/2. Então, incluindo todos os termos quadráticos, não parece ser uma boa idéia. Pois são muitas variáveis, e você pode acabar sobreajustando para o conjunto de treino, além disso pode ser computacionalmente custoso, ter tantas variáveis. O que você pode fazer é incluir apenas um subconjunto disso. Então, se você incluir apenas os termos x₁², x₂², x₃², até talvez, x₁₀₀², então o número de variáveis será bem menor. Aqui você tem apenas 100 termos quadráticos. Mas essas variáveis não são suficientes, e certamente não irá ajustar ao conjunto de dados,
como essa acima. Na verdade, se você incluir apenas essas variáveis quadráticas com x₁ , ..., até x₁₀₀, você pode ajustar hipóteses muito interessantes, você pode ajustar coisas como uma elipse alinhada ao eixo "x", como essa. Mas, você não pode ajustar a dados mais complexos, como o que mostrei. Então, 5,000 variáveis parece muito, se você fosse incluir termos cúbicos, ou de terceira ordem, x₁x₂x₃ , x₁²x₂ , x₁₀x₁₁x₁₇ , e assim por diante. Como você pode imaginar,
terremos muitos desses termos. Na verdade, haverá mais de n³ variáveis, e se n for 100, podemos calcular, e teríamos algo em torno de 170,000 termos cúbicos. Então, incluir esses termos polinomiais de ordem maior, quando seu conjunto de variáveis é tão grande, aumenta drasticamente a sua quantidade de variáveis. E isso não parece ser uma boa forma de construir classificadores não-lineares, quando o "n" é grande. Em muitos dos problemas de Aprendizado
de Máquina, o "n" será muito grande. Aqui está um exemplo. Vamos considerar um problema
de Visão Computacional. Suponha que você queria usar Aprendizado de Máquina para treinar um classificador, para examinar uma imagem, e nos dizer se a imagem é, ou não, um carro. Muitas pessoas perguntam: "Por que visão computacional é complicado?". Quando olhamos para essa imagem, é tão óbvio o que ela é. Você imagina como é que um algoritmo de aprendizado poderia falhar em saber o que há na imagem. Para entender porque Visão Computacional é difícil, vamos aproximar em uma pequena parte da imagem, como essa área, onde está esse retângulo vermelho. Aconte que, onde nós vemos um carro,
o computador vê isto. O que ele vê é essa matriz, ou essa grade, com os valores de
intensidade de cada pixel, que nos diz o brilho de
cada pixel, nessa imagem. E, o problema da Visão Computacional, é olhar para essa matriz de valores de intensidade de pixels, e nos dizer que isso representa
a maçaneta da porta de um carro. Na verdade, quando nós usamos Aprendizado de Máquina para construir um detector de carro, o que nós fazemos é: nós pegamos um conjunto de treinamento, com alguns exemplos de carros conhecidos, e alguns exemplos de outras coisas, que não sejam carros. Nós fornecemos nosso conjunto de treinamento para o algoritmo, treinamos um classificador. E então podemos testá-lo, mostrando novas imagens e perguntando: "O que é isso?". E esperamos que ele reconheça que é um carro. Para enteder o porquê de precisarmos de uma hipótese não-linear, vamos dar uma olhada em algumas imagens de carro, e "não carros", que poderíamos fornecer ao nosso algoritmo. Vamos pegar alguns pixels de nossas imagens, essa é a localidade do pixel 1, e essa a do pixel 2. E vamos plotar esse carro, em posições do gráfico, dependendo das intensidades do pixel 1 e do pixel 2. E façamos isso com outras imagens. Vamos pegar um exemplo diferente de carro, e olhar nas mesmas posições desses dois pixels, e a imagem tem diferentes intensidades para os pixels 1 e 2. Então, ela fica um
lugar diferente no gráfico. E então, vamos plotar alguns
exemplos negativos também. Isso não é um carro, isso também não. E se fizermos isso para cada vez mais exemplos, usando o sinal "+" para carros, e "-" para "não-carros", descobriremos que carros, e não-carros se encontram em diferentes regiões do gráfico. E, o que precisaremos, é algum tipo de função não-linear, para tentar separar essas duas classes. Qual é a dimensão do espaço das variáveis? Suponha que vamos usar
apenas imagens de 50x50 pixels. Isso supõe que nossas imagens são bem pequenas,
apenas 50 pixels de cada lado. Então, teríamos 2,500 pixels. Então, a dimensão das nossas variáveis será: "n = 2,500", onde o vetor de variáveis "X" é uma lista das intensidades de todos os pixels. O brilho do pixel 1, brilho do pixel 2, e assim por diante, até o brilho do último pixel. Onde, em uma representação computacional típica, cada um desses valores seria algo entre 0 e 255, se estiverem em escala de cinza. Então temos "n=2,500". E isso se tivermos imagens em tons de cinza. Se usássemos RGB, imagens com valores de: vermelho, verde, e azul; nós teríamos "n=7,500". Portanto, se tentarmos treinar uma hipótese não-linear, incluindo todos os termos quadráticos, ou seja, todos os termos na forma "xi · xj", uma vez que temos 2,500 pixels, teríamos algo em torno de 3 milhões de variáveis. E isso ultrapassa o razoável. O custo para encontrar e representar todas essas 3 milhões de variáveis, para cada exemplo do conjunto de treino, seria muito alto. Então, Regressões Logísticas simples, talvez acrescidas de termos quadráticos e cúbicos, não é uma boa maneira de aprendier hipóteses
complexas, não-lineares, quando "n" é grande. Pois você acaba criando variáveis demais. Nos próximos vídeos, gostaria de falar sobre "Redes Neurais". Que são uma forma muito melhor para o aprendizado de hipóteses complexas não-lineares, e mesmo quando a dimensão das variáveis, "n", é grande. E no decorrer dos vídeos, eu também gostaria de mostrar alguns vídeos divertidos de aplicações importantes das Redes Neurais. E eu espero que esses vídeos sejam proveitosos e divertidos para vocês também!
Tradução: Luís Moneda | Revisão: Pablo de Morais Andrade