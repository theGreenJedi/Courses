1
00:00:00,440 --> 00:00:01,400
このビデオと、

2
00:00:01,480 --> 00:00:02,640
これに続く一連のビデオで、

3
00:00:02,780 --> 00:00:04,270
ニューラルネットワークと呼ばれる

4
00:00:04,550 --> 00:00:06,110
学習アルゴリズムをやっていきたい。

5
00:00:07,190 --> 00:00:07,900
最初にまず表現について

6
00:00:08,079 --> 00:00:09,330
説明し、その後で、

7
00:00:09,600 --> 00:00:10,390
その後の一連のビデオで

8
00:00:10,410 --> 00:00:12,160
それに関する学習アルゴリズムについて話す。

9
00:00:12,660 --> 00:00:14,070
ニューラルネットワークは実は

10
00:00:14,510 --> 00:00:15,870
かなり古くからあるアイデアだ。

11
00:00:16,290 --> 00:00:17,680
だがしばらくは下火だった。

12
00:00:18,200 --> 00:00:19,270
だが今日では、

13
00:00:19,580 --> 00:00:20,820
最先端の技術で、

14
00:00:21,090 --> 00:00:22,390
たくさんの異なる機械学習の問題で使われている。

15
00:00:23,740 --> 00:00:25,740
で、何故また別の学習アルゴリズムをやらなきゃいけないのか？

16
00:00:26,300 --> 00:00:28,030
既に線形回帰もあるし、

17
00:00:28,180 --> 00:00:31,260
ロジスティック回帰もある。
では何故、ニューラルネットワークなんて物が必要なんだ？

18
00:00:32,280 --> 00:00:34,260
ニューラルネットワークの議論を

19
00:00:34,790 --> 00:00:35,970
興味深くする為に、

20
00:00:36,120 --> 00:00:37,130
複雑で非線形の仮説を

21
00:00:37,310 --> 00:00:38,720
学ぶ必要があるような

22
00:00:38,930 --> 00:00:40,100
機械学習の問題の例を

23
00:00:40,300 --> 00:00:41,850
いくつか挙げてみよう。

24
00:00:43,850 --> 00:00:45,650
こんなトレーニングセットを用いた、

25
00:00:46,530 --> 00:00:48,440
教師有りの分類問題の学習を考えてみよう。

26
00:00:49,280 --> 00:00:50,530
この問題にロジスティック回帰を

27
00:00:50,960 --> 00:00:52,710
適用しよう、とすると、

28
00:00:52,900 --> 00:00:54,250
たくさんの非線形のフィーチャーに対して

29
00:00:54,660 --> 00:00:56,140
ロジスティック回帰を

30
00:00:56,190 --> 00:00:57,720
適用していく、というのが考えられる。

31
00:00:58,170 --> 00:00:59,580
このように、gはいつも通り

32
00:01:00,070 --> 00:01:01,710
sigmoid関数で、

33
00:01:01,780 --> 00:01:04,680
これらのようにたくさんの多項式の項を含められる。

34
00:01:05,450 --> 00:01:06,790
そしてもし十分にたくさんの

35
00:01:07,370 --> 00:01:08,280
多項式の項を入れれば、

36
00:01:08,950 --> 00:01:10,280
陽性と陰性を分ける仮説を

37
00:01:11,600 --> 00:01:13,780
得られるかもしれない。

38
00:01:14,630 --> 00:01:16,080
このやり方は、

39
00:01:16,470 --> 00:01:18,400
たとえばフィーチャーが2つだけ、

40
00:01:18,620 --> 00:01:20,180
x1とx2、とかの場合はうまく行く。

41
00:01:20,190 --> 00:01:20,980
何故ならその場合なら、x1とx2を含む

42
00:01:21,500 --> 00:01:22,880
全ての多項式を

43
00:01:23,400 --> 00:01:24,620
含むことが出来るからだ。

44
00:01:24,810 --> 00:01:26,280
だが、興味が湧くような機械学習の問題の中には

45
00:01:26,520 --> 00:01:27,730
フィーチャーがたったの2つよりはずっと多い物も

46
00:01:27,910 --> 00:01:29,230
たくさん存在する。

47
00:01:30,780 --> 00:01:31,760
ここまで家の価格の予測を

48
00:01:32,320 --> 00:01:34,560
議論してきたが、ここで

49
00:01:35,130 --> 00:01:36,990
回帰では無く

50
00:01:38,020 --> 00:01:39,280
分類の問題、例えば

51
00:01:39,390 --> 00:01:41,170
異なるフィーチャーの家があるとして、

52
00:01:41,580 --> 00:01:43,350
その時に半年以内に

53
00:01:43,440 --> 00:01:44,760
その家が売れるオッズを

54
00:01:45,010 --> 00:01:46,000
予測したい、というような問題が

55
00:01:46,050 --> 00:01:47,590
あるとすると、

56
00:01:47,700 --> 00:01:48,710
それは分類問題と

57
00:01:48,910 --> 00:01:51,040
なる訳だ。

58
00:01:52,100 --> 00:01:53,060
そしてそれは、前に見たように、

59
00:01:53,260 --> 00:01:55,130
最終的にはとてもたくさんの

60
00:01:55,260 --> 00:01:56,480
フィーチャー、数百という

61
00:01:56,840 --> 00:01:58,270
異なるフィーチャーを持つ家たちたりえる。

62
00:02:00,130 --> 00:02:01,610
こんな問題の時は、

63
00:02:01,880 --> 00:02:03,260
全ての二次の項を含めようなんて

64
00:02:03,370 --> 00:02:04,980
考えたら、

65
00:02:05,100 --> 00:02:06,260
これら全部は、

66
00:02:06,540 --> 00:02:07,540
二次の項だけでも、

67
00:02:07,930 --> 00:02:10,450
たくさんある。

68
00:02:10,560 --> 00:02:11,580
それはx1の二乗の項とか、

69
00:02:12,960 --> 00:02:17,610
x1x2、x1x3、x1x4という風に、

70
00:02:18,750 --> 00:02:21,880
x1x100まで続く。そして、

71
00:02:21,980 --> 00:02:23,620
x2 の二乗、x2x3、

72
00:02:25,620 --> 00:02:25,980
などと続いていく。

73
00:02:26,510 --> 00:02:27,770
そして二次の項だけを含めたとしても、

74
00:02:28,060 --> 00:02:29,200
つまり、

75
00:02:29,330 --> 00:02:30,750
これらの2つの

76
00:02:30,840 --> 00:02:32,090
積の項、

77
00:02:32,220 --> 00:02:33,390
つまりx1掛けるx1などで、

78
00:02:33,510 --> 00:02:35,010
そしてnが

79
00:02:35,780 --> 00:02:36,920
100の場合だと、

80
00:02:38,180 --> 00:02:40,280
結局は約5000程のフィーチャーとなる。

81
00:02:41,890 --> 00:02:44,880
そして漸近的に、

82
00:02:45,000 --> 00:02:46,330
二次のフィーチャーの数は

83
00:02:46,770 --> 00:02:48,670
オーダーnの二乗で

84
00:02:48,820 --> 00:02:50,330
成長する。

85
00:02:50,460 --> 00:02:52,790
ここでnはもとのフィーチャーの数。

86
00:02:53,370 --> 00:02:54,780
たとえばx1からx100まであった訳だが、

87
00:02:55,700 --> 00:02:58,750
その場合は実際 nの二乗 割る 2 に近かった。

88
00:02:59,920 --> 00:03:01,440
だから、すべての

89
00:03:01,560 --> 00:03:02,920
2次式フィーチャーを含めるのは

90
00:03:03,220 --> 00:03:04,220
よいアイデアではなさそうだ。

91
00:03:04,300 --> 00:03:05,380
なぜなら、たくさんのフィーチャーが

92
00:03:05,580 --> 00:03:07,050
あり、トレーニングセットを

93
00:03:07,220 --> 00:03:08,920
オーバーフィットする結果になるからだ。

94
00:03:09,330 --> 00:03:10,500
また、そのような

95
00:03:10,740 --> 00:03:12,800
たくさんのフィーチャーを扱うと、

96
00:03:14,080 --> 00:03:15,120
計算コストが高くなりうる。

97
00:03:16,450 --> 00:03:17,540
一つできることは

98
00:03:17,770 --> 00:03:19,090
これらのサブセットのみを

99
00:03:19,290 --> 00:03:20,950
入れることだ。例えば

100
00:03:21,050 --> 00:03:22,630
x1 の自乗、x2 の自乗、x3 の自乗

101
00:03:23,590 --> 00:03:25,180
から x100 の自乗

102
00:03:25,580 --> 00:03:27,750
までを入れる。すると

103
00:03:28,100 --> 00:03:29,500
フィーチャーの数はかなり少なくなる。

104
00:03:29,980 --> 00:03:31,720
そのような 2次式フィーチャーが

105
00:03:32,070 --> 00:03:33,850
100 個だけあるとして、

106
00:03:34,120 --> 00:03:35,950
これは不十分なフィーチャーであり

107
00:03:36,100 --> 00:03:37,170
左上のようなデータセットには

108
00:03:37,290 --> 00:03:39,330
まずフィットさせられないだろう。

109
00:03:39,570 --> 00:03:40,550
実のところ、これらの

110
00:03:41,040 --> 00:03:42,720
2次式フィーチャーと共に

111
00:03:43,170 --> 00:03:44,870
元々の x1 等々から

112
00:03:45,350 --> 00:03:46,500
x100 までを入れるだけでも、

113
00:03:47,460 --> 00:03:48,530
かなり興味深い仮説を

114
00:03:48,910 --> 00:03:50,210
フィットさせることは出来る。

115
00:03:50,330 --> 00:03:52,350
例えば楕円の仮説、

116
00:03:52,490 --> 00:03:53,860
例えばこんなのとかにはフィットさせる事が出来る。

117
00:03:55,080 --> 00:03:56,240
だが、ここに示したような

118
00:03:56,340 --> 00:03:57,930
より複雑なデータにはフィットさせられない。

119
00:03:59,360 --> 00:04:00,530
5000のフィーチャーと聞くと

120
00:04:00,620 --> 00:04:03,090
凄いたくさんに感じるだろうが、

121
00:04:03,230 --> 00:04:04,860
三乗の項を含めると

122
00:04:05,140 --> 00:04:06,100
つまり三次の多項式を含めると、

123
00:04:06,440 --> 00:04:08,050
x1x2x3とか、

124
00:04:08,400 --> 00:04:09,800
x1の二乗掛けるx2、

125
00:04:10,310 --> 00:04:12,240
x10x11x17

126
00:04:12,900 --> 00:04:15,280
などなど。

127
00:04:15,700 --> 00:04:18,110
こんなフィーチャーがたくさんになるのは想像出来るだろう。

128
00:04:19,040 --> 00:04:19,770
実際、オーダー三乗の

129
00:04:20,050 --> 00:04:21,260
フィーチャーは、

130
00:04:22,210 --> 00:04:23,830
nが100の時は

131
00:04:24,150 --> 00:04:25,660
計算してみると、

132
00:04:25,740 --> 00:04:26,870
なんと17000もの

133
00:04:27,730 --> 00:04:29,650
三次の項がある事になる。

134
00:04:30,040 --> 00:04:31,670
だから元々の

135
00:04:32,260 --> 00:04:34,470
フィーチャーセットが大きいとき

136
00:04:34,920 --> 00:04:36,050
これらの高次の多項式フィーチャーを

137
00:04:36,230 --> 00:04:37,730
含めることは、実に劇的に

138
00:04:38,530 --> 00:04:40,440
フィーチャー空間を膨張させる。

139
00:04:41,070 --> 00:04:42,180
n が大きいときに

140
00:04:42,320 --> 00:04:43,320
非線形の分類器を作るのに、

141
00:04:43,560 --> 00:04:45,050
追加のフィーチャーを用意することは

142
00:04:45,240 --> 00:04:48,100
よい方法ではなさそうだ。

143
00:04:49,590 --> 00:04:52,560
多くの機械学習問題にとって、n はかなり大きいだろう。

144
00:04:53,270 --> 00:04:53,560
例を示そう。

145
00:04:55,000 --> 00:04:58,140
コンピュータビジョンの問題について考える。

146
00:04:59,670 --> 00:05:00,770
機械学習を使って

147
00:05:01,260 --> 00:05:02,620
分類器を学習させたいとする。

148
00:05:02,710 --> 00:05:04,610
画像を調べ、それが

149
00:05:04,710 --> 00:05:05,880
車かどうかを

150
00:05:06,160 --> 00:05:08,030
判定するものだ。

151
00:05:09,480 --> 00:05:11,900
多くの人は、コンピュータビジョンがどうして難しいのかと疑問に思う。

152
00:05:12,390 --> 00:05:13,140
例えばあなたや私が

153
00:05:13,270 --> 00:05:15,670
この写真を見ると、これが何であるかはとても明白だ。

154
00:05:15,900 --> 00:05:17,000
どうして学習アルゴリズムが

155
00:05:17,190 --> 00:05:18,320
この写真が何であるかを

156
00:05:18,910 --> 00:05:20,880
判定し損ねるだろうかと思うだろう。

157
00:05:22,110 --> 00:05:23,330
コンピュータビジョンが難しい理由を

158
00:05:23,720 --> 00:05:25,380
理解するために、

159
00:05:25,650 --> 00:05:26,690
画像の小さな部分を

160
00:05:26,940 --> 00:05:28,180
拡大してみよう。

161
00:05:28,510 --> 00:05:30,240
この小さな赤い矩形の領域だ。

162
00:05:30,400 --> 00:05:31,330
あなたや私が車を見るとき、

163
00:05:31,450 --> 00:05:34,270
コンピュータは実はこのように見ている。

164
00:05:34,780 --> 00:05:35,930
コンピュータが見ているのは

165
00:05:36,600 --> 00:05:38,110
ピクセル輝度値のマトリックスあるいは

166
00:05:38,580 --> 00:05:40,350
グリッドで、画像の

167
00:05:40,610 --> 00:05:42,930
各ピクセルの明るさを表している。

168
00:05:43,640 --> 00:05:45,170
つまりコンピュータビジョン問題は

169
00:05:45,550 --> 00:05:47,230
このようなピクセル輝度値の

170
00:05:47,310 --> 00:05:49,140
マトリックスを見て、

171
00:05:49,410 --> 00:05:52,440
数値が車のドアハンドルを表しているとわかることだ。

172
00:05:54,230 --> 00:05:55,740
具体的には、我々が

173
00:05:56,030 --> 00:05:57,220
機械学習を使って車の検出器を

174
00:05:57,430 --> 00:05:59,060
作るときに行うのは、

175
00:05:59,360 --> 00:06:00,510
分類トレーニングセットを考えること、

176
00:06:00,800 --> 00:06:02,690
例えばそれは、

177
00:06:02,890 --> 00:06:04,250
車の分類例をいくつか含み、

178
00:06:04,730 --> 00:06:05,850
そして

179
00:06:06,000 --> 00:06:07,150
車ではない物の例をいくつか含む。

180
00:06:07,380 --> 00:06:08,780
そしてその

181
00:06:09,090 --> 00:06:10,590
トレーニングセットを

182
00:06:10,720 --> 00:06:12,230
学習アルゴリズムに与え

183
00:06:12,310 --> 00:06:13,500
分類器を学習させる。

184
00:06:13,680 --> 00:06:14,700
そしてテストする。新しい画像を

185
00:06:14,890 --> 00:06:16,710
提示して「この新しいものは何？」と尋ね、

186
00:06:17,980 --> 00:06:20,030
上手くいけば検出器はそれが車であると認識する。

187
00:06:21,410 --> 00:06:24,000
どうして非線形の仮説が

188
00:06:24,120 --> 00:06:26,810
必要かを理解するために、

189
00:06:27,050 --> 00:06:27,940
我々が学習アルゴリズムに

190
00:06:28,190 --> 00:06:29,360
与えるであろう車の画像や

191
00:06:29,480 --> 00:06:31,780
車でない画像をいくつか見てみよう。

192
00:06:32,960 --> 00:06:33,920
画像の中から一対の

193
00:06:34,090 --> 00:06:35,630
ピクセルを選んで、

194
00:06:35,750 --> 00:06:37,040
ピクセル1 の座標、

195
00:06:37,180 --> 00:06:39,500
ピクセル2 の座標とする。

196
00:06:39,730 --> 00:06:42,390
そしてこの車を特定の位置に

197
00:06:42,510 --> 00:06:44,010
プロットする。

198
00:06:44,360 --> 00:06:45,890
ピクセル1 とピクセル2 の

199
00:06:46,430 --> 00:06:47,870
輝度に応じて。

200
00:06:49,260 --> 00:06:50,630
他の画像についてもいくつかやってみよう。

201
00:06:51,060 --> 00:06:52,450
車の別の例をとると、

202
00:06:52,980 --> 00:06:53,980
同じ 2つの

203
00:06:54,080 --> 00:06:55,010
ピクセル座標を見て、

204
00:06:56,160 --> 00:06:57,570
この画像は

205
00:06:57,770 --> 00:06:58,970
ピクセル1 が異なる輝度を持ち、

206
00:06:59,230 --> 00:07:00,660
ピクセル2 も別の輝度を持っている。

207
00:07:00,960 --> 00:07:02,940
そのため結局 図上の異なる位置に置かれる。

208
00:07:03,360 --> 00:07:05,740
次に陰性の例もいくつか同様にプロットしてみよう。

209
00:07:05,990 --> 00:07:07,590
これは車でない物、

210
00:07:07,720 --> 00:07:09,470
これは車でない物。

211
00:07:09,730 --> 00:07:10,910
そしてこれを

212
00:07:11,070 --> 00:07:12,720
よりたくさんの例について行ったとする。

213
00:07:13,280 --> 00:07:14,680
プラスは車を表し

214
00:07:15,080 --> 00:07:16,310
マイナスは車でない物を表す。

215
00:07:16,890 --> 00:07:18,500
最終的にわかるのは

216
00:07:18,830 --> 00:07:20,680
車と車でない物が

217
00:07:20,890 --> 00:07:22,430
空間内の異なる領域に分布

218
00:07:22,570 --> 00:07:24,910
しているということで、

219
00:07:25,180 --> 00:07:26,570
我々に必要なのは

220
00:07:26,750 --> 00:07:28,790
2つのクラスを分離しようとする

221
00:07:29,000 --> 00:07:30,900
何らかの非線形の仮説だ。

222
00:07:32,480 --> 00:07:34,300
フィーチャー空間の次元は何だろうか？

223
00:07:35,290 --> 00:07:38,210
仮にたった 50x50 のピクセル画像を使うとしよう。

224
00:07:38,770 --> 00:07:40,050
かなり小さな画像、

225
00:07:40,520 --> 00:07:42,760
一辺が 50 ピクセルしかない画像を想定する。

226
00:07:43,470 --> 00:07:44,990
すると 2500 ピクセルあることになるので、

227
00:07:46,330 --> 00:07:47,650
フィーチャーサイズの

228
00:07:47,740 --> 00:07:49,310
次元は n=2500、

229
00:07:49,520 --> 00:07:51,450
ここでフィーチャーベクトル x は

230
00:07:51,700 --> 00:07:52,910
すべてのピクセル検査の

231
00:07:53,180 --> 00:07:54,570
リストだ。これは

232
00:07:54,710 --> 00:07:56,690
ピクセル1 の明るさ、

233
00:07:57,080 --> 00:07:58,030
ピクセル2 の明るさ、

234
00:07:58,330 --> 00:07:59,580
等々から

235
00:07:59,870 --> 00:08:01,310
最後のピクセルの明るさ

236
00:08:01,400 --> 00:08:03,420
までのことで、

237
00:08:03,590 --> 00:08:05,450
典型的なコンピュータ表現では

238
00:08:05,540 --> 00:08:07,190
各々は、グレイスケール値の場合

239
00:08:07,480 --> 00:08:09,020
例えば  0 から 255 までの

240
00:08:09,230 --> 00:08:12,110
値となるだろう。

241
00:08:12,520 --> 00:08:13,290
n=2500 は

242
00:08:13,950 --> 00:08:15,580
グレイスケール画像を

243
00:08:15,740 --> 00:08:17,140
使う場合だ。

244
00:08:17,790 --> 00:08:18,800
RGB 画像を使い

245
00:08:19,440 --> 00:08:21,140
赤、緑、青を分ける

246
00:08:21,420 --> 00:08:23,870
場合は、n=7500 となるだろう。

247
00:08:27,650 --> 00:08:28,630
だからもし

248
00:08:29,000 --> 00:08:29,920
すべての 2次式フィーチャーを

249
00:08:30,370 --> 00:08:32,020
取り込んで、非線形の

250
00:08:32,300 --> 00:08:33,710
仮説を学習させようとしたら

251
00:08:33,810 --> 00:08:34,680
つまり、Xi かける Xj の形の

252
00:08:35,430 --> 00:08:38,900
すべての項を取り込み、

253
00:08:39,130 --> 00:08:40,370
それが 2500 ピクセルあったら

254
00:08:40,580 --> 00:08:42,500
結局合計で 300万ピクセルになる。

255
00:08:43,050 --> 00:08:44,620
そしてこれは妥当な大きさから

256
00:08:44,720 --> 00:08:46,430
程遠い。1つのトレーニング例

257
00:08:46,600 --> 00:08:48,680
当たりに、これら 300万フィーチャーの

258
00:08:48,840 --> 00:08:50,070
すべてを見つけて、表現する

259
00:08:50,310 --> 00:08:52,250
計算は非常に高くつくだろう。

260
00:08:55,470 --> 00:08:57,580
だから、単純なロジスティック回帰に

261
00:08:58,100 --> 00:08:59,230
2次や 3次のフィーチャーを

262
00:08:59,300 --> 00:09:00,510
加えたもの

263
00:09:00,930 --> 00:09:01,910
- これは n が大きい時に

264
00:09:01,980 --> 00:09:03,950
非線形の仮説を学習させるのには

265
00:09:04,550 --> 00:09:06,090
まったく向いていない。

266
00:09:06,310 --> 00:09:08,410
フィーチャーが多くなりすぎるからだ。

267
00:09:09,370 --> 00:09:10,620
以降のいくつかのビデオでは、

268
00:09:10,840 --> 00:09:11,890
ニューラルネットワークについて

269
00:09:12,080 --> 00:09:13,670
教えよう。これは複雑な

270
00:09:13,980 --> 00:09:15,370
仮説、複雑な非線形の仮説を

271
00:09:15,650 --> 00:09:17,720
学習させるのにずっとよい方法

272
00:09:17,960 --> 00:09:19,780
だということがわかる。

273
00:09:20,070 --> 00:09:22,080
フィーチャー空間、n が大きい時でも。

274
00:09:22,860 --> 00:09:24,080
そしてついでに

275
00:09:24,420 --> 00:09:25,580
ニューラルネットワークの

276
00:09:25,780 --> 00:09:26,730
歴史的に重要な応用についての

277
00:09:27,240 --> 00:09:29,030
おもしろい動画をいくつか

278
00:09:30,300 --> 00:09:31,290
お見せしよう。

279
00:09:32,100 --> 00:09:33,480
あなたも後ほど見るこれらの動画を

280
00:09:33,570 --> 00:09:35,460
おもしろいと思ってくれるといいが。