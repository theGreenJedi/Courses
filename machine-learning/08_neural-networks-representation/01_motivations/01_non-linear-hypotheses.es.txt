En este video y en los siguientes videos, me gustaría hablarte acerca de un algoritmo de aprendizaje, llamado red neuronal. Primero vamos a hablar sobre la representación, y después, en la siguiente serie de videos hablaremos de los algoritmos de aprendizaje para ésta. Las redes neuronales en realidad son una idea bastante vieja, pero han caído en desuso por un tiempo. Pero hoy en día, es la técnica de vanguardia para muchos problemas diferentes de aprendizaje automático. Entonces, ¿por qué necesitamos otro algoritmo de aprendizaje? Ya tenemos la regresión lineal y tenemos regresión logística, así que ¿por qué necesitamos las redes neuronales? Con el fin de motivar la discusión en torno a las redes neuronales, voy a empezar por mostrarte algunos ejemplos de problemas de aprendizaje automático en los que necesitamos aprender hipótesis complejas no lineales. Considera un problema de clasificación de aprendizaje supervisado en el que tienes un conjunto de enseñanza como este. Si quieres aplicar una regresión logística a este problema, una cosa que podrías hacer es aplicar una regresión logística con una gran cantidad de variables no lineales de este modo. Entonces aquí, como siempre, g es la función sigmoidea, y podemos incluir varios términos polinomiales como esos. Y, si incluyes suficientes términos polinomiales, ya sabes, quizás puedas obtener una hipótesis que separe los ejemplos positivos y negativos. Este método particular funciona bien cuando sólo tienes, digamos, dos variables - x1 y x2 - porque entonces puedes incluir todos esos términos polinomiales de x1 y x2. Pero muchos problemas interesantes de aprendizaje automático tendrán muchas más variables que solo dos. hemos estado hablando sobre una predicción de vivienda, y supón que tienes un problema de clasificación de vivienda en lugar de un problema de regresión, quizás, como si tuvieras diferentes variables de una casa, y deseas predecir cuáles son las posibilidades de que tu casa se venda en los próximos seis meses, por lo que sería un problema de clasificación. Y como vimos que podemos tener un gran número de variables, quizás cien variables diferentes de diferentes casas. Para un problema como este, si fueras a incluir todos los términos cuadráticos, todos esos, incluso todos los términos cuadráticos que son el segundo término o el término polinomial, habrían muchos de ellos. Habrían términos como x1 al cuadrado, x1x2, x1x3, ya sabes, x1x4 hasta x1x100 y luego tienes 2x al cuadrado, x2x3 y así sucesivamente. Y si sólo incluyes los términos de segundo orden, esto es, los términos que son un producto de, ya sabes, dos de estos términos, x1 multiplicado por x1 y así sucesivamente, entonces, para el caso de n igual a 100, terminas con unas 5 mil variables. Y, asintóticamente, el número de variables cuadráticas crece aproximadamente como orden de n al cuadrado, en donde n es el número de variables originales, como 1x hasta x100, que teníamos anteriormente. Y esto, en realidad, está más cerca a n al cuadrado sobre dos. Entonces, incluir todas las variables cuadráticas no parece una buena idea, porque son muchas variables y podrías terminar sobreajustando el conjunto de aprendizaje, y también puede ser computacionalmente costoso, ya sabes, trabajar con tantas variables. Algo que puedes hacer es incluir sólo un subconjunto de éstos, entonces, si sólo incluyes las variables x1 al cuadrado, x2 al cuadrado, x3 al cuadrado, hasta, quizás, x100 al cuadrado, entonces el número de variables es mucho menor. Aquí sólo tienes 100 funciones cuadráticas como esta, pero ésta no tiene suficientes variables y, ciertamente, no te dejará ajustar el conjunto de datos como la que está en la parte superior izquierda. De hecho, si incluyes sólo estas funciones cuadráticas junto con el x1 original, y así sucesivamente, hasta x100 variables, entonces realmente podrás ajustar hipótesis muy interesantes. Entonces, puedes ajustar cosas como, ya sabes, acceder a una línea de las elipses como éstas, pero, ciertamente, no puedes ajustar un conjunto de datos más complejos como el que se muestra aquí. Entonces, 5000 variables parecen demasiadas, si fueras a incluir las cúbicas o las conocidas de tercer orden de los demás, los x1, x2, x3. Ya sabes, x1 al cuadrado, x2, x10 y x11, x17 y así sucesivamente. Puedes imaginar que habrán muchas de estas variables. De hecho, van a ser variables de orden y de cubo, y si cualquiera es 100, puedes calcular eso, terminarás con el orden de aproximadamente unas 170,000 de estas variables cúbicas y, por lo tanto, incluyendo estas variables auto-polinomiales más elevadas y, cuando el final de tu conjunto original de variables es grande, esto realmente aumenta de forma dramática tu espacio de variables y no parece una buena forma de proponer variables adicionales con las cuales construir no muchos clasificadores cuando n es grande. Para muchos problemas de aprendizaje automático, n será bastante grande. Aquí hay un ejemplo. Consideremos el problema de la visión por computadora. Y supongamos que quieres usar aprendizaje automático para entrenar a un clasificador para examinar una imagen y decirnos si la imagen es un auto o no. Mucha gente se pregunta por qué la visión por computadora puede ser complicada. Quiero decir, cuando tú y yo vemos esta imagen, es demasiado obvio lo que es. Te preguntas cómo es que un algoritmo de aprendizaje podría no saber lo que es esta imagen. Para entender por qué la visión por computadora es complicada, hagamos un acercamiento a una pequeña parte de la imagen, como el área en la que está el pequeño rectángulo rojo. resulta que cuando tú y yo vemos un auto, la computadora ve eso. Lo que ve es esta matriz, o esta cuadrícula, de valores de intensidades de pixeles que nos dice el brillo de cada pixel en la imagen. Entonces, el problema de la visión por computadora es ver esta matriz de valores de intensidad de pixeles y decirnos que esos número representan la manija de la puerta de un auto. Concretamente, cuando utilizamos aprendizaje automático para construir un detector de autos, lo que hacemos es crear un conjunto de aprendizaje con valores asignados, con, digamos, algunos ejemplos de asignación de valores de autos y algunos ejemplos de asignación de valores de cosas que no son autos, entonces le damos nuestro conjunto de entrenamiento al algoritmo de aprendizaje al que se le enseñó un clasificador y, después, ya sabes, podemos probarlo y mostrar la nueva imagen y preguntar, "¿Qué es esta cosa nueva?". Y, con suerte, reconocerá que esto es un auto. Para entender por qué necesita hipótesis no lineales, vamos a ver algunas de las imágenes de autos y, tal vez, no-autos que podamos introducir a nuestro algoritmo de aprendizaje. Vamos a elegir un par de ubicaciones de pixeles en nuestras imágenes, de forma que sea la ubicación del pixel uno y la ubicación del pixel dos, y vamos a trazar este auto, ya sabes, en la ubicación, en un determinado punto, dependiendo de las intensidades del pixel uno y del pixel dos. Y vamos a hacer esto con algunas otras imágenes. Ahora, tomemos un ejemplo diferente del auto, ya sabes, veamos las mismas ubicaciones de los dos pixeles y esa imagen tiene una diferente intensidad para el pixel uno y una diferente intensidad para el pixel dos. Entonces, termina siendo una ubicación diferente en la figura. Y luego vamos a trazar algunos ejemplos negativos también. Ese no es un auto, ese no es un auto. Y si hacemos esto para más y más ejemplos usando los positivos para denotar a los autos y los negativos para denotar a los no-autos, encontraremos que los autos y no-autos terminan en diferentes regiones del espacio, y lo que necesitamos, por lo tanto, es algún tipo de hipótesis no lineal para intentar separar las dos clases. ¿Cuál es la dimensión del espacio de variables? Supongamos que fuéramos a usar sólo imágenes de 50 por 50 pixeles. Ahora supongamos que nuestras imágenes fueran muy pequeñas, de sólo 50 pixeles por lado. Entonces tendríamos 2500 pixeles, y así la dimensión de nuestro tamaño de variable será N igual a 2500, donde nuestro vector de variable x es una lista de todas las pruebas de pixeles, ya sabes, del brillo del pixel del pixel uno, el brillo del pixel dos, y así sucesivamente hasta el brillo del pixel del último píxel donde, ya sabes, en una representación computacional típica, cada uno de estos pueden ser valores entre, digamos, 0 a 255, si nos da el valor en escala de grises. Entonces, tenemos que n es igual a 2500, y eso es si estuviéramos usando imágenes en escala de grises. Si estuviéramos usando imágenes en RGB, con valores separados para rojo, verde y azul, tendríamos que n es igual a 7500. Ahora, si quisiéramos tratar de aprender una hipótesis no lineal incluyendo todas las variables cuadráticas, esto es, todos los términos de la forma, ya sabes, Xi veces Xj, mientras que con los 2500 pixeles terminaríamos con un total de 3 millones de variables. Y esto es demasiado grande para ser razonable; el cálculo sería demasiado costoso de encontrar y para representar todos estos tres millones de variables por ejemplo de entrenamiento. Entonces, una simple regresión logística junto con añadir, quizás, las variables cuadráticas o cúbicas - no es una buena manera de aprender hipótesis no lineales complejas cuando n es grande porque acabas con demasiadas variables. En los siguientes videos, me me gustaría hablarles sobre las redes neuronales, que resulta ser una forma mucho mejor para aprender hipótesis complejas, hipótesis complejas no lineales, incluso cuando tu espacio de entrada de variables, incluso cuando n es grande. Sobre la marcha también te mostraré un par de videos divertidos de aplicaciones históricamente importantes de las redes neuronales, así como también espero que esos videos que veremos más adelante también sean divertidos de ver.