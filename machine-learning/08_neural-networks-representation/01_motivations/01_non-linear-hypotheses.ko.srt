1
00:00:00,440 --> 00:00:01,400
이번과

2
00:00:01,480 --> 00:00:02,640
다음 여러편의 비디오에서, 

3
00:00:02,780 --> 00:00:04,270
저는 뉴럴 네트워크라고 불리는 

4
00:00:04,550 --> 00:00:06,110
학습 알고리즘에 대해 설명하려고 합니다.

5
00:00:07,190 --> 00:00:07,900
우리는 먼저 

6
00:00:08,079 --> 00:00:09,330
뉴럴 네트워크가 어떻게 작동하는지 설명하고

7
00:00:09,600 --> 00:00:10,390
다음 여러편의 비디오에서

8
00:00:10,410 --> 00:00:12,160
구체적인 학습 알고리즘을 설명하려고 합니다.

9
00:00:12,660 --> 00:00:14,070
뉴럴 네트워크는 사실

10
00:00:14,510 --> 00:00:15,870
꽤 오래전에 제시된 방법이지만,

11
00:00:16,290 --> 00:00:17,680
한동안 사람들의 관심을 받지 못했습니다.

12
00:00:18,200 --> 00:00:19,270
그러나 오늘날, 뉴럴 네트워크는

13
00:00:19,580 --> 00:00:20,820
다양한 분야에 기계학습을 적용하기 위한

14
00:00:21,090 --> 00:00:22,390
최첨단의 기법입니다.

15
00:00:23,740 --> 00:00:25,740
그렇다면 왜 신경망 알고리즘이 필요할까요?

16
00:00:26,300 --> 00:00:28,030
이미 선형 회귀와

17
00:00:28,180 --> 00:00:31,260
로지스틱 회귀가 기계 학습에 사용되고 있습니다. 그런데 왜 우리는 신경망 알고리즘을 필요로 할까요?

18
00:00:32,280 --> 00:00:34,260
신경망 이론에 대한

19
00:00:34,790 --> 00:00:35,970
관심을 불러일으키기 위해,

20
00:00:36,120 --> 00:00:37,130
먼저 몇가지 

21
00:00:37,310 --> 00:00:38,720
기계학습의 예를 보여드리겠습니다.

22
00:00:38,930 --> 00:00:40,100
이와 같은 문제에서는

23
00:00:40,300 --> 00:00:41,850
복잡한 비선형 가설들이 필요합니다.

24
00:00:43,850 --> 00:00:45,650
지도 학습 분류 문제를 생각해봅시다.

25
00:00:46,530 --> 00:00:48,440
이 그림과 같은 훈련 데이터가 주어진 문제에

26
00:00:49,280 --> 00:00:50,530
로지스틱 회귀를

27
00:00:50,960 --> 00:00:52,710
적용하려 한다면, 

28
00:00:52,900 --> 00:00:54,250
이 식과 같이

29
00:00:54,660 --> 00:00:56,140
수많은 비선형적 요소들을 사용해

30
00:00:56,190 --> 00:00:57,720
로직스틱 회귀를 적용해볼 수 밖에 없을것입니다.

31
00:00:58,170 --> 00:00:59,580
여기 있는 g 함수는 주로

32
00:01:00,070 --> 00:01:01,710
시그모이드 함수를 나타냅니다.

33
00:01:01,780 --> 00:01:04,680
여기에서 시그모이드 함수를 사용하기 때문에, 수많은 다항식들을 포함할 수 있습니다.

34
00:01:05,450 --> 00:01:06,790
만약 이 식에 충분히 많은 항들을 포함한다면,

35
00:01:07,370 --> 00:01:08,280
아마도

36
00:01:08,950 --> 00:01:10,280
양수값을 가지는 데이터와 음의 값을 가지는 데이터들을 구분할 수 있는

37
00:01:11,600 --> 00:01:13,780
가설을 세울 수 있습니다.

38
00:01:14,630 --> 00:01:16,080
여기에 제시된 특별한 방법은 

39
00:01:16,470 --> 00:01:18,400
오직 두개의 요소 -

40
00:01:18,620 --> 00:01:20,180
 x1과 x2만으로 데이터가 주어졌을때에만 잘 동작합니다.

41
00:01:20,190 --> 00:01:20,980
왜냐하면, x1과 x2로 이루어진

42
00:01:21,500 --> 00:01:22,880
모든 항들을 포함할 수 

43
00:01:23,400 --> 00:01:24,620
있기 때문입니다.

44
00:01:24,810 --> 00:01:26,280
그러나 많은 기계 학습 문제에서는

45
00:01:26,520 --> 00:01:27,730
단지 두개가 아닌

46
00:01:27,910 --> 00:01:29,230
훨씬 많은 요소들을 고려하게 될 것입니다.

47
00:01:30,780 --> 00:01:31,760
이제 문제를

48
00:01:32,320 --> 00:01:34,560
주택 수급 예측 문제로 돌려봅시다.

49
00:01:35,130 --> 00:01:36,990
만약 당신이 회귀 문제가 아닌

50
00:01:38,020 --> 00:01:39,280
주택 분류 문제를 가지고 있다면,

51
00:01:39,390 --> 00:01:41,170
아마도 이런 상황일겁니다.

52
00:01:41,580 --> 00:01:43,350
당신이 어떤 주택이 어떤 요소와 특징을 가졌는지

53
00:01:43,440 --> 00:01:44,760
 알고 있을 것입니다.

54
00:01:45,010 --> 00:01:46,000
그 상태에서 당신이

55
00:01:46,050 --> 00:01:47,590
앞으로 6개월 이내에 그 주택이 팔릴 가능성이

56
00:01:47,700 --> 00:01:48,710
얼마나 될지 예측하려고 한다면,

57
00:01:48,910 --> 00:01:51,040
그것은 분류 문제일 것입니다.

58
00:01:52,100 --> 00:01:53,060
그리고 이와 같이

59
00:01:53,260 --> 00:01:55,130
우리는 아주 많은 특징들을

60
00:01:55,260 --> 00:01:56,480
떠올릴 수 있습니다.

61
00:01:56,840 --> 00:01:58,270
주택마다 각각 다른 요소를 100가지 쯤은 떠올릴 수 있겠죠.

62
00:02:00,130 --> 00:02:01,610
이렇게 수없이 다양한 항들이 있는 문제를 푸는 데에

63
00:02:01,880 --> 00:02:03,260
모든 항과 이차항들을

64
00:02:03,370 --> 00:02:04,980
포함하려고 한다면,

65
00:02:05,100 --> 00:02:06,260
게다가 심지어

66
00:02:06,540 --> 00:02:07,540
제곱항인 이차항에

67
00:02:07,930 --> 00:02:10,450
다변수 이차항까지 포함한다면, 항이 엄청 많아질 겁니다.

68
00:02:10,560 --> 00:02:11,580
x1 제곱도 있을 거구요,

69
00:02:12,960 --> 00:02:17,610
x1x2, x1x3, 계속하자면 x1x4,

70
00:02:18,750 --> 00:02:21,880
결국 x1x100까지 모두 포함되겠죠.

71
00:02:21,980 --> 00:02:23,620
다음 x2 제곱, x2x3,

72
00:02:25,620 --> 00:02:25,980
등등이 계속 이어집니다.

73
00:02:26,510 --> 00:02:27,770
만약 당신이

74
00:02:28,060 --> 00:02:29,200
단지 이차항들만을,

75
00:02:29,330 --> 00:02:30,750
즉

76
00:02:30,840 --> 00:02:32,090
두 항의 곱,

77
00:02:32,220 --> 00:02:33,390
x1 곱하기 x1

78
00:02:33,510 --> 00:02:35,010
등등 만을 포함한다고 해도

79
00:02:35,780 --> 00:02:36,920
항이 100개인 경우에

80
00:02:38,180 --> 00:02:40,280
5000개가 넘는 요소를 포함하게 됩니다.

81
00:02:41,890 --> 00:02:44,880
그리고, 점근적으로, 

82
00:02:45,000 --> 00:02:46,330
이차항의 수는

83
00:02:46,770 --> 00:02:48,670
O(n*n)으로 증가하는데,

84
00:02:48,820 --> 00:02:50,330
여기에서 n은

85
00:02:50,460 --> 00:02:52,790
원래 요소의 갯수입니다.

86
00:02:53,370 --> 00:02:54,780
주택 문제에서 떠올렸던 x1 부터 x100까지의 변수들 처럼요.

87
00:02:55,700 --> 00:02:58,750
그리고, 그것은 사실 2분의 n 제곱에 가깝습니다.

88
00:02:59,920 --> 00:03:01,440
그래서, 모든

89
00:03:01,560 --> 00:03:02,920
이차 요소들을 포함하는 것은

90
00:03:03,220 --> 00:03:04,220
좋은 생각같지는 않습니다.

91
00:03:04,300 --> 00:03:05,380
왜냐하면 그렇게 되면

92
00:03:05,580 --> 00:03:07,050
너무 많은 요소들을 포함하게 되어

93
00:03:07,220 --> 00:03:08,920
훈련 데이터에 너무 딱 맞는

94
00:03:09,330 --> 00:03:10,500
결과를 보게 될 것입니다.

95
00:03:10,740 --> 00:03:12,800
그리고, 계산하는데에도 오래 걸리겠죠

96
00:03:14,080 --> 00:03:15,120
그 수많은 요소들을 다루게 된다면요.

97
00:03:16,450 --> 00:03:17,540
한가지 당신이 할수 있는것은

98
00:03:17,770 --> 00:03:19,090
수많은 요소들 중 어떤 부분집합만을 포함하는 것입니다.

99
00:03:19,290 --> 00:03:20,950
이런 경우를 생각해봅시다. 단지

100
00:03:21,050 --> 00:03:22,630
 x1 제곱, x2 제곱,

101
00:03:23,590 --> 00:03:25,180
 x3 제곱,

102
00:03:25,580 --> 00:03:27,750
 x100 제곱까지만 포함한다면,

103
00:03:28,100 --> 00:03:29,500
요소의 수가 훨씬 적어집니다.

104
00:03:29,980 --> 00:03:31,720
이렇게 오직 100개의

105
00:03:32,070 --> 00:03:33,850
이차 요소들을 뽑아낼 수 있죠.

106
00:03:34,120 --> 00:03:35,950
그러나 이것은 충분한 요소가 아니며

107
00:03:36,100 --> 00:03:37,170
확실히

108
00:03:37,290 --> 00:03:39,330
이 그림처럼 데이터 셋을 잘 가려내지는 못할 겁니다.

109
00:03:39,570 --> 00:03:40,550
사실, 정말로

110
00:03:41,040 --> 00:03:42,720
이들 이차 요소들과

111
00:03:43,170 --> 00:03:44,870
원래 x1에서

112
00:03:45,350 --> 00:03:46,500
x100까지의 요소들을 함께 포함한다면,

113
00:03:47,460 --> 00:03:48,530
실제로는 매우

114
00:03:48,910 --> 00:03:50,210
흥미로운 가설을 세울 수 있습니다.

115
00:03:50,330 --> 00:03:52,350
그러니까, 이 그림처럼

116
00:03:52,490 --> 00:03:53,860
타원 모양의 구분선을 그려낼 수 있습니다.

117
00:03:55,080 --> 00:03:56,240
하지만 틀림없이

118
00:03:56,340 --> 00:03:57,930
이 그림과 같은 복잡한 구분선은 그려낼 수 없을겁니다.

119
00:03:59,360 --> 00:04:00,530
요소가 5000개나 된다고 생각할 수도 있습니다.

120
00:04:00,620 --> 00:04:03,090
하지만 만약

121
00:04:03,230 --> 00:04:04,860
세제곱이나

122
00:04:05,140 --> 00:04:06,100
다른 3차항들,

123
00:04:06,440 --> 00:04:08,050
x1x2x3 라던가,

124
00:04:08,400 --> 00:04:09,800
x1제곱,

125
00:04:10,310 --> 00:04:12,240
x2x10,

126
00:04:12,900 --> 00:04:15,280
x11x17 등을 포함하기 시작하면

127
00:04:15,700 --> 00:04:18,110
얼마나 많은 요소가 있을 지 상상해 보세요.

128
00:04:19,040 --> 00:04:19,770
요소들은

129
00:04:20,050 --> 00:04:21,260
n이 늘어날수록 세제곱에 비례해 수가 늘어날 것이고

130
00:04:22,210 --> 00:04:23,830
만약 n 이 100이면, 

131
00:04:24,150 --> 00:04:25,660
계산해보세요.

132
00:04:25,740 --> 00:04:26,870
대략

133
00:04:27,730 --> 00:04:29,650
170,000개의 삼차 요소들을

134
00:04:30,040 --> 00:04:31,670
포함하게 됩니다.

135
00:04:32,260 --> 00:04:34,470
이렇게 고차 다항 요소들을 포함하는 것은

136
00:04:34,920 --> 00:04:36,050
당신의 원래 요소의 수가 많을수록

137
00:04:36,230 --> 00:04:37,730
더욱 급격하게

138
00:04:38,530 --> 00:04:40,440
요소 공간을 확대하게 될 것입니다.

139
00:04:41,070 --> 00:04:42,180
이 방식이

140
00:04:42,320 --> 00:04:43,320
좋은 방식같진 않습니다.

141
00:04:43,560 --> 00:04:45,050
즉 n이 클때, 비선형적인 분류를 위해서

142
00:04:45,240 --> 00:04:48,100
요소를 추가하는 방식은 좋지 않습니다.

143
00:04:49,590 --> 00:04:52,560
많은 기계학습 문제에서 n은 아주 큽니다.

144
00:04:53,270 --> 00:04:53,560
예를 들어봅시다

145
00:04:55,000 --> 00:04:58,140
컴퓨터 비전 문제를 고려해 봅시다.

146
00:04:59,670 --> 00:05:00,770
그리고 당신이

147
00:05:01,260 --> 00:05:02,620
기계학습을 이용하여

148
00:05:02,710 --> 00:05:04,610
분류기를 트레이닝하여 

149
00:05:04,710 --> 00:05:05,880
이미지를 조사하고

150
00:05:06,160 --> 00:05:08,030
이미지가 차인지 아닌지를 판단하게 하고 싶다고 가정해봅시다.

151
00:05:09,480 --> 00:05:11,900
많은 사람들은 왜 컴퓨터 비전이 어려운지 궁금해합니다.

152
00:05:12,390 --> 00:05:13,140
당신과 내가

153
00:05:13,270 --> 00:05:15,670
이 그림을 바라볼때, 이것이 무엇인지는 매우 명백하죠.

154
00:05:15,900 --> 00:05:17,000
당신은

155
00:05:17,190 --> 00:05:18,320
어떻게 학습 알고리즘이

156
00:05:18,910 --> 00:05:20,880
이 그림이 무엇인지 아는데 실패할 수 있는지 궁금할테죠.

157
00:05:22,110 --> 00:05:23,330
왜 컴퓨터 비전이 어려운지를

158
00:05:23,720 --> 00:05:25,380
이해하기 위해서,

159
00:05:25,650 --> 00:05:26,690
작은 영역의 이미지를

160
00:05:26,940 --> 00:05:28,180
확대해서 보도록 합시다.

161
00:05:28,510 --> 00:05:30,240
여기 이 빨간 사각형을 확대해보죠.

162
00:05:30,400 --> 00:05:31,330
우리는 자동차를 보고 있지만

163
00:05:31,450 --> 00:05:34,270
컴퓨터는 다른 것을 봅니다.

164
00:05:34,780 --> 00:05:35,930
컴퓨터가 보는 것은 행렬,

165
00:05:36,600 --> 00:05:38,110
혹은 픽셀 값의 격자인데,

166
00:05:38,580 --> 00:05:40,350
이 값은

167
00:05:40,610 --> 00:05:42,930
이미지에서 각 픽셀의 밝기를 나타냅니다.

168
00:05:43,640 --> 00:05:45,170
컴퓨터 비전 문제는

169
00:05:45,550 --> 00:05:47,230
컴퓨터가 픽셀 밝기 값들로 이루어진

170
00:05:47,310 --> 00:05:49,140
이런 행렬을 보고,

171
00:05:49,410 --> 00:05:52,440
이 숫자들이 어떤 자동차의 문 손잡이를 나타낸다고 우리에게 말해줘야 하는 문제입니다.

172
00:05:54,230 --> 00:05:55,740
구체적으로, 우리가 

173
00:05:56,030 --> 00:05:57,220
기계학습을 이용하여, 

174
00:05:57,430 --> 00:05:59,060
차량 식별기를 구축하기 위해

175
00:05:59,360 --> 00:06:00,510
우리가 해야 하는 것은

176
00:06:00,800 --> 00:06:02,690
라벨 트레이닝 셋을 마련하는 것입니다.

177
00:06:02,890 --> 00:06:04,250
즉 "자동차"라고 표시된 예제들과

178
00:06:04,730 --> 00:06:05,850
"자동차가 아님"이라고 표시된

179
00:06:06,000 --> 00:06:07,150
다른 예제들을 준비해야 합니다.

180
00:06:07,380 --> 00:06:08,780
그 후에 우리는 트레이닝 셋을

181
00:06:09,090 --> 00:06:10,590
학습 알고리즘에 전달하여

182
00:06:10,720 --> 00:06:12,230
분류기를 학습시키고

183
00:06:12,310 --> 00:06:13,500
그 다음

184
00:06:13,680 --> 00:06:14,700
학습된 알고리즘을 테스트할 것입니다.

185
00:06:14,890 --> 00:06:16,710
이렇게, 새로운 이미지를 보여주고는 "이것은 무엇입니까?" 라고 질문하겠죠.

186
00:06:17,980 --> 00:06:20,030
그리곤 분류기가 그것이 차라고 대답하기를 기대하겠죠.

187
00:06:21,410 --> 00:06:24,000
왜 비선형적인 가설이

188
00:06:24,120 --> 00:06:26,810
필요한지 이해하기 위해서

189
00:06:27,050 --> 00:06:27,940
몇 개의 자동차 이미지와

190
00:06:28,190 --> 00:06:29,360
자동차가 아닌 이미지를 봅시다.

191
00:06:29,480 --> 00:06:31,780
우리의 학습 알고리즘에게 예제로 주었던 그 이미집니다.

192
00:06:32,960 --> 00:06:33,920
이 이미지에서 몇개의 픽셀들을

193
00:06:34,090 --> 00:06:35,630
골라봅시다. 그러면

194
00:06:35,750 --> 00:06:37,040
픽셀 1 은 여기서,

195
00:06:37,180 --> 00:06:39,500
픽셀 2 는 여기서 고르겠습니다. 그리고

196
00:06:39,730 --> 00:06:42,390
이 차를 도면에 표시하겠습니다.

197
00:06:42,510 --> 00:06:44,010
그러니까, 이 좌표값은

198
00:06:44,360 --> 00:06:45,890
픽셀1 과 픽셀2 의

199
00:06:46,430 --> 00:06:47,870
밝기를 값으로 가지는 좌표값입니다.

200
00:06:49,260 --> 00:06:50,630
다른 이미지들도 같은 작업을 해 봅시다.

201
00:06:51,060 --> 00:06:52,450
다른 "자동차" 예제를

202
00:06:52,980 --> 00:06:53,980
고르고,

203
00:06:54,080 --> 00:06:55,010
동일한 위치의 픽셀 두 개를 봅시다.

204
00:06:56,160 --> 00:06:57,570
그러면 그 이미지의 픽셀 1 은 다른 밝기를,

205
00:06:57,770 --> 00:06:58,970
픽셀 2도 역시

206
00:06:59,230 --> 00:07:00,660
다른 밝기를 가지고 있겠죠.

207
00:07:00,960 --> 00:07:02,940
따라서 다른 "자동차" 그림은 도표상에서 다른 위치에 놓이게 됩니다.

208
00:07:03,360 --> 00:07:05,740
"자동차"가 아닌 예제들도 도면에 표시해볼까요.

209
00:07:05,990 --> 00:07:07,590
이 이미지는 "자동차가 아님" 예제구요,

210
00:07:07,720 --> 00:07:09,470
이 이미지도 "자동차가 아님" 예제입니다.

211
00:07:09,730 --> 00:07:10,910
그리고 만약 우리가

212
00:07:11,070 --> 00:07:12,720
더욱더 많은 예제를 사용해

213
00:07:13,280 --> 00:07:14,680
"자동차"들을 나타내기 위해 +를,

214
00:07:15,080 --> 00:07:16,310
"자동차가 아님"을 나타내기 위해 -를 사용해 표시하면,

215
00:07:16,890 --> 00:07:18,500
우리는

216
00:07:18,830 --> 00:07:20,680
"자동차"와 "자동차가 아님"들이 좌표 공간에서

217
00:07:20,890 --> 00:07:22,430
서로 다른 영역에 위치하게 된다는 것을 발견할 겁니다.

218
00:07:22,570 --> 00:07:24,910
그러므로 우리가

219
00:07:25,180 --> 00:07:26,570
필요로 하는 것은

220
00:07:26,750 --> 00:07:28,790
어떤 비선형적인 가설이며

221
00:07:29,000 --> 00:07:30,900
이를 사용해 두가지 경우를 구분하도록 시도하는 것입니다.

222
00:07:32,480 --> 00:07:34,300
특징 공간은 몇 차원일까요?

223
00:07:35,290 --> 00:07:38,210
우리가 단지 50x50 픽셀짜리 이미지를 이용한다고 가정해 봅시다.

224
00:07:38,770 --> 00:07:40,050
우리가 사용하는 이미지들은

225
00:07:40,520 --> 00:07:42,760
단지 가로 길이가 50픽셀인 작은 이미지입니다.

226
00:07:43,470 --> 00:07:44,990
그러나 결국 우리는 2500개의 픽셀을 가지게 됩니다.

227
00:07:46,330 --> 00:07:47,650
따라서 특징 공간의 차원은

228
00:07:47,740 --> 00:07:49,310
n 이 2500 이며,

229
00:07:49,520 --> 00:07:51,450
우리의 특징 벡터 

230
00:07:51,700 --> 00:07:52,910
x는

231
00:07:53,180 --> 00:07:54,570
모든 픽셀 값의 목록입니다.

232
00:07:54,710 --> 00:07:56,690
픽셀 1의 밝기,

233
00:07:57,080 --> 00:07:58,030
픽셀 2의 밝기,

234
00:07:58,330 --> 00:07:59,580
등등 

235
00:07:59,870 --> 00:08:01,310
마지막 픽셀의 밝기까지

236
00:08:01,400 --> 00:08:03,420
이어집니다.

237
00:08:03,590 --> 00:08:05,450
전형적인 컴퓨터 표현에서

238
00:08:05,540 --> 00:08:07,190
밝기 값은

239
00:08:07,480 --> 00:08:09,020
0에서 255사이의 값입니다.

240
00:08:09,230 --> 00:08:12,110
흑백 이미지를 사용할 경우에, 말이죠.

241
00:08:12,520 --> 00:08:13,290
즉 n은 2500입니다.

242
00:08:13,950 --> 00:08:15,580
만약 우리가 

243
00:08:15,740 --> 00:08:17,140
흑백 이미지들을 이용한다는 가정 하에서는요.

244
00:08:17,790 --> 00:08:18,800
만약 우리가 분리된 RGB 이미지를

245
00:08:19,440 --> 00:08:21,140
이용한다면, 이미지는 적색, 녹색, 청색 값을 각각 가지며

246
00:08:21,420 --> 00:08:23,870
n이 7500이 됩니다.

247
00:08:27,650 --> 00:08:28,630
우리가 

248
00:08:29,000 --> 00:08:29,920
모든 이차 요소들을 

249
00:08:30,370 --> 00:08:32,020
포함하는 비선형 가설을 

250
00:08:32,300 --> 00:08:33,710
학습하려고 하면

251
00:08:33,810 --> 00:08:34,680
이차 요소라는게

252
00:08:35,430 --> 00:08:38,900
Xi 곱하기 Xj 형태의 모든 항이니까

253
00:08:39,130 --> 00:08:40,370
2500 픽셀이 결국엔

254
00:08:40,580 --> 00:08:42,500
3백만개에 달하는 요소항으로 늘어납니다.

255
00:08:43,050 --> 00:08:44,620
이성적으로, 그건 너무 많습니다. 

256
00:08:44,720 --> 00:08:46,430
계산이 무척

257
00:08:46,600 --> 00:08:48,680
오래 걸릴겁니다.

258
00:08:48,840 --> 00:08:50,070
매 예제마다 300만개의

259
00:08:50,310 --> 00:08:52,250
특징을 찾고 표현하려면요.

260
00:08:55,470 --> 00:08:57,580
단순 로지스틱 회귀를 

261
00:08:58,100 --> 00:08:59,230
이차, 또는 삼차 특징들과 함께하여,

262
00:08:59,300 --> 00:09:00,510
n 이 클때

263
00:09:00,930 --> 00:09:01,910
복잡한 비선형 가설을 학습하는데 사용하는 것은

264
00:09:01,980 --> 00:09:03,950
좋은 방법이 아니다.

265
00:09:04,550 --> 00:09:06,090
왜냐하면, 너무 많은 특징들이 

266
00:09:06,310 --> 00:09:08,410
될 것이기 때문이다.

267
00:09:09,370 --> 00:09:10,620
다음 몇편의 비디오에서

268
00:09:10,840 --> 00:09:11,890
나는 뉴럴 네트워크에 대해서 

269
00:09:12,080 --> 00:09:13,670
설명할 것이다.

270
00:09:13,980 --> 00:09:15,370
이는 복잡한 비선형 가설을 학습하는데 

271
00:09:15,650 --> 00:09:17,720
훨씬 좋은 방법이다.

272
00:09:17,960 --> 00:09:19,780
심지어 당신의 입력 특징 공간 n 이 

273
00:09:20,070 --> 00:09:22,080
클때의 경우에도 말이다

274
00:09:22,860 --> 00:09:24,080
중간에 

275
00:09:24,420 --> 00:09:25,580
나는 몇개의

276
00:09:25,780 --> 00:09:26,730
재미있는 비디오를 보여줄 것이다. 

277
00:09:27,240 --> 00:09:29,030
이들은 뉴럴 네트워크의 역사적으로 중요한 응용이며,

278
00:09:30,300 --> 00:09:31,290
나는 우리가 나중에 볼 이들 비디오가 

279
00:09:32,100 --> 00:09:33,480
당신이 보기에도 

280
00:09:33,570 --> 00:09:35,460
역시나 재미있기를 바란다.