1
00:00:00,440 --> 00:00:01,400
在这节课和接下来的课程中

2
00:00:01,480 --> 00:00:02,640
我将给大家介绍

3
00:00:02,780 --> 00:00:04,270
一种叫“神经网络”(Neural Network)

4
00:00:04,550 --> 00:00:06,110
的机器学习算法

5
00:00:07,190 --> 00:00:07,900
我们将首先讨论

6
00:00:08,079 --> 00:00:09,330
神经网络的表层结构

7
00:00:09,600 --> 00:00:10,390
在后续课程中

8
00:00:10,410 --> 00:00:12,160
再来具体讨论的学习算法

9
00:00:12,660 --> 00:00:14,070
神经网络实际上是一个

10
00:00:14,510 --> 00:00:15,870
相对古老的算法

11
00:00:16,290 --> 00:00:17,680
并且后来沉寂了一段时间

12
00:00:18,200 --> 00:00:19,270
不过到了现在

13
00:00:19,580 --> 00:00:20,820
它又成为许多机器学习问题

14
00:00:21,090 --> 00:00:22,390
的首选技术

15
00:00:23,740 --> 00:00:25,740
不过我们为什么还需要这个学习算法？

16
00:00:26,300 --> 00:00:28,030
我们已经有线性回归和逻辑回归算法了

17
00:00:28,180 --> 00:00:31,260
为什么还要研究神经网络？

18
00:00:32,280 --> 00:00:34,260
为了阐述研究

19
00:00:34,790 --> 00:00:35,970
神经网络算法的目的

20
00:00:36,120 --> 00:00:37,130
我们首先来看几个

21
00:00:37,310 --> 00:00:38,720
机器学习问题作为例子

22
00:00:38,930 --> 00:00:40,100
这几个问题的解决

23
00:00:40,300 --> 00:00:41,850
都依赖于研究复杂的非线性分类器

24
00:00:43,850 --> 00:00:45,650
考虑这个监督学习分类的问题

25
00:00:46,530 --> 00:00:48,440
我们已经有了对应的训练集

26
00:00:49,280 --> 00:00:50,530
如果利用逻辑回归算法

27
00:00:50,960 --> 00:00:52,710
来解决这个问题

28
00:00:52,900 --> 00:00:54,250
首先需要构造

29
00:00:54,660 --> 00:00:56,140
一个包含很多非线性项的

30
00:00:56,190 --> 00:00:57,720
逻辑回归函数

31
00:00:58,170 --> 00:00:59,580
这里g仍是s型函数 (即f(x)=1/(1+e^-x) )

32
00:01:00,070 --> 00:01:01,710
我们能让函数

33
00:01:01,780 --> 00:01:04,680
包含很多像这样的多项式项

34
00:01:05,450 --> 00:01:06,790
事实上  当多项式项数足够多时

35
00:01:07,370 --> 00:01:08,280
那么可能

36
00:01:08,950 --> 00:01:10,280
你能够得到一个

37
00:01:11,600 --> 00:01:13,780
分开正样本和负样本的分界线

38
00:01:14,630 --> 00:01:16,080
当只有两项时

39
00:01:16,470 --> 00:01:18,400
比如 x1 x2

40
00:01:18,620 --> 00:01:20,180
这种方法确实能得到不错的结果

41
00:01:20,190 --> 00:01:20,980
因为你可以

42
00:01:21,500 --> 00:01:22,880
把x1和x2的所有组合

43
00:01:23,400 --> 00:01:24,620
都包含到多项式中

44
00:01:24,810 --> 00:01:26,280
但是对于许多

45
00:01:26,520 --> 00:01:27,730
复杂的机器学习问题

46
00:01:27,910 --> 00:01:29,230
涉及的项往往多于两项

47
00:01:30,780 --> 00:01:31,760
我们之前已经讨论过

48
00:01:32,320 --> 00:01:34,560
房价预测的问题

49
00:01:35,130 --> 00:01:36,990
假设现在要处理的是

50
00:01:38,020 --> 00:01:39,280
关于住房的分类问题

51
00:01:39,390 --> 00:01:41,170
而不是一个回归问题

52
00:01:41,580 --> 00:01:43,350
假设你对一栋房子的多方面特点

53
00:01:43,440 --> 00:01:44,760
都有所了解

54
00:01:45,010 --> 00:01:46,000
你想预测

55
00:01:46,050 --> 00:01:47,590
房子在未来半年内

56
00:01:47,700 --> 00:01:48,710
能被卖出去的概率

57
00:01:48,910 --> 00:01:51,040
这是一个分类问题

58
00:01:52,100 --> 00:01:53,060
我们可以想出

59
00:01:53,260 --> 00:01:55,130
很多特征

60
00:01:55,260 --> 00:01:56,480
对于不同的房子有可能

61
00:01:56,840 --> 00:01:58,270
就有上百个特征

62
00:02:00,130 --> 00:02:01,610
对于这类问题

63
00:02:01,880 --> 00:02:03,260
如果要包含

64
00:02:03,370 --> 00:02:04,980
所有的二次项

65
00:02:05,100 --> 00:02:06,260
即使只包含

66
00:02:06,540 --> 00:02:07,540
二项式或多项式的计算

67
00:02:07,930 --> 00:02:10,450
最终的多项式也可能有很多项

68
00:02:10,560 --> 00:02:11,580
比如x1^2

69
00:02:12,960 --> 00:02:17,610
x1x2   x1x3   x1x4

70
00:02:18,750 --> 00:02:21,880
直到x1x100

71
00:02:21,980 --> 00:02:23,620
还有x2^2  x2x3

72
00:02:25,620 --> 00:02:25,980
等等很多项

73
00:02:26,510 --> 00:02:27,770
因此

74
00:02:28,060 --> 00:02:29,200
即使只考虑二阶项

75
00:02:29,330 --> 00:02:30,750
也就是说

76
00:02:30,840 --> 00:02:32,090
两个项的乘积

77
00:02:32,220 --> 00:02:33,390
x1乘以x1

78
00:02:33,510 --> 00:02:35,010
等等类似于此的项

79
00:02:35,780 --> 00:02:36,920
那么  在n=100的情况下

80
00:02:38,180 --> 00:02:40,280
最终也有5000个二次项

81
00:02:41,890 --> 00:02:44,880
而且渐渐地

82
00:02:45,000 --> 00:02:46,330
随着特征个数n的增加

83
00:02:46,770 --> 00:02:48,670
二次项的个数大约以n^2的量级增长

84
00:02:48,820 --> 00:02:50,330
其中

85
00:02:50,460 --> 00:02:52,790
n是原始项的个数

86
00:02:53,370 --> 00:02:54,780
即我们之前说过的x1到x100这些项

87
00:02:55,700 --> 00:02:58,750
事实上二次项的个数大约是(n^2)/2

88
00:02:59,920 --> 00:03:01,440
因此要包含所有的

89
00:03:01,560 --> 00:03:02,920
二次项是很困难的

90
00:03:03,220 --> 00:03:04,220
所以这可能

91
00:03:04,300 --> 00:03:05,380
不是一个好的做法

92
00:03:05,580 --> 00:03:07,050
而且由于项数过多

93
00:03:07,220 --> 00:03:08,920
最后的结果很有可能是过拟合的

94
00:03:09,330 --> 00:03:10,500
此外

95
00:03:10,740 --> 00:03:12,800
在处理这么多项时

96
00:03:14,080 --> 00:03:15,120
也存在运算量过大的问题

97
00:03:16,450 --> 00:03:17,540
当然  你也可以试试

98
00:03:17,770 --> 00:03:19,090
只包含上边这些二次项的子集

99
00:03:19,290 --> 00:03:20,950
例如 我们只考虑

100
00:03:21,050 --> 00:03:22,630
x1^2  x2^2

101
00:03:23,590 --> 00:03:25,180
x3^2直到

102
00:03:25,580 --> 00:03:27,750
x100^2 这些项

103
00:03:28,100 --> 00:03:29,500
这样就可以将二次项的数量大幅度减少

104
00:03:29,980 --> 00:03:31,720
减少到只有100个二次项

105
00:03:32,070 --> 00:03:33,850
但是由于

106
00:03:34,120 --> 00:03:35,950
忽略了太多相关项

107
00:03:36,100 --> 00:03:37,170
在处理类似左上角的数据时

108
00:03:37,290 --> 00:03:39,330
不可能得到理想的结果

109
00:03:39,570 --> 00:03:40,550
实际上

110
00:03:41,040 --> 00:03:42,720
如果只考虑x1的平方

111
00:03:43,170 --> 00:03:44,870
到x100的平方

112
00:03:45,350 --> 00:03:46,500
这一百个二次项

113
00:03:47,460 --> 00:03:48,530
那么你可能会

114
00:03:48,910 --> 00:03:50,210
拟合出一些特别的假设

115
00:03:50,330 --> 00:03:52,350
比如可能拟合出

116
00:03:52,490 --> 00:03:53,860
一个椭圆状的曲线

117
00:03:55,080 --> 00:03:56,240
但是肯定不能拟合出

118
00:03:56,340 --> 00:03:57,930
像左上角这个数据集的分界线

119
00:03:59,360 --> 00:04:00,530
所以5000个二次项看起来已经很多了

120
00:04:00,620 --> 00:04:03,090
而现在假设

121
00:04:03,230 --> 00:04:04,860
包括三次项

122
00:04:05,140 --> 00:04:06,100
或者三阶项

123
00:04:06,440 --> 00:04:08,050
例如x1 x2 x3

124
00:04:08,400 --> 00:04:09,800
x1^2

125
00:04:10,310 --> 00:04:12,240
x2 x10

126
00:04:12,900 --> 00:04:15,280
x11 x17等等

127
00:04:15,700 --> 00:04:18,110
类似的三次项有很多很多

128
00:04:19,040 --> 00:04:19,770
事实上

129
00:04:20,050 --> 00:04:21,260
三次项的个数是以n^3的量级增加

130
00:04:22,210 --> 00:04:23,830
当n=100时

131
00:04:24,150 --> 00:04:25,660
可以计算出来

132
00:04:25,740 --> 00:04:26,870
最后能得到

133
00:04:27,730 --> 00:04:29,650
大概17000个三次项

134
00:04:30,040 --> 00:04:31,670
所以

135
00:04:32,260 --> 00:04:34,470
当初始特征个数n增大时

136
00:04:34,920 --> 00:04:36,050
这些高阶多项式项数

137
00:04:36,230 --> 00:04:37,730
将以几何级数递增

138
00:04:38,530 --> 00:04:40,440
特征空间也随之急剧膨胀

139
00:04:41,070 --> 00:04:42,180
当特征个数n很大时

140
00:04:42,320 --> 00:04:43,320
如果找出附加项

141
00:04:43,560 --> 00:04:45,050
来建立一些分类器

142
00:04:45,240 --> 00:04:48,100
这并不是一个好做法

143
00:04:49,590 --> 00:04:52,560
对于许多实际的机器学习问题 特征个数n是很大的

144
00:04:53,270 --> 00:04:53,560
举个例子

145
00:04:55,000 --> 00:04:58,140
关于计算机视觉中的一个问题

146
00:04:59,670 --> 00:05:00,770
假设你想要

147
00:05:01,260 --> 00:05:02,620
使用机器学习算法

148
00:05:02,710 --> 00:05:04,610
来训练一个分类器

149
00:05:04,710 --> 00:05:05,880
使它检测一个图像

150
00:05:06,160 --> 00:05:08,030
来判断图像是否为一辆汽车

151
00:05:09,480 --> 00:05:11,900
很多人可能会好奇  这对计算机视觉来说有什么难的

152
00:05:12,390 --> 00:05:13,140
当我们自己看这幅图像时

153
00:05:13,270 --> 00:05:15,670
里面有什么是一目了然的事情

154
00:05:15,900 --> 00:05:17,000
你肯定会很奇怪

155
00:05:17,190 --> 00:05:18,320
为什么学习算法竟可能会不知道

156
00:05:18,910 --> 00:05:20,880
图像是什么

157
00:05:22,110 --> 00:05:23,330
为了解答这个疑问

158
00:05:23,720 --> 00:05:25,380
我们取出这幅图片中的一小部分

159
00:05:25,650 --> 00:05:26,690
将其放大

160
00:05:26,940 --> 00:05:28,180
比如图中

161
00:05:28,510 --> 00:05:30,240
这个红色方框内的部分

162
00:05:30,400 --> 00:05:31,330
结果表明

163
00:05:31,450 --> 00:05:34,270
当人眼看到一辆汽车时 计算机实际上看到的却是这个

164
00:05:34,780 --> 00:05:35,930
一个数据矩阵

165
00:05:36,600 --> 00:05:38,110
或像这种格网

166
00:05:38,580 --> 00:05:40,350
它们表示了像素强度值

167
00:05:40,610 --> 00:05:42,930
告诉我们图像中每个像素的亮度值

168
00:05:43,640 --> 00:05:45,170
因此 对于计算机视觉来说问题就变成了

169
00:05:45,550 --> 00:05:47,230
根据这个像素点亮度矩阵

170
00:05:47,310 --> 00:05:49,140
来告诉我们

171
00:05:49,410 --> 00:05:52,440
这些数值代表一个汽车门把手

172
00:05:54,230 --> 00:05:55,740
具体而言

173
00:05:56,030 --> 00:05:57,220
当用机器学习算法构造

174
00:05:57,430 --> 00:05:59,060
一个汽车识别器时

175
00:05:59,360 --> 00:06:00,510
我们要想出

176
00:06:00,800 --> 00:06:02,690
一个带标签的样本集

177
00:06:02,890 --> 00:06:04,250
其中一些样本

178
00:06:04,730 --> 00:06:05,850
是各类汽车

179
00:06:06,000 --> 00:06:07,150
另一部分样本

180
00:06:07,380 --> 00:06:08,780
是其他任何东西

181
00:06:09,090 --> 00:06:10,590
将这个样本集输入给学习算法

182
00:06:10,720 --> 00:06:12,230
以训练出一个分类器

183
00:06:12,310 --> 00:06:13,500
训练完毕后

184
00:06:13,680 --> 00:06:14,700
我们输入一幅新的图片

185
00:06:14,890 --> 00:06:16,710
让分类器判定  “这是什么东西？”

186
00:06:17,980 --> 00:06:20,030
理想情况下 分类器能识别出这是一辆汽车

187
00:06:21,410 --> 00:06:24,000
为了理解引入

188
00:06:24,120 --> 00:06:26,810
非线性分类器的必要性

189
00:06:27,050 --> 00:06:27,940
我们从学习算法的训练样本中

190
00:06:28,190 --> 00:06:29,360
挑出一些汽车图片

191
00:06:29,480 --> 00:06:31,780
和一些非汽车图片

192
00:06:32,960 --> 00:06:33,920
让我们从其中

193
00:06:34,090 --> 00:06:35,630
每幅图片中挑出一组像素点

194
00:06:35,750 --> 00:06:37,040
这是像素点1的位置

195
00:06:37,180 --> 00:06:39,500
这是像素点2的位置

196
00:06:39,730 --> 00:06:42,390
在坐标系中标出这幅汽车的位置

197
00:06:42,510 --> 00:06:44,010
在某一点上

198
00:06:44,360 --> 00:06:45,890
车的位置取决于

199
00:06:46,430 --> 00:06:47,870
像素点1和像素点2的亮度

200
00:06:49,260 --> 00:06:50,630
让我们用同样的方法标出其他图片中汽车的位置

201
00:06:51,060 --> 00:06:52,450
然后我们再举一个

202
00:06:52,980 --> 00:06:53,980
关于汽车的不同的例子

203
00:06:54,080 --> 00:06:55,010
观察这两个相同的像素位置

204
00:06:56,160 --> 00:06:57,570
这幅图片中

205
00:06:57,770 --> 00:06:58,970
像素1有一个像素强度

206
00:06:59,230 --> 00:07:00,660
像素2也有一个不同的像素强度

207
00:07:00,960 --> 00:07:02,940
所以在这幅图中它们两个处于不同的位置

208
00:07:03,360 --> 00:07:05,740
我们继续画上两个非汽车样本

209
00:07:05,990 --> 00:07:07,590
这个不是汽车

210
00:07:07,720 --> 00:07:09,470
这个也不是汽车

211
00:07:09,730 --> 00:07:10,910
然后我们继续

212
00:07:11,070 --> 00:07:12,720
在坐标系中画上更多的新样本

213
00:07:13,280 --> 00:07:14,680
用''+"表示汽车图片

214
00:07:15,080 --> 00:07:16,310
用“-”表示非汽车图片

215
00:07:16,890 --> 00:07:18,500
我们将发现

216
00:07:18,830 --> 00:07:20,680
汽车样本和非汽车样本

217
00:07:20,890 --> 00:07:22,430
分布在坐标系中的不同区域

218
00:07:22,570 --> 00:07:24,910
因此

219
00:07:25,180 --> 00:07:26,570
我们现在需要一个

220
00:07:26,750 --> 00:07:28,790
非线性分类器

221
00:07:29,000 --> 00:07:30,900
来尽量分开这两类样本

222
00:07:32,480 --> 00:07:34,300
这个分类问题中特征空间的维数是多少？

223
00:07:35,290 --> 00:07:38,210
假设我们用50*50像素的图片

224
00:07:38,770 --> 00:07:40,050
我们的图片已经很小了

225
00:07:40,520 --> 00:07:42,760
长宽只各有50个像素

226
00:07:43,470 --> 00:07:44,990
但这依然是2500个像素点

227
00:07:46,330 --> 00:07:47,650
因此

228
00:07:47,740 --> 00:07:49,310
我们的特征向量的元素数量

229
00:07:49,520 --> 00:07:51,450
N=2500

230
00:07:51,700 --> 00:07:52,910
特征向量X

231
00:07:53,180 --> 00:07:54,570
包含了所有像素点的亮度值

232
00:07:54,710 --> 00:07:56,690
这是像素点1的亮度

233
00:07:57,080 --> 00:07:58,030
这是像素点2的亮度

234
00:07:58,330 --> 00:07:59,580
如此类推

235
00:07:59,870 --> 00:08:01,310
直到最后一个

236
00:08:01,400 --> 00:08:03,420
像素点的亮度

237
00:08:03,590 --> 00:08:05,450
对于典型的计算机图片表示方法

238
00:08:05,540 --> 00:08:07,190
如果存储的是每个像素点的灰度值 (色彩的强烈程度)

239
00:08:07,480 --> 00:08:09,020
那么每个元素的值

240
00:08:09,230 --> 00:08:12,110
应该在0到255之间

241
00:08:12,520 --> 00:08:13,290
因此 这个问题中n=2500

242
00:08:13,950 --> 00:08:15,580
但是

243
00:08:15,740 --> 00:08:17,140
这只是使用灰度图片的情况

244
00:08:17,790 --> 00:08:18,800
如果我们用的是RGB彩色图像

245
00:08:19,440 --> 00:08:21,140
每个像素点包含红、绿、蓝三个子像素

246
00:08:21,420 --> 00:08:23,870
那么n=7500

247
00:08:27,650 --> 00:08:28,630
因此 如果我们非要

248
00:08:29,000 --> 00:08:29,920
通过包含所有的二次项

249
00:08:30,370 --> 00:08:32,020
来解决这个非线性问题

250
00:08:32,300 --> 00:08:33,710
那么

251
00:08:33,810 --> 00:08:34,680
这就是式子中的所有条件

252
00:08:35,430 --> 00:08:38,900
xi*xj

253
00:08:39,130 --> 00:08:40,370
连同开始的2500像素

254
00:08:40,580 --> 00:08:42,500
总共大约有300万个

255
00:08:43,050 --> 00:08:44,620
这数字大得有点离谱了

256
00:08:44,720 --> 00:08:46,430
对于每个样本来说

257
00:08:46,600 --> 00:08:48,680
要发现并表示

258
00:08:48,840 --> 00:08:50,070
所有这300万个项

259
00:08:50,310 --> 00:08:52,250
这计算成本太高了

260
00:08:55,470 --> 00:08:57,580
因此 只是简单的增加

261
00:08:58,100 --> 00:08:59,230
二次项或者三次项

262
00:08:59,300 --> 00:09:00,510
之类的逻辑回归算法

263
00:09:00,930 --> 00:09:01,910
并不是一个解决

264
00:09:01,980 --> 00:09:03,950
复杂非线性问题的好办法

265
00:09:04,550 --> 00:09:06,090
因为当n很大时

266
00:09:06,310 --> 00:09:08,410
将会产生非常多的特征项

267
00:09:09,370 --> 00:09:10,620
在接下来的视频课程中

268
00:09:10,840 --> 00:09:11,890
我将为大家讲解神经网络

269
00:09:12,080 --> 00:09:13,670
它在解决复杂的非线性分类问题上

270
00:09:13,980 --> 00:09:15,370
被证明是

271
00:09:15,650 --> 00:09:17,720
是一种好得多的算法

272
00:09:17,960 --> 00:09:19,780
即使你输入特征空间

273
00:09:20,070 --> 00:09:22,080
或输入的特征维数n很大也能轻松搞定

274
00:09:22,860 --> 00:09:24,080
在后面的课程中

275
00:09:24,420 --> 00:09:25,580
我将给大家展示

276
00:09:25,780 --> 00:09:26,730
一些有趣的视频

277
00:09:27,240 --> 00:09:29,030
视频中讲述了神经网络在历史上的重要应用

278
00:09:30,300 --> 00:09:31,290
我也希望

279
00:09:32,100 --> 00:09:33,480
这些我们即将看到的视频

280
00:09:33,570 --> 00:09:35,460
能给你的学习过程带来一些乐趣 ^ ^ 【教育无边界字幕组】翻译：伽罗 校对：sherry  审核：所羅門捷列夫