このビデオと、 これに続く一連のビデオで、 ニューラルネットワークと呼ばれる 学習アルゴリズムをやっていきたい。 最初にまず表現について 説明し、その後で、 その後の一連のビデオで それに関する学習アルゴリズムについて話す。 ニューラルネットワークは実は かなり古くからあるアイデアだ。 だがしばらくは下火だった。 だが今日では、 最先端の技術で、 たくさんの異なる機械学習の問題で使われている。 で、何故また別の学習アルゴリズムをやらなきゃいけないのか？ 既に線形回帰もあるし、 ロジスティック回帰もある。
では何故、ニューラルネットワークなんて物が必要なんだ？ ニューラルネットワークの議論を 興味深くする為に、 複雑で非線形の仮説を 学ぶ必要があるような 機械学習の問題の例を いくつか挙げてみよう。 こんなトレーニングセットを用いた、 教師有りの分類問題の学習を考えてみよう。 この問題にロジスティック回帰を 適用しよう、とすると、 たくさんの非線形のフィーチャーに対して ロジスティック回帰を 適用していく、というのが考えられる。 このように、gはいつも通り sigmoid関数で、 これらのようにたくさんの多項式の項を含められる。 そしてもし十分にたくさんの 多項式の項を入れれば、 陽性と陰性を分ける仮説を 得られるかもしれない。 このやり方は、 たとえばフィーチャーが2つだけ、 x1とx2、とかの場合はうまく行く。 何故ならその場合なら、x1とx2を含む 全ての多項式を 含むことが出来るからだ。 だが、興味が湧くような機械学習の問題の中には フィーチャーがたったの2つよりはずっと多い物も たくさん存在する。 ここまで家の価格の予測を 議論してきたが、ここで 回帰では無く 分類の問題、例えば 異なるフィーチャーの家があるとして、 その時に半年以内に その家が売れるオッズを 予測したい、というような問題が あるとすると、 それは分類問題と なる訳だ。 そしてそれは、前に見たように、 最終的にはとてもたくさんの フィーチャー、数百という 異なるフィーチャーを持つ家たちたりえる。 こんな問題の時は、 全ての二次の項を含めようなんて 考えたら、 これら全部は、 二次の項だけでも、 たくさんある。 それはx1の二乗の項とか、 x1x2、x1x3、x1x4という風に、 x1x100まで続く。そして、 x2 の二乗、x2x3、 などと続いていく。 そして二次の項だけを含めたとしても、 つまり、 これらの2つの 積の項、 つまりx1掛けるx1などで、 そしてnが 100の場合だと、 結局は約5000程のフィーチャーとなる。 そして漸近的に、 二次のフィーチャーの数は オーダーnの二乗で 成長する。 ここでnはもとのフィーチャーの数。 たとえばx1からx100まであった訳だが、 その場合は実際 nの二乗 割る 2 に近かった。 だから、すべての 2次式フィーチャーを含めるのは よいアイデアではなさそうだ。 なぜなら、たくさんのフィーチャーが あり、トレーニングセットを オーバーフィットする結果になるからだ。 また、そのような たくさんのフィーチャーを扱うと、 計算コストが高くなりうる。 一つできることは これらのサブセットのみを 入れることだ。例えば x1 の自乗、x2 の自乗、x3 の自乗 から x100 の自乗 までを入れる。すると フィーチャーの数はかなり少なくなる。 そのような 2次式フィーチャーが 100 個だけあるとして、 これは不十分なフィーチャーであり 左上のようなデータセットには まずフィットさせられないだろう。 実のところ、これらの 2次式フィーチャーと共に 元々の x1 等々から x100 までを入れるだけでも、 かなり興味深い仮説を フィットさせることは出来る。 例えば楕円の仮説、 例えばこんなのとかにはフィットさせる事が出来る。 だが、ここに示したような より複雑なデータにはフィットさせられない。 5000のフィーチャーと聞くと 凄いたくさんに感じるだろうが、 三乗の項を含めると つまり三次の多項式を含めると、 x1x2x3とか、 x1の二乗掛けるx2、 x10x11x17 などなど。 こんなフィーチャーがたくさんになるのは想像出来るだろう。 実際、オーダー三乗の フィーチャーは、 nが100の時は 計算してみると、 なんと17000もの 三次の項がある事になる。 だから元々の フィーチャーセットが大きいとき これらの高次の多項式フィーチャーを 含めることは、実に劇的に フィーチャー空間を膨張させる。 n が大きいときに 非線形の分類器を作るのに、 追加のフィーチャーを用意することは よい方法ではなさそうだ。 多くの機械学習問題にとって、n はかなり大きいだろう。 例を示そう。 コンピュータビジョンの問題について考える。 機械学習を使って 分類器を学習させたいとする。 画像を調べ、それが 車かどうかを 判定するものだ。 多くの人は、コンピュータビジョンがどうして難しいのかと疑問に思う。 例えばあなたや私が この写真を見ると、これが何であるかはとても明白だ。 どうして学習アルゴリズムが この写真が何であるかを 判定し損ねるだろうかと思うだろう。 コンピュータビジョンが難しい理由を 理解するために、 画像の小さな部分を 拡大してみよう。 この小さな赤い矩形の領域だ。 あなたや私が車を見るとき、 コンピュータは実はこのように見ている。 コンピュータが見ているのは ピクセル輝度値のマトリックスあるいは グリッドで、画像の 各ピクセルの明るさを表している。 つまりコンピュータビジョン問題は このようなピクセル輝度値の マトリックスを見て、 数値が車のドアハンドルを表しているとわかることだ。 具体的には、我々が 機械学習を使って車の検出器を 作るときに行うのは、 分類トレーニングセットを考えること、 例えばそれは、 車の分類例をいくつか含み、 そして 車ではない物の例をいくつか含む。 そしてその トレーニングセットを 学習アルゴリズムに与え 分類器を学習させる。 そしてテストする。新しい画像を 提示して「この新しいものは何？」と尋ね、 上手くいけば検出器はそれが車であると認識する。 どうして非線形の仮説が 必要かを理解するために、 我々が学習アルゴリズムに 与えるであろう車の画像や 車でない画像をいくつか見てみよう。 画像の中から一対の ピクセルを選んで、 ピクセル1 の座標、 ピクセル2 の座標とする。 そしてこの車を特定の位置に プロットする。 ピクセル1 とピクセル2 の 輝度に応じて。 他の画像についてもいくつかやってみよう。 車の別の例をとると、 同じ 2つの ピクセル座標を見て、 この画像は ピクセル1 が異なる輝度を持ち、 ピクセル2 も別の輝度を持っている。 そのため結局 図上の異なる位置に置かれる。 次に陰性の例もいくつか同様にプロットしてみよう。 これは車でない物、 これは車でない物。 そしてこれを よりたくさんの例について行ったとする。 プラスは車を表し マイナスは車でない物を表す。 最終的にわかるのは 車と車でない物が 空間内の異なる領域に分布 しているということで、 我々に必要なのは 2つのクラスを分離しようとする 何らかの非線形の仮説だ。 フィーチャー空間の次元は何だろうか？ 仮にたった 50x50 のピクセル画像を使うとしよう。 かなり小さな画像、 一辺が 50 ピクセルしかない画像を想定する。 すると 2500 ピクセルあることになるので、 フィーチャーサイズの 次元は n=2500、 ここでフィーチャーベクトル x は すべてのピクセル検査の リストだ。これは ピクセル1 の明るさ、 ピクセル2 の明るさ、 等々から 最後のピクセルの明るさ までのことで、 典型的なコンピュータ表現では 各々は、グレイスケール値の場合 例えば  0 から 255 までの 値となるだろう。 n=2500 は グレイスケール画像を 使う場合だ。 RGB 画像を使い 赤、緑、青を分ける 場合は、n=7500 となるだろう。 だからもし すべての 2次式フィーチャーを 取り込んで、非線形の 仮説を学習させようとしたら つまり、Xi かける Xj の形の すべての項を取り込み、 それが 2500 ピクセルあったら 結局合計で 300万ピクセルになる。 そしてこれは妥当な大きさから 程遠い。1つのトレーニング例 当たりに、これら 300万フィーチャーの すべてを見つけて、表現する 計算は非常に高くつくだろう。 だから、単純なロジスティック回帰に 2次や 3次のフィーチャーを 加えたもの - これは n が大きい時に 非線形の仮説を学習させるのには まったく向いていない。 フィーチャーが多くなりすぎるからだ。 以降のいくつかのビデオでは、 ニューラルネットワークについて 教えよう。これは複雑な 仮説、複雑な非線形の仮説を 学習させるのにずっとよい方法 だということがわかる。 フィーチャー空間、n が大きい時でも。 そしてついでに ニューラルネットワークの 歴史的に重要な応用についての おもしろい動画をいくつか お見せしよう。 あなたも後ほど見るこれらの動画を おもしろいと思ってくれるといいが。