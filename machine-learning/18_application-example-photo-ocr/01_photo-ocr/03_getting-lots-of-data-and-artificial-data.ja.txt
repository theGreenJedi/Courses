何度も見てきたが 高いパフォーマンスの 機械学習システムを得る もっとも信頼出来る方法の一つに、 低バイアスの学習アルゴリズムに 大量のトレーニングセットで訓練する、というのがある。 だが、そんな大量のトレーニングデータをどこから得たら良いだろうか？ 機械学習においては、 人工データ合成、と呼ばれる 魅力的なアイデアが考え出されている。
このアイデアは どの問題でも使えるという訳では無いし 特定の問題に適用する時にも なんらかの思索、イノベーション、そして洞察が必要となる事が多い。 だがもしこのアイデアが あなたの機械学習の問題に適用出来たら、 それはあなたの学習アルゴリズムに 膨大なトレーニングセットを与える 簡単な方法となる事がある。 人工データ合成は 2つのバリエーションから構成されている。 最初の主要なバリエーションは 本質的には無から データを作り出す、つまり新しいデータをスクラッチから作り出す、という物。 そして二番目は、 もし既に少量のラベルつきトレーニングセットを 持っていたら、 そのトレーニングセットをどうにか増幅する。 言い換えると少しのトレーニングセットから より大きなトレーニングセットに かえる。 このビデオでは両方のアイデアを見ていく。 人口データ合成のアイデアを 議論する為に Photo OCRパイプラインの中の 文字認識の部分を例にとろう。 入力の画像を取り、 その文字が何なのかを認識する。 外に出て多くのラベルづけされたデータセットを 収集してくると、 こんな感じとなる。 この具体例に関しては、正方形のアスペクト比を選んだ。 つまり正方形の画像パッチをとる。 そして目標は、画像のパッチを取り、 その画像パッチの真ん中にある 文字を認識する事だ。 シンプルにする為に、 これらの画像は カラー画像では無くグレースケール画像として扱う。 色を使ってもこの問題に関しては 大して変わらない事が分かっている。 さて、この画像パッチが与えられて、 これがTだ、と認識したい。 この画像パッチが与えられたら これはSだと認識したい。 この画像パッチを与えられたら、 これはIだと認識したい、などなど。 つまりこれら全て、 我らの生画像の手本に対し、 どうやったらもっと多くのトレーニングセットが得られるか？ 最近のコンピュータなら、普通 膨大なフォントのライブラリを持ってる物だ。 もしワープロのソフトを使ってるなら どのワープロを使っているかに応じて これらのフォントを全て 持ってるかもしれないし、 さらにもっと多くの物が既に内部に保存されているだろう。 そして実の所、いろいろなwebサイトには そこにもまた、膨大な フリーのフォントのライブラリがインターネット上にはある。 我らは様々な種類のフォント、それこそ 何百とか何千ものフォントをダウンロード出来る。 だからもっと多くのトレーニング手本が欲しければ 考えられる手としては一つには 別々のフォントから 文字を取り出して、 別々のランダムの背景に ペーストしていく、というのが考えられる。 つまりこれを取って、このCをランダムの背景にペーストする。 そうすれば、文字Cの 画像のトレーニングセットを 得る事が出来る。 いくらかの仕事を行えば、 本当っぽく見せる為の これらの行程は ちょっとした仕事ではあるが、 だがこの幾らかの仕事を行ったあとには、 こんな感じの合成されたトレーニングセットが得られる。 右側に示した各画像は全て実際に合成された画像だ。 前面にはフォント、例えば webからダウンロードしたランダムのフォントなどから 一文字か数文字を 背景画像の上に ペーストした物で その背景画像はそれぞれ別々のランダムの背景画像。 そしてそこに、ちょっとした ブラーの操作をしても良い --- 適当なアフィン変換で歪めたり、 アフィン変換というのはシアーしたり 拡大縮小したり、ちょっとだけ回転させたりといった 操作の事だ。それを行うと、 合成されたトレーニングセットが 出来上がり、それはここに示したような物となる。 そしてこの合成されたデータが 本物っぽく見えるようにするには 真面目に考えて 作業もしなくてはならない。 そして合成データを 作る所で手抜きの仕事をしてしまうと、 合成データはそんなにうまくは機能しなくなるだろう。 だがもし実際のデータと 驚くほど似たデータを合成する事が出来たら、 その合成されたデータを使う事で、 人工トレーニングセット合成による 実質的には無制限の トレーニング手本を供給出来る。 つまり、この合成データを使う事で ラベルデータの供給を 実質無制限に行う事が出来、 それを用いて 文字認識の教師あり学習のアルゴリズムを トレーニング出来る。 以上が人工データ合成の 例だ。そこでは、 基本的にはデータをスクラッチから作る、 スクラッチから全く新しい画像を生成する。 これとは別の、人工データ合成の良くあるアプローチとしては、 既に持っている 手本を持ってきて、 つまり本当の画像などの 本当の手本を持ってきて、 追加のデータを 作成して、トレーニングセットを 増幅する。 さてここに文字Aの画像がある、 これは実際の画像から取ってきた物だ、 合成された画像では無い。 そして格子上の線を 例示の為に重ねて表示している。 実際はこの線は無いよ、もちろん。 さて、ここで取れる手段として、 このここにあるアルファベットを取り、 この画像を取り、 人工的なたわみ、あるいは 人工的な歪みを画像に導入する。 つまり画像を取り、 そこから16個の新しい手本を 生成する。 つまりこうやって、 少量のラベル付きトレーニングセットを持ってきて それを一気に増幅して もっとたくさんの手本を、 これ全部を得る事が出来る。 繰り返しになるが、具体的なアプリケーションの為に これを行うには、 我らにとっての 合理的な歪みとはどんな物か、 トレーニングセットを増幅する 合理的な方法はどんな物か、について、 良く考えて、洞察を元に探す 必要がある。 この文字認識の 具体例に関しては、 これらのたわみを導入するのは、自然な選択に思える。 だが異なる 機械学習の問題に対しては、 別の種類の歪みの方がより良さそう、という事は十分ありえる。 全く異なる分野の例として、 音声認識の例を見てみよう。 音声認識とは、 オーディオクリップがあったとして、 オーディオクリップから、 その中になんという単語が喋られているかを 認識するように学習したい。 ラベル付きトレーニング手本の一つがどんな物か、見てみよう。 あなたは一つのラベル付き手本を 持ってるとして、それは誰かが 幾つかの特定の単語を喋ってる物だとしよう。 そのオーディオクリップを再生してみる。 0-1-2-3-4-5。 こう。誰かが 0から5まで数えているとする、 そしてその中で言われている単語が 何なのかを認識する為に 学習アルゴリズムを適用したいとする。 その時、どうやってデータセットを増幅するか？ うーん、一つ考えられるのは、 データセットに追加のオーディオ的な歪みを加えるという事。 ここでは背後の音を加えて 携帯電話の接続が悪い状態をシミュレートしてみよう。 ビープ音が聞こえても、 それは実際にオーディオトラックの一部だから スピーカーの故障じゃありません。
では再生しよう。 0-1-2-3-4-5。 この種のオーディオクリップも あなたは聞き取れて、 音を認識出来るのだから、 これもまた追加するに値する トレーニング手本に思える。
これはまた別の例で、うるさい背後の例だ。 0, 1, 2, 3, 4, 5。 車が横を通って、人々が 背後を歩いている。これはまた別のだ。 元のクリーンな オーディオクリップを取ってきて、 つまり誰かが クリアに言っている、0, 1, 2, 3, 4, 5という オーディオを取ってきて、そして自動的に これらの追加的なトレーニング手本を合成する事が出来、 かくして一つのトレーニング手本を 4つの別々のトレーニング手本に増幅出来る。 ではこの最後の例を 再生してみよう。同様に、 0、1、2、3、4、5。 つまり、一つのラベル付き手本を持ってくるだけで、 一つのラベルつき手本を 収集する労力を払うだけで、 0, 1, 2, 3, 4, 5と言っている手本を得るだけで、 歪みを追加して合成する事によって、 異なる背後の音を導入するだけで、 いまやこの一つの手本を たくさんの手本に掛け算する事が出来た、 そんなにたくさんの仕事をせずに。 単に自動でこれら異なる背後の音を クリーンな音声に足すだけで。 歪みを導入する事でデータを合成する事に関して 一言警告をしておく。 これを自分でやる時には、 あなたが導入する 歪みは、テストセットで見られそうな ノイズ音源や歪みを 代表しているべきだ。 つまり文字認識の例では、 この類の歪みは 実際にアリだと思われる物だ、 何故なら画像のAは Aっぽく見えるから。つまり 実際にテストセットでも見そうな 物だから。 これはAだと言える。そして 右上の画像は、 見る事もありえそうな画像と言える。 そしてオーディオに関しては、 以下のような悪条件でも会話を認識したい： 携帯の接続が悪かったり、 異なる種類の背後のノイズの中だったり。 だからオーディオの場合も、 我らが合成した手本は、 実際に分類したい、 実際に正しく認識したい種類の物を 本当に代表している。 逆に、無意味なノイズを データに付加するのは、 だいたいの場合にはたぶん役に立たないだろう。 これを見て気づくか分からないが、 これにやった事は、 画像を持ってきて、 各ピクセルに対し、 これら4つの画像のそれぞれに、 ランダムのガウス分布のノイズを各ピクセルに付加したのだ。 各ピクセルに対し、 ピクセルの明るさとかに、 各ピクセルにガウス分布のランダムのノイズを付加する。 つまり、それは完全に無意味なノイズだ。でしょ？ だから、テストセットにこの種の ピクセル全体に渡るノイズを 観測する場合があると想定出来る場合を除いては、 この種の純粋にランダムで 無意味なノイズは、あまり役に立たない。 しかし、人工データ合成の 過程においては、 ちょっとしたアートも 必要となるし、 時には試してみてうまく行くかを見てみるというのも必要だ。 だが、もしどの種の歪みを 加えるかを決めようとしているなら、 それ以外にどんな意味がありそうな 歪みがありそうかを 真面目に考える必要がある。 その歪みは少なくとも 幾らかはテストセットを 代表した画像を 生成する物の範囲で。 最後に、このビデオをまとめる為に 人工データ合成から たくさんのデータを得るというアイデアについて 2, 3言っておきたい 事がある。 毎度の事だが、人工的にトレーニングデータを 作り出す方法を編み出す為に たくさんの労力を払う前に、 本当に手持ちの分類器が 低バイアスか、そして より多くのトレーニングデータが本当に 役に立つのかを確認しておくのは 多くの場合で良い習慣だ。 そしてこれをやる標準的な方法は、 学習曲線をプロットして、 確かに低バイアスの 分類器を持っていて、 高バリアンスの分類器では無い事を確認する。 または、もし低バイアスの分類器を 持っていなければ、 もう一つ試す価値のある事としては、 分類器の持つフィーチャーの 数を増やしてみる、というのがある。 または、ニューラルネットワークの 隠れユニットの総数を増やしてみる、というのがある。 これを低バイアスの分類器になるまで 続けてみる。そしてそこまで行ってはじめて、 大量の人工トレーニングセットを 生成する事に 労力を投入すべきだ。 本当に避けなくてはいけない事態は まるまる一週間とか 何ヶ月も費やして とても良い人工合成されたデータセットを 作る方法を 発見した後で、 結局あなたの学習アルゴリズムの パフォーマンスは大量のトレーニングセットがあっても あまり改善しない、と判明する事だ。 以上がいつも通りのアドバイスである、 大量のデータを実際に あなたが有効活用出来るかの テストを、大量のトレーニングセットを 収集する努力を費やす前に行え、という事だ。 二番目。私が機械学習の問題の 仕事をしている時に、 一緒に働いているチームに しょっちゅう尋ねる質問は、しょっちゅう生徒に尋ねる質問は、 現在持ってるデータセットの10倍を得るのに どれだけの仕事が必要だろうか？という物がある。 私が新しい機械学習の適用の場に 直面した時には、とてもよく チームと共に座って まさにこの質問を尋ねる。 この問いを何度も何度も なーんども尋ねてきた。 そして私はこの問いの答えが 何度も以下のようであるかを知る事になり、しばしば驚く： 実際はそんなに大変じゃなくて、 せいぜい2〜3日の仕事で 現在持っているデータの 10倍のデータを得る事が出来て 機械学習のアプリケーションに 使う事が出来、 そしてしょっちゅう、 もし10倍のデータが得られたら、 あなたのアルゴリズムはずっと良くなる、という事が起こる。 だからもし 何らかの機械学習のアプリケーションの 仕事をしているチームに 参加する時には、 これは自問してみるのにとても良い問いであり、 チームに尋ねてみるのにとても良い問いだ。 そして数分のブレーンストーミングの後に 文字通り10倍のデータを 得る方法を 考え出したとしても、 それほど驚くべき事では無い。 その場合、あなたはたぶん そのチームのヒーローになるだろう。 何故なら10倍ものデータがあれば、 たぶんあなたは、本当にずっと良いパフォーマンスを 得る事になるだろうから、そんなにたくさんのデータから学習するだけで。 データを集めるには幾つかの方法があり、このビデオでは人工データ合成について議論してきた。 それは二つのアイデアから構成されている： 一つ目はランダムのフォントなどを使って データをスクラッチから作る方法で、 二つ目はすでに存在する 手本を取ってきて それに歪ませて、トレーニングセットを さらにたくさんに増幅する。 その他の手段としては、 自分でデータを収集して ラベルづけ していく、という物。 だから私が良くやる 有用な計算として、 ある数の手本を集めるのに 何分かかるか 何時間かかるか、を。 実際に座って、 考えてみる、 1つの手本をラベルづけするのに 10秒かかるとして、 我らのアプリケーションは 現在1000個のラベルづけされた 手本があるとしてみよう。 すると、それを 10倍すると mは1万となる。 二番目の データをたくさん集める方法は、 単にデータを収集して自分でラベルづけする事だった。 以上の意味する事は、 私は良く座って 良くこんな計算を行う。 どれだけの時間が、 何時間かかるか、 何日かかるか、 椅子に座って考えてみるのだ。 現在持っているデータの 10倍のデータを集めてきて、 自分たちの手でラベル付けして行ったら どれだけかかるのか、 椅子に座って考えてみるのだ。 例えば、 我らの機械学習のアプリケーションは現在、 1000個の手本があるとする、つまりm=1000。 そこで我らがやるべき事は、椅子に座って、 こう問うてみる事だ：一つのラベルつき手本を集めるのには、 実際どれだけの時間がかかるだろう？と。 例えばそれは、 10秒かかるとする、 新しいラベル付き手本1つを 得るのに。 つまり、10倍の数の手本を得たいと思えば、計算を行ってみると、 もし一つの手本に10秒かかり、 10倍の数の手本を得たいと思えば、 その場合は1万手本が必要となる。 そこで計算を行う。人力で 1万個のラベルつき手本を集めたら どれだけの時間がかかるか？を。 1手本につき10秒かかるとすると、 その場合にこの計算を行うと、 割とよく、あなたがたは 驚くことになる、 いかにちょっとの仕事で済むのか、 時にはほんの2、3日の仕事で、 またある時にはほんの数日で済む事を知る事で。 私は多くのチームが いかにちょっとの仕事で済むかを 知ってとても驚く事を、何度も見てきた。 単にもっと多くのデータを集めて、 そのデータを学習アプリに食わせるだけで、 膨大なパフォーマンスの向上が 望める。 そしてそれをあなたが 成し遂げたら、 あなたはどんな製品を開発していようと、 どんなチームで働いていようと、 きっとヒーローになる。 何故ならこれは、パフォーマンスを改善する 素晴らしい方法になりえるからだ。 三番目は、これが最後だが、 データをたくさん集めるのに 時には良い方法たりえる物として、 クラウドソーシングと呼ばれる物がある。 こんにちでは、幾つかのサービスで あなたがかなり安い価格で あたなの為にラベル付きトレーニングセットを たくさん集めてくれる人を 雇わせてくれるサービスが 存在している。 クラウドソーシング、あるいは データのラベル付のクラウドソースは、 いろいろと 複雑な 事があり、 そこでは、 信頼出来るラベル付けを行う人を得られるかもしれない。 世界中の何百、何千もの 比較的安くラベル付けを手伝ってくれる ラベル付けを行う人を 得るという手が使えるかもしれない。 私が既に言及したように、 そういう選択肢もまたありうる。 そしてたぶん、AmazonのMechanical Turkシステムが 現時点ではもっとも人気のある クラウドソースの選択肢だ。 これはしばしば、 ちゃんと機能させる為には かなりの作業を必要とする。 もし高いクオリティのラベルを得たいと思えば。 だがそれでも、これは検討するに値する 選択肢となる事もある。 もしラベル付けをあなたの為に 行ってくれるような大量の人を、比較的安価に web上で探したい時には。 このビデオでは、 人工データ合成の アイデアについて議論した。その中でも スクラッチからデータ全体を作る 方法とーーこの例としては ランダムフォントの例を見たがーー それか、既に存在しているトレーニングセットを 増幅する、という方法ーー 既存のラベル付きトレーニング手本を持ってきて そこに歪みを導入し、 そこから新しい手本を生成するーーを見た。 そして最後に、このビデオから あなたに覚えておいて 欲しい事としては、 もしあなたが機械学習の問題に直面していたら、 二つの事はしばしば試してみる価値がある。 一つは単純なサニティチェックを 学習曲線で行い、もっと多くのデータが役に立つかを確認する事。 二つ目は、もっと多くのデータが役に立つ場合には、 椅子に座って自分自身に真剣に こう問うてみる： 現在持っているデータの 10倍のデータを得るには、 どれだけ時間がかかるか、を。そしていつもでは無いにしても、 ときには、それがいかに簡単かが判明して 驚く事もある。 2〜3日とか、 2〜3週間とかの作業で十分だったり。 そしてそれはあなたの学習アルゴリズムのパフォーマンスを 猛烈に引き上げる素晴らしい方法になりうる。