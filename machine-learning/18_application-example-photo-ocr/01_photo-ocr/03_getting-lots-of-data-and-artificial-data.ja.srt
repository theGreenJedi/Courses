1
00:00:00,090 --> 00:00:01,270
何度も見てきたが

2
00:00:01,570 --> 00:00:03,160
高いパフォーマンスの

3
00:00:03,300 --> 00:00:04,800
機械学習システムを得る

4
00:00:05,040 --> 00:00:06,170
もっとも信頼出来る方法の一つに、

5
00:00:06,550 --> 00:00:07,860
低バイアスの学習アルゴリズムに

6
00:00:08,750 --> 00:00:10,220
大量のトレーニングセットで訓練する、というのがある。

7
00:00:11,230 --> 00:00:12,830
だが、そんな大量のトレーニングデータをどこから得たら良いだろうか？

8
00:00:13,510 --> 00:00:14,440
機械学習においては、

9
00:00:14,820 --> 00:00:16,520
人工データ合成、と呼ばれる

10
00:00:17,220 --> 00:00:19,000
魅力的なアイデアが考え出されている。
このアイデアは

11
00:00:19,370 --> 00:00:20,740
どの問題でも使えるという訳では無いし

12
00:00:20,980 --> 00:00:22,120
特定の問題に適用する時にも

13
00:00:22,360 --> 00:00:25,060
なんらかの思索、イノベーション、そして洞察が必要となる事が多い。

14
00:00:25,780 --> 00:00:27,170
だがもしこのアイデアが

15
00:00:27,580 --> 00:00:29,120
あなたの機械学習の問題に適用出来たら、

16
00:00:29,230 --> 00:00:30,270
それはあなたの学習アルゴリズムに

17
00:00:30,510 --> 00:00:31,600
膨大なトレーニングセットを与える

18
00:00:31,680 --> 00:00:33,470
簡単な方法となる事がある。

19
00:00:33,900 --> 00:00:35,520
人工データ合成は

20
00:00:36,230 --> 00:00:38,410
2つのバリエーションから構成されている。

21
00:00:38,590 --> 00:00:40,210
最初の主要なバリエーションは

22
00:00:40,650 --> 00:00:41,940
本質的には無から

23
00:00:42,520 --> 00:00:44,940
データを作り出す、つまり新しいデータをスクラッチから作り出す、という物。

24
00:00:45,380 --> 00:00:46,700
そして二番目は、

25
00:00:46,930 --> 00:00:48,350
もし既に少量のラベルつきトレーニングセットを

26
00:00:48,590 --> 00:00:49,970
持っていたら、

27
00:00:50,210 --> 00:00:51,490
そのトレーニングセットをどうにか増幅する。

28
00:00:51,840 --> 00:00:52,680
言い換えると少しのトレーニングセットから

29
00:00:52,980 --> 00:00:54,390
より大きなトレーニングセットに

30
00:00:54,660 --> 00:00:56,290
かえる。

31
00:00:56,450 --> 00:00:58,120
このビデオでは両方のアイデアを見ていく。

32
00:01:00,350 --> 00:01:02,220
人口データ合成のアイデアを

33
00:01:02,440 --> 00:01:04,030
議論する為に

34
00:01:04,330 --> 00:01:06,930
Photo OCRパイプラインの中の

35
00:01:07,090 --> 00:01:08,470
文字認識の部分を例にとろう。

36
00:01:08,690 --> 00:01:09,710
入力の画像を取り、

37
00:01:10,060 --> 00:01:11,370
その文字が何なのかを認識する。

38
00:01:13,330 --> 00:01:14,690
外に出て多くのラベルづけされたデータセットを

39
00:01:14,880 --> 00:01:16,270
収集してくると、

40
00:01:16,890 --> 00:01:17,980
こんな感じとなる。

41
00:01:18,580 --> 00:01:21,770
この具体例に関しては、正方形のアスペクト比を選んだ。

42
00:01:22,130 --> 00:01:23,250
つまり正方形の画像パッチをとる。

43
00:01:24,180 --> 00:01:25,110
そして目標は、画像のパッチを取り、

44
00:01:25,770 --> 00:01:27,420
その画像パッチの真ん中にある

45
00:01:27,530 --> 00:01:29,270
文字を認識する事だ。

46
00:01:31,090 --> 00:01:31,990
シンプルにする為に、

47
00:01:32,660 --> 00:01:33,740
これらの画像は

48
00:01:34,240 --> 00:01:36,380
カラー画像では無くグレースケール画像として扱う。

49
00:01:36,870 --> 00:01:38,550
色を使ってもこの問題に関しては

50
00:01:38,930 --> 00:01:41,180
大して変わらない事が分かっている。

51
00:01:42,190 --> 00:01:43,530
さて、この画像パッチが与えられて、

52
00:01:43,660 --> 00:01:44,890
これがTだ、と認識したい。

53
00:01:45,010 --> 00:01:46,230
この画像パッチが与えられたら

54
00:01:46,550 --> 00:01:47,920
これはSだと認識したい。

55
00:01:49,540 --> 00:01:50,740
この画像パッチを与えられたら、

56
00:01:50,850 --> 00:01:52,950
これはIだと認識したい、などなど。

57
00:01:54,110 --> 00:01:55,310
つまりこれら全て、

58
00:01:55,450 --> 00:01:57,240
我らの生画像の手本に対し、

59
00:01:57,380 --> 00:01:59,460
どうやったらもっと多くのトレーニングセットが得られるか？

60
00:02:00,000 --> 00:02:01,580
最近のコンピュータなら、普通

61
00:02:01,640 --> 00:02:03,700
膨大なフォントのライブラリを持ってる物だ。

62
00:02:03,890 --> 00:02:05,330
もしワープロのソフトを使ってるなら

63
00:02:05,950 --> 00:02:07,090
どのワープロを使っているかに応じて

64
00:02:07,240 --> 00:02:08,580
これらのフォントを全て

65
00:02:08,800 --> 00:02:09,980
持ってるかもしれないし、

66
00:02:10,120 --> 00:02:12,490
さらにもっと多くの物が既に内部に保存されているだろう。

67
00:02:12,950 --> 00:02:14,350
そして実の所、いろいろなwebサイトには

68
00:02:14,680 --> 00:02:16,280
そこにもまた、膨大な

69
00:02:16,690 --> 00:02:18,200
フリーのフォントのライブラリがインターネット上にはある。

70
00:02:18,370 --> 00:02:19,960
我らは様々な種類のフォント、それこそ

71
00:02:20,250 --> 00:02:22,580
何百とか何千ものフォントをダウンロード出来る。

72
00:02:23,960 --> 00:02:25,180
だからもっと多くのトレーニング手本が欲しければ

73
00:02:25,570 --> 00:02:27,020
考えられる手としては一つには

74
00:02:27,100 --> 00:02:28,340
別々のフォントから

75
00:02:28,870 --> 00:02:30,220
文字を取り出して、

76
00:02:31,240 --> 00:02:32,870
別々のランダムの背景に

77
00:02:33,290 --> 00:02:35,890
ペーストしていく、というのが考えられる。

78
00:02:36,730 --> 00:02:39,500
つまりこれを取って、このCをランダムの背景にペーストする。

79
00:02:40,680 --> 00:02:41,640
そうすれば、文字Cの

80
00:02:42,060 --> 00:02:43,830
画像のトレーニングセットを

81
00:02:44,080 --> 00:02:45,260
得る事が出来る。

82
00:02:46,360 --> 00:02:47,500
いくらかの仕事を行えば、

83
00:02:47,570 --> 00:02:48,920
本当っぽく見せる為の

84
00:02:48,980 --> 00:02:49,710
これらの行程は

85
00:02:49,830 --> 00:02:51,760
ちょっとした仕事ではあるが、

86
00:02:52,180 --> 00:02:53,080
だがこの幾らかの仕事を行ったあとには、

87
00:02:53,700 --> 00:02:56,130
こんな感じの合成されたトレーニングセットが得られる。

88
00:02:57,180 --> 00:02:59,910
右側に示した各画像は全て実際に合成された画像だ。

89
00:03:00,360 --> 00:03:02,080
前面にはフォント、例えば

90
00:03:02,810 --> 00:03:04,240
webからダウンロードしたランダムのフォントなどから

91
00:03:04,400 --> 00:03:05,680
一文字か数文字を

92
00:03:06,160 --> 00:03:07,320
背景画像の上に

93
00:03:07,800 --> 00:03:08,870
ペーストした物で

94
00:03:09,570 --> 00:03:11,440
その背景画像はそれぞれ別々のランダムの背景画像。

95
00:03:12,140 --> 00:03:12,840
そしてそこに、ちょっとした

96
00:03:13,540 --> 00:03:15,160
ブラーの操作をしても良い ---

97
00:03:15,680 --> 00:03:17,380
適当なアフィン変換で歪めたり、

98
00:03:17,620 --> 00:03:18,650
アフィン変換というのはシアーしたり

99
00:03:19,350 --> 00:03:20,740
拡大縮小したり、ちょっとだけ回転させたりといった

100
00:03:21,000 --> 00:03:22,260
操作の事だ。それを行うと、

101
00:03:22,370 --> 00:03:23,330
合成されたトレーニングセットが

102
00:03:23,580 --> 00:03:25,520
出来上がり、それはここに示したような物となる。

103
00:03:26,510 --> 00:03:28,050
そしてこの合成されたデータが

104
00:03:28,530 --> 00:03:29,640
本物っぽく見えるようにするには

105
00:03:29,970 --> 00:03:31,460
真面目に考えて

106
00:03:31,700 --> 00:03:33,250
作業もしなくてはならない。

107
00:03:34,020 --> 00:03:34,710
そして合成データを

108
00:03:35,120 --> 00:03:36,200
作る所で手抜きの仕事をしてしまうと、

109
00:03:36,250 --> 00:03:38,910
合成データはそんなにうまくは機能しなくなるだろう。

110
00:03:39,620 --> 00:03:40,600
だがもし実際のデータと

111
00:03:40,790 --> 00:03:43,940
驚くほど似たデータを合成する事が出来たら、

112
00:03:45,120 --> 00:03:46,850
その合成されたデータを使う事で、

113
00:03:47,340 --> 00:03:48,550
人工トレーニングセット合成による

114
00:03:48,990 --> 00:03:50,970
実質的には無制限の

115
00:03:51,310 --> 00:03:53,060
トレーニング手本を供給出来る。

116
00:03:53,150 --> 00:03:54,110
つまり、この合成データを使う事で

117
00:03:54,330 --> 00:03:55,820
ラベルデータの供給を

118
00:03:56,150 --> 00:03:58,100
実質無制限に行う事が出来、

119
00:03:58,560 --> 00:04:00,000
それを用いて

120
00:04:00,140 --> 00:04:01,610
文字認識の教師あり学習のアルゴリズムを

121
00:04:02,300 --> 00:04:03,990
トレーニング出来る。

122
00:04:05,120 --> 00:04:06,540
以上が人工データ合成の

123
00:04:07,000 --> 00:04:08,500
例だ。そこでは、

124
00:04:09,040 --> 00:04:10,870
基本的にはデータをスクラッチから作る、

125
00:04:11,080 --> 00:04:13,780
スクラッチから全く新しい画像を生成する。

126
00:04:14,880 --> 00:04:16,450
これとは別の、人工データ合成の良くあるアプローチとしては、

127
00:04:16,710 --> 00:04:18,210
既に持っている

128
00:04:18,370 --> 00:04:19,610
手本を持ってきて、

129
00:04:19,740 --> 00:04:20,780
つまり本当の画像などの

130
00:04:21,020 --> 00:04:22,430
本当の手本を持ってきて、

131
00:04:22,700 --> 00:04:24,130
追加のデータを

132
00:04:24,770 --> 00:04:26,130
作成して、トレーニングセットを

133
00:04:26,380 --> 00:04:27,900
増幅する。

134
00:04:28,070 --> 00:04:28,810
さてここに文字Aの画像がある、

135
00:04:28,910 --> 00:04:30,490
これは実際の画像から取ってきた物だ、

136
00:04:31,410 --> 00:04:32,550
合成された画像では無い。

137
00:04:32,630 --> 00:04:33,790
そして格子上の線を

138
00:04:33,880 --> 00:04:35,750
例示の為に重ねて表示している。

139
00:04:36,430 --> 00:04:36,880
実際はこの線は無いよ、もちろん。

140
00:04:36,970 --> 00:04:39,030
さて、ここで取れる手段として、

141
00:04:39,100 --> 00:04:40,110
このここにあるアルファベットを取り、

142
00:04:40,480 --> 00:04:41,500
この画像を取り、

143
00:04:42,240 --> 00:04:43,760
人工的なたわみ、あるいは

144
00:04:44,290 --> 00:04:45,810
人工的な歪みを画像に導入する。

145
00:04:46,040 --> 00:04:47,030
つまり画像を取り、

146
00:04:47,220 --> 00:04:48,240
そこから16個の新しい手本を

147
00:04:48,430 --> 00:04:50,060
生成する。

148
00:04:51,110 --> 00:04:52,000
つまりこうやって、

149
00:04:52,450 --> 00:04:53,740
少量のラベル付きトレーニングセットを持ってきて

150
00:04:54,090 --> 00:04:55,360
それを一気に増幅して

151
00:04:56,180 --> 00:04:57,190
もっとたくさんの手本を、

152
00:04:57,300 --> 00:05:00,020
これ全部を得る事が出来る。

153
00:05:01,210 --> 00:05:02,360
繰り返しになるが、具体的なアプリケーションの為に

154
00:05:02,560 --> 00:05:03,940
これを行うには、

155
00:05:04,120 --> 00:05:05,060
我らにとっての

156
00:05:05,140 --> 00:05:06,270
合理的な歪みとはどんな物か、

157
00:05:06,430 --> 00:05:07,840
トレーニングセットを増幅する

158
00:05:08,420 --> 00:05:09,460
合理的な方法はどんな物か、について、

159
00:05:09,720 --> 00:05:11,000
良く考えて、洞察を元に探す

160
00:05:11,470 --> 00:05:12,760
必要がある。

161
00:05:13,070 --> 00:05:15,130
この文字認識の

162
00:05:15,260 --> 00:05:17,170
具体例に関しては、

163
00:05:17,480 --> 00:05:18,310
これらのたわみを導入するのは、自然な選択に思える。

164
00:05:18,780 --> 00:05:19,910
だが異なる

165
00:05:20,090 --> 00:05:21,970
機械学習の問題に対しては、

166
00:05:22,080 --> 00:05:24,180
別の種類の歪みの方がより良さそう、という事は十分ありえる。

167
00:05:24,860 --> 00:05:25,600
全く異なる分野の例として、

168
00:05:26,180 --> 00:05:28,750
音声認識の例を見てみよう。

169
00:05:30,230 --> 00:05:31,480
音声認識とは、

170
00:05:31,580 --> 00:05:33,450
オーディオクリップがあったとして、

171
00:05:33,600 --> 00:05:35,010
オーディオクリップから、

172
00:05:35,350 --> 00:05:37,240
その中になんという単語が喋られているかを

173
00:05:37,460 --> 00:05:38,780
認識するように学習したい。

174
00:05:39,510 --> 00:05:41,340
ラベル付きトレーニング手本の一つがどんな物か、見てみよう。

175
00:05:42,290 --> 00:05:43,190
あなたは一つのラベル付き手本を

176
00:05:43,400 --> 00:05:45,000
持ってるとして、それは誰かが

177
00:05:45,330 --> 00:05:46,660
幾つかの特定の単語を喋ってる物だとしよう。

178
00:05:46,860 --> 00:05:48,720
そのオーディオクリップを再生してみる。

179
00:05:49,150 --> 00:05:51,230
0-1-2-3-4-5。

180
00:05:51,570 --> 00:05:53,810
こう。誰かが

181
00:05:54,220 --> 00:05:55,110
0から5まで数えているとする、

182
00:05:55,450 --> 00:05:57,180
そしてその中で言われている単語が

183
00:05:57,290 --> 00:05:58,460
何なのかを認識する為に

184
00:05:59,380 --> 00:06:01,320
学習アルゴリズムを適用したいとする。

185
00:06:02,040 --> 00:06:04,030
その時、どうやってデータセットを増幅するか？

186
00:06:04,390 --> 00:06:05,340
うーん、一つ考えられるのは、

187
00:06:06,020 --> 00:06:09,180
データセットに追加のオーディオ的な歪みを加えるという事。

188
00:06:09,970 --> 00:06:10,960
ここでは背後の音を加えて

189
00:06:11,640 --> 00:06:14,700
携帯電話の接続が悪い状態をシミュレートしてみよう。

190
00:06:15,360 --> 00:06:16,800
ビープ音が聞こえても、

191
00:06:16,980 --> 00:06:17,710
それは実際にオーディオトラックの一部だから

192
00:06:17,740 --> 00:06:20,350
スピーカーの故障じゃありません。
では再生しよう。

193
00:06:20,580 --> 00:06:20,580
0-1-2-3-4-5。

194
00:06:21,380 --> 00:06:22,260
この種のオーディオクリップも

195
00:06:22,640 --> 00:06:24,890
あなたは聞き取れて、

196
00:06:25,720 --> 00:06:28,600
音を認識出来るのだから、

197
00:06:28,960 --> 00:06:30,800
これもまた追加するに値する

198
00:06:31,370 --> 00:06:33,230
トレーニング手本に思える。
これはまた別の例で、うるさい背後の例だ。

199
00:06:34,890 --> 00:06:36,870
0, 1, 2, 3,

200
00:06:37,560 --> 00:06:39,060
4, 5。

201
00:06:39,090 --> 00:06:40,280
車が横を通って、人々が

202
00:06:40,580 --> 00:06:42,200
背後を歩いている。これはまた別のだ。

203
00:06:42,450 --> 00:06:43,880
元のクリーンな

204
00:06:44,430 --> 00:06:45,980
オーディオクリップを取ってきて、

205
00:06:46,090 --> 00:06:47,810
つまり誰かが

206
00:06:47,990 --> 00:06:48,960
クリアに言っている、0, 1, 2, 3, 4, 5という

207
00:06:49,090 --> 00:06:50,490
オーディオを取ってきて、そして自動的に

208
00:06:51,790 --> 00:06:54,090
これらの追加的なトレーニング手本を合成する事が出来、

209
00:06:54,470 --> 00:06:55,850
かくして一つのトレーニング手本を

210
00:06:56,410 --> 00:06:57,860
4つの別々のトレーニング手本に増幅出来る。

211
00:07:00,110 --> 00:07:00,940
ではこの最後の例を

212
00:07:01,300 --> 00:07:03,180
再生してみよう。同様に、

213
00:07:03,340 --> 00:07:07,180
0、1、2、3、4、5。

214
00:07:07,530 --> 00:07:08,510
つまり、一つのラベル付き手本を持ってくるだけで、

215
00:07:09,000 --> 00:07:10,260
一つのラベルつき手本を

216
00:07:10,360 --> 00:07:11,760
収集する労力を払うだけで、

217
00:07:11,950 --> 00:07:13,270
0, 1, 2, 3, 4, 5と言っている手本を得るだけで、

218
00:07:14,140 --> 00:07:16,520
歪みを追加して合成する事によって、

219
00:07:17,290 --> 00:07:18,560
異なる背後の音を導入するだけで、

220
00:07:19,000 --> 00:07:20,240
いまやこの一つの手本を

221
00:07:20,370 --> 00:07:21,810
たくさんの手本に掛け算する事が出来た、

222
00:07:23,420 --> 00:07:24,480
そんなにたくさんの仕事をせずに。

223
00:07:25,270 --> 00:07:27,090
単に自動でこれら異なる背後の音を

224
00:07:27,680 --> 00:07:30,510
クリーンな音声に足すだけで。

225
00:07:30,740 --> 00:07:31,980
歪みを導入する事でデータを合成する事に関して

226
00:07:33,190 --> 00:07:35,220
一言警告をしておく。

227
00:07:35,310 --> 00:07:36,630
これを自分でやる時には、

228
00:07:36,810 --> 00:07:38,580
あなたが導入する

229
00:07:39,020 --> 00:07:40,300
歪みは、テストセットで見られそうな

230
00:07:40,660 --> 00:07:42,010
ノイズ音源や歪みを

231
00:07:42,110 --> 00:07:43,680
代表しているべきだ。

232
00:07:44,010 --> 00:07:45,350
つまり文字認識の例では、

233
00:07:45,930 --> 00:07:47,230
この類の歪みは

234
00:07:47,440 --> 00:07:48,620
実際にアリだと思われる物だ、

235
00:07:48,770 --> 00:07:49,980
何故なら画像のAは

236
00:07:50,340 --> 00:07:51,510
Aっぽく見えるから。つまり

237
00:07:52,000 --> 00:07:53,020
実際にテストセットでも見そうな

238
00:07:53,210 --> 00:07:55,170
物だから。

239
00:07:55,370 --> 00:07:57,180
これはAだと言える。そして

240
00:07:57,380 --> 00:08:00,200
右上の画像は、

241
00:08:00,350 --> 00:08:01,800
見る事もありえそうな画像と言える。

242
00:08:03,280 --> 00:08:04,570
そしてオーディオに関しては、

243
00:08:04,740 --> 00:08:06,560
以下のような悪条件でも会話を認識したい：

244
00:08:06,970 --> 00:08:07,990
携帯の接続が悪かったり、

245
00:08:08,480 --> 00:08:09,440
異なる種類の背後のノイズの中だったり。

246
00:08:09,590 --> 00:08:10,920
だからオーディオの場合も、

247
00:08:11,230 --> 00:08:12,800
我らが合成した手本は、

248
00:08:13,530 --> 00:08:14,770
実際に分類したい、

249
00:08:14,850 --> 00:08:15,830
実際に正しく認識したい種類の物を

250
00:08:15,990 --> 00:08:17,360
本当に代表している。

251
00:08:18,770 --> 00:08:20,660
逆に、無意味なノイズを

252
00:08:20,770 --> 00:08:21,940
データに付加するのは、

253
00:08:22,170 --> 00:08:23,760
だいたいの場合にはたぶん役に立たないだろう。

254
00:08:24,420 --> 00:08:25,170
これを見て気づくか分からないが、

255
00:08:25,440 --> 00:08:26,400
これにやった事は、

256
00:08:26,620 --> 00:08:28,050
画像を持ってきて、

257
00:08:28,210 --> 00:08:29,540
各ピクセルに対し、

258
00:08:29,720 --> 00:08:30,710
これら4つの画像のそれぞれに、

259
00:08:30,990 --> 00:08:32,970
ランダムのガウス分布のノイズを各ピクセルに付加したのだ。

260
00:08:33,240 --> 00:08:34,690
各ピクセルに対し、

261
00:08:35,060 --> 00:08:36,370
ピクセルの明るさとかに、

262
00:08:36,500 --> 00:08:38,880
各ピクセルにガウス分布のランダムのノイズを付加する。

263
00:08:39,360 --> 00:08:40,940
つまり、それは完全に無意味なノイズだ。でしょ？

264
00:08:41,650 --> 00:08:43,280
だから、テストセットにこの種の

265
00:08:43,800 --> 00:08:45,510
ピクセル全体に渡るノイズを

266
00:08:45,910 --> 00:08:46,830
観測する場合があると想定出来る場合を除いては、

267
00:08:46,910 --> 00:08:48,190
この種の純粋にランダムで

268
00:08:48,660 --> 00:08:51,540
無意味なノイズは、あまり役に立たない。

269
00:08:52,880 --> 00:08:53,750
しかし、人工データ合成の

270
00:08:54,250 --> 00:08:55,570
過程においては、

271
00:08:55,640 --> 00:08:56,660
ちょっとしたアートも

272
00:08:56,710 --> 00:08:57,850
必要となるし、

273
00:08:58,140 --> 00:09:00,250
時には試してみてうまく行くかを見てみるというのも必要だ。

274
00:09:01,280 --> 00:09:02,060
だが、もしどの種の歪みを

275
00:09:02,140 --> 00:09:03,170
加えるかを決めようとしているなら、

276
00:09:03,870 --> 00:09:04,720
それ以外にどんな意味がありそうな

277
00:09:04,820 --> 00:09:06,260
歪みがありそうかを

278
00:09:06,670 --> 00:09:08,180
真面目に考える必要がある。

279
00:09:08,660 --> 00:09:09,720
その歪みは少なくとも

280
00:09:10,110 --> 00:09:11,370
幾らかはテストセットを

281
00:09:11,880 --> 00:09:13,410
代表した画像を

282
00:09:13,480 --> 00:09:15,830
生成する物の範囲で。

283
00:09:18,100 --> 00:09:19,000
最後に、このビデオをまとめる為に

284
00:09:19,150 --> 00:09:19,920
人工データ合成から

285
00:09:20,140 --> 00:09:21,420
たくさんのデータを得るというアイデアについて

286
00:09:21,790 --> 00:09:23,360
2, 3言っておきたい

287
00:09:23,600 --> 00:09:25,610
事がある。

288
00:09:26,920 --> 00:09:28,780
毎度の事だが、人工的にトレーニングデータを

289
00:09:29,170 --> 00:09:30,280
作り出す方法を編み出す為に

290
00:09:30,450 --> 00:09:32,020
たくさんの労力を払う前に、

291
00:09:33,060 --> 00:09:34,140
本当に手持ちの分類器が

292
00:09:34,220 --> 00:09:35,310
低バイアスか、そして

293
00:09:35,650 --> 00:09:36,540
より多くのトレーニングデータが本当に

294
00:09:36,920 --> 00:09:38,350
役に立つのかを確認しておくのは

295
00:09:38,460 --> 00:09:40,320
多くの場合で良い習慣だ。

296
00:09:41,010 --> 00:09:41,840
そしてこれをやる標準的な方法は、

297
00:09:41,970 --> 00:09:42,810
学習曲線をプロットして、

298
00:09:43,030 --> 00:09:43,970
確かに低バイアスの

299
00:09:44,130 --> 00:09:44,920
分類器を持っていて、

300
00:09:45,000 --> 00:09:47,470
高バリアンスの分類器では無い事を確認する。

301
00:09:47,760 --> 00:09:48,650
または、もし低バイアスの分類器を

302
00:09:48,720 --> 00:09:50,090
持っていなければ、

303
00:09:50,160 --> 00:09:51,040
もう一つ試す価値のある事としては、

304
00:09:51,450 --> 00:09:53,270
分類器の持つフィーチャーの

305
00:09:53,540 --> 00:09:54,440
数を増やしてみる、というのがある。

306
00:09:54,600 --> 00:09:55,650
または、ニューラルネットワークの

307
00:09:55,740 --> 00:09:56,710
隠れユニットの総数を増やしてみる、というのがある。

308
00:09:57,180 --> 00:09:58,470
これを低バイアスの分類器になるまで

309
00:09:58,540 --> 00:10:00,000
続けてみる。そしてそこまで行ってはじめて、

310
00:10:00,310 --> 00:10:01,820
大量の人工トレーニングセットを

311
00:10:02,040 --> 00:10:04,020
生成する事に

312
00:10:04,260 --> 00:10:05,760
労力を投入すべきだ。

313
00:10:05,860 --> 00:10:06,660
本当に避けなくてはいけない事態は

314
00:10:06,870 --> 00:10:07,930
まるまる一週間とか

315
00:10:08,110 --> 00:10:08,890
何ヶ月も費やして

316
00:10:09,090 --> 00:10:10,370
とても良い人工合成されたデータセットを

317
00:10:10,540 --> 00:10:11,720
作る方法を

318
00:10:12,450 --> 00:10:13,260
発見した後で、

319
00:10:13,820 --> 00:10:15,520
結局あなたの学習アルゴリズムの

320
00:10:15,760 --> 00:10:17,410
パフォーマンスは大量のトレーニングセットがあっても

321
00:10:18,030 --> 00:10:20,730
あまり改善しない、と判明する事だ。

322
00:10:22,190 --> 00:10:23,060
以上がいつも通りのアドバイスである、

323
00:10:23,420 --> 00:10:24,690
大量のデータを実際に

324
00:10:25,030 --> 00:10:26,290
あなたが有効活用出来るかの

325
00:10:26,530 --> 00:10:27,760
テストを、大量のトレーニングセットを

326
00:10:28,080 --> 00:10:30,530
収集する努力を費やす前に行え、という事だ。

327
00:10:31,960 --> 00:10:33,280
二番目。私が機械学習の問題の

328
00:10:33,590 --> 00:10:35,250
仕事をしている時に、

329
00:10:35,690 --> 00:10:37,520
一緒に働いているチームに

330
00:10:37,880 --> 00:10:39,210
しょっちゅう尋ねる質問は、しょっちゅう生徒に尋ねる質問は、

331
00:10:39,430 --> 00:10:40,550
現在持ってるデータセットの10倍を得るのに

332
00:10:40,620 --> 00:10:42,810
どれだけの仕事が必要だろうか？という物がある。

333
00:10:46,720 --> 00:10:47,850
私が新しい機械学習の適用の場に

334
00:10:48,200 --> 00:10:49,760
直面した時には、とてもよく

335
00:10:49,980 --> 00:10:50,940
チームと共に座って

336
00:10:51,210 --> 00:10:52,440
まさにこの質問を尋ねる。

337
00:10:52,920 --> 00:10:53,870
この問いを何度も何度も

338
00:10:53,970 --> 00:10:55,870
なーんども尋ねてきた。

339
00:10:56,000 --> 00:10:57,540
そして私はこの問いの答えが

340
00:10:58,390 --> 00:10:59,660
何度も以下のようであるかを知る事になり、しばしば驚く：

341
00:11:00,010 --> 00:11:01,070
実際はそんなに大変じゃなくて、

342
00:11:01,680 --> 00:11:02,670
せいぜい2〜3日の仕事で

343
00:11:02,930 --> 00:11:03,930
現在持っているデータの

344
00:11:04,250 --> 00:11:05,300
10倍のデータを得る事が出来て

345
00:11:05,450 --> 00:11:06,650
機械学習のアプリケーションに

346
00:11:06,810 --> 00:11:08,820
使う事が出来、

347
00:11:09,080 --> 00:11:09,830
そしてしょっちゅう、

348
00:11:09,950 --> 00:11:11,030
もし10倍のデータが得られたら、

349
00:11:11,270 --> 00:11:13,680
あなたのアルゴリズムはずっと良くなる、という事が起こる。

350
00:11:14,060 --> 00:11:15,040
だからもし

351
00:11:15,260 --> 00:11:16,510
何らかの機械学習のアプリケーションの

352
00:11:17,820 --> 00:11:18,880
仕事をしているチームに

353
00:11:19,110 --> 00:11:20,430
参加する時には、

354
00:11:20,550 --> 00:11:21,710
これは自問してみるのにとても良い問いであり、

355
00:11:22,290 --> 00:11:23,500
チームに尋ねてみるのにとても良い問いだ。

356
00:11:23,650 --> 00:11:25,120
そして数分のブレーンストーミングの後に

357
00:11:25,240 --> 00:11:26,530
文字通り10倍のデータを

358
00:11:26,650 --> 00:11:27,520
得る方法を

359
00:11:27,660 --> 00:11:28,950
考え出したとしても、

360
00:11:29,200 --> 00:11:30,250
それほど驚くべき事では無い。

361
00:11:30,380 --> 00:11:31,320
その場合、あなたはたぶん

362
00:11:31,430 --> 00:11:32,330
そのチームのヒーローになるだろう。

363
00:11:32,940 --> 00:11:34,000
何故なら10倍ものデータがあれば、

364
00:11:34,240 --> 00:11:35,360
たぶんあなたは、本当にずっと良いパフォーマンスを

365
00:11:35,450 --> 00:11:38,460
得る事になるだろうから、そんなにたくさんのデータから学習するだけで。

366
00:11:39,650 --> 00:11:44,500
データを集めるには幾つかの方法があり、このビデオでは人工データ合成について議論してきた。

367
00:11:47,450 --> 00:11:48,510
それは二つのアイデアから構成されている：

368
00:11:48,970 --> 00:11:50,440
一つ目はランダムのフォントなどを使って

369
00:11:50,640 --> 00:11:53,050
データをスクラッチから作る方法で、

370
00:11:53,570 --> 00:11:54,430
二つ目はすでに存在する

371
00:11:54,840 --> 00:11:56,600
手本を取ってきて

372
00:11:56,670 --> 00:11:58,100
それに歪ませて、トレーニングセットを

373
00:11:58,280 --> 00:12:00,910
さらにたくさんに増幅する。

374
00:12:01,090 --> 00:12:02,150
その他の手段としては、

375
00:12:02,280 --> 00:12:03,130
自分でデータを収集して

376
00:12:03,270 --> 00:12:04,610
ラベルづけ

377
00:12:04,670 --> 00:12:06,600
していく、という物。

378
00:12:07,600 --> 00:12:09,090
だから私が良くやる

379
00:12:09,210 --> 00:12:11,580
有用な計算として、

380
00:12:11,780 --> 00:12:13,320
ある数の手本を集めるのに

381
00:12:13,520 --> 00:12:15,140
何分かかるか

382
00:12:15,350 --> 00:12:16,420
何時間かかるか、を。

383
00:12:16,610 --> 00:12:17,780
実際に座って、

384
00:12:17,900 --> 00:12:19,410
考えてみる、

385
00:12:19,550 --> 00:12:21,830
1つの手本をラベルづけするのに

386
00:12:22,060 --> 00:12:23,990
10秒かかるとして、

387
00:12:24,120 --> 00:12:25,820
我らのアプリケーションは

388
00:12:26,190 --> 00:12:29,050
現在1000個のラベルづけされた

389
00:12:29,190 --> 00:12:31,500
手本があるとしてみよう。

390
00:12:31,620 --> 00:12:32,730
すると、それを

391
00:12:32,860 --> 00:12:34,090
10倍すると

392
00:12:34,200 --> 00:12:35,940
mは1万となる。

393
00:12:37,440 --> 00:12:40,260
二番目の

394
00:12:40,400 --> 00:12:41,530
データをたくさん集める方法は、

395
00:12:41,800 --> 00:12:43,540
単にデータを収集して自分でラベルづけする事だった。

396
00:12:44,510 --> 00:12:45,380
以上の意味する事は、

397
00:12:45,690 --> 00:12:46,970
私は良く座って

398
00:12:47,240 --> 00:12:48,570
良くこんな計算を行う。

399
00:12:48,950 --> 00:12:50,190
どれだけの時間が、

400
00:12:50,350 --> 00:12:51,140
何時間かかるか、

401
00:12:52,640 --> 00:12:54,000
何日かかるか、

402
00:12:54,200 --> 00:12:55,130
椅子に座って考えてみるのだ。

403
00:12:55,230 --> 00:12:56,890
現在持っているデータの

404
00:12:57,020 --> 00:12:58,400
10倍のデータを集めてきて、

405
00:12:58,640 --> 00:12:59,870
自分たちの手でラベル付けして行ったら

406
00:13:00,190 --> 00:13:01,490
どれだけかかるのか、

407
00:13:01,800 --> 00:13:03,560
椅子に座って考えてみるのだ。

408
00:13:05,260 --> 00:13:06,550
例えば、

409
00:13:06,630 --> 00:13:08,200
我らの機械学習のアプリケーションは現在、

410
00:13:08,690 --> 00:13:10,180
1000個の手本があるとする、つまりm=1000。

411
00:13:12,010 --> 00:13:12,750
そこで我らがやるべき事は、椅子に座って、

412
00:13:12,870 --> 00:13:14,500
こう問うてみる事だ：一つのラベルつき手本を集めるのには、

413
00:13:14,720 --> 00:13:16,930
実際どれだけの時間がかかるだろう？と。

414
00:13:17,340 --> 00:13:18,480
例えばそれは、

415
00:13:18,600 --> 00:13:19,510
10秒かかるとする、

416
00:13:19,790 --> 00:13:22,100
新しいラベル付き手本1つを

417
00:13:23,310 --> 00:13:25,120
得るのに。

418
00:13:25,520 --> 00:13:27,720
つまり、10倍の数の手本を得たいと思えば、計算を行ってみると、

419
00:13:28,360 --> 00:13:30,400
もし一つの手本に10秒かかり、

420
00:13:31,370 --> 00:13:32,340
10倍の数の手本を得たいと思えば、

421
00:13:32,580 --> 00:13:35,320
その場合は1万手本が必要となる。

422
00:13:35,830 --> 00:13:38,470
そこで計算を行う。人力で

423
00:13:38,770 --> 00:13:40,380
1万個のラベルつき手本を集めたら

424
00:13:40,840 --> 00:13:42,640
どれだけの時間がかかるか？を。

425
00:13:43,340 --> 00:13:45,280
1手本につき10秒かかるとすると、

426
00:13:47,070 --> 00:13:47,940
その場合にこの計算を行うと、

427
00:13:48,840 --> 00:13:49,920
割とよく、あなたがたは

428
00:13:50,390 --> 00:13:51,780
驚くことになる、

429
00:13:51,870 --> 00:13:53,140
いかにちょっとの仕事で済むのか、

430
00:13:53,240 --> 00:13:54,730
時にはほんの2、3日の仕事で、

431
00:13:54,880 --> 00:13:55,560
またある時にはほんの数日で済む事を知る事で。

432
00:13:55,780 --> 00:13:57,180
私は多くのチームが

433
00:13:57,500 --> 00:13:59,160
いかにちょっとの仕事で済むかを

434
00:13:59,340 --> 00:14:00,280
知ってとても驚く事を、何度も見てきた。

435
00:14:00,410 --> 00:14:01,200
単にもっと多くのデータを集めて、

436
00:14:01,370 --> 00:14:02,510
そのデータを学習アプリに食わせるだけで、

437
00:14:02,580 --> 00:14:03,470
膨大なパフォーマンスの向上が

438
00:14:03,580 --> 00:14:04,310
望める。

439
00:14:04,640 --> 00:14:06,350
そしてそれをあなたが

440
00:14:06,450 --> 00:14:07,550
成し遂げたら、

441
00:14:07,790 --> 00:14:08,900
あなたはどんな製品を開発していようと、

442
00:14:09,190 --> 00:14:10,780
どんなチームで働いていようと、

443
00:14:11,360 --> 00:14:12,520
きっとヒーローになる。

444
00:14:12,910 --> 00:14:14,150
何故ならこれは、パフォーマンスを改善する

445
00:14:14,320 --> 00:14:15,760
素晴らしい方法になりえるからだ。

446
00:14:17,650 --> 00:14:19,490
三番目は、これが最後だが、

447
00:14:20,020 --> 00:14:21,230
データをたくさん集めるのに

448
00:14:21,450 --> 00:14:22,650
時には良い方法たりえる物として、

449
00:14:23,080 --> 00:14:24,350
クラウドソーシングと呼ばれる物がある。

450
00:14:25,280 --> 00:14:26,350
こんにちでは、幾つかのサービスで

451
00:14:26,520 --> 00:14:27,270
あなたがかなり安い価格で

452
00:14:27,460 --> 00:14:29,520
あたなの為にラベル付きトレーニングセットを

453
00:14:29,920 --> 00:14:32,210
たくさん集めてくれる人を

454
00:14:32,350 --> 00:14:33,410
雇わせてくれるサービスが

455
00:14:33,730 --> 00:14:36,140
存在している。

456
00:14:36,810 --> 00:14:37,870
クラウドソーシング、あるいは

457
00:14:38,190 --> 00:14:39,460
データのラベル付のクラウドソースは、

458
00:14:39,950 --> 00:14:41,390
いろいろと

459
00:14:41,810 --> 00:14:43,180
複雑な

460
00:14:43,340 --> 00:14:45,200
事があり、

461
00:14:45,660 --> 00:14:47,040
そこでは、

462
00:14:47,210 --> 00:14:49,390
信頼出来るラベル付けを行う人を得られるかもしれない。

463
00:14:50,440 --> 00:14:51,470
世界中の何百、何千もの

464
00:14:51,860 --> 00:14:53,420
比較的安くラベル付けを手伝ってくれる

465
00:14:53,580 --> 00:14:55,530
ラベル付けを行う人を

466
00:14:55,630 --> 00:14:56,810
得るという手が使えるかもしれない。

467
00:14:57,030 --> 00:14:58,580
私が既に言及したように、

468
00:14:58,930 --> 00:15:00,120
そういう選択肢もまたありうる。

469
00:15:00,390 --> 00:15:02,170
そしてたぶん、AmazonのMechanical Turkシステムが

470
00:15:02,510 --> 00:15:03,750
現時点ではもっとも人気のある

471
00:15:03,900 --> 00:15:05,860
クラウドソースの選択肢だ。

472
00:15:06,860 --> 00:15:08,070
これはしばしば、

473
00:15:08,220 --> 00:15:10,040
ちゃんと機能させる為には

474
00:15:10,190 --> 00:15:10,940
かなりの作業を必要とする。

475
00:15:11,150 --> 00:15:12,520
もし高いクオリティのラベルを得たいと思えば。

476
00:15:12,780 --> 00:15:14,160
だがそれでも、これは検討するに値する

477
00:15:14,240 --> 00:15:15,760
選択肢となる事もある。

478
00:15:17,330 --> 00:15:18,870
もしラベル付けをあなたの為に

479
00:15:19,320 --> 00:15:21,000
行ってくれるような大量の人を、比較的安価に

480
00:15:21,810 --> 00:15:24,220
web上で探したい時には。

481
00:15:26,320 --> 00:15:27,570
このビデオでは、

482
00:15:27,660 --> 00:15:28,840
人工データ合成の

483
00:15:29,100 --> 00:15:30,870
アイデアについて議論した。その中でも

484
00:15:31,120 --> 00:15:32,440
スクラッチからデータ全体を作る

485
00:15:32,750 --> 00:15:34,400
方法とーーこの例としては

486
00:15:34,640 --> 00:15:35,400
ランダムフォントの例を見たがーー

487
00:15:35,830 --> 00:15:37,710
それか、既に存在しているトレーニングセットを

488
00:15:37,790 --> 00:15:38,980
増幅する、という方法ーー

489
00:15:39,420 --> 00:15:41,340
既存のラベル付きトレーニング手本を持ってきて

490
00:15:41,560 --> 00:15:42,980
そこに歪みを導入し、

491
00:15:43,240 --> 00:15:44,880
そこから新しい手本を生成するーーを見た。

492
00:15:46,010 --> 00:15:47,450
そして最後に、このビデオから

493
00:15:47,630 --> 00:15:48,810
あなたに覚えておいて

494
00:15:49,120 --> 00:15:49,970
欲しい事としては、

495
00:15:50,540 --> 00:15:51,540
もしあなたが機械学習の問題に直面していたら、

496
00:15:51,830 --> 00:15:54,350
二つの事はしばしば試してみる価値がある。

497
00:15:54,660 --> 00:15:55,830
一つは単純なサニティチェックを

498
00:15:56,160 --> 00:15:58,600
学習曲線で行い、もっと多くのデータが役に立つかを確認する事。

499
00:15:59,520 --> 00:16:00,340
二つ目は、もっと多くのデータが役に立つ場合には、

500
00:16:00,730 --> 00:16:01,780
椅子に座って自分自身に真剣に

501
00:16:01,850 --> 00:16:03,670
こう問うてみる：

502
00:16:04,050 --> 00:16:05,150
現在持っているデータの

503
00:16:05,260 --> 00:16:06,510
10倍のデータを得るには、

504
00:16:06,630 --> 00:16:08,450
どれだけ時間がかかるか、を。そしていつもでは無いにしても、

505
00:16:08,960 --> 00:16:10,440
ときには、それがいかに簡単かが判明して

506
00:16:10,640 --> 00:16:12,310
驚く事もある。

507
00:16:12,580 --> 00:16:13,990
2〜3日とか、

508
00:16:14,060 --> 00:16:15,020
2〜3週間とかの作業で十分だったり。

509
00:16:15,150 --> 00:16:16,160
そしてそれはあなたの学習アルゴリズムのパフォーマンスを

510
00:16:16,260 --> 00:16:18,700
猛烈に引き上げる素晴らしい方法になりうる。