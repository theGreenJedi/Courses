En vídeos anteriores, he comentado una y otra vez que, cuando desarrollen sistemas de aprendizaje automático, uno de los recursos más valiosos es su tiempo como el desarrollador en cuanto a escoger en qué van a trabajar después; o tienen un equipo de desarrolladores, o un equipo de ingenieros que trabajan juntos en un sistema de aprendizaje automático,  nuevamente uno de los recursos más valiosos es el tiempo de los ingenieros o los desarrolladores que trabajan en el sistema. Y lo que realmente quieren evitar es que ustedes o sus colegas, o sus amigos, pasen mucho tiempo de trabajando en algún componente, sólo para darse cuenta, después de semanas o meses, del tiempo que invirtieron, que todo ese trabajo, ya saben, simplemente no hace una gran diferencia en el rendimiento del sistema final. En este video, lo que me gustaría hacer es hablar acerca de algo llamado análisis de límite máximo. Cuando ustedes o su equipo están trabajando en un sistema de aprendizaje electrónico de flujo de proyecto, éste puede a veces darles una señal muy fuerte, una guía muy fuerte, sobre cuáles partes del flujo de proyecto podrían invertir mejor su tiempo. Para hablar acerca del análisis de límite máximo, voy va a seguir usando el ejemplo del flujo de proyecto de «foto OCR», y ya dije antes que cada uno de estos cuadros, detección de texto, segmentación de caracteres, reconocimiento de caracteres, cada uno de estos cuadros pueden tener hasta un pequeño equipo de trabajo de ingeniería en él, o tal vez ustedes construyeron todo el sistema; de cualquier manera, la pregunta es, ¿en dónde debe asignar los recursos? ¿Cuál de estos cuadros vale más su esfuerzo, tratando de mejorar su desempeño?. Con el fin de explicar la idea del análisis de límite máximo, voy a seguir usando el ejemplo de nuestro flujo de proyecto de «foto OCR». Como mencioné antes, cada uno de estos cuadros aquí, cada uno de estos componentes del aprendizaje automático podría ser el trabajo de incluso un pequeño equipo de ingenieros, o tal vez todo el sistema pudo haber sido construido por una persona solamente. Pero la pregunta es, ¿a dónde deben asignar sus escasos recursos? esto es, ¿cuál de estos componentes, o cuál componente, o dos, o tal vez los tres componentes, vale más la pena para darle su tiempo para tratar de mejorar su desempeño? Así que aquí está la idea del análisis de límite máximo. Al igual que en el proceso de desarrollo para otros sistemas de aprendizaje automático también, a fin de tomar decisiones sobre qué hacer para desarrollar el sistema, va a ser muy útil contar con una métrica de evaluación de números de una sola hilera para este sistema de aprendizaje. Así que digamos que seleccionamos exactitud de nivel de los caracteres. Así que si, ya saben, teniendo una imagen del conjunto de pruebas, ¿cuál es una fracción de alfabetos de caracteres en la imagen de prueba que reconocemos correctamente?. O pueden escoger alguna otra métrica de evaluación de números de una sola hilera, si desean, pero digamos que cualquiera que sea la métrica de evaluación que seleccionemos, encontramos que el sistema global tiene actualmente 72% de exactitud. De modo que, en otras palabras, tenemos algunos conjuntos de imágenes del conjunto de pruebas, y para cada imagen del conjunto de pruebas, la ejecutamos a través de la detección de texto, después la segmentación de caracteres, después el reconocimiento de caracteres, y hallamos que en nuestro conjunto de pruebas, la exactitud global del sistema completo, fue de 72% en una de las métricas que eligieron. Ahora, sólo la idea detrás del análisis de límite máximo, que es que vamos a permitir que se vea el primer módulo de una detección de texto de los flujos de proyecto de las máquinas, y lo que vamos a hacer es que vamos a juguetear con el conjunto de prueba, vamos a ir al conjunto de prueba, y para cada ejemplo de prueba simplemente le vamos a proporcionar las salidas de detección de texto correctas. En otras palabras, iremos al conjunto de pruebas y simplemente le diremos manualmente al algoritmo en dónde está el texto en cada uno de los ejemplos de prueba. En otras palabras, vamos simular lo que ocurre si tenemos un sistema de detección de texto con una exactitud del 100%, con el propósito de detectar el texto en una imagen. Y realmente, la manera en la que hacen eso es muy sencilla, ¿cierto?, en lugar de de permitir que el algoritmo de aprendizaje detecte el texto en las imágenes, en su lugar, irían a las imágenes y simplemente asignan valores manualmente cuál es la ubicación del texto en mi imagen de prueba, y entonces permitirían que estas asignaciones de valor correctas, dejarían que estas asignaciones de valor verdaderas de en dónde es el texto parte de su conjunto de texto, y usar estas asignaciones de valores verdaderas como lo que alimentan en la siguiente etapa del flujo de trabajo, al flujo de trabajo de la segmentación de caracteres. Así que para decirlo de nuevo, al colocar una marca de verificación por aquí, lo que quiero decir es que voy a ir a mi conjunto de pruebas y simplemente le daré las respuestas correctas, le daré las asignaciones de valor correctas para la parte de detección de texto del flujo de proyecto. Así que eso es como si tuviera un sistema de detección de texto perfecto en mi conjunto de prueba. Lo que vamos a hacer después es ejecutar estos datos al resto del flujo de proyecto, a través de la segmentación de caracteres y el reconocimiento de caracteres, y después usar la misma métrica de evaluación, como antes, para medir lo que es la exactitud general de todo el sistema. Y con la detección de texto perfecta, esperamos que aumente el desempeño. Digamos que sube 89%, y luego vamos a seguir adelante, después vamos ir a la siguiente selección del flujo de proyecto, a la segmentación de caracteres y nuevamente vamos a mi conjunto de pruebas. Y ahora vamos a dar la salidas de detección de texto correctas y dar las salidas de segmentación de caracteres correctas y vamos a ir al conjunto de pruebas y asignar el valor manualmente las segmentaciones correctas del texto en caracteres individuales, y ver cuánto ayuda eso. Y digamos que aumenta hasta 90% de exactitud para el sistema global, ¿bien?, así que, como siempre, la exactitud es la exactitud de los sistemas globales, así que sea cual sea el resultado final del sistema de reconocimiento de caracteres, cualquiera que sea el resultado final del flujo de proyecto general, va a medir la exactitud de eso. Y finalmente, voy a ir al sistema de  reconocimiento de caracteres y darle a eso la asignación de valor correcta también. Y si hago eso también entonces, no es de extrañar que obtendré un 100% de exactitud. Ahora, lo bueno de haber hecho este análisis, es que ahora podemos entender cuál es el potencial de crecimiento para mejorar cada uno de estos componentes. De modo que vemos que si conseguimos la detección de texto perfecta, nuestro desempeño se elevó de 72 a 89 por ciento, así que eso es un aumento de 17 por ciento en el desempeño. Así que esto significa que tienen que tomar su sistema en el que pasan mucho tiempo para mejorar la detección de texto; eso significa que podríamos mejorar potencialmente el desempeño de nuestro sistema en un 17 por ciento. Parece que esto bien vale la pena por nuestro tiempo. Mientras que, en contraste, cuando vamos de la detección de texto, cuando le damos la segmentación de caracteres perfecta, el desempeño sólo aumentó uno por ciento, así que eso es un mensaje más aleccionador. Quiere decir que no importa cuánto tiempo pasan en la segmentación de caracteres, tal vez el potencial de crecimiento va va a ser muy pequeño, y tal vez no querrán tener un gran equipo de ingenieros trabajando en la segmentación de caracteres de la que este tipo de análisis muestra que, incluso cuando le dan la segmentación de caracteres perfecta, su desempeño sólo se incrementa en uno por ciento. Así que esto realmente calcula lo que es el techo, o qué es un límite superior a la cantidad en la que pueden mejorar el desempeño de su sistema, por medio de trabajar en uno de estos componentes. Y, por último, en cuanto a los caracteres, cuando obtenemos un mejor reconocimiento de caracteres, el desempeño aumentó en un diez por ciento. Así que, ya saben, nuevamente pueden decidir: el diez por ciento de mejoría, ¿qué tanto está resolviendo? Les dice que tal vez, con más esfuerzo invertido en la última estación del flujo de proyecto, pueden mejorar el desempeño de los sistemas también. Otra forma de pensar sobre esto es que, al pasar a través de este tipo de análisis, están tratando de averiguar, ya saben, cuál es el potencial de crecimiento, de mejorar cada uno de estos componentes o cuánto podrían ganar si cada uno de estos componentes se hiciera absolutamente perfecto, y realmente sólo coloca un límite superior en el desempeño de ese sistema. Así que la idea del análisis de límite máximo es bastante importante. Permítanme ilustrar esta idea de nuevo, pero con un ejemplo diferente pero más complejo. Digamos que quieren hacer reconocimiento facial de imágenes, así que digamos que quieren ver una fotografía y reconocer si la persona en esta foto es o no un amigo suyo en particular, tratando de reconocer a la persona que se muestra en esta imagen. Este es un ejemplo ligeramente artificial, esta no es realmente la forma en la se hace el reconocimiento facial en práctica, pero quiero pasar a través de un ejemplo de cómo se vería un flujo de proyecto para darles otro ejemplo de cómo se vería un proceso de análisis de límite máximo. Así que tenemos una imagen de la cámara y digamos que diseñamos un flujo de proyecto de la siguiente manera. Digamos que lo primero que quieren hacer es un pre-procesamiento de la imagen, así que vamos a tomar esas imágenes como les he mostrado en la parte superior derecha, y digamos que queremos quitar el fondo, así que, a través del pre-procesamiento, el fondo desaparece. Después, queremos, digamos, detectar el rostro de la persona. Eso se hace generalmente con un algoritmo de aprendizaje, Así que ejecutamos un fuego cruzado de Windows de deslizamiento para dibujar un cuadro  alrededor de la cara de la persona. Después de haber detectado la cara, resulta que si quieren reconocer personas, los ojos son una señal de gran utilidad. En realidad, en términos de, ya saben, reconocer a sus amigos, la apariencia de sus ojos es en realidad una de las claves más importantes que usan. Así que vamos a ejecutar otro fuego cruzado para detectar los ojos de la persona. De modo que segmenten los ojos, y a continuación, y dado que esto nos dará variables útiles para reconocer a una persona y después otras partes de la cara de interés físico, tal vez segmenten la nariz, segmenten la boca y después, ya que encontraron los ojos, la nariz y la boca, todos ellos nos dan variables útiles para, quizás, alimentar nuestro fuego cruzado de regresión logística. Y es el trabajo del fuego cruzado darnos entonces la asignación de valor general para encontrar el valor asignado para la que pensamos que es la identidad de esta persona. Así que este es un flujo de proyecto un poco complicado, De hecho, es probablemente más complicado de lo que deberían utilizar si en en realidad quieren reconocer personas. Pero hay un ejemplo ilustrativo que es útil para pensar en el análisis de límite máximo. Así que, ¿cómo pasan a través del análisis de límite máximo para este flujo de proyecto? Bueno, pasamos a través de estas piezas de una en una. Digamos que su sistema general tiene 85 por ciento de exactitud; lo primero que hago es ir a mi conjunto de pruebas y darle segmentaciones de primer plano, fondo y, a continuación, ir manualmente al conjunto de pruebas, y usar Photoshop o algo así, para simplemente decirle en dónde está el fondo, y sólo quitar el fondo de manera manual, así que, del primer plano al fondo, y ver cuánto cambia la exactitud. En este ejemplo, la exactitud aumenta 0.1%, así que esto es una fuerte señal de que, incluso si tenían la segmentación de fondo perfecto, su desempeño, aún si la eliminación del fondo es perfecta, el desempeño de su sistema no va a aumentar mucho. Así que esto tal vez no vale el enorme esfuerzo para trabajar en el pre-procesamiento, en la eliminación del fondo. Entonces, todo se dirige al conjunto de pruebas, dadas las correctas imágenes de detección facial, de nuevo pasan a través de las segmentaciones de los ojos, la nariz, la boca en algún orden; elijan un orden. Vamos a dar la ubicación correcta de los ojos, la ubicación correcta de la nariz, la ubicación correcta de la boca y finalmente, si le dí la asignación de valor general correcta, obtengo 100% de exactitud. Así que, ya saben, a medida que paso a través del sistema y sólo doy más y más asignaciones de valor correctas a los componentes en el conjunto de pruebas, el desempeño del sistema global aumenta, y pueden ver cuánto aumentó el desempeño en diferentes etapas, así que, ya saben, por darle la detección facial perfecta, y parece que el desempeño general de este sistema aumentó 5.9 por ciento. Así que ése fue un gran salto, significa que tal vez vale la pena un poco de esfuerzo para una mejor detección facial. Subió cuatro por ciento allí, subió uno por ciento allá, uno por ciento allí y tres por ciento allí. Así que parece que los componentes que más valen nuestro tiempo son cuando les di detección facial perfecta, el sistema subió por 5.9 de desempeño, le podría dar segmentación de ojo perfecta y subir 4%, y después mi fuego cruzado logístico, bueno, hay otra brecha de 3 por ciento allí posiblemente. Entonces, esto nos dice uno de los componentes en los que más vale la pena trabajar. Por cierto, quiero contarles, es una verdadera historia con moraleja; la razón por la que puse este pre-procesamiento para eliminar el fondo es porque realmente conozco una historia real en la que hubo un equipo de investigación que en realidad, literalmente, tuvo dos personas que pasaron alrededor de un año y medio, pasaron 18 meses trabajando en una mejor eliminación del fondo. Nos apresuramos aquí ... Estoy ocultando los detalles por obvias razones, pero hubo una aplicación de visión por computadora en la que hubo un equipo de dos ingenieros que literalmente pasaron, creo que alrededor de un año y medio, trabajando en una mejor eliminación del fondo. En realidad elaboraron algoritmos muy complicados, y terminaron publicando creo, un trabajo de investigación. Pero después de todo ese trabajo, encontraron que simplemente no tuvo un gran impacto en el desempeño global de la aplicación real en la que estaban trabajando. Y si sólo, ya saben, si sólo alguien hubiera hecho un análisis desde el vacío de antemano, quizás nos podríamos haber dado cuenta de esto. Y uno de ellos me dijo después, ya saben, si tan sólo hubieran hecho un tipo de análisis así, tal vez se habrían dado cuenta antes de los 18 meses de trabajo, que deberían haber invertido su esfuerzo centrándose en algún componente diferente en lugar de, literalmente, pasarse 18 meses trabajando en la eliminación del fondo. Así que para resumir, los flujos de proyecto son aplicaciones de aprendizaje automático bastante generalizadas y complejas, y cuando están trabajando en una gran aplicación de aprendizaje automático, quiero decir, creo que su tiempo como desarrolladores es muy valioso. Así que no pierdan su tiempo trabajando en algo que finalmente no va a importar, y en este vídeo hablamos acerca de esta idea del análisis de límite máximo, que a menudo he encontrado que es una muy buena herramienta para identificar el componente, y si en verdad invierten un esfuerzo enfocado en ese componente y hacen una gran diferencia, en realidad tendría un efecto enorme en el desempeño general de su sistema final. Así que, en los que años que he trabajado con el aprendizaje automático, en realidad he aprendido a no confiar en mi propio instinto, a sentir sobre cuál componente trabajar. Así que, muy a menudo, cuando han trabajado con el aprendizaje automático durante mucho tiempo, pero a menudo, nuestro problema de aprendizaje automático local, y quizá tenga alguna intuición acerca de él, ¡oh, vamos, ya saben, a saltar sobre ese componente, y a invertir más tiempo en él!. Pero a través de los años en los que he llegado hasta a confiar en mis instintos y en los que he aprendido a no confiar tanto en mis instintos y, en lugar de eso, si tenemos un problema de aprendizaje automático sólido, en donde es posible estructurar las cosas, realizar un análisis de límite máximo es a menudo mucho mejor y es una manera mucho más confiable para decidir en dónde poner un esfuerzo centrado, para realmente mejorar esto, el desempeño de algún componente, y estaremos seguros de que, cuando hagamos eso, en realidad tendrá un enorme impacto en el desempeño final de su sistema de procesamiento.