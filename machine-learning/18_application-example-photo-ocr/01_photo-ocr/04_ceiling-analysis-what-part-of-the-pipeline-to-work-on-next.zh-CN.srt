1
00:00:00,090 --> 00:00:01,140
在前面的视频中

2
00:00:01,260 --> 00:00:02,510
我不止一次地说过

3
00:00:02,650 --> 00:00:03,980
在你开发机器学习系统时

4
00:00:04,770 --> 00:00:06,630
你最宝贵的资源

5
00:00:06,810 --> 00:00:08,050
就是你的时间

6
00:00:08,490 --> 00:00:09,820
作为一个开发者

7
00:00:09,950 --> 00:00:11,520
你需要正确选择下一步的工作

8
00:00:11,950 --> 00:00:12,710
或者也许你有一个开发团队

9
00:00:13,300 --> 00:00:14,610
或者一个工程师小组

10
00:00:15,090 --> 00:00:16,620
共同开发一个机器学习系统

11
00:00:16,930 --> 00:00:18,420
同样 最宝贵的还是

12
00:00:18,990 --> 00:00:20,790
开发系统所花费的时间

13
00:00:22,420 --> 00:00:23,340
你需要尽量避免的

14
00:00:23,430 --> 00:00:25,340
情况是你或者

15
00:00:25,360 --> 00:00:26,410
你的同事 你的朋友

16
00:00:26,680 --> 00:00:27,560
花费了大量时间

17
00:00:27,970 --> 00:00:29,510
在某一个模块上

18
00:00:30,470 --> 00:00:31,540
在几周甚至几个月的努力以后

19
00:00:31,620 --> 00:00:33,070
才意识到所有这些付出的劳动

20
00:00:33,310 --> 00:00:35,090
都对你最终系统的表现

21
00:00:35,380 --> 00:00:38,120
并没有太大的帮助

22
00:00:39,350 --> 00:00:40,430
在这段视频中

23
00:00:40,550 --> 00:00:42,960
我将介绍一下关于上限分析(ceiling analysis)的内容

24
00:00:44,510 --> 00:00:45,760
当你自己或你跟

25
00:00:46,280 --> 00:00:47,270
你的团队在开发

26
00:00:47,520 --> 00:00:48,860
机器学习系统的流水线时

27
00:00:49,020 --> 00:00:50,380
这种方式通常能

28
00:00:50,630 --> 00:00:51,650
提供一种很有价值的信号

29
00:00:52,340 --> 00:00:53,730
或者说很有用的导向

30
00:00:54,150 --> 00:00:56,550
告诉你流水线中的哪个部分最值得你花时间

31
00:00:59,740 --> 00:01:01,700
为了介绍上限分析

32
00:01:01,860 --> 00:01:03,140
我将继续使用之前用过的

33
00:01:03,690 --> 00:01:04,910
照片OCR流水线的例子

34
00:01:05,640 --> 00:01:06,870
在之前的课程中

35
00:01:07,170 --> 00:01:08,270
我讲过这些方框

36
00:01:08,480 --> 00:01:09,900
文字检测、字符分割

37
00:01:10,200 --> 00:01:12,140
字符识别

38
00:01:12,310 --> 00:01:13,730
这每一个方框都可能

39
00:01:14,100 --> 00:01:15,550
需要一个小团队来完成

40
00:01:15,920 --> 00:01:17,370
当然也可能

41
00:01:17,690 --> 00:01:18,640
你一个人来构建整个系统

42
00:01:18,800 --> 00:01:19,700
不管怎样

43
00:01:19,960 --> 00:01:22,340
问题是 你应该怎样分配资源呢？

44
00:01:22,730 --> 00:01:24,250
哪一个方框最值得

45
00:01:24,430 --> 00:01:26,630
你投入精力去做

46
00:01:26,920 --> 00:01:28,260
投入时间去改善效果

47
00:01:29,070 --> 00:01:30,350
为了解释上限分析的原理

48
00:01:30,840 --> 00:01:32,560
为了解释上限分析的原理

49
00:01:32,730 --> 00:01:35,690
我将继续使用照片OCR流水线的例子

50
00:01:37,000 --> 00:01:38,320
在之前的视频中我讲过

51
00:01:38,430 --> 00:01:39,630
这里的每个方框

52
00:01:39,850 --> 00:01:41,860
都表示一个机器学习的组成部分

53
00:01:42,170 --> 00:01:43,270
需要一个小工程师团队来完成

54
00:01:43,470 --> 00:01:44,720
当然也可能

55
00:01:45,280 --> 00:01:48,110
整个系统都由一个人来完成

56
00:01:48,780 --> 00:01:49,920
但问题是

57
00:01:50,100 --> 00:01:51,990
你应该如何分配有限的资源呢？

58
00:01:52,130 --> 00:01:53,200
也就是说

59
00:01:53,690 --> 00:01:54,860
这些模块中

60
00:01:54,950 --> 00:01:56,250
哪一个 或者哪两个、三个

61
00:01:57,080 --> 00:01:58,540
是最值得你花更多的

62
00:01:59,200 --> 00:02:01,060
精力去改善它的效果的？

63
00:02:01,660 --> 00:02:02,810
这便是上限分析要做的事

64
00:02:04,140 --> 00:02:05,520
跟其他机器学习系统的

65
00:02:05,890 --> 00:02:07,170
开发过程一样

66
00:02:07,340 --> 00:02:08,490
为了决定

67
00:02:08,670 --> 00:02:09,740
要开发这个系统应该

68
00:02:09,970 --> 00:02:11,150
采取什么样的行动

69
00:02:11,710 --> 00:02:12,770
一个有效的方法是

70
00:02:12,900 --> 00:02:14,070
对学习系统使用一个

71
00:02:14,580 --> 00:02:17,650
数值评价量度

72
00:02:18,450 --> 00:02:19,390
所以假如我们用字符准确度作为这个量度

73
00:02:19,530 --> 00:02:21,140
因此 给定一个

74
00:02:21,570 --> 00:02:22,840
测试样本图像

75
00:02:22,860 --> 00:02:24,710
那么这个数值就表示

76
00:02:25,060 --> 00:02:26,570
我们对测试图像中的文字

77
00:02:28,980 --> 00:02:29,390
识别正确的比例

78
00:02:29,550 --> 00:02:30,830
或者你也可以选择

79
00:02:31,030 --> 00:02:32,270
其他的某个数值评价度量值

80
00:02:32,370 --> 00:02:33,740
随你选择

81
00:02:34,040 --> 00:02:35,820
但不管选择什么评价量度值

82
00:02:35,920 --> 00:02:37,680
我们只是假设

83
00:02:37,880 --> 00:02:40,090
整个系统的估计准确率为72%

84
00:02:40,350 --> 00:02:42,210
所以换句话说

85
00:02:42,350 --> 00:02:43,380
我们有一些测试集图像

86
00:02:43,520 --> 00:02:44,960
并且对测试集中的

87
00:02:45,180 --> 00:02:46,460
每一幅图像

88
00:02:46,640 --> 00:02:47,850
我们都对其分别运行

89
00:02:47,980 --> 00:02:49,280
文字检测、字符分割

90
00:02:49,560 --> 00:02:50,680
然后字符识别

91
00:02:51,010 --> 00:02:52,240
然后我们发现

92
00:02:52,370 --> 00:02:53,570
根据你选用的度量方法

93
00:02:53,800 --> 00:02:56,220
整个测试集的准确率是72%

94
00:02:58,120 --> 00:02:59,700
下面是上限分析的

95
00:03:00,070 --> 00:03:01,610
主要思想

96
00:03:01,910 --> 00:03:03,530
首先我们关注

97
00:03:03,670 --> 00:03:05,100
这个机器学习流程中的

98
00:03:05,400 --> 00:03:06,810
第一个模块 文字检测

99
00:03:07,270 --> 00:03:08,400
而我们要做的

100
00:03:08,420 --> 00:03:09,170
实际上是在

101
00:03:09,270 --> 00:03:11,310
给测试集样本捣点儿乱

102
00:03:11,980 --> 00:03:12,920
我们要对

103
00:03:12,990 --> 00:03:14,270
每一个测试集样本

104
00:03:14,830 --> 00:03:16,170
都给它提供一个

105
00:03:16,380 --> 00:03:18,230
正确的文字检测结果

106
00:03:19,210 --> 00:03:20,300
换句话说

107
00:03:20,560 --> 00:03:21,760
我们要遍历每个测试集样本

108
00:03:21,960 --> 00:03:23,340
然后人为地告诉算法

109
00:03:24,350 --> 00:03:26,210
每一个测试样本中

110
00:03:26,780 --> 00:03:27,940
什么地方出现了文字

111
00:03:28,950 --> 00:03:29,960
因此换句话说

112
00:03:30,030 --> 00:03:31,510
我们是要模拟

113
00:03:32,030 --> 00:03:33,640
如果是100%

114
00:03:33,890 --> 00:03:35,350
正确地检测出

115
00:03:35,610 --> 00:03:37,180
图片中的文字信息

116
00:03:38,300 --> 00:03:40,410
应该是什么样的

117
00:03:42,050 --> 00:03:43,070
当然 要做到这个

118
00:03:43,110 --> 00:03:44,210
是很容易的

119
00:03:44,620 --> 00:03:45,840
现在不用你的学习算法

120
00:03:46,340 --> 00:03:47,630
来检测图像中的文字了

121
00:03:48,180 --> 00:03:49,110
你只需要找到对应的图像

122
00:03:49,340 --> 00:03:51,230
然后人为地识别出

123
00:03:51,540 --> 00:03:53,620
测试集图像中出现文字的区域

124
00:03:54,200 --> 00:03:55,040
然后你要做的就是让这些

125
00:03:55,530 --> 00:03:56,620
绝对正确的结果

126
00:03:56,990 --> 00:03:58,370
这些绝对为真的标签

127
00:03:58,560 --> 00:04:00,010
也就是告诉你

128
00:04:00,090 --> 00:04:01,330
图像中哪些位置

129
00:04:01,580 --> 00:04:02,990
有文字信息的标签

130
00:04:03,110 --> 00:04:04,200
把它们传给下一个模块

131
00:04:04,470 --> 00:04:07,550
也就是传给字符分割模块

132
00:04:07,710 --> 00:04:09,250
我再说一遍

133
00:04:09,680 --> 00:04:10,790
这里打钩的地方

134
00:04:11,500 --> 00:04:12,590
我想做的是

135
00:04:12,750 --> 00:04:13,750
遍历我的测试集

136
00:04:13,860 --> 00:04:14,970
直接向它公布“标准答案”

137
00:04:15,480 --> 00:04:16,520
为这个流程中的文字检测部分

138
00:04:16,650 --> 00:04:18,250
直接提供正确的标签

139
00:04:19,240 --> 00:04:20,280
这样好像我就会

140
00:04:20,410 --> 00:04:21,700
有一个非常棒的文字检测系统

141
00:04:22,370 --> 00:04:24,270
能很好地检测我的测试样本

142
00:04:24,460 --> 00:04:26,570
然后我们要做的是

143
00:04:27,190 --> 00:04:28,150
继续运行完接下来的几个模块

144
00:04:28,530 --> 00:04:29,860
也就是字符分割和字符识别

145
00:04:30,680 --> 00:04:31,930
然后使用跟之前一样的

146
00:04:32,300 --> 00:04:33,310
评价量度指标

147
00:04:34,000 --> 00:04:35,240
来测量整个系统的

148
00:04:35,450 --> 00:04:36,900
总体准确度

149
00:04:37,790 --> 00:04:39,890
这样用准确的文字检测结果 系统的表现应该会有提升

150
00:04:40,330 --> 00:04:41,870
假如说 准确率

151
00:04:41,930 --> 00:04:44,550
提高到89%

152
00:04:44,680 --> 00:04:45,830
然后我们继续进行

153
00:04:46,090 --> 00:04:47,120
接着执行流水线中的下一模块 字符分割

154
00:04:47,330 --> 00:04:50,230
同前面一样 我还是去找出我的测试集

155
00:04:50,540 --> 00:04:52,300
然后现在我不仅用

156
00:04:52,390 --> 00:04:54,140
标准的文字检测结果

157
00:04:54,900 --> 00:04:55,970
我还同时用标准的

158
00:04:56,490 --> 00:04:58,220
字符分割结果

159
00:04:59,400 --> 00:05:00,780
所以还是遍历测试样本

160
00:05:01,330 --> 00:05:03,710
人工地给出正确的字符分割结果

161
00:05:04,730 --> 00:05:05,560
然后看看这样做以后 效果怎样变化

162
00:05:05,810 --> 00:05:06,670
假如我们这样做以后

163
00:05:06,800 --> 00:05:09,140
整个系统准确率提高到90%

164
00:05:10,070 --> 00:05:11,060
注意跟前面一样 这里说的准确率

165
00:05:11,340 --> 00:05:13,420
是指整个系统的准确率

166
00:05:14,120 --> 00:05:15,460
所以无论最后一个模块

167
00:05:15,830 --> 00:05:17,450
字符识别模块给出的最终输出是什么

168
00:05:17,560 --> 00:05:18,870
无论整个流水线的

169
00:05:19,040 --> 00:05:19,660
最后输出结果是什么

170
00:05:19,930 --> 00:05:22,400
我们都是测出的整个系统的准确率

171
00:05:22,520 --> 00:05:23,720
最后我们还是执行最后一个模块 字符识别

172
00:05:24,170 --> 00:05:26,170
同样也是人工给出这一模块的正确标签

173
00:05:26,780 --> 00:05:29,270
这样做以后 我应该理所当然得到100%准确率

174
00:05:31,270 --> 00:05:32,530
进行上限分析的

175
00:05:32,850 --> 00:05:34,340
一个好处是

176
00:05:34,450 --> 00:05:36,080
我们现在就知道了

177
00:05:36,700 --> 00:05:40,250
如果对每一个模块进行改善 它们各自的上升空间是多大

178
00:05:41,390 --> 00:05:44,180
所以 我们可以看到 如果我们拥有完美的文字检测模块

179
00:05:44,950 --> 00:05:46,360
那么整个系统的表现将会从

180
00:05:46,710 --> 00:05:48,080
准确率72%上升到89%

181
00:05:48,420 --> 00:05:50,670
因此效果的增益是17%

182
00:05:51,640 --> 00:05:52,680
这就意味着

183
00:05:52,890 --> 00:05:54,030
如果你在现有系统的基础上

184
00:05:54,160 --> 00:05:56,130
花费时间和精力改善文字检测模块的效果

185
00:05:57,330 --> 00:05:58,750
那么系统的表现

186
00:05:59,200 --> 00:06:00,640
可能会提高17%

187
00:06:01,020 --> 00:06:02,850
看起来这还挺值得

188
00:06:03,770 --> 00:06:05,840
而相对来讲

189
00:06:06,200 --> 00:06:08,360
如果我们取得完美的字符分割模块

190
00:06:08,640 --> 00:06:12,450
那么最终系统表现只提升了1%

191
00:06:13,020 --> 00:06:14,820
这便提供了一个很重要的信息

192
00:06:15,250 --> 00:06:16,880
这就告诉我们

193
00:06:17,090 --> 00:06:18,510
不管我们投入多大精力在字符分割上

194
00:06:19,800 --> 00:06:20,990
系统效果的潜在上升空间

195
00:06:21,080 --> 00:06:22,280
也都是很小很小

196
00:06:22,460 --> 00:06:23,420
所以你就不会让一个

197
00:06:23,580 --> 00:06:24,340
比较大的工程师团队

198
00:06:24,860 --> 00:06:26,860
花时间忙于字符分割模块

199
00:06:26,990 --> 00:06:28,860
因为通过上限分析我们知道了

200
00:06:29,150 --> 00:06:30,180
即使你把字符分割模块做得再好

201
00:06:30,260 --> 00:06:32,480
再怎么完美 你的系统表现

202
00:06:32,620 --> 00:06:34,180
最多也只能提升1%

203
00:06:34,620 --> 00:06:36,090
所以这就估计出

204
00:06:36,890 --> 00:06:38,080
通过改善各个模块的质量

205
00:06:38,300 --> 00:06:39,360
你的系统表现

206
00:06:39,550 --> 00:06:40,690
所能提升的上限值

207
00:06:40,740 --> 00:06:42,710
或者说最大值 是多少

208
00:06:44,330 --> 00:06:45,600
最后

209
00:06:46,320 --> 00:06:47,700
如果我们取得完美的字符识别模块

210
00:06:47,900 --> 00:06:50,080
那么整个系统的表现将提高10%

211
00:06:50,530 --> 00:06:51,640
所以 同样

212
00:06:51,750 --> 00:06:52,570
你也可以分析

213
00:06:52,860 --> 00:06:55,630
10%的效果提升值得投入多少工作量

214
00:06:55,830 --> 00:06:57,200
也许这也告诉你

215
00:06:57,400 --> 00:06:58,670
如果把精力投入在

216
00:06:58,730 --> 00:06:59,690
流水线的最后这个模块

217
00:07:00,360 --> 00:07:02,840
那么系统的性能

218
00:07:03,760 --> 00:07:04,500
还是能得到较大的提高

219
00:07:05,610 --> 00:07:06,580
另一种认识这种分析方法的角度是

220
00:07:06,870 --> 00:07:08,090
通过这样的分析

221
00:07:08,290 --> 00:07:09,470
你就能总结出

222
00:07:09,570 --> 00:07:10,640
改善每个模块的性能

223
00:07:10,740 --> 00:07:12,700
系统的上升空间是多少

224
00:07:13,480 --> 00:07:14,980
或者说如果其中的某个模块

225
00:07:15,080 --> 00:07:16,730
变得绝对完美时

226
00:07:17,260 --> 00:07:18,910
你能得到什么收获

227
00:07:19,380 --> 00:07:20,780
这就像是给系统表现

228
00:07:21,060 --> 00:07:23,230
加上了一个提升的上限值

229
00:07:24,220 --> 00:07:26,290
所以 上限分析的概念是很重要的

230
00:07:26,900 --> 00:07:29,840
下面我换一个复杂一点的例子再来演绎一下上限分析的原理

231
00:07:31,860 --> 00:07:32,990
假如说你想对这张图像

232
00:07:33,260 --> 00:07:34,830
进行人脸识别

233
00:07:35,280 --> 00:07:35,960
也就是说看着这张照片

234
00:07:35,990 --> 00:07:37,650
你希望识别出

235
00:07:37,820 --> 00:07:38,770
照片里这个人

236
00:07:39,470 --> 00:07:40,640
是不是你的朋友

237
00:07:40,670 --> 00:07:43,880
希望辨识出图像中的人

238
00:07:44,180 --> 00:07:46,260
这是一个偏人工智能的例子

239
00:07:47,130 --> 00:07:51,080
当然这并不是现实中的

240
00:07:51,320 --> 00:07:52,790
人脸识别技术

241
00:07:52,800 --> 00:07:53,660
但我想通过这个例子

242
00:07:53,870 --> 00:07:54,800
来向你展示一个流水线

243
00:07:54,940 --> 00:07:56,220
并且给你另一个关于

244
00:07:56,450 --> 00:07:57,820
上限分析的实例

245
00:07:58,710 --> 00:07:59,980
假如我们有张照片

246
00:08:00,160 --> 00:08:03,830
我们设计了如下的流水线

247
00:08:04,420 --> 00:08:05,120
假如我们第一步要做的

248
00:08:05,380 --> 00:08:07,480
是图像预处理

249
00:08:07,560 --> 00:08:08,770
假如我们就用

250
00:08:08,910 --> 00:08:10,310
右上角这张照片

251
00:08:10,390 --> 00:08:11,040
现在我们想要

252
00:08:11,140 --> 00:08:12,510
把背景去掉

253
00:08:13,030 --> 00:08:14,790
那么经过预处理 背景就被去掉了

254
00:08:16,070 --> 00:08:18,820
下一步我们希望检测出人脸的位置

255
00:08:19,370 --> 00:08:20,550
这通常通过一个学习算法来实现

256
00:08:20,930 --> 00:08:21,960
我们会运行一个滑动窗分类器

257
00:08:22,180 --> 00:08:24,900
在人脸周围画一个框

258
00:08:25,680 --> 00:08:26,720
在检测到脸部以后

259
00:08:26,790 --> 00:08:27,650
如果你想要

260
00:08:27,770 --> 00:08:29,320
识别出这个人

261
00:08:29,530 --> 00:08:31,630
那么眼睛是一个很重要的线索

262
00:08:32,000 --> 00:08:33,860
事实上

263
00:08:34,130 --> 00:08:35,420
要辨认出你的朋友

264
00:08:35,700 --> 00:08:36,870
你通常会看眼睛

265
00:08:37,330 --> 00:08:38,680
这是个比较重要的线索

266
00:08:39,470 --> 00:08:41,610
所以 我们需要运行另一个分类器来检测人的眼睛

267
00:08:42,500 --> 00:08:43,660
分割出眼睛

268
00:08:44,410 --> 00:08:45,650
这样就提供了

269
00:08:45,900 --> 00:08:47,290
识别出一个人的

270
00:08:47,380 --> 00:08:48,840
很重要的特征

271
00:08:49,100 --> 00:08:50,400
然后继续识别脸上其他重要的部位

272
00:08:50,990 --> 00:08:52,330
比如分割出鼻子

273
00:08:52,830 --> 00:08:54,750
分割出嘴巴

274
00:08:54,980 --> 00:08:56,230
这样找出了

275
00:08:56,370 --> 00:08:57,060
眼睛、鼻子、嘴巴

276
00:08:57,340 --> 00:08:58,420
所有这些都是非常有用的特征

277
00:08:58,740 --> 00:08:59,920
然后这些特征可以被输入给某个

278
00:09:00,580 --> 00:09:01,540
逻辑回归的分类器

279
00:09:02,500 --> 00:09:03,200
然后这个分类器的任务

280
00:09:03,480 --> 00:09:04,420
就是给出最终的标签

281
00:09:04,710 --> 00:09:05,850
找出我们认为能

282
00:09:05,970 --> 00:09:06,930
辨别出这个人是谁的

283
00:09:07,190 --> 00:09:08,450
最终的标签

284
00:09:10,110 --> 00:09:11,730
这是一个稍微复杂一些的流水线

285
00:09:12,160 --> 00:09:13,300
如果你真的想识别出人的话

286
00:09:13,950 --> 00:09:16,810
可能实际的流程比这个还要复杂

287
00:09:17,620 --> 00:09:20,330
但这给出了很好的一个上限分析的例子

288
00:09:22,150 --> 00:09:24,510
那么对这个流水线怎么进行上限分析呢？

289
00:09:25,000 --> 00:09:26,790
我们还是每次关注一个步骤

290
00:09:27,470 --> 00:09:28,900
假如说你整个系统的准确率

291
00:09:29,150 --> 00:09:30,560
达到了85%

292
00:09:30,720 --> 00:09:31,670
那么我要做的第一件事情

293
00:09:31,750 --> 00:09:32,890
还是找到我的测试集

294
00:09:33,860 --> 00:09:36,200
然后对前景和背景

295
00:09:36,740 --> 00:09:38,090
进行分割

296
00:09:38,150 --> 00:09:39,670
然后使用Photoshop

297
00:09:40,290 --> 00:09:41,750
或者别的什么软件

298
00:09:41,950 --> 00:09:43,130
识别出哪些区域是背景

299
00:09:43,360 --> 00:09:45,230
然后手动把背景删掉

300
00:09:45,470 --> 00:09:48,050
然后观察准确率提高多少

301
00:09:48,990 --> 00:09:50,320
假设在这个例子中

302
00:09:50,800 --> 00:09:53,700
准确率提高了0.1%

303
00:09:53,860 --> 00:09:54,900
这是个很明显的信号

304
00:09:55,100 --> 00:09:56,240
它告诉你即便你

305
00:09:56,630 --> 00:09:59,680
把背景分割做得很好

306
00:09:59,840 --> 00:10:01,650
完全去除了背景图案

307
00:10:01,730 --> 00:10:03,740
但整个系统的表现也并不会提高多少

308
00:10:03,880 --> 00:10:05,000
所以似乎并不值得

309
00:10:05,190 --> 00:10:07,720
花太多精力在预处理或者背景移除上

310
00:10:09,270 --> 00:10:10,170
接下来 再遍历测试集

311
00:10:10,230 --> 00:10:11,290
给出正确的脸部识别图案

312
00:10:11,780 --> 00:10:13,650
接下来还是依次运行

313
00:10:14,140 --> 00:10:16,690
眼睛、鼻子和嘴巴的分割

314
00:10:17,100 --> 00:10:17,470
选择一种顺序就行了

315
00:10:17,700 --> 00:10:18,890
给出眼睛的正确位置

316
00:10:19,340 --> 00:10:20,520
鼻子的正确位置

317
00:10:20,750 --> 00:10:22,510
嘴巴的正确位置

318
00:10:22,520 --> 00:10:23,740
最后 再给出最终的正确标签

319
00:10:24,130 --> 00:10:26,200
准确率提高到100%

320
00:10:27,900 --> 00:10:29,390
在我每次通过这个系统的时候

321
00:10:29,500 --> 00:10:30,430
在我每次通过这个系统的时候

322
00:10:31,040 --> 00:10:32,080
随着使用有正确标签的测试集

323
00:10:32,210 --> 00:10:33,900
的模块越来越多

324
00:10:34,370 --> 00:10:35,350
整个系统的表现

325
00:10:35,830 --> 00:10:37,550
逐步上升

326
00:10:37,730 --> 00:10:38,640
这样你就能很清楚地看到

327
00:10:38,890 --> 00:10:39,860
通过不同的步骤

328
00:10:40,240 --> 00:10:41,660
系统的表现增加了多少

329
00:10:42,550 --> 00:10:43,830
比如 有了完美的脸部识别

330
00:10:44,440 --> 00:10:45,270
整个系统的表现似乎

331
00:10:45,570 --> 00:10:48,290
提高了5.9%

332
00:10:49,710 --> 00:10:50,670
这算是比较大的提高了

333
00:10:50,980 --> 00:10:52,100
这告诉你也许在脸部检测上

334
00:10:52,370 --> 00:10:53,660
多做点努力是有意义的

335
00:10:54,670 --> 00:10:56,290
这里提高4%

336
00:10:56,710 --> 00:10:58,680
这两步都是提高1%

337
00:10:59,160 --> 00:11:00,600
这一步提高3%

338
00:11:01,520 --> 00:11:02,840
所以从整体上看

339
00:11:02,980 --> 00:11:04,250
最值得我付出努力的模块

340
00:11:04,730 --> 00:11:06,520
按顺序排列一下

341
00:11:06,680 --> 00:11:08,540
排在最前的是脸部检测

342
00:11:09,680 --> 00:11:10,190
系统表现提高了

343
00:11:10,490 --> 00:11:11,990
5.9%

344
00:11:12,170 --> 00:11:14,170
给它完美的眼部分割

345
00:11:14,380 --> 00:11:15,540
系统表现提高4%

346
00:11:16,000 --> 00:11:19,220
最终是我的逻辑回归分类器 提高大约3%

347
00:11:19,570 --> 00:11:20,580
因此 这很清楚地指出了

348
00:11:20,810 --> 00:11:23,400
哪一个模块是最值得花精力去完善的

349
00:11:24,610 --> 00:11:25,690
顺便一提

350
00:11:25,830 --> 00:11:28,110
我还想讲一个真实的故事

351
00:11:28,680 --> 00:11:29,620
我在预处理这里

352
00:11:29,850 --> 00:11:32,350
放入背景移除这个部分的原因是

353
00:11:32,600 --> 00:11:34,050
我知道一件真实的事情

354
00:11:34,340 --> 00:11:35,530
原来有一个研究小组

355
00:11:35,770 --> 00:11:37,140
大概有两个人

356
00:11:37,480 --> 00:11:38,990
不夸张地说

357
00:11:39,580 --> 00:11:40,250
花了一年半的时间

358
00:11:40,530 --> 00:11:42,410
整整18个月

359
00:11:42,770 --> 00:11:44,050
都在完善背景移除的效果

360
00:11:44,480 --> 00:11:45,680
我不详细地讲

361
00:11:46,120 --> 00:11:47,490
具体的细节和原因是什么

362
00:11:47,970 --> 00:11:48,770
但确实是有两个工程师

363
00:11:48,820 --> 00:11:50,610
为了开发某个

364
00:11:50,720 --> 00:11:51,660
计算机视觉的应用系统

365
00:11:51,770 --> 00:11:52,850
大概花了一年半的时间

366
00:11:52,990 --> 00:11:54,210
就为了得到一个

367
00:11:54,550 --> 00:11:56,050
更好的背景移除效果

368
00:11:56,550 --> 00:11:57,720
事实上他们确实研究出了非常复杂的算法 

369
00:11:57,820 --> 00:12:00,270
貌似最后还发表了一篇文章

370
00:12:01,080 --> 00:12:02,000
但最终他们发现

371
00:12:02,110 --> 00:12:03,020
所有付出的这些劳动

372
00:12:03,260 --> 00:12:04,910
都不能给他们研发系统

373
00:12:05,200 --> 00:12:06,490
的整体表现带来

374
00:12:06,710 --> 00:12:09,120
比较大的提升

375
00:12:09,450 --> 00:12:10,770
而如果要是之前

376
00:12:10,770 --> 00:12:13,170
他们组某个人做一下上限分析

377
00:12:13,700 --> 00:12:15,790
他们就会提前意识到这个问题

378
00:12:17,240 --> 00:12:18,360
后来 他们中有一个人

379
00:12:18,480 --> 00:12:19,510
跟我说 如果他们之前

380
00:12:19,640 --> 00:12:20,580
也做了某种这样的分析

381
00:12:20,850 --> 00:12:21,710
他们就会在长达

382
00:12:21,990 --> 00:12:23,190
18个月的辛苦劳动以前

383
00:12:23,440 --> 00:12:25,180
意识到这个问题

384
00:12:25,240 --> 00:12:26,300
他们就可以把精力花在

385
00:12:26,680 --> 00:12:28,920
其他更重要的模块上

386
00:12:29,380 --> 00:12:31,230
而不是把18个月花在背景移除上

387
00:12:33,910 --> 00:12:36,140
总结一下

388
00:12:36,390 --> 00:12:38,630
流水线是非常常用却又很复杂的机器学习应用

389
00:12:39,890 --> 00:12:40,950
当你在开发某个

390
00:12:41,200 --> 00:12:42,780
机器学习应用的时候

391
00:12:42,830 --> 00:12:45,450
作为一个开发者 你的时间是相当宝贵的

392
00:12:46,090 --> 00:12:47,360
所以真的不要花时间

393
00:12:47,460 --> 00:12:50,120
去做一些到头来没意义的事情

394
00:12:51,350 --> 00:12:52,370
因此在这节课中

395
00:12:52,490 --> 00:12:53,570
我给大家介绍了上限分析的概念

396
00:12:54,340 --> 00:12:55,750
我经常觉得上限分析

397
00:12:55,850 --> 00:12:57,000
在判断应该改进哪个模块上

398
00:12:57,130 --> 00:12:58,660
是个很有用的工具

399
00:12:58,760 --> 00:12:59,830
当你真的把精力

400
00:13:00,050 --> 00:13:01,010
花在那个模块上

401
00:13:01,250 --> 00:13:02,420
并且改进了它

402
00:13:03,050 --> 00:13:04,360
它真的会让整个系统的表现

403
00:13:04,620 --> 00:13:06,040
有一个显著的提高

404
00:13:07,070 --> 00:13:08,010
所以 经过这么多年

405
00:13:08,340 --> 00:13:09,520
在机器学习中的摸爬滚打

406
00:13:09,710 --> 00:13:10,900
我已经学会了不要凭自己的直觉

407
00:13:11,100 --> 00:13:13,200
来判断应该改进哪个模块

408
00:13:13,280 --> 00:13:14,310
虽然我从事

409
00:13:14,540 --> 00:13:15,440
机器学习的工作

410
00:13:15,570 --> 00:13:17,160
已经很多年了

411
00:13:17,360 --> 00:13:18,770
但经常遇到某个机器学习问题时

412
00:13:18,950 --> 00:13:20,130
总有一些直觉告诉我

413
00:13:20,450 --> 00:13:22,970
我们应该跳到那一个模块 应该把时间花在那儿

414
00:13:24,120 --> 00:13:25,050
但经过这么多年

415
00:13:25,160 --> 00:13:26,600
我现在也开始慢慢意识到

416
00:13:26,740 --> 00:13:27,800
还是不能太相信

417
00:13:28,130 --> 00:13:29,310
自己的感觉

418
00:13:29,980 --> 00:13:31,450
相反地 如果要解决某个机器学习问题

419
00:13:31,520 --> 00:13:33,060
最好能把问题

420
00:13:33,180 --> 00:13:34,750
分成多个模块

421
00:13:34,960 --> 00:13:36,340
然后做一下上限分析

422
00:13:36,660 --> 00:13:37,720
这通常是一个更可靠

423
00:13:37,910 --> 00:13:39,110
更好的方法

424
00:13:39,670 --> 00:13:40,900
来为你决定

425
00:13:40,940 --> 00:13:42,270
该把劲儿往哪儿使

426
00:13:42,690 --> 00:13:44,570
该提高哪个模块的效果

427
00:13:44,680 --> 00:13:45,900
这样我们就会非常确信

428
00:13:46,180 --> 00:13:46,960
把这个模块做好就能提高

429
00:13:47,200 --> 00:13:49,460
整个系统的最终表现【教育无边界字幕组】翻译：所罗门捷列夫 校对：竹二个 审核：Naplessss