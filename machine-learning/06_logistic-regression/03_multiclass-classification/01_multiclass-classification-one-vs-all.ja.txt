このビデオでは、ロジスティック回帰を マルチクラスの分類問題に 適用する方法を議論する。 具体的には、one vs all分類と呼ばれる アルゴリズムを話したい。 マルチクラスの分類問題とは何だろう？ ここにその例がある。 例えばあなたのe-mailを 学習アルゴリズムに 別々のフォルダに移動させたい、としよう。 あるいは自動でe-mailにタグをつけたい、とする。 あなたは様々なフォルダやタグを使うだろう、 仕事用のe-mailとか 友達から来たe-mailとか 家族から来たe-mailとか、趣味に関するe-mailとか。 すると、我らは4つのクラスへの 分類問題に直面する訳だ。 そこに数字を振っておくと、 クラスy=1, y=2, y=3, y=4と。 もう一つ別の例としては、 医療の診断とかも考えられる： 患者があなたのオフィスに やってきて、 鼻づまりだと言う。 考えられる診断結果としては、 病気じゃないとか、これをy=1とする、 または風邪とか、これを2とする。 またはインフルエンザかもしれない。 最後に、三番目の例としては、 天気の分類の為に 機械学習を使いたいとする。 例えば晴れ、曇り、 雨、雪を、雪が降りそうな場所ならだが、 分類したいとする。 以上の三つの例は全て yが離散的な 小数の値を とる事が出来る。1から3までとか 1から4までとか。 これらがマルチクラスの分類問題だ。 ところで、インデックスを 0123とふるか1234とふるかは どうでもいい。 私はクラスを1からふって、 0からふらない事が多いが。 だがどっちでもいい。 他方、以前の二択分類問題は 我らのデータセットはこんな感じだった。 マルチクラスの分類問題では、 我らのデータセットはこんな感じになるだろう、 ここで三つの異なる記号を 三つのクラスを表すのに用いた。 そこで問題はこうだ： 三つのクラスのデータセットを与えられて、 ここでは これが一つのクラスの手本で これがそれとは別のクラスの手本で、 これがそれとも別のさらに別のクラスの手本だ。 で、これらの状況で、どうやって学習アルゴリズムを機能させられるだろうか？ 我らは既に、二択の分類を どうやるのかは知っている。 ロジスティック回帰を用いる事で、 陽性と陰性のクラスを直線で どうやって分離するかを知っている。 one vs all分類と言われるアイデアを 用いる事で、 これを、マルチクラスの分類にも 使えるように出来る。 one vs allはこんな風に機能する。 ところで、この戦略はone vs restと呼ばれる事もある。 では、左に示したような トレーニングセットがあるとしよう。 3つのクラスがあり、 y1で三角形を示し、 y2で四角形を y3でクロスを示すとする。 そこでやる事は、 トレーニングセットを持ってきて、 これを三つの異なる、二択問題に変換する事だ。 つまり、私はこれを、三つの別個の 2クラス分類問題に変換する。 クラス1、三角形から始めよう。 本質的には、新しい三角形の 偽のトレーニングセットを作る。 そこではクラス2と3は 陰性のクラスに割り当てて、 クラス1は 陽性のクラスに割り当てる。 こうして右側に示したような トレーニングセットを作り そこに分類器を フィッティングさせる。これを hの下付き添字シータ、 上付き添字1のxと呼ぼう。 ここでは、三角形が陽性の手本で 丸は陰性の手本となる。 つまり三角形に 値1を割り振って、 丸に値0を割り振る。 そして通常のロジスティック回帰の分類器で トレーニングを行う。 その結果、ある決定境界が得られる訳だ。 OK? 上付き添字1は、クラス1を。 つまりこれを最初の三角形のクラスの為に行う。 次に、同様の事をクラス2に対して行う。 四角形をとり、 四角形に陽性のクラスを 割り振って、それ以外の全てに つまり三角とバッテンに陰性のクラスを割り振る。 そして二番目のロジスティック回帰分類器をフィッティングする。 これはhの上付き添字2、と 呼ぶ事にする。ここでこの 上付き添字2は、現在我らは これ、四角形のクラスを 陽性のクラスと扱っている、という事を表している。 そしてこんな分類器が得られるだろう。 そして最後に、同様の事を 三番目のクラスに対して行い、 そして三番目の分類器、 h上付き添字3のxをフィッティングし、 これが我らに こんな感じの決定境界、あるいは 陽性と陰性を分離する分類器を 与える。 まとめよう。我らがやった事は、 3つの分類器をフィッティングした、という事。 i=1, 2, 3について h上付き添字i 下付き添字シータ xの 分類器をフィッティングしていく。 かくして、与えられたxと パラメータシータに対して、 yがiとなる確率を 推計しようと試みる訳だ。 いいかい？ 最初のインスタンスでは、 ここの最初の奴は、 この分類器は三角形で 学習している。 つまり三角形を陽性のクラスとみなしている。 つまり、hの上付き添字1は 本質的には y=1となる確率を 所与のxとパラメータシータの条件の元で 推計している。 同様に、これは四角形のクラスを 陽性の手本と みなしているので、 これはyが2 となる確率を推計しているのだ。以下同様。 つまり今や、我らは3つの分類器を持ち、 おのおのは3つのクラスのうち一つに向けてトレーニングされている。 まとめると、我らがやった事は 我らはロジスティック回帰の 分類器、hの上付き添字iのxを yがiとなる確率を 各h iが推計するように トレーニングしたい。 そして最終的に、 予測を行いたい時には、、、 新規の入力xを与えられた時に、 予測を行いたければ、 我らがやる事は、 我らの手持ちの 3つの分類器を 入力xに対して実行し、 その中で一番大きなクラスのiを 選ぶ、という事をする。 つまり基本的には、 それがなんであれ、 もっとも自信のありそうな分類器を、 言い換えるともっとも大声でこれが正しいクラスだ、と 言っている物を選ぶ。 つまり最も高い確率を 与えるiの値ならなんでもいい。 そしてyの値を予測する。 以上がマルチクラスの分類問題であり、 one vs all法だ。 このちょっとした手法を用いる事で あなたは今や、ロジスティック回帰 を用いて、マルチクラスの分類器の問題で それを同様に機能させる事が出来るようになった。