1
00:00:00,200 --> 00:00:01,596
В этом видео мы поговорим о том,

2
00:00:01,620 --> 00:00:03,659
как применять логистическую регрессию в задачах

3
00:00:03,659 --> 00:00:06,089
многоклассовой классификации, в

4
00:00:06,089 --> 00:00:07,526
частности, я хочу рассказать об алгоритме

5
00:00:07,526 --> 00:00:12,070
классификации «один против всех».

6
00:00:12,150 --> 00:00:14,316
В чем состоит задача многоклассовой классификации?

7
00:00:14,316 --> 00:00:15,945
Вот несколько примеров.

8
00:00:15,945 --> 00:00:17,318
Предположим, вам нужен алгоритм, чтобы научить

9
00:00:17,320 --> 00:00:19,691
программу автоматически распределять электронные

10
00:00:19,710 --> 00:00:21,076
письма по папкам или ставить на них

11
00:00:21,076 --> 00:00:23,398
метки.

12
00:00:23,398 --> 00:00:24,749
То есть, для писем по работе, писем от друзей, от

13
00:00:24,790 --> 00:00:27,052
родственников и писем о вашем хобби у

14
00:00:27,060 --> 00:00:28,236
вас есть разные папки или

15
00:00:28,236 --> 00:00:31,561
разные метки.

16
00:00:31,590 --> 00:00:33,145
Таким образом, это задача

17
00:00:33,145 --> 00:00:34,856
классификации

18
00:00:34,900 --> 00:00:36,164
с четырьмя

19
00:00:36,180 --> 00:00:38,129
классами.

20
00:00:38,129 --> 00:00:41,326
Мы можем

21
00:00:41,326 --> 00:00:43,530
поставить им

22
00:00:44,490 --> 00:00:45,790
в соответствие числа: y = 1, y = 2 , y = 3, y = 4.

23
00:00:46,000 --> 00:00:47,260
Другой пример: постановка

24
00:00:47,800 --> 00:00:48,910
диагноза. К вам приходит пациент,

25
00:00:48,930 --> 00:00:51,395
скажем, с заложенным носом,

26
00:00:51,395 --> 00:00:52,762
тогда диагноз может быть «не болен»,

27
00:00:52,762 --> 00:00:54,140
пусть это будет y = 1,

28
00:00:54,140 --> 00:00:55,474
«простуда», 2, или

29
00:00:55,490 --> 00:00:59,026
«грипп», 3.

30
00:00:59,026 --> 00:01:00,541
Третий и последний пример: если вы

31
00:01:00,541 --> 00:01:02,056
используете машинное

32
00:01:02,090 --> 00:01:03,906
обучение для классификации погоды и

33
00:01:03,910 --> 00:01:05,299
хотите определить, например,

34
00:01:05,299 --> 00:01:07,937
что погода солнечная, пасмурная,

35
00:01:07,950 --> 00:01:10,211
идет дождь или идет снег.

36
00:01:10,230 --> 00:01:11,165
Во всех этих примерах y может

37
00:01:11,165 --> 00:01:12,808
принимать небольшое

38
00:01:12,808 --> 00:01:14,300
количество дискретных значений: от 1 до 3 или от

39
00:01:14,300 --> 00:01:16,498
1 до 4, и так далее.

40
00:01:16,498 --> 00:01:17,810
Задачи такого типа называются

41
00:01:17,890 --> 00:01:20,659
задачами многоклассовой классификации.

42
00:01:20,659 --> 00:01:21,904
К слову, неважно, с чего начинать

43
00:01:21,904 --> 00:01:23,632
нумерацию, вы можете

44
00:01:23,632 --> 00:01:27,063
использовать 0, 1, 2, 3 или 1, 2, 3, 4.

45
00:01:27,090 --> 00:01:29,138
Я привык нумеровать классы,

46
00:01:29,138 --> 00:01:31,569
начиная с единицы, а не с нуля.

47
00:01:31,569 --> 00:01:33,756
Можно и так, и так, это несущественно.

48
00:01:33,756 --> 00:01:35,243
Если раньше, в задаче бинарной классификации,

49
00:01:35,243 --> 00:01:39,375
наш набор данных был таким,

50
00:01:39,375 --> 00:01:41,617
то в случае нескольких классов набор

51
00:01:41,617 --> 00:01:42,792
данных может выглядеть так.

52
00:01:42,792 --> 00:01:44,362
Я использую три разных

53
00:01:44,362 --> 00:01:48,399
символа для обозначения трех классов.

54
00:01:48,410 --> 00:01:49,858
Тогда вопрос стоит так: если в наших

55
00:01:49,858 --> 00:01:51,613
данных есть примеры трех

56
00:01:51,613 --> 00:01:53,193
классов, скажем, вот эти

57
00:01:53,193 --> 00:01:54,651
наблюдения относятся к первому

58
00:01:54,651 --> 00:01:55,768
классу, эти — ко второму, а эти — к

59
00:01:55,790 --> 00:01:58,389
третьему,

60
00:01:58,410 --> 00:02:01,421
как научить алгоритм разделять их?

61
00:02:01,421 --> 00:02:02,598
Мы уже умеем с помощью логистической

62
00:02:02,598 --> 00:02:05,096
регрессии проводить бинарную классификацию,

63
00:02:05,096 --> 00:02:06,594
знаем, как подобрать, скажем,

64
00:02:06,594 --> 00:02:07,736
прямую линию, разделяющую

65
00:02:07,736 --> 00:02:10,613
положительные и отрицательные случаи.

66
00:02:10,613 --> 00:02:12,116
При помощи идеи «один против всех» мы

67
00:02:12,116 --> 00:02:14,399
можем применить тот

68
00:02:14,400 --> 00:02:15,730
же подход для задач

69
00:02:15,730 --> 00:02:18,646
многоклассовой классификации.

70
00:02:18,650 --> 00:02:21,617
Вот как это будет работать.

71
00:02:21,620 --> 00:02:25,777
Иногда этот алгоритм также называют

72
00:02:25,777 --> 00:02:26,941
«один против остальных».

73
00:02:26,941 --> 00:02:28,138
Пусть наш обучающий набор

74
00:02:28,150 --> 00:02:30,456
выглядит, как на рисунке слева. У нас есть три класса:

75
00:02:30,470 --> 00:02:32,310
если y = 1, обозначим этот случай

76
00:02:32,310 --> 00:02:34,405
треугольником, если y = 2 — квадратом,

77
00:02:34,405 --> 00:02:37,970
если y = 3 — крестиком.

78
00:02:37,980 --> 00:02:39,460
Теперь мы возьмем этот обучающий набор и

79
00:02:39,480 --> 00:02:41,350
будем решать три независимые задачи

80
00:02:41,350 --> 00:02:44,816
бинарной классификации.

81
00:02:44,816 --> 00:02:46,719
То есть я превращу нашу задачу в три

82
00:02:46,750 --> 00:02:49,450
различных задачи с двумя классами.

83
00:02:49,450 --> 00:02:51,660
Начнем с класса 1, то есть с треугольников.

84
00:02:51,660 --> 00:02:52,990
По сути, мы создадим новый, как

85
00:02:53,050 --> 00:02:55,418
бы фиктивный, обучающий набор,

86
00:02:55,440 --> 00:02:56,913
в котором классы 2 и 3 превратятся

87
00:02:56,920 --> 00:02:58,151
в отрицательный класс, а

88
00:02:58,151 --> 00:02:59,873
класс 1 будет

89
00:02:59,873 --> 00:03:01,134
положительным классом.

90
00:03:01,134 --> 00:03:02,352
Получится обучающий набор

91
00:03:02,380 --> 00:03:03,700
как на рисунке

92
00:03:03,700 --> 00:03:05,508
справа. И теперь мы

93
00:03:05,508 --> 00:03:07,573
построим классификатор, я

94
00:03:07,573 --> 00:03:10,200
обозначу его h(x) c нижним

95
00:03:10,220 --> 00:03:12,626
индексом тета и верхним

96
00:03:12,640 --> 00:03:15,659
индексом (1), который

97
00:03:15,659 --> 00:03:19,008
будет относить треугольники к положительному классу, а кружочки — к отрицательному.

98
00:03:19,008 --> 00:03:20,649
То есть треугольникам

99
00:03:20,649 --> 00:03:21,800
соответствует значение 1, а

100
00:03:21,800 --> 00:03:25,291
кружочкам — значение 0.

101
00:03:25,300 --> 00:03:26,723
Мы просто применим обычную

102
00:03:26,723 --> 00:03:29,556
логистическую регрессию

103
00:03:29,556 --> 00:03:34,173
для бинарной классификации, и, допустим, получим такую границу решений.

104
00:03:34,173 --> 00:03:34,173
Понятно?

105
00:03:34,890 --> 00:03:37,693
Верхний индекс (1) обозначает класс 1.

106
00:03:37,693 --> 00:03:40,777
То есть мы отделили класс треугольников, класс 1.

107
00:03:40,800 --> 00:03:42,302
Теперь сделаем то же для класса 2:

108
00:03:42,302 --> 00:03:44,013
выделим квадраты

109
00:03:44,020 --> 00:03:45,456
в положительный класс,

110
00:03:45,470 --> 00:03:47,001
а все остальное — треугольники и крестики — отнесем к

111
00:03:47,001 --> 00:03:50,213
отрицательному классу,

112
00:03:50,220 --> 00:03:54,173
и с помощью логистической регрессии обучим второй классификатор.

113
00:03:54,173 --> 00:03:56,410
Я обозначу его h(x) с верхним

114
00:03:56,420 --> 00:03:58,352
индексом (2), что соответствует определению

115
00:03:58,352 --> 00:04:00,029
квадратов, второго класса, как положительных

116
00:04:00,029 --> 00:04:01,860
примеров. У нас, получится, к

117
00:04:01,870 --> 00:04:03,310
примеру, такой

118
00:04:03,350 --> 00:04:07,518
классификатор.

119
00:04:07,518 --> 00:04:08,854
Наконец, обучим третий классификатор

120
00:04:08,854 --> 00:04:10,143
для третьего класса, h(x) с верхним

121
00:04:10,143 --> 00:04:11,598
индексом (3), и он даст нам,

122
00:04:11,610 --> 00:04:14,632
предположим, такую

123
00:04:14,632 --> 00:04:16,424
границу решений,

124
00:04:16,440 --> 00:04:18,106
то есть будет разделять

125
00:04:18,106 --> 00:04:19,749
положительные и

126
00:04:19,750 --> 00:04:22,863
отрицательные примеры вот так.

127
00:04:22,870 --> 00:04:24,353
Итак, пока что мы подобрали

128
00:04:24,353 --> 00:04:27,872
три классификатора.

129
00:04:27,890 --> 00:04:29,403
Для i, равного 1, 2 и 3 мы

130
00:04:29,403 --> 00:04:31,836
обучили классификатор h(x) с

131
00:04:31,880 --> 00:04:33,855
верхним индексом (i),

132
00:04:33,855 --> 00:04:35,193
который оценивает

133
00:04:35,220 --> 00:04:36,446
вероятность того, что y

134
00:04:36,450 --> 00:04:38,208
равняется i при данном x

135
00:04:38,208 --> 00:04:41,834
и векторе параметров тета.

136
00:04:41,834 --> 00:04:41,834
Так?

137
00:04:41,834 --> 00:04:43,229
В первом случае, наверху,

138
00:04:43,230 --> 00:04:44,903
классификатор учится

139
00:04:44,910 --> 00:04:47,277
отличать

140
00:04:47,280 --> 00:04:49,364
треугольники.

141
00:04:49,364 --> 00:04:52,037
То есть, считает треугольники положительным классом.

142
00:04:52,060 --> 00:04:53,840
И h(x) с верхним индексом (1), по

143
00:04:53,840 --> 00:04:55,163
сути, рассчитывает вероятность

144
00:04:55,170 --> 00:04:57,343
того, что y = 1 при

145
00:04:57,350 --> 00:04:59,083
данном x и векторе

146
00:04:59,083 --> 00:05:02,037
параметров тета.

147
00:05:02,037 --> 00:05:04,475
Аналогично, второй классификатор

148
00:05:04,480 --> 00:05:05,859
относит к положительному

149
00:05:05,859 --> 00:05:07,400
классу квадраты и

150
00:05:07,400 --> 00:05:10,748
рассчитывает вероятность того, что y = 2, и так далее.

151
00:05:10,750 --> 00:05:13,300
Итак, у нас есть три классификатора, каждый

152
00:05:13,310 --> 00:05:16,649
из которых обучен распознавать один из трех классов.

153
00:05:16,670 --> 00:05:17,859
Резюмирую, что мы сделали, для общего

154
00:05:17,860 --> 00:05:19,685
случая. Для каждого

155
00:05:19,700 --> 00:05:21,280
класса i мы с помощью

156
00:05:21,300 --> 00:05:23,560
логистической регрессии обучаем

157
00:05:23,560 --> 00:05:24,947
классификатор h(x) с верхним

158
00:05:24,950 --> 00:05:26,183
индексом (i), предсказывающий,

159
00:05:26,183 --> 00:05:28,550
насколько вероятно, что y = i.

160
00:05:28,570 --> 00:05:29,740
Теперь, чтобы

161
00:05:29,820 --> 00:05:31,772
предсказать, к какому

162
00:05:31,772 --> 00:05:33,326
классу относится новый случай x, мы

163
00:05:33,340 --> 00:05:34,729
запускаем все классификаторы, в нашем

164
00:05:34,730 --> 00:05:36,706
примере все три, и

165
00:05:36,706 --> 00:05:38,557
затем указываем

166
00:05:38,557 --> 00:05:40,010
класс i, для которого полученное

167
00:05:40,010 --> 00:05:41,535
значение

168
00:05:41,535 --> 00:05:44,068
было максимальным.

169
00:05:44,068 --> 00:05:45,387
То есть, по сути, мы

170
00:05:45,387 --> 00:05:47,180
выбираем тот классификатор

171
00:05:47,180 --> 00:05:49,163
из трех, который наиболее

172
00:05:49,210 --> 00:05:52,178
уверенно, с наибольшим

173
00:05:52,178 --> 00:05:54,352
энтузиазмом утверждает, что случай попал в его класс.

174
00:05:54,352 --> 00:05:56,153
Для какого i мы получили самую высокую

175
00:05:56,190 --> 00:05:58,069
вероятность, такое значение y мы и

176
00:05:58,069 --> 00:06:01,056
предсказываем.

177
00:06:02,660 --> 00:06:04,453
Вот так работает многоклассовая классификация и

178
00:06:04,470 --> 00:06:07,677
алгоритм «один против всех».

179
00:06:07,677 --> 00:06:09,120
Этот нехитрый прием позволяет вам

180
00:06:09,120 --> 00:06:10,521
использовать

181
00:06:10,521 --> 00:06:12,033
логистическую регрессию и в задачах

182
00:06:12,033 --> 00:06:15,051
многоклассовой классификации.