В этом видео мы поговорим о том, как применять логистическую регрессию в задачах многоклассовой классификации, в частности, я хочу рассказать об алгоритме классификации «один против всех». В чем состоит задача многоклассовой классификации? Вот несколько примеров. Предположим, вам нужен алгоритм, чтобы научить программу автоматически распределять электронные письма по папкам или ставить на них метки. То есть, для писем по работе, писем от друзей, от родственников и писем о вашем хобби у вас есть разные папки или разные метки. Таким образом, это задача классификации с четырьмя классами. Мы можем поставить им в соответствие числа: y = 1, y = 2 , y = 3, y = 4. Другой пример: постановка диагноза. К вам приходит пациент, скажем, с заложенным носом, тогда диагноз может быть «не болен», пусть это будет y = 1, «простуда», 2, или «грипп», 3. Третий и последний пример: если вы используете машинное обучение для классификации погоды и хотите определить, например, что погода солнечная, пасмурная, идет дождь или идет снег. Во всех этих примерах y может принимать небольшое количество дискретных значений: от 1 до 3 или от 1 до 4, и так далее. Задачи такого типа называются задачами многоклассовой классификации. К слову, неважно, с чего начинать нумерацию, вы можете использовать 0, 1, 2, 3 или 1, 2, 3, 4. Я привык нумеровать классы, начиная с единицы, а не с нуля. Можно и так, и так, это несущественно. Если раньше, в задаче бинарной классификации, наш набор данных был таким, то в случае нескольких классов набор данных может выглядеть так. Я использую три разных символа для обозначения трех классов. Тогда вопрос стоит так: если в наших данных есть примеры трех классов, скажем, вот эти наблюдения относятся к первому классу, эти — ко второму, а эти — к третьему, как научить алгоритм разделять их? Мы уже умеем с помощью логистической регрессии проводить бинарную классификацию, знаем, как подобрать, скажем, прямую линию, разделяющую положительные и отрицательные случаи. При помощи идеи «один против всех» мы можем применить тот же подход для задач многоклассовой классификации. Вот как это будет работать. Иногда этот алгоритм также называют «один против остальных». Пусть наш обучающий набор выглядит, как на рисунке слева. У нас есть три класса: если y = 1, обозначим этот случай треугольником, если y = 2 — квадратом, если y = 3 — крестиком. Теперь мы возьмем этот обучающий набор и будем решать три независимые задачи бинарной классификации. То есть я превращу нашу задачу в три различных задачи с двумя классами. Начнем с класса 1, то есть с треугольников. По сути, мы создадим новый, как бы фиктивный, обучающий набор, в котором классы 2 и 3 превратятся в отрицательный класс, а класс 1 будет положительным классом. Получится обучающий набор как на рисунке справа. И теперь мы построим классификатор, я обозначу его h(x) c нижним индексом тета и верхним индексом (1), который будет относить треугольники к положительному классу, а кружочки — к отрицательному. То есть треугольникам соответствует значение 1, а кружочкам — значение 0. Мы просто применим обычную логистическую регрессию для бинарной классификации, и, допустим, получим такую границу решений. Понятно? Верхний индекс (1) обозначает класс 1. То есть мы отделили класс треугольников, класс 1. Теперь сделаем то же для класса 2: выделим квадраты в положительный класс, а все остальное — треугольники и крестики — отнесем к отрицательному классу, и с помощью логистической регрессии обучим второй классификатор. Я обозначу его h(x) с верхним индексом (2), что соответствует определению квадратов, второго класса, как положительных примеров. У нас, получится, к примеру, такой классификатор. Наконец, обучим третий классификатор для третьего класса, h(x) с верхним индексом (3), и он даст нам, предположим, такую границу решений, то есть будет разделять положительные и отрицательные примеры вот так. Итак, пока что мы подобрали три классификатора. Для i, равного 1, 2 и 3 мы обучили классификатор h(x) с верхним индексом (i), который оценивает вероятность того, что y равняется i при данном x и векторе параметров тета. Так? В первом случае, наверху, классификатор учится отличать треугольники. То есть, считает треугольники положительным классом. И h(x) с верхним индексом (1), по сути, рассчитывает вероятность того, что y = 1 при данном x и векторе параметров тета. Аналогично, второй классификатор относит к положительному классу квадраты и рассчитывает вероятность того, что y = 2, и так далее. Итак, у нас есть три классификатора, каждый из которых обучен распознавать один из трех классов. Резюмирую, что мы сделали, для общего случая. Для каждого класса i мы с помощью логистической регрессии обучаем классификатор h(x) с верхним индексом (i), предсказывающий, насколько вероятно, что y = i. Теперь, чтобы предсказать, к какому классу относится новый случай x, мы запускаем все классификаторы, в нашем примере все три, и затем указываем класс i, для которого полученное значение было максимальным. То есть, по сути, мы выбираем тот классификатор из трех, который наиболее уверенно, с наибольшим энтузиазмом утверждает, что случай попал в его класс. Для какого i мы получили самую высокую вероятность, такое значение y мы и предсказываем. Вот так работает многоклассовая классификация и алгоритм «один против всех». Этот нехитрый прием позволяет вам использовать логистическую регрессию и в задачах многоклассовой классификации.