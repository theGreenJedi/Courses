1
00:00:00,160 --> 00:00:01,704
В этом видео мы обсудим, как подбирать значения

2
00:00:01,704 --> 00:00:04,010
параметров тета в задачах логистической

3
00:00:04,040 --> 00:00:05,869
регрессии.

4
00:00:05,880 --> 00:00:06,982
В частности, я хочу определить целевую функцию оптимизации,

5
00:00:07,020 --> 00:00:10,386
функцию стоимости, которую мы будем использовать для подбора

6
00:00:10,400 --> 00:00:14,470
параметров.

7
00:00:15,390 --> 00:00:17,370
Вот наша задача обучения с учителем. Нам нужно найти подходящие

8
00:00:17,370 --> 00:00:19,892
параметры для модели логистической регрессии.

9
00:00:19,960 --> 00:00:22,210
У нас есть обучающий набор из

10
00:00:22,210 --> 00:00:24,964
m учебных примеров.

11
00:00:24,964 --> 00:00:26,577
Как обычно, каждый из наших примеров представляет

12
00:00:26,577 --> 00:00:28,130
собой вектор параметров

13
00:00:28,150 --> 00:00:32,830
размерности n+1.

14
00:00:32,830 --> 00:00:35,133
И, как обычно, х нулевое

15
00:00:35,180 --> 00:00:36,498
равно 1.

16
00:00:36,498 --> 00:00:38,315
Первая характеристика, то есть характеристика

17
00:00:38,315 --> 00:00:39,951
номер ноль, всегда равняется

18
00:00:39,970 --> 00:00:41,203
единице. И поскольку это задача

19
00:00:41,203 --> 00:00:43,335
классификации, для каждого примера

20
00:00:43,350 --> 00:00:44,999
из обучающего набора

21
00:00:45,010 --> 00:00:48,422
y равняется нулю или единице.

22
00:00:48,422 --> 00:00:50,576
Это гипотеза, а

23
00:00:50,576 --> 00:00:52,007
вектор ее

24
00:00:52,007 --> 00:00:54,460
параметров тета — здесь.

25
00:00:54,490 --> 00:00:55,572
И вопрос, который я хочу

26
00:00:55,610 --> 00:00:57,339
разобрать — как найти, как подобрать параметры тета

27
00:00:57,340 --> 00:00:58,846
исходя из этого обучающего

28
00:00:58,880 --> 00:01:02,482
набора.

29
00:01:02,510 --> 00:01:04,125
Когда мы строили модель линейной регрессии, наша функция

30
00:01:04,125 --> 00:01:08,463
затрат выглядела так.

31
00:01:08,480 --> 00:01:10,868
Я записал ее немного по

32
00:01:10,900 --> 00:01:12,663
другому: перенес 1/2 из единицы,

33
00:01:12,670 --> 00:01:16,440
деленной на 2m, под знак суммы.

34
00:01:16,440 --> 00:01:17,440
Это позволит

35
00:01:17,440 --> 00:01:19,132
мне

36
00:01:19,140 --> 00:01:20,663
представить функцию

37
00:01:20,700 --> 00:01:22,009
затрат иначе.

38
00:01:22,030 --> 00:01:23,920
Вместо этого квадрата

39
00:01:23,920 --> 00:01:27,100
ошибки я

40
00:01:28,310 --> 00:01:31,476
напишу «стоимость» от h(x) и y, а «стоимость» определю

41
00:01:31,500 --> 00:01:33,605
равной

42
00:01:33,605 --> 00:01:37,176
этому

43
00:01:37,210 --> 00:01:39,727
выражению.

44
00:01:39,740 --> 00:01:42,641
Тому самому квадрату ошибки, деленному пополам.

45
00:01:42,670 --> 00:01:43,800
В такой записи яснее

46
00:01:43,800 --> 00:01:46,018
видно, что функция затрат

47
00:01:46,018 --> 00:01:48,145
равна средней ошибке по обучающему

48
00:01:48,145 --> 00:01:49,740
набору, то есть 1/m умножить на

49
00:01:49,740 --> 00:01:51,427
сумму этих выражений для обучающих

50
00:01:51,427 --> 00:01:56,046
примеров.

51
00:01:56,050 --> 00:01:58,065
Чтобы упростить еще немного, я

52
00:01:58,065 --> 00:01:59,470
уберу верхние

53
00:01:59,490 --> 00:02:02,587
индексы.

54
00:02:02,610 --> 00:02:04,408
То есть

55
00:02:04,408 --> 00:02:05,527
«стоимость» от h(x) и y

56
00:02:05,527 --> 00:02:06,618
равна половине

57
00:02:06,618 --> 00:02:08,925
квадрата ошибки, и по сути эта

58
00:02:08,925 --> 00:02:10,336
величина — цена, которую я хочу,

59
00:02:10,360 --> 00:02:11,876
чтобы мой алгоритм

60
00:02:11,890 --> 00:02:13,447
«заплатил», если он

61
00:02:13,460 --> 00:02:15,110
выдает предсказание

62
00:02:15,110 --> 00:02:16,701
h(x) при том,

63
00:02:16,750 --> 00:02:18,737
что реальное

64
00:02:18,737 --> 00:02:19,912
значение

65
00:02:19,912 --> 00:02:21,258
равно y.

66
00:02:21,310 --> 00:02:24,035
Так что я просто зачеркну

67
00:02:24,050 --> 00:02:27,836
эти индексы.Так?

68
00:02:27,840 --> 00:02:29,756
Для линейной регрессии

69
00:02:29,756 --> 00:02:31,537
мы установили именно такую цену,

70
00:02:31,537 --> 00:02:32,757
то есть половину

71
00:02:32,757 --> 00:02:34,535
квадрата

72
00:02:34,540 --> 00:02:36,232
разницы между

73
00:02:36,232 --> 00:02:37,663
предсказанным и наблюдаемым значением y.

74
00:02:37,670 --> 00:02:38,943
Ну хорошо, такая

75
00:02:38,943 --> 00:02:41,103
функция затрат сработала для линейной

76
00:02:41,103 --> 00:02:42,848
регрессии, но мы - то имеем дело с

77
00:02:42,848 --> 00:02:47,418
логистической.

78
00:02:47,430 --> 00:02:49,146
Если бы мы могли минимизировать

79
00:02:49,150 --> 00:02:51,992
эту функцию затрат,

80
00:02:52,020 --> 00:02:53,817
она бы нам подошла.

81
00:02:53,817 --> 00:02:55,476
Но такая функция затрат

82
00:02:55,480 --> 00:02:57,640
для логистической регрессии

83
00:02:57,640 --> 00:03:01,807
оказывается невыпуклой функцией параметров тета.

84
00:03:01,820 --> 00:03:03,968
Вот что я имею в виду под невыпуклой.

85
00:03:03,990 --> 00:03:05,313
Попробуем изобразить нашу

86
00:03:05,313 --> 00:03:08,118
функцию затрат J от

87
00:03:08,140 --> 00:03:12,113
тета. В логистической

88
00:03:12,113 --> 00:03:13,495
регрессии функция h нелинейна, верно?

89
00:03:13,500 --> 00:03:14,538
Она равна единице, деленной на

90
00:03:14,538 --> 00:03:16,384
один плюс e в степени

91
00:03:16,384 --> 00:03:19,591
минус транспонированный вектор тета на x. Довольно сложная нелинейная функция.

92
00:03:19,591 --> 00:03:21,108
И если мы

93
00:03:21,130 --> 00:03:22,104
возьмем эту

94
00:03:22,104 --> 00:03:23,239
сигмоиду, подставим ее сюда, затем

95
00:03:23,300 --> 00:03:25,016
возьмем эту стоимость и

96
00:03:25,020 --> 00:03:26,746
подставим ее сюда, получится

97
00:03:26,746 --> 00:03:28,200
функция J, график которой может

98
00:03:28,210 --> 00:03:29,650
выглядеть

99
00:03:29,650 --> 00:03:33,493
как-то так.

100
00:03:33,500 --> 00:03:35,958
Со множеством

101
00:03:35,958 --> 00:03:37,321
локальных минимумов. Такие

102
00:03:37,340 --> 00:03:39,488
функции называются невыпуклыми.

103
00:03:39,500 --> 00:03:40,644
Сами видите,

104
00:03:40,644 --> 00:03:41,880
если применить градиентный

105
00:03:41,880 --> 00:03:43,192
спуск к такой функции,

106
00:03:43,192 --> 00:03:45,160
он необязательно придет

107
00:03:45,170 --> 00:03:47,747
в глобальный минимум.

108
00:03:47,747 --> 00:03:48,867
Мы же, напротив, хотели

109
00:03:48,870 --> 00:03:50,350
бы получить

110
00:03:50,350 --> 00:03:52,100
выпуклую функцию стоимости J

111
00:03:52,100 --> 00:03:53,599
от тета, график

112
00:03:53,599 --> 00:03:55,250
которой —

113
00:03:55,250 --> 00:03:56,675
одна чашеобразная кривая, чтобы

114
00:03:56,675 --> 00:03:58,543
градиентный спуск гарантированно

115
00:03:58,543 --> 00:04:01,147
сошелся в

116
00:04:01,170 --> 00:04:04,917
глобальном минимуме.

117
00:04:04,917 --> 00:04:07,020
Таким

118
00:04:07,020 --> 00:04:08,460
образом, проблема с использованием

119
00:04:08,520 --> 00:04:10,400
квадратичной

120
00:04:10,400 --> 00:04:12,371
ошибки в том, что

121
00:04:12,371 --> 00:04:14,107
из-за нелинейного элемента,

122
00:04:14,107 --> 00:04:15,987
сигмоиды, которая

123
00:04:15,987 --> 00:04:17,962
входит вот сюда, J от тета

124
00:04:17,962 --> 00:04:21,294
становится невыпуклой.

125
00:04:21,294 --> 00:04:22,313
Так что нам нужно

126
00:04:22,320 --> 00:04:23,822
найти другую функцию затрат,

127
00:04:23,822 --> 00:04:25,576
выпуклую, чтобы мы

128
00:04:25,576 --> 00:04:28,063
могли применять

129
00:04:28,063 --> 00:04:29,257
мощный алгоритм вроде

130
00:04:29,280 --> 00:04:30,919
градиентного спуска и быть уверены,

131
00:04:30,940 --> 00:04:33,683
что получим глобальный минимум.

132
00:04:33,683 --> 00:04:37,295
И вот какую функцию стоимости мы будем использовать для логистической регрессии.

133
00:04:37,295 --> 00:04:39,313
Мы положим цену или

134
00:04:39,320 --> 00:04:40,710
штраф,

135
00:04:40,710 --> 00:04:42,924
который

136
00:04:42,924 --> 00:04:44,596
алгоритм платит, предсказывая h(x) —

137
00:04:44,620 --> 00:04:46,722
какое-то число, например, 0,7, — когда

138
00:04:46,722 --> 00:04:48,670
в действительности

139
00:04:48,670 --> 00:04:50,780
значение

140
00:04:50,780 --> 00:04:52,032
метки равно y,

141
00:04:52,032 --> 00:04:54,087
равным минус

142
00:04:54,090 --> 00:04:56,061
логарифму h(x), если y

143
00:04:56,100 --> 00:04:57,861
равно 1,

144
00:04:57,861 --> 00:04:59,447
и минус логарифму от единицы

145
00:04:59,460 --> 00:05:02,010
минус h(x), если y равно 0.

146
00:05:02,020 --> 00:05:04,205
На вид это довольно замысловатая функция,

147
00:05:04,230 --> 00:05:05,773
но давайте построим ее график

148
00:05:05,773 --> 00:05:08,147
и попробуем понять, как она устроена.

149
00:05:08,160 --> 00:05:11,054
Начнем со случая, когда y = 1.

150
00:05:11,070 --> 00:05:12,461
Если y = 1,

151
00:05:12,461 --> 00:05:14,958
функция

152
00:05:14,958 --> 00:05:18,240
стоимости

153
00:05:18,240 --> 00:05:19,601
равна −log(h(x)), и ее

154
00:05:19,601 --> 00:05:21,564
график...

155
00:05:21,580 --> 00:05:22,961
Пусть наша ось абсцисс

156
00:05:22,961 --> 00:05:24,722
соответствует h(x). Значение

157
00:05:24,730 --> 00:05:26,611
гипотезы всегда

158
00:05:26,630 --> 00:05:28,465
лежит между 0 и 1.

159
00:05:28,465 --> 00:05:28,465
Так?

160
00:05:28,490 --> 00:05:30,514
Значит, h(x) лежит в пределах

161
00:05:30,530 --> 00:05:31,940
от 0 до 1.

162
00:05:31,940 --> 00:05:35,469
Итак, если вы построите график функции затрат,

163
00:05:35,470 --> 00:05:37,981
он окажется таким.

164
00:05:37,981 --> 00:05:39,044
Это можно понять, построив

165
00:05:39,044 --> 00:05:41,363
сначала график

166
00:05:41,440 --> 00:05:44,988
функции log(z),

167
00:05:45,000 --> 00:05:47,656
где z откладывается по оси абсцисс.

168
00:05:47,656 --> 00:05:48,794
Он выглядит так.

169
00:05:48,794 --> 00:05:50,369
Здесь он уходит к минус бесконечности.

170
00:05:50,369 --> 00:05:53,700
Вот как выглядит логарифмическая функция.

171
00:05:53,700 --> 00:05:55,963
Здесь ноль, здесь единица.

172
00:05:55,980 --> 00:05:57,560
А z, конечно же,

173
00:05:57,560 --> 00:05:59,653
играет

174
00:05:59,653 --> 00:06:02,030
роль h(x). Соответственно, −log(z)

175
00:06:02,030 --> 00:06:06,329
выглядит так:

176
00:06:06,330 --> 00:06:08,098
просто поменяем знак,

177
00:06:08,100 --> 00:06:09,822
получится минус логарифм.

178
00:06:09,822 --> 00:06:11,013
А поскольку нас

179
00:06:11,020 --> 00:06:12,580
интересуют значения функции

180
00:06:12,610 --> 00:06:14,014
только на интервале от 0 до 1,

181
00:06:14,014 --> 00:06:15,924
эту часть можно отбросить,

182
00:06:15,924 --> 00:06:17,962
и у нас останется вот эта часть

183
00:06:17,980 --> 00:06:21,555
кривой.

184
00:06:21,630 --> 00:06:23,200
Именно так выглядит график слева.

185
00:06:23,200 --> 00:06:25,472
И у этой функции затрат есть несколько интересных и

186
00:06:25,500 --> 00:06:29,666
полезных свойств.

187
00:06:29,690 --> 00:06:32,103
Во-первых, обратите внимание,

188
00:06:32,103 --> 00:06:35,003
что когда y = 1 и h(x) = 1,

189
00:06:35,010 --> 00:06:37,367
то есть если предсказанное

190
00:06:37,410 --> 00:06:39,000
гипотезой значение

191
00:06:39,000 --> 00:06:40,261
в точности равно

192
00:06:40,261 --> 00:06:42,744
известному нам y,

193
00:06:42,744 --> 00:06:44,432
то стоимость равна нулю.

194
00:06:44,432 --> 00:06:44,432
Так?

195
00:06:44,432 --> 00:06:47,475
На самом деле кривая здесь не становится горизонтальной,

196
00:06:47,475 --> 00:06:49,866
она пересекает ось.

197
00:06:49,880 --> 00:06:51,006
Итак, если h(x) = 1, если

198
00:06:51,006 --> 00:06:53,056
гипотеза предсказывает

199
00:06:53,056 --> 00:06:55,113
y = 1,

200
00:06:55,113 --> 00:06:56,342
и это в самом деле так, то

201
00:06:56,342 --> 00:06:58,502
стоимость равна нулю.

202
00:06:58,530 --> 00:07:00,975
Вот эта точка на графике.

203
00:07:00,975 --> 00:07:00,975
Так?

204
00:07:01,030 --> 00:07:02,332
Поскольку мы пока

205
00:07:02,332 --> 00:07:04,068
рассматриваем случай

206
00:07:04,068 --> 00:07:06,273
y = 1,

207
00:07:06,273 --> 00:07:08,366
когда h(x) равно 1,

208
00:07:08,366 --> 00:07:11,063
cтоимость равна нулю.

209
00:07:11,063 --> 00:07:13,082
А этого нам и надо.

210
00:07:13,082 --> 00:07:13,968
Потому что, если мы правильно предсказали y, стоимость

211
00:07:13,968 --> 00:07:17,673
должна быть нулевой.

212
00:07:17,673 --> 00:07:21,466
Во-вторых, обратите внимание,

213
00:07:21,470 --> 00:07:23,456
что когда h(x) приближается к нулю,

214
00:07:23,456 --> 00:07:25,037
когда предсказание гипотезы

215
00:07:25,037 --> 00:07:26,909
близко к нулю, стоимость

216
00:07:26,909 --> 00:07:30,163
резко возрастает и стремится к бесконечности.

217
00:07:30,163 --> 00:07:31,513
Это воплощает наше

218
00:07:31,513 --> 00:07:34,271
понимание гипотезы: если гипотеза

219
00:07:34,310 --> 00:07:36,890
предсказывает 0,

220
00:07:36,890 --> 00:07:38,574
то есть утверждает, что

221
00:07:38,574 --> 00:07:39,960
вероятность y = 1 равняется

222
00:07:39,960 --> 00:07:41,541
нулю,

223
00:07:41,541 --> 00:07:42,516
а это все равно что

224
00:07:42,520 --> 00:07:44,010
прийти к пациенту

225
00:07:44,020 --> 00:07:45,594
и сказать ему:

226
00:07:45,610 --> 00:07:47,337
«вероятность того,

227
00:07:47,337 --> 00:07:49,807
что ваша опухоль злокачественная,

228
00:07:49,807 --> 00:07:52,154
равняется нулю», то есть сказать,

229
00:07:52,160 --> 00:07:55,130
что это совершенно невероятно,

230
00:07:55,150 --> 00:07:56,776
но при этом на деле опухоль

231
00:07:56,776 --> 00:08:00,111
пациента злокачественная,

232
00:08:00,111 --> 00:08:01,879
то есть в действительности

233
00:08:01,880 --> 00:08:03,291
y = 1, хотя мы и сказали, что вероятность

234
00:08:03,300 --> 00:08:05,375
этого равна нулю,

235
00:08:05,390 --> 00:08:08,716
что опухоль никак не может быть злокачественной, —

236
00:08:08,716 --> 00:08:09,759
так вот, если мы с большой

237
00:08:09,760 --> 00:08:11,186
уверенностью предсказали, что y не равно 1, и

238
00:08:11,240 --> 00:08:13,018
оказались неправы,

239
00:08:13,018 --> 00:08:14,688
нам нужно наложить на

240
00:08:14,690 --> 00:08:16,122
алгоритм очень-очень

241
00:08:16,122 --> 00:08:17,963
большой штраф.

242
00:08:17,963 --> 00:08:20,474
И этому соответствует рост

243
00:08:20,474 --> 00:08:21,900
стоимости к бесконечности в случае,

244
00:08:21,900 --> 00:08:24,334
когда y = 1, а h(x) приближается к нулю.

245
00:08:24,334 --> 00:08:26,725
Мы рассмотрели

246
00:08:26,725 --> 00:08:28,875
случай y = 1, теперь посмотрим,

247
00:08:28,875 --> 00:08:32,371
как выглядит функция затрат при y = 0.

248
00:08:32,410 --> 00:08:35,710
Когда y = 0, стоимость определяется вот этим

249
00:08:35,720 --> 00:08:39,121
выражением.

250
00:08:39,121 --> 00:08:40,403
График

251
00:08:40,403 --> 00:08:42,751
функции −log(1 − z)

252
00:08:42,780 --> 00:08:45,839
выглядит

253
00:08:45,839 --> 00:08:49,245
так.

254
00:08:49,245 --> 00:08:50,256
Здесь у меня 0, здесь 1.

255
00:08:50,270 --> 00:08:53,263
Что-то вроде этого.

256
00:08:53,280 --> 00:08:54,611
Соответственно, так

257
00:08:54,611 --> 00:08:55,872
выглядит и график функции затрат

258
00:08:55,872 --> 00:08:57,823
в случае y = 0. Кривая теперь

259
00:08:57,823 --> 00:09:00,763
резко возрастает,

260
00:09:00,763 --> 00:09:02,404
уходит к бесконечности

261
00:09:02,404 --> 00:09:04,937
при приближении h(x) к

262
00:09:04,937 --> 00:09:08,273
единице.

263
00:09:08,290 --> 00:09:09,880
Поскольку это значит, что

264
00:09:09,900 --> 00:09:11,199
при y = 0 мы

265
00:09:11,200 --> 00:09:12,168
предсказали, что y = 1

266
00:09:12,168 --> 00:09:13,966
почти наверняка, с

267
00:09:13,966 --> 00:09:15,286
вероятностью,

268
00:09:15,320 --> 00:09:17,281
близкой к единице, и

269
00:09:17,281 --> 00:09:21,569
тогда нам нужно заплатить очень большой штраф.

270
00:09:21,569 --> 00:09:24,609
И, наоборот, если h(x) = 0

271
00:09:24,610 --> 00:09:25,942
притом, что y = 0,

272
00:09:25,950 --> 00:09:27,483
гипотеза попала в

273
00:09:27,483 --> 00:09:28,983
яблочко.

274
00:09:29,000 --> 00:09:30,626
Предсказанное значение — ноль,

275
00:09:30,630 --> 00:09:32,371
и y в действительности

276
00:09:32,371 --> 00:09:34,376
равно нулю,

277
00:09:34,376 --> 00:09:36,701
так что в этой

278
00:09:36,750 --> 00:09:40,139
точке стоимость нулевая.

279
00:09:40,160 --> 00:09:42,163
В этом видео

280
00:09:42,163 --> 00:09:43,886
мы рассмотрели функцию затрат

281
00:09:43,886 --> 00:09:46,428
для одного обучающего примера.

282
00:09:46,428 --> 00:09:50,251
Вопрос анализа выпуклости лежит за пределами этого курса,

283
00:09:50,270 --> 00:09:51,594
но можно показать, что при

284
00:09:51,620 --> 00:09:53,080
таком выборе функции затрат

285
00:09:53,150 --> 00:09:54,774
мы получим выпуклую

286
00:09:54,774 --> 00:09:57,926
задачу оптимизации, то есть

287
00:09:57,960 --> 00:10:00,081
полная функция затрат J от

288
00:10:00,081 --> 00:10:01,463
тета — выпуклая и

289
00:10:01,463 --> 00:10:04,368
не имеет локальных экстремумов.

290
00:10:04,370 --> 00:10:05,691
В следующем видео мы возьмем функцию затрат

291
00:10:05,691 --> 00:10:07,753
для одного обучающего

292
00:10:07,753 --> 00:10:08,923
примера и

293
00:10:08,923 --> 00:10:10,839
пойдем дальше,

294
00:10:10,839 --> 00:10:12,522
определив функцию затрат

295
00:10:12,522 --> 00:10:13,773
и для всего

296
00:10:13,780 --> 00:10:16,104
обучающего набора. Кроме того, мы

297
00:10:16,104 --> 00:10:17,404
найдем способ записать ее проще, чем

298
00:10:17,404 --> 00:10:19,699
до того.

299
00:10:19,699 --> 00:10:21,016
Основываясь на этой функции, мы сможем применить

300
00:10:21,030 --> 00:10:22,779
градиентный спуск и завершить создание

301
00:10:22,779 --> 00:10:25,835
алгоритма логистической регрессии.