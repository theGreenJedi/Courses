前回のビデオで、 ロジスティック回帰のコスト関数である Jのシータを最小化する為の最急降下法について話した。 このビデオでは、 よりアドバンスドな最適化アルゴリズムを幾つかと いくつかのよりアドバンスドな最適化の概念について 議論したい。 これらのアイデアを使う事で 最急降下法で可能な範囲を越えて、 さらに早くロジスティック回帰を 実行出来る。 そしてまたこれは アルゴリズムがとても大きな学習問題に対して ずっと良くスケールさせてくれる。 例えばとてもたくさんの数のフィーチャーがある時など。 これは最急降下法が何をしているか、についての 別の角度からの見方だ。 何らかのコスト関数Jがあって、それを最小化したい。 だから必要なのは こんな事を行うコードを 書く事だ、それは シータを入力に受け取り 2つの事を計算するという物、一つはJのシータで もう一つはこれら偏微分の項、 それが j=0、1、、、と nまで。 これら2つを与えるコードを元に 最急降下法が行う事は 以下の更新を繰り返し行う、という事。 でしょ？ つまりこれらの 偏微分を計算するコードを与えると 最急降下法はそれを ここに代入して、パラメータのシータをアップデートするのに使う。 つまり最急降下法について 別の角度からの考え方としては Jのシータを これらの偏微分を計算する コードを与える必要があり、そうすれば これらを最急降下法に代入する。 するとそれは我らの為に関数を最小化してくれる。 最急降下法の場合 実際にはコスト関数Jのシータを 計算するコードは必要無い。 偏微分の項を計算するコードだけで十分だ。 だがもし同時に 収束のモニタリングとかも 一緒にやりたいと思えば、 コスト関数と偏微分の項の 計算をする両方のコードを 与えている、と みなせる。 だからこれら2つを計算するコードを 書いたら、 使用出来る一つのアルゴリズムとしては最急降下法がある。 だが最急降下法だけが我らの使えるアルゴリズムという訳では無い。 他にもアルゴリズムはある。 よりアドバンスドな、より洗練された物が。 これら2つの計算方法だけを 提供してやれば これらのコスト関数を 最適化してくれるような 別のアプローチがある。 それら洗練された最適化アルゴリズムの例としては 共役勾配法、BFGS、そして L-BFGSなどがある。 それらがJのシータと その微分を渡してやると 最急降下法よりももっと 洗練された戦略に基づいて コスト関数を最小化してくれる。 これら3つのアルゴリズムの詳細は このコースの範囲を超えている。 実際これらのアルゴリズムを学ぶには 何日も、時には何週間もの 時間を費やすことになる。 数値計算のアドバンスドなコースを取るとそうなる。 でもそれらのアルゴリズムの性質は幾つか述べておこう。 これら3つのアルゴリズムはたくさんの利点がある。 一つには、これらのアルゴリズムはどれも ラーニングレートのアルファを 自分で選ばなくて良い。 これらのアルゴリズムの 考え方としては コスト関数とその微分項を与えられて、 これらのアルゴリズムはループ間が賢い、と考えて良い。 実際、これらは賢い ループの間である、ラインサーチと呼ばれる アルゴリズムがあり、それが 自動的に異なるラーニングレートのアルファを 試して、自動的に 良いラーニングレートのアルファを選ぶ。 それはイテレーションごとにですら それぞれ異なるラーニングレートを選ぶ事が出来る。 だからラーニングレートを自分で選ぶ必要は無い。 これらのアルゴリズムは 単に良いラーニングレートを選ぶだけよりは もっと洗練された事をやる、 そうであるからしばしば 最急降下法よりも早く収束する事になる。 これらのアルゴリズムは 単に良いラーニングレートを選ぶだけよりは もっと洗練された事をやる、 そうであるからしばしば 最急降下法よりも早く収束する事になる。 だがそれらが何をやるかの詳細な議論は このコースのスコープを超えている。 実の所、私はこれらのアルゴリズムを 長いこと、そうだなぁ、それこそ10年以上 とてもしょっちゅう 使ってきたが、 共役勾配法やBFGSやO-BFGSが 実際になにをやっているかの詳細を 自分でちゃんと理解したのは ここ二、三年の事に過ぎない。 つまり言いたいのは、実際に ループ間にこれらのアルゴリズムが 実際に何をしているのかを理解しないでも これらのアルゴリズムを正しく使って 様々な学習の問題に適用する事が出来る。 これらのアルゴリズムにもし欠点があるとすれば 主な問題点は 最急降下法よりもずっと 複雑な事だ。 そして特に、これらのアルゴリズム、 ー 共役勾配法、L-BFGS，BFGS ー は、自前で実装すべきでは無い。 数値計算のエキスパートで無い限り。 それは私が ルートを計算したり 行列の逆行列を計算したりするコードを 自前で書くのを推奨しないのと 同様だ。 これらのアルゴリズムも同様に ソフトウェアライブラリを使うべき。 つまりさ。ルートをとる必要がある時は みんな、なんらかの 誰か別の人が書いた 関数を使って、 自分の目的の数字のルートを計算するよね。 そして幸運なことにOctaveと、 それととても関連の深いMATLAB言語には ー 我らはそれを使う事になる ー Octaveはとても良い、 とても手頃な、これらのアドバンスドな最適化アルゴリズムの実装を持っている。 つまり、単にビルトインのライブラリを使うだけで とても良い結果が得られるという事だ。 これらのアルゴリズムには 良い実装と悪い実装がある、 といわざるをえない。 だからもし異なる言語を使って 機械学習のアプリケーションを書くなら、 もしCとかC++とかJavaを 使うなら、 幾つかのライブラリを 試して、 これらのアルゴリズムの良い実装を含む ライブラリを探す方が良い。 何故なら良い実装の 共役勾配法なりL-BFGSなりと より劣る実装の 共役勾配法なりL-BFGSなりとでは、 パフォーマンスに違いがあるからだ。 ではこれらのアルゴリズムをどう使うか 説明していこう。例を挙げて説明していく。 2つのパラメータの問題があるとしよう、 それら2つのパラメータは シータ0とシータ1とする。 そしてその問題のコスト関数は Jのシータで、それはイコール (シータ1 - 5)の二乗 足すことの (シータ2 - 5)の二乗 としよう。 このコスト関数なら シータ1とシータ2の値があなたにも分かる事だろう。 もしJのシータをシータの関数として最小化したければ、 そのシータの値は シータ1=5、 シータ2=5 だろう。 ここで、あなたがたには、 解析学が詳しい人もあまり詳しくない人も居るだろう。 なんにせよ、このコスト関数Jの 微分は、この2つの式となる。 私が計算した。 だからもしあなたが コスト関数Jを最小化する為に アドバンスドなアルゴリズムの一つを使いたければ、 つまり、もし仮に 我らは5、5で最小になるのを 知らなくて、でもコスト関数を最小にする値、5を 数値的に、最急降下法のような何らかの アルゴリズムを使って知りたければ、でも 最急降下法よりはもっと好ましい、 よりアドバンスドな物を使いたければ、 あなたがやるべき事は、 こんなoctave関数を実装する事で、 だからコスト関数を実装した、 コスト関数はシータの関数でこんな感じだ。 これがやってる事は 2つの引数を返してる、 最初のjValは コスト関数をどう計算するか。 つまりこれの意味する所は jValイコール、えーと、 (シータ1 - 5)の二乗 足すことの (シータ2 - 5)の二乗だ。 つまりこれは、ここのコスト関数をただ計算しているだけ。 そしてこの関数が返してる 二番目の引数は、gradientだ。 gradientは 2x1のベクトルで そのgradientベクトルの 2つの要素は、それぞれ ここにある2つの偏微分の項に対応する。 このコスト関数を実装すれば、 あなたはついに アドバンスドな最適化関数、 その名もfminuncを呼ぶ事が出来る、 fminuncはfunction minimization uncostrained (制約無し関数最小化)の略だ、 そしてそれの呼び方は以下のようになる。 幾つかオプションを設定する。 これはデータ構造として オプションを保持する物。 GradObj、onは Gradient Objectiveパラメータをonにセットする。 それはgradientをアルゴリズムに渡す事を意味する。 最大の繰り返し回数を 100回にセットしている。 最初のシータの予測値を与える。 それは2x1ベクトル。 そしてこのコマンドがfminuncを呼ぶ。 ここにあるアットマークは コスト関数のポインタで ここで定義した物。 そしてこれを呼べば、 これはよりアドバンスドなアルゴリズムの一つを用いて 計算してくれる。 思いたければ最急降下法みたいなもんだと思っておけば良い。 けど、ラーニングレートのアルファは自動で選ばれるので 自分で選ぶ必要は無い。 だがそれはなんらかのアドバンスドな アルゴリズムを使うというのが違う。 最急降下法にステロイド剤使ったみたいに。 最適な値シータをあなたの為に探してくれる。 ではこれがOctaveではどんな感じか見てみよう。 私はコスト関数をシータの関数として 前掲の通り書いた。 これはjValをコスト関数として計算し、 さらにgradientは2つの要素を持ち コスト関数の偏微分としての gradientを計算する、 2つの要素は2つのパラメータ、 シータ1とシータ2についての偏微分項。 ではOctaveのwindowに切り替えて、と。 さっき見せたコマンドをここに打つ。 optionsはイコールoptimsetの これは最適化アルゴリズムの為の オプションをセットする 記法だ。GradObjオプションをonにして、MaxIterを100に。 つまり繰り返し回数は 100回という事。 そしてgradientをアルゴリズムに提供する。 シータの初期値をzeroの2x1。 これがシータの初期値の予想値だ。 そしてここで、シータ、 functionval、そしてexitflagは イコールの、、、fminuncは制約無しで コスト関数へのポインタと 最初の推測値を提供し、 オプションはさっきの通り、 そしてエンターキーを叩けば、最適化アルゴリズムを走らせる事になる。 そしてそれはとても早く帰ってくる。 このおかしなフォーマットは この行が 折り返ししてるからだ。 このおかしなのは、単に コマンドの行が折り返しているからだ。 だがこれが言ってるのはようするに 最急降下法のステロイド版を 数値的に実行すると シータの最適値を見つけて、 シータ1は5、 シータ2も5だ。我らが期待する物と完全に一致してる。 function valueは(注: この場合はコスト関数の値) 最適値ではようするに10の-30乗で つまりは実質的には0だ。 これまた我らの期待通り。 そしてexitFlagは1、 これはこの収束のステータスが 何なのかを示している。 もしこのフラグが何を意味しているのか 知りたければ、 help fminuncして このexitflagをどう解釈したら良いか、ドキュメントを読めば良いが、 このexitflagはこのアルゴリズムがちゃんと収束したかを確認する為の物だ。 以上がOctaveでこれらのアルゴリズムをどう実行するかだ。 ところで、一つ言っておかなくてはならない事として Octaveの実装は、この値シータ、 パラメータベクトルであるシータは、 Rのd次元で、 dは2以上じゃないといけない。 だからシータが単なる実数の時、 つまり2次元に満たない ベクトル、二次元より大きなベクトルで 無い場合は このfminuncは 機能しないかもしれない。 だからもし一次元の関数を 最適化したい場合は、 Octaveのドキュメントのfminuncで さらなる詳細を 調べてくれ。 以上がこの簡単な 二次関数のコスト関数で 最適化を試してみるという試みだ。 ではこれをどうやってロジスティック回帰に適用すれば良いか？ ロジスティック回帰では パラメータベクトルであるシータがあるが、 Octaveのノーテーションと 数学のノーテーションを混ぜて使う。 だがこの説明で分かると思う。 我らのパラメータベクトル シータはこれらのパラメータ、 シータ0からシータnで構成されている。 Octaveはインデックスを 1から振るので シータ0とこれまで言っていたのは Octaveではシータ1と書く。 ではシータ1はどうなるかと言えば Octaveではシータ2となる。 それはシータn+1まで 続く。 そうする理由は、Octaveのインデックスは ベクトルはインデックスが1から 始まって、0から始まらないから。 で、我らがやらなくてはならない事は ロジスティック回帰のコストを捉えた コスト関数を 書くという事。 具体的に言うと、そのコスト関数は jValを返す必要があり、 それはJのシータを計算する これらの偏微分を計算する また、gradientも提供する必要がある。 gradient 1は シータ0による偏微分を 計算するなんらかのコードで、 次はシータ1での 偏微分の項、などなどだ。 もう一度言うが、これは gradient 1とかgradient 2であって、 gradient 0とgradient 1では無い。 何故ならOctaveはベクトルのインデックスを 0からではなく1から始めるから。 だがこのスライドから 読み取って欲しい一番のポイントは、 やらなくてはいけない事は コスト関数とgradientを返す 関数を書かなきゃいけないって事だ。 だからこれをロジスティック回帰や 線形回帰ですら、 もし線形回帰をこれらのアルゴリズムを使って 最適化したい場合は、 やらなくてはならない事は ここにあるこれらを計算する 適切なコードを代入するという事。 さて、今やあなたはこれらのアドバンスドな最適化アルゴリズムの使い方を知った。 これらのアルゴリズムを使うと これらはとても洗練された 最適化ライブラリなので どうしてもちょっと、 不透明で、 だからちょっとだけデバッグが大変になっちゃう。 でもこれらのアルゴリズムはしばしば 最急降下法より早く走るので、 だいたい普段は 大規模な機械学習の問題があると私は 最急降下法の代わりにこれらのアルゴリズムを 使っている。 そしてこれらのアイデアを知る事で あなたもロジスティック回帰や 線形回帰も、もっと大規模な問題に 適用出来るようになると思う。 以上がアドバンスドな最適化、というコンセプトだ。 次のビデオでは それはロジスティック回帰の最後のビデオとなるが、 すでにご存知の ロジスティック回帰を使って、 それをどうマルチクラスの分類問題に 適用するかを議論する。