در ویدیو ی قبلی درباره کاربرد روش گرادیان کاهشی برای کمینه سازی تابع هزینه J(theta) برای رگرسیون لاجیستیک صحبت کردیم. در این ویدئو می خواهم به شما راجع به الگوریتمهای بهینه سازی  و برخی موضوعات پیشرفته بهینه سازی صحبت کنم. با استفاده از برخی از این ایده ها، ما می توانیم رگرسیون لاجیستیک را با بسیار سریع تر از مقدار ممکن با گرادیان کاهشی اجرا کنیم. و این همچنین کمک می کند که الگوریتم ها  بسیار بهتر در مقیاس مسایل یادگیری ماشین بسیار بزرگ،  قابل گسترش باشند. مثل حالتی که تعداد  خصیصه ها بسیار زیاد باشد. این دید متفاوتی از عملکرد گرادیان کاهشی است. ما یک تابع هزینه J داریم و می خواهیم آن را کمینه کنیم. پس چیزی که می خواهیم این است، باید کدی بنویسیم که بتواند به عنوان ورودی پارامتر theta را بگیرد و و قادر باشد دو چیز را محاسبه کند: هزینه J در نقطه theta و این مشتقات جزئی  برای J = 0 , J=1 تا j=N.   با داشتن چنین کدی، کاری که و می تواند این دو چیز را انجام دهد گرادیان کاهشی می کند این است که به تکرار این به روز رسانی را انجام می دهد. مگه نه؟ خوب پس اگر کدی را که که برای محاسبه این مشتقات جزئی نوشتیم داشته باشیم، گرادیان کاهشی در اینجا کارش می گذارد و برای به روز رسانی پارامتر ما theta  آن را به کار می برد. ا نگاه دیگر به گرادیان کاهشی این است که ما باید کدی را تامین کنیم که J(theta) و این مشتقات را  محاسبه کند و بعد اینها  داخل گرادیان کاهشی تعبیه می شود که به کمک آن بتواند تابع را برای ما کمینه کند فکر کنم، برای گرادیان کاهشی از لحاظ تئوری حتی به کدی که تابع هزینه  J(theta) را  محاسبه کند احتیاج ندارید. تنها باید بتوانید عبارات مشتقی را محاسبه کنید. ولی اگر به کد خود به عنوان یک ناظر بر همگرایی روش نگاه کنیم می بینیم که می خواهیم کدی تامین کنیم که هم تابع هزینه و هم مشتقات جزئی را محاسبه کند. پس، حالا که کدی نوشته ایم که این دو مورد را محاسبه می کند یکی از الگوریتمهایی که می توانیم استفاده کنیم گرادیان کاهشی است. ولی گرادیان کاهشی تنها الگوریتمی که می توانیم استفاده کنیم نیست. الگوریتم های دیگری هم هستند، خیلی پیچیده تر، خیلی پیشرفته تر، که اگر فقط ما برای آنها راهی برای محاسبه این دو مورد تامین کنیم، آن وقت آنها با رویکردهای متفاوتی تابع هزینه را برای ما بهینه سازی می کنند. خوب، روش Congugate gradient BFGS و LBFGS مثالهایی از روشهای بهینه سازی پیچیده تر هستند که به راهی برای محاسبه J(Theta) و را هی برای مشتقات نیاز دارند و پس از آن می توانند روشهای پیچیده تری از گرادیان کاهشی برای کمینه کردن تابع هزینه را به کار ببرند. جزئیات دقیق اینکه این سه الگوریتم، چه میکنند، فراتر از حدود این درس است. و معمولا در نهایت کار به اینجا می کشد که شما چندین روز، یا هفته های کمی را به مطالعه این الگوریتمها بگذرانید اگر کلاسی در زمینه محاسبات عددی پیشرفته بگیرید. اما فقط بگذارید برخی از ویژگیهایشان را بگویم. این سه الگوریتم چند مزیت دارند. یکی اینکه، برای هیچ کدامشان معمولا لازم نیست نرخ یادگیری آلفا را دستی انتخاب کنید. پس یک جور دیگر نگاه به این الگوریتمها، روشی برای محاسبه مشتق و تابع هزینه است. می توانید تصور کنید که این الگوریتم ها یک حلقه هوشمند داخلی دارند. و در واقع حلقه داخلی هوشمند این الگوریتمها جستجوی خط  نام گرفته است. که به طور خودکار مقادیر مختلفی را برای نرخ یادگیری امتحان می کند و یک نرخ یادگیری خوب را انتخاب می کند حتی می تواند برای هر مرحله نرخ یادگیری مختلفی انتخاب کند. در نتیجه شما لازم نیست خودتان این کار را بکنید. این الگوریتمها معمولا کارهای پیچیده تری از انتخاب یک نرخ یادگیری خوب می کنند در نتیجه معولا خیلی سریعتر از گرادیان کاهشی همگرا می شوند. این الگوریتمها معمولا کارهای پیچیده تری از انتخاب یک نرخ یادگیری خوب می کنند در نتیجه معولا خیلی سریعتر از گرادیان کاهشی همگرا می شوند. ولی جزئیات اینکه دقیقا چه کار می کنند فراتر از حیطه این درس است. من خودم قبلها این الگوریتم ها را برای مدت زیادی به فراوانی به کار برده ام مثلا شاید بیشتر از یک دهه و می دونید فقط چند سالی هست که واقعا جزئیات اینکه روشهای conjugate gradient، BFGS و O-BFGS چه کار می کنند را در آوردم. یعنی کاملا میشه که این الگوریتمها را با موفقیت استفاده کرد و و در بسیاری از مسائل یادگیری به کارشان برد بدون اینکه واقعا حلقه داخلی آنها را بفهمی. اگر این الگوریتمها عیبی داشته باشند، اگر من باشم می گم عیب اصلیشان این است که خیلی پیچیده تر از گرادیان کاهشی هستند. و مخصوصا، شما نباید این الگوریتمها را گرادیان در هم آمیخته، LBFGS و BFGS. خودتان به شخصه پیاده سازی کنید مگر آنکه خبره محاسبات عددی باشید. همانطور که  توصیه نمی کنم که خودتان کدی را بنویسید که که جذر اعداد را محاسبه کند یا یا ماتریسها را معکوس کند. کاری که در مورد این الگوریتمها سفارش می کنم این است که که به سادگی یک کتابخانه نرم افزاری را به کار برید. می دونید، برای جذر گرفتن  کاری که که همه ما می کنیم این است که تابعی را که کس دیگری نوشته برای جذر گرفتن از اعداد خود استفاده می کنیم. خوشبختانه اکتاو و زبان مرتبط به آن متلب ما  از آن استفاده خواهیم کرد. اکتاو کتابخانه قابل قبولی دارد که برخی از این الگوریتمهای پیچیده را پیاده سازی کرده است. و فقط اگر کتابخانه تعبیه شده در آن را بکار ببرید جوابهای خوبی می گیرید. البته باید بگویم بین پیاده سازیهای خوب یا بد این الگوریتمها فرق هست. و در نتیجه اگر شما زبان متفاوتی برای برنامه یادگیری ماشین خود به کار می برید مثل C ،C++، جاوا در آنصورت بهتر است چند تا کتابخانه مختلف را امتحان کنید که تا یک پیاده سازی خوبشان را پیدا کنید. چون در کارایی یک پیاده سازی خوب مثلا گرادیان کانتور یا  LBFGS در مقابل یک پیاده سازی نه چندان خوب فرق هست. حالا بگذارید بگم چطور این الگوریتمها را بکار ببریم و این را با یک مثال نشان خواهم داد. فرض کنید مسئله ای با دو پارامتر داریم با نامهای theta0, theta1 و فرض کنید تابع هزینه شما J(theta)  برابر J(theta) = (theta1 - 5) ^2 + (theta2 -5)^2 خوب با این تابع هزینه مقدارهای کمینه کننده theta1 و theta2 اگر بخواهید تابع هزینه را به عنوان تابعی از theta  کمینه کنید مقداری که آن را کمینه می کند خواهد بود theta1 برابر است با 5 و theta2 برابر است با 5 حالا، باز باید تاکید کنم که، میدانم بعضی از شما، بیشتر از بعضیهای دیگه حسابان بلدید. ولی مشتق تابع هزینه برابر خواهد بود با این دو عبارات پایین اینجا. من حسابانش را انجام داده ام. پس اگر بخواهید تابع هزینه را داشته باشید یکی از الگوریتمهای بهینه سازی پیشرفته را برای کمینه کردن این تابع هزینه J به کار ببرید و اگر ما نمی دانستیم که مینیمم در 5و5 هستش ولی اگر بخواهید تابع هزینه را داشته باشید و کمینه آن را به صورت عددی با استفاده از روشی مثل گرادیان کاهشی ولی ترجیحا پیشرفته تر از گرادیان کاهشی محاسبه کنید کاری که می کنید این است که یک تابع اکتیو شبیه به این پیاده سازی کنید، پس ما یک تابع هزینه پیاده سازی می کنیم یک تابع هزینه theta، یک تابع مانند آن، و کاری که انجام میدهد این است که دو آرگومان برمیگرداند، اولی، به نام jVal بیانگر این است که چطور مقدار تابع هزینه J را محاسبه می کنیم، در نتیجه این می گوید مقدار J برابر، میدونید، theta1 منهای 5 به توان دو به اضافه theta2 منهای 5 به توان دو است. پس فقط اینجا فقط این تابع هزینه حساب میشود. و دومین آرگومانی که تابع برمیگرداند گرادیان است. پس گرادیان باید یک بردار دو در یک باشد، و دو مولفه بردار گرادیان متناظر با دو عبارت مشتق جزئی اینجاست. با پیاده سازی این تابع هزینه، شما بعدش می توانید تابع بهینه سازی پیشرفته را فراخوانی کنید fminunc را فراخوانی کنید. که مخفف کمینه سازی تابعی بدون قید در اکتیو هست. و طریقه فراخوانی آن به صورت زیر است. یک تعدادی گزینه را تعیین می کنید. options ساختار داده ای است که گزینه هایی را که شما می خواهید ذخیره می کند. آن را فعال کنید این پارامتر هدف گرادیان را on میکند. معنی آن این است که واقعا شما قرار است برای این الگوریتم گرادیان فراهم کنید. من می خواهم بیشترین تعداد مراحل را بگذارم، مثلا صد. ما یک حدس اولیه از مقدار theta به آن می دهیم. که یک بردار 2 در 1 است. و سپس این دستور fminunc را فراخوانی میکند. این علامت @ یک اشاره گر به تابع هزینه را نشان می دهد که بالاتر تعریف کردیم. و اگر این را فراخوانی کنید، می دونید، یکی از الگوریتمهای پیچیده تر بهینه سازی را استفاده می کند. و اگر می خواهید می توانید آن را درست شبیه گرادیان کاهشی ببینید ولی نرخ یادگیری به طور خودکار انتخاب میشود و شما مجبور نیستید خودتان آن را تعیین کنید. ولی بعدش  میخواهد یکی از الگوریتمهای بهینه سازی پیشرفته را استفاده کند مثل حالت خیلی قوی گرادیان کاهشی تا مقدار بهینه theta را برای شما پیدا کند. بگذارید نشانتان بدهم این در اکتیو چه شکلی است. خوب. من این تابع هزینه را نوشته ام. یک تابع از theta، دقیقا به صورتی که قبلا آن را داشتیم. مقدار J را محاسبه می کند که تابع هزینه است. و گرادیان را که دو مولفه دارد که که مشتقات جزئی تابع هزینه  هستند بر حسب دو پارامتر، theta1 و theta2. حالا بیایید به پنجره اکتیو من برویم. من این دو تابعی را که دارم تایپ میکنم. پس options برابر است با optimset. نمادگزاری که برای انتصاب پارامترها در انتخابهای من در الگوریتم بهینه سازی من بکار می رود. option را فعال کنید و maxIter را مقدار 100 بدهید که اعلام کند تا 100 مرحله پیش می رویم و من گرادیان را برای الگوریتم خود فراهم می کنم. فرض کنیم theta اولیه برابر بردار صفر دو در یک باشد. پس این مقدار حدس اولیه من برای theta است. حالا دارم optTheta functionVal   exitFlag مساوی است با fmin بدون قید، یک اشاره گر به تابع هزینه، و حدس اولیه ام را ارائه میدهم. و  انتخابها. و اگر enter بزنم،  این الگوریتم بهینه سازی را اجرا می کند. و خیلی هم سریع کارش تمام می شود. این شکل خنده دار خط من به خاطر ایت است که که کد چرخیده خط بعد. این چیز خنده دار فقط به خاطر آن است که خط دستور من چرخیده است ولی چیزی که می گوید این است عددی می سازد، می دونید، از آن به دید گرادیان کاهشی ّبهبود یافته نگاه کنید که مقدار بهینه که پیدا کرده است theta1 برابر 5 و theta2 ّبرابر 5 است، دقیفا همانطور که انتظار داشتیم. مقدار تابع در نقطه بهینه 10 به توان منفی 30 است. پس اساسا صفر است که آن هم همان مقداریست که منتظرش بودیم. و پرچم خروج 1 است و این نشان می دهد که که وضعیت همگرایی چیست. و اگر بخواهید میتوانید مستندات fminunc را بخوانید که توضیح میدهد چطور پرچم خروج را تعبیر کنید. ولی درکل پرچم خروج کمک می کند که مطمئن شویم که آیا الگوریتم همگرا شده است یا نه. خوب این نحوه اجرای این الگوریتمها در اکتیو است. راستی باید بگویم که برای پیاده سازی اکتیو این مقدار theta، یعنی بردار پارامتر thetaی شما باید در R^d باشد که در آن d بزرگتر یا مساوی 2 است. حالا اگر theta  فقط یک عدد حقیقی باشد یعنی حداقل یک بردار دو بعدی نباشد یا برداری با بعد بیشتر از دو نباشد، این fminunc ممکن است کار نکند و در نتیجه در صورتی که یک تابع یک بعدی دارید که می خواهید بهینه سازی کنید، باید به مستندات اکتیو برای fminunc مراجعه کنید برای جزئیات بیشتر. خوب... این روشی است که ما  برای بهینه سازی مثال غیرواقعی بازیگونه  خود با تابع هزینه درجه دوم  ساده خود بکار میبریم. چطور این را درمورد  به طور مثال رگرسیون لاجیستیک به کار می بریم؟ در رگرسیون لاجیستیک ما یک بردار پارامتر theta داریم و من مخلوطی از نمادگذاری اکتیو و نوعی نمادگذاری ریاضی به کار می بریم. امیدوارم این توضیح واضح باشد بردار theta  تشکیل شده از این پارامترها: theta0 تا theta n به خاطر اینکه اکتیو  اندیس گذاری بردارها را از یک آغاز می کند در نتیجه  اندیس صفرم theta theta 1  نوشته می شود در اکتیو،  اندیس اول نوشته خواهد شد theta 2 و این نوشته خواهد شد theta n+1  درسته؟ و این بخاطر این است که اکتیو اندیس گذاری بردارها را  با شروع از اندیس 1 انجام می دهد. به جای اندیس 0. پس چیزی که ما احتیاج داریم نوشتن یک تابع هزینه است که هزینه رگرسیون لاجیستیک را در بر بگیرد. مشخصا، تابع هزینه باید JVal را برگرداند، می دانید همانطور که کدهایی لازم دارید که که J(theta) را محاسبه کند همینطور لازم است به آن گرادیان را بدهیم. پس گرادیان 1 کدی خواهد بود که مشتق جزئی نسبت به را نسیت به theta0  مشتق پاره ای بعدی نسبت به theta 1  و بقیه را حساب می کند. باید مجددا تاکید کنم که این گرادیان 1، گرادیان 2 و غیره است، نه گرادیان 0 و گرادیان 1 بخاطر اینکه اندیس گذاری اکتیو از یک شروع می شود به جای 0. ولی مفهوم اصلی که امیدوارم از این اسلاید به همراه ببرید این است که کاری که شما لازم است انجام دهید نوشتن تابعی است که تابع هزینه و گرادیان را برمیگدراند. در نتیجه  برای اعمال این به رگرسیون لاجستیک یا حتی به رگرسیون خطی، اگر اگر می خواهید که این الگوریتمهای بهینه سازی را  برای رگرسیون خطی به کار ببرید کاری که لازم است انجام دهید این است که کد مناسب برای محاسبه این چیزهای این بالا را وارد کنید. در نتیجه، الان شما میدانید که چطور این الگوریتمهای پیشرفته بهینه سازی را بکاربرید. بخاطر اینکه برای این الگوریتمها شما یک کتابخانه بهینه سازی پیچیده را بکار می برید، این مسئله کد را کمی پنهانتر میکند و در نتیجه ممکن است کمی مشکل زدایی آن دشوارتر باشد. ولی بخاطر اینکه این الگوریتمها معمولا بسیار سریعتر از گرادیان کاهشی اجرا میشوند اکثر اوقات وقتی من یک مسئله یادگیری ماشین بزرگ دارم من این الگوریتمها را به جای گرادیان کاهشی به کار می برم. و با این ایده ها امیدوارم که که شما بتوانید رگرسیون لاجیستیک و همینطور رگرسیون خطی را روی مسایل بزرگتری به کار بیندازید. خوب، مفاهیم بهینه سازی پیشرفته ما همینها بود. و در ویدیوی بعد و آخری در مورد رگرسیون لاجیستیک، می خواهم به شما بگویم که جطور الگوریتم رگرسیون لاجیستیکی که راجع به آن میدانید را بردارید  و آن را برای مسایل طبقه بندی چند کلاسه به کار برید.