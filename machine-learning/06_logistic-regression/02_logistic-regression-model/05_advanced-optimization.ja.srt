1
00:00:00,300 --> 00:00:01,680
前回のビデオで、

2
00:00:01,990 --> 00:00:03,920
ロジスティック回帰のコスト関数である

3
00:00:04,440 --> 00:00:06,700
Jのシータを最小化する為の最急降下法について話した。

4
00:00:07,800 --> 00:00:08,930
このビデオでは、

5
00:00:09,020 --> 00:00:10,250
よりアドバンスドな最適化アルゴリズムを幾つかと

6
00:00:10,850 --> 00:00:12,340
いくつかのよりアドバンスドな最適化の概念について

7
00:00:12,670 --> 00:00:14,060
議論したい。

8
00:00:15,180 --> 00:00:16,480
これらのアイデアを使う事で

9
00:00:16,630 --> 00:00:17,930
最急降下法で可能な範囲を越えて、

10
00:00:19,010 --> 00:00:20,220
さらに早くロジスティック回帰を

11
00:00:20,350 --> 00:00:21,970
実行出来る。

12
00:00:22,880 --> 00:00:24,190
そしてまたこれは

13
00:00:24,320 --> 00:00:26,060
アルゴリズムがとても大きな学習問題に対して

14
00:00:26,670 --> 00:00:28,030
ずっと良くスケールさせてくれる。

15
00:00:28,660 --> 00:00:30,950
例えばとてもたくさんの数のフィーチャーがある時など。

16
00:00:31,850 --> 00:00:33,360
これは最急降下法が何をしているか、についての

17
00:00:33,750 --> 00:00:34,910
別の角度からの見方だ。

18
00:00:35,590 --> 00:00:38,030
何らかのコスト関数Jがあって、それを最小化したい。

19
00:00:38,950 --> 00:00:39,980
だから必要なのは

20
00:00:40,340 --> 00:00:41,080
こんな事を行うコードを

21
00:00:41,330 --> 00:00:42,640
書く事だ、それは

22
00:00:42,850 --> 00:00:44,980
シータを入力に受け取り

23
00:00:45,200 --> 00:00:46,470
2つの事を計算するという物、一つはJのシータで

24
00:00:46,700 --> 00:00:48,190
もう一つはこれら偏微分の項、

25
00:00:48,620 --> 00:00:50,280
それが

26
00:00:50,530 --> 00:00:51,820
j=0、1、、、と

27
00:00:51,890 --> 00:00:53,700
nまで。

28
00:00:53,830 --> 00:00:54,980
これら2つを与えるコードを元に

29
00:00:55,160 --> 00:00:56,710
最急降下法が行う事は

30
00:00:56,790 --> 00:00:58,620
以下の更新を繰り返し行う、という事。

31
00:00:59,100 --> 00:00:59,100
でしょ？

32
00:00:59,280 --> 00:01:00,500
つまりこれらの

33
00:01:00,670 --> 00:01:01,750
偏微分を計算するコードを与えると

34
00:01:02,090 --> 00:01:03,800
最急降下法はそれを

35
00:01:04,480 --> 00:01:07,330
ここに代入して、パラメータのシータをアップデートするのに使う。

36
00:01:08,650 --> 00:01:09,590
つまり最急降下法について

37
00:01:09,910 --> 00:01:11,070
別の角度からの考え方としては

38
00:01:11,350 --> 00:01:12,670
Jのシータを

39
00:01:12,810 --> 00:01:14,050
これらの偏微分を計算する

40
00:01:14,230 --> 00:01:15,700
コードを与える必要があり、そうすれば

41
00:01:15,900 --> 00:01:16,930
これらを最急降下法に代入する。

42
00:01:17,370 --> 00:01:20,110
するとそれは我らの為に関数を最小化してくれる。

43
00:01:20,970 --> 00:01:21,970
最急降下法の場合

44
00:01:22,480 --> 00:01:23,790
実際にはコスト関数Jのシータを

45
00:01:24,170 --> 00:01:26,520
計算するコードは必要無い。

46
00:01:26,940 --> 00:01:28,980
偏微分の項を計算するコードだけで十分だ。

47
00:01:29,740 --> 00:01:30,480
だがもし同時に

48
00:01:30,590 --> 00:01:32,300
収束のモニタリングとかも

49
00:01:33,000 --> 00:01:34,060
一緒にやりたいと思えば、

50
00:01:34,190 --> 00:01:35,440
コスト関数と偏微分の項の

51
00:01:35,530 --> 00:01:37,380
計算をする両方のコードを

52
00:01:37,510 --> 00:01:38,530
与えている、と

53
00:01:38,890 --> 00:01:40,250
みなせる。

54
00:01:42,700 --> 00:01:44,130
だからこれら2つを計算するコードを

55
00:01:44,280 --> 00:01:45,860
書いたら、

56
00:01:46,090 --> 00:01:47,820
使用出来る一つのアルゴリズムとしては最急降下法がある。

57
00:01:48,910 --> 00:01:51,590
だが最急降下法だけが我らの使えるアルゴリズムという訳では無い。

58
00:01:52,280 --> 00:01:53,690
他にもアルゴリズムはある。

59
00:01:54,330 --> 00:01:55,930
よりアドバンスドな、より洗練された物が。

60
00:01:56,720 --> 00:01:57,880
これら2つの計算方法だけを

61
00:01:58,400 --> 00:01:59,520
提供してやれば

62
00:01:59,960 --> 00:02:01,550
これらのコスト関数を

63
00:02:01,760 --> 00:02:03,040
最適化してくれるような

64
00:02:03,490 --> 00:02:04,790
別のアプローチがある。

65
00:02:05,110 --> 00:02:07,910
それら洗練された最適化アルゴリズムの例としては

66
00:02:08,110 --> 00:02:09,240
共役勾配法、BFGS、そして

67
00:02:09,460 --> 00:02:11,490
L-BFGSなどがある。

68
00:02:11,640 --> 00:02:12,610
それらがJのシータと

69
00:02:12,810 --> 00:02:13,670
その微分を渡してやると

70
00:02:13,750 --> 00:02:15,430
最急降下法よりももっと

71
00:02:15,670 --> 00:02:16,940
洗練された戦略に基づいて

72
00:02:17,620 --> 00:02:19,880
コスト関数を最小化してくれる。

73
00:02:21,260 --> 00:02:22,560
これら3つのアルゴリズムの詳細は

74
00:02:22,780 --> 00:02:25,920
このコースの範囲を超えている。

75
00:02:26,490 --> 00:02:28,200
実際これらのアルゴリズムを学ぶには

76
00:02:28,650 --> 00:02:30,570
何日も、時には何週間もの

77
00:02:31,060 --> 00:02:32,670
時間を費やすことになる。

78
00:02:33,240 --> 00:02:35,840
数値計算のアドバンスドなコースを取るとそうなる。

79
00:02:36,920 --> 00:02:38,200
でもそれらのアルゴリズムの性質は幾つか述べておこう。

80
00:02:40,080 --> 00:02:42,150
これら3つのアルゴリズムはたくさんの利点がある。

81
00:02:42,900 --> 00:02:44,070
一つには、これらのアルゴリズムはどれも

82
00:02:44,290 --> 00:02:45,850
ラーニングレートのアルファを

83
00:02:46,000 --> 00:02:48,970
自分で選ばなくて良い。

84
00:02:50,670 --> 00:02:51,450
これらのアルゴリズムの

85
00:02:51,650 --> 00:02:53,630
考え方としては

86
00:02:54,230 --> 00:02:56,900
コスト関数とその微分項を与えられて、

87
00:02:57,320 --> 00:02:59,740
これらのアルゴリズムはループ間が賢い、と考えて良い。

88
00:03:00,060 --> 00:03:00,680
実際、これらは賢い

89
00:03:01,810 --> 00:03:03,780
ループの間である、ラインサーチと呼ばれる

90
00:03:04,200 --> 00:03:05,840
アルゴリズムがあり、それが

91
00:03:06,520 --> 00:03:08,010
自動的に異なるラーニングレートのアルファを

92
00:03:08,080 --> 00:03:09,360
試して、自動的に

93
00:03:10,010 --> 00:03:11,090
良いラーニングレートのアルファを選ぶ。

94
00:03:12,030 --> 00:03:12,900
それはイテレーションごとにですら

95
00:03:13,130 --> 00:03:14,570
それぞれ異なるラーニングレートを選ぶ事が出来る。

96
00:03:15,490 --> 00:03:18,230
だからラーニングレートを自分で選ぶ必要は無い。

97
00:03:21,430 --> 00:03:22,770
これらのアルゴリズムは

98
00:03:22,910 --> 00:03:24,260
単に良いラーニングレートを選ぶだけよりは

99
00:03:24,470 --> 00:03:25,640
もっと洗練された事をやる、

100
00:03:25,800 --> 00:03:27,300
そうであるからしばしば

101
00:03:27,490 --> 00:03:30,320
最急降下法よりも早く収束する事になる。

102
00:03:32,470 --> 00:03:33,740
これらのアルゴリズムは

103
00:03:33,980 --> 00:03:35,160
単に良いラーニングレートを選ぶだけよりは

104
00:03:35,360 --> 00:03:36,740
もっと洗練された事をやる、

105
00:03:36,880 --> 00:03:38,770
そうであるからしばしば

106
00:03:39,020 --> 00:03:40,840
最急降下法よりも早く収束する事になる。

107
00:03:41,040 --> 00:03:42,230
だがそれらが何をやるかの詳細な議論は

108
00:03:42,710 --> 00:03:44,420
このコースのスコープを超えている。

109
00:03:45,580 --> 00:03:47,060
実の所、私はこれらのアルゴリズムを

110
00:03:47,570 --> 00:03:49,020
長いこと、そうだなぁ、それこそ10年以上

111
00:03:49,170 --> 00:03:50,170
とてもしょっちゅう

112
00:03:50,470 --> 00:03:53,070
使ってきたが、

113
00:03:53,290 --> 00:03:54,410
共役勾配法やBFGSやO-BFGSが

114
00:03:54,510 --> 00:03:55,460
実際になにをやっているかの詳細を

115
00:03:56,150 --> 00:03:57,200
自分でちゃんと理解したのは

116
00:03:57,780 --> 00:04:00,220
ここ二、三年の事に過ぎない。

117
00:04:00,980 --> 00:04:02,740
つまり言いたいのは、実際に

118
00:04:03,560 --> 00:04:05,380
ループ間にこれらのアルゴリズムが

119
00:04:05,480 --> 00:04:06,530
実際に何をしているのかを理解しないでも

120
00:04:06,780 --> 00:04:08,490
これらのアルゴリズムを正しく使って

121
00:04:09,460 --> 00:04:11,140
様々な学習の問題に適用する事が出来る。

122
00:04:12,270 --> 00:04:13,630
これらのアルゴリズムにもし欠点があるとすれば

123
00:04:14,200 --> 00:04:15,350
主な問題点は

124
00:04:15,610 --> 00:04:16,970
最急降下法よりもずっと

125
00:04:17,110 --> 00:04:19,390
複雑な事だ。

126
00:04:20,180 --> 00:04:21,700
そして特に、これらのアルゴリズム、

127
00:04:21,970 --> 00:04:23,290
ー 共役勾配法、L-BFGS，BFGS ー

128
00:04:23,850 --> 00:04:26,060
は、自前で実装すべきでは無い。

129
00:04:26,360 --> 00:04:29,520
数値計算のエキスパートで無い限り。

130
00:04:30,720 --> 00:04:32,320
それは私が

131
00:04:32,420 --> 00:04:33,640
ルートを計算したり

132
00:04:33,850 --> 00:04:35,240
行列の逆行列を計算したりするコードを

133
00:04:35,590 --> 00:04:36,660
自前で書くのを推奨しないのと

134
00:04:36,770 --> 00:04:39,010
同様だ。

135
00:04:39,140 --> 00:04:40,600
これらのアルゴリズムも同様に

136
00:04:40,710 --> 00:04:42,530
ソフトウェアライブラリを使うべき。

137
00:04:43,030 --> 00:04:43,770
つまりさ。ルートをとる必要がある時は

138
00:04:44,120 --> 00:04:44,940
みんな、なんらかの

139
00:04:45,150 --> 00:04:46,440
誰か別の人が書いた

140
00:04:47,080 --> 00:04:48,310
関数を使って、

141
00:04:48,530 --> 00:04:50,200
自分の目的の数字のルートを計算するよね。

142
00:04:51,330 --> 00:04:53,530
そして幸運なことにOctaveと、

143
00:04:53,760 --> 00:04:55,070
それととても関連の深いMATLAB言語には

144
00:04:55,430 --> 00:04:57,110
ー 我らはそれを使う事になる ー

145
00:04:57,140 --> 00:04:58,370
Octaveはとても良い、

146
00:04:58,530 --> 00:05:02,410
とても手頃な、これらのアドバンスドな最適化アルゴリズムの実装を持っている。

147
00:05:03,380 --> 00:05:04,350
つまり、単にビルトインのライブラリを使うだけで

148
00:05:04,600 --> 00:05:06,800
とても良い結果が得られるという事だ。

149
00:05:08,010 --> 00:05:08,880
これらのアルゴリズムには

150
00:05:09,370 --> 00:05:10,880
良い実装と悪い実装がある、

151
00:05:11,230 --> 00:05:12,740
といわざるをえない。

152
00:05:13,690 --> 00:05:15,010
だからもし異なる言語を使って

153
00:05:15,120 --> 00:05:16,270
機械学習のアプリケーションを書くなら、

154
00:05:16,470 --> 00:05:17,560
もしCとかC++とかJavaを

155
00:05:18,190 --> 00:05:20,090
使うなら、

156
00:05:20,250 --> 00:05:24,060
幾つかのライブラリを

157
00:05:24,210 --> 00:05:24,710
試して、

158
00:05:24,730 --> 00:05:25,660
これらのアルゴリズムの良い実装を含む

159
00:05:25,740 --> 00:05:27,790
ライブラリを探す方が良い。

160
00:05:28,250 --> 00:05:29,410
何故なら良い実装の

161
00:05:29,480 --> 00:05:30,740
共役勾配法なりL-BFGSなりと

162
00:05:31,680 --> 00:05:33,150
より劣る実装の

163
00:05:33,530 --> 00:05:35,150
共役勾配法なりL-BFGSなりとでは、

164
00:05:35,350 --> 00:05:37,680
パフォーマンスに違いがあるからだ。

165
00:05:43,060 --> 00:05:44,310
ではこれらのアルゴリズムをどう使うか

166
00:05:44,580 --> 00:05:47,080
説明していこう。例を挙げて説明していく。

167
00:05:48,970 --> 00:05:50,220
2つのパラメータの問題があるとしよう、

168
00:05:50,370 --> 00:05:51,620
それら2つのパラメータは

169
00:05:53,380 --> 00:05:55,580
シータ0とシータ1とする。

170
00:05:56,410 --> 00:05:57,450
そしてその問題のコスト関数は

171
00:05:57,970 --> 00:05:59,210
Jのシータで、それはイコール

172
00:05:59,430 --> 00:06:01,540
(シータ1 - 5)の二乗 足すことの (シータ2 - 5)の二乗 としよう。

173
00:06:02,630 --> 00:06:04,080
このコスト関数なら

174
00:06:04,590 --> 00:06:06,960
シータ1とシータ2の値があなたにも分かる事だろう。

175
00:06:07,080 --> 00:06:09,590
もしJのシータをシータの関数として最小化したければ、

176
00:06:09,940 --> 00:06:10,910
そのシータの値は

177
00:06:11,030 --> 00:06:12,040
シータ1=5、

178
00:06:12,420 --> 00:06:14,220
シータ2=5 だろう。

179
00:06:15,230 --> 00:06:16,620
ここで、あなたがたには、

180
00:06:16,950 --> 00:06:18,320
解析学が詳しい人もあまり詳しくない人も居るだろう。

181
00:06:19,010 --> 00:06:20,770
なんにせよ、このコスト関数Jの

182
00:06:20,850 --> 00:06:23,420
微分は、この2つの式となる。

183
00:06:24,270 --> 00:06:25,060
私が計算した。

184
00:06:26,260 --> 00:06:27,250
だからもしあなたが

185
00:06:27,480 --> 00:06:29,220
コスト関数Jを最小化する為に

186
00:06:29,810 --> 00:06:31,380
アドバンスドなアルゴリズムの一つを使いたければ、

187
00:06:31,660 --> 00:06:32,630
つまり、もし仮に

188
00:06:32,880 --> 00:06:34,680
我らは5、5で最小になるのを

189
00:06:34,780 --> 00:06:36,140
知らなくて、でもコスト関数を最小にする値、5を

190
00:06:36,240 --> 00:06:37,550
数値的に、最急降下法のような何らかの

191
00:06:37,970 --> 00:06:39,840
アルゴリズムを使って知りたければ、でも

192
00:06:40,040 --> 00:06:41,560
最急降下法よりはもっと好ましい、

193
00:06:41,730 --> 00:06:43,430
よりアドバンスドな物を使いたければ、

194
00:06:43,550 --> 00:06:45,010
あなたがやるべき事は、

195
00:06:45,570 --> 00:06:46,690
こんなoctave関数を実装する事で、

196
00:06:46,860 --> 00:06:48,190
だからコスト関数を実装した、

197
00:06:49,210 --> 00:06:51,180
コスト関数はシータの関数でこんな感じだ。

198
00:06:52,180 --> 00:06:53,250
これがやってる事は

199
00:06:53,380 --> 00:06:55,660
2つの引数を返してる、

200
00:06:55,760 --> 00:06:57,780
最初のjValは

201
00:06:58,910 --> 00:07:00,020
コスト関数をどう計算するか。

202
00:07:00,680 --> 00:07:01,780
つまりこれの意味する所は

203
00:07:02,080 --> 00:07:03,210
jValイコール、えーと、

204
00:07:03,440 --> 00:07:04,630
(シータ1 - 5)の二乗 足すことの

205
00:07:05,330 --> 00:07:06,230
(シータ2 - 5)の二乗だ。

206
00:07:06,540 --> 00:07:09,140
つまりこれは、ここのコスト関数をただ計算しているだけ。

207
00:07:10,540 --> 00:07:12,040
そしてこの関数が返してる

208
00:07:12,260 --> 00:07:14,190
二番目の引数は、gradientだ。

209
00:07:14,840 --> 00:07:16,030
gradientは

210
00:07:16,160 --> 00:07:17,320
2x1のベクトルで

211
00:07:18,870 --> 00:07:20,050
そのgradientベクトルの

212
00:07:20,120 --> 00:07:22,100
2つの要素は、それぞれ

213
00:07:22,800 --> 00:07:24,670
ここにある2つの偏微分の項に対応する。

214
00:07:27,150 --> 00:07:28,570
このコスト関数を実装すれば、

215
00:07:29,580 --> 00:07:30,390
あなたはついに

216
00:07:31,510 --> 00:07:33,010
アドバンスドな最適化関数、

217
00:07:34,270 --> 00:07:35,720
その名もfminuncを呼ぶ事が出来る、

218
00:07:35,950 --> 00:07:36,900
fminuncはfunction minimization uncostrained

219
00:07:37,610 --> 00:07:39,360
(制約無し関数最小化)の略だ、

220
00:07:40,300 --> 00:07:41,520
そしてそれの呼び方は以下のようになる。

221
00:07:41,790 --> 00:07:42,350
幾つかオプションを設定する。

222
00:07:43,230 --> 00:07:43,580
これはデータ構造として

223
00:07:44,330 --> 00:07:46,680
オプションを保持する物。

224
00:07:47,320 --> 00:07:48,960
GradObj、onは

225
00:07:49,160 --> 00:07:52,100
Gradient Objectiveパラメータをonにセットする。

226
00:07:52,270 --> 00:07:55,180
それはgradientをアルゴリズムに渡す事を意味する。

227
00:07:56,150 --> 00:07:57,550
最大の繰り返し回数を

228
00:07:57,840 --> 00:07:59,280
100回にセットしている。

229
00:07:59,580 --> 00:08:02,230
最初のシータの予測値を与える。

230
00:08:02,720 --> 00:08:03,680
それは2x1ベクトル。

231
00:08:04,440 --> 00:08:06,860
そしてこのコマンドがfminuncを呼ぶ。

232
00:08:07,530 --> 00:08:10,290
ここにあるアットマークは

233
00:08:10,420 --> 00:08:11,810
コスト関数のポインタで

234
00:08:13,010 --> 00:08:14,320
ここで定義した物。

235
00:08:15,060 --> 00:08:16,020
そしてこれを呼べば、

236
00:08:16,270 --> 00:08:18,290
これはよりアドバンスドなアルゴリズムの一つを用いて

237
00:08:18,620 --> 00:08:20,490
計算してくれる。

238
00:08:21,110 --> 00:08:23,350
思いたければ最急降下法みたいなもんだと思っておけば良い。

239
00:08:23,690 --> 00:08:25,170
けど、ラーニングレートのアルファは自動で選ばれるので

240
00:08:25,500 --> 00:08:27,290
自分で選ぶ必要は無い。

241
00:08:28,210 --> 00:08:29,880
だがそれはなんらかのアドバンスドな

242
00:08:30,160 --> 00:08:32,000
アルゴリズムを使うというのが違う。

243
00:08:32,640 --> 00:08:33,770
最急降下法にステロイド剤使ったみたいに。

244
00:08:34,400 --> 00:08:36,490
最適な値シータをあなたの為に探してくれる。

245
00:08:37,180 --> 00:08:39,040
ではこれがOctaveではどんな感じか見てみよう。

246
00:08:40,690 --> 00:08:42,460
私はコスト関数をシータの関数として

247
00:08:42,900 --> 00:08:46,440
前掲の通り書いた。

248
00:08:46,650 --> 00:08:49,070
これはjValをコスト関数として計算し、

249
00:08:49,920 --> 00:08:51,810
さらにgradientは2つの要素を持ち

250
00:08:52,040 --> 00:08:53,050
コスト関数の偏微分としての

251
00:08:53,450 --> 00:08:54,430
gradientを計算する、

252
00:08:55,220 --> 00:08:56,200
2つの要素は2つのパラメータ、

253
00:08:56,360 --> 00:08:57,910
シータ1とシータ2についての偏微分項。

254
00:08:59,040 --> 00:09:00,360
ではOctaveのwindowに切り替えて、と。

255
00:09:00,710 --> 00:09:02,900
さっき見せたコマンドをここに打つ。

256
00:09:03,470 --> 00:09:05,850
optionsはイコールoptimsetの

257
00:09:06,630 --> 00:09:08,510
これは最適化アルゴリズムの為の

258
00:09:09,670 --> 00:09:11,190
オプションをセットする

259
00:09:11,710 --> 00:09:13,850
記法だ。GradObjオプションをonにして、MaxIterを100に。

260
00:09:14,130 --> 00:09:17,600
つまり繰り返し回数は

261
00:09:18,310 --> 00:09:19,610
100回という事。

262
00:09:19,730 --> 00:09:22,090
そしてgradientをアルゴリズムに提供する。

263
00:09:23,490 --> 00:09:27,190
シータの初期値をzeroの2x1。

264
00:09:27,980 --> 00:09:29,280
これがシータの初期値の予想値だ。

265
00:09:30,500 --> 00:09:31,390
そしてここで、シータ、

266
00:09:32,620 --> 00:09:35,100
functionval、そしてexitflagは

267
00:09:37,610 --> 00:09:39,430
イコールの、、、fminuncは制約無しで

268
00:09:40,570 --> 00:09:41,600
コスト関数へのポインタと

269
00:09:43,010 --> 00:09:44,700
最初の推測値を提供し、

270
00:09:46,090 --> 00:09:49,060
オプションはさっきの通り、

271
00:09:49,820 --> 00:09:52,760
そしてエンターキーを叩けば、最適化アルゴリズムを走らせる事になる。

272
00:09:53,940 --> 00:09:54,810
そしてそれはとても早く帰ってくる。

273
00:09:55,790 --> 00:09:57,040
このおかしなフォーマットは

274
00:09:57,430 --> 00:09:58,430
この行が

275
00:09:59,700 --> 00:10:00,290
折り返ししてるからだ。

276
00:10:00,680 --> 00:10:02,540
このおかしなのは、単に

277
00:10:02,760 --> 00:10:04,890
コマンドの行が折り返しているからだ。

278
00:10:05,490 --> 00:10:06,290
だがこれが言ってるのはようするに

279
00:10:06,550 --> 00:10:08,500
最急降下法のステロイド版を

280
00:10:08,670 --> 00:10:10,400
数値的に実行すると

281
00:10:10,440 --> 00:10:11,620
シータの最適値を見つけて、

282
00:10:11,760 --> 00:10:13,150
シータ1は5、

283
00:10:13,400 --> 00:10:15,670
シータ2も5だ。我らが期待する物と完全に一致してる。

284
00:10:16,520 --> 00:10:18,760
function valueは(注: この場合はコスト関数の値)

285
00:10:18,840 --> 00:10:21,430
最適値ではようするに10の-30乗で

286
00:10:21,670 --> 00:10:23,160
つまりは実質的には0だ。

287
00:10:23,370 --> 00:10:24,760
これまた我らの期待通り。

288
00:10:24,840 --> 00:10:27,060
そしてexitFlagは1、

289
00:10:27,240 --> 00:10:29,080
これはこの収束のステータスが

290
00:10:29,730 --> 00:10:31,400
何なのかを示している。

291
00:10:31,800 --> 00:10:33,010
もしこのフラグが何を意味しているのか

292
00:10:33,150 --> 00:10:35,020
知りたければ、

293
00:10:35,130 --> 00:10:36,480
help fminuncして

294
00:10:36,680 --> 00:10:38,650
このexitflagをどう解釈したら良いか、ドキュメントを読めば良いが、

295
00:10:38,760 --> 00:10:41,600
このexitflagはこのアルゴリズムがちゃんと収束したかを確認する為の物だ。

296
00:10:43,960 --> 00:10:46,450
以上がOctaveでこれらのアルゴリズムをどう実行するかだ。

297
00:10:47,480 --> 00:10:48,920
ところで、一つ言っておかなくてはならない事として

298
00:10:48,940 --> 00:10:51,020
Octaveの実装は、この値シータ、

299
00:10:51,640 --> 00:10:53,010
パラメータベクトルであるシータは、

300
00:10:53,370 --> 00:10:54,940
Rのd次元で、

301
00:10:55,280 --> 00:10:58,210
dは2以上じゃないといけない。

302
00:10:58,450 --> 00:11:00,330
だからシータが単なる実数の時、

303
00:11:00,770 --> 00:11:02,040
つまり2次元に満たない

304
00:11:02,160 --> 00:11:03,160
ベクトル、二次元より大きなベクトルで

305
00:11:03,800 --> 00:11:04,860
無い場合は

306
00:11:05,160 --> 00:11:06,840
このfminuncは

307
00:11:07,560 --> 00:11:08,760
機能しないかもしれない。

308
00:11:09,140 --> 00:11:10,310
だからもし一次元の関数を

309
00:11:10,590 --> 00:11:11,590
最適化したい場合は、

310
00:11:11,830 --> 00:11:12,930
Octaveのドキュメントのfminuncで

311
00:11:13,100 --> 00:11:14,680
さらなる詳細を

312
00:11:14,950 --> 00:11:16,230
調べてくれ。

313
00:11:18,230 --> 00:11:19,360
以上がこの簡単な

314
00:11:19,620 --> 00:11:21,640
二次関数のコスト関数で

315
00:11:22,190 --> 00:11:23,810
最適化を試してみるという試みだ。

316
00:11:24,440 --> 00:11:26,520
ではこれをどうやってロジスティック回帰に適用すれば良いか？

317
00:11:27,720 --> 00:11:29,270
ロジスティック回帰では

318
00:11:29,520 --> 00:11:31,290
パラメータベクトルであるシータがあるが、

319
00:11:31,430 --> 00:11:32,210
Octaveのノーテーションと

320
00:11:32,620 --> 00:11:34,880
数学のノーテーションを混ぜて使う。

321
00:11:35,300 --> 00:11:36,400
だがこの説明で分かると思う。

322
00:11:36,870 --> 00:11:38,050
我らのパラメータベクトル

323
00:11:38,520 --> 00:11:40,360
シータはこれらのパラメータ、

324
00:11:40,540 --> 00:11:41,780
シータ0からシータnで構成されている。

325
00:11:42,210 --> 00:11:44,230
Octaveはインデックスを

326
00:11:46,090 --> 00:11:48,040
1から振るので

327
00:11:48,460 --> 00:11:49,640
シータ0とこれまで言っていたのは

328
00:11:49,710 --> 00:11:51,190
Octaveではシータ1と書く。

329
00:11:51,330 --> 00:11:53,290
ではシータ1はどうなるかと言えば

330
00:11:53,930 --> 00:11:54,690
Octaveではシータ2となる。

331
00:11:55,280 --> 00:11:56,180
それはシータn+1まで

332
00:11:56,780 --> 00:11:58,430
続く。

333
00:11:58,610 --> 00:12:00,650
そうする理由は、Octaveのインデックスは

334
00:12:01,320 --> 00:12:03,070
ベクトルはインデックスが1から

335
00:12:03,430 --> 00:12:05,200
始まって、0から始まらないから。

336
00:12:06,920 --> 00:12:07,950
で、我らがやらなくてはならない事は

337
00:12:08,160 --> 00:12:09,670
ロジスティック回帰のコストを捉えた

338
00:12:09,880 --> 00:12:12,070
コスト関数を

339
00:12:12,710 --> 00:12:14,210
書くという事。

340
00:12:15,170 --> 00:12:16,450
具体的に言うと、そのコスト関数は

341
00:12:16,880 --> 00:12:18,310
jValを返す必要があり、

342
00:12:18,940 --> 00:12:20,430
それはJのシータを計算する

343
00:12:20,640 --> 00:12:22,440
これらの偏微分を計算する

344
00:12:22,710 --> 00:12:24,010
また、gradientも提供する必要がある。

345
00:12:24,540 --> 00:12:25,460
gradient 1は

346
00:12:25,920 --> 00:12:27,080
シータ0による偏微分を

347
00:12:27,280 --> 00:12:29,100
計算するなんらかのコードで、

348
00:12:29,390 --> 00:12:31,250
次はシータ1での

349
00:12:31,600 --> 00:12:34,300
偏微分の項、などなどだ。

350
00:12:34,770 --> 00:12:36,260
もう一度言うが、これは

351
00:12:37,500 --> 00:12:38,390
gradient 1とかgradient 2であって、

352
00:12:39,030 --> 00:12:40,330
gradient 0とgradient 1では無い。

353
00:12:40,500 --> 00:12:42,730
何故ならOctaveはベクトルのインデックスを

354
00:12:43,460 --> 00:12:46,200
0からではなく1から始めるから。

355
00:12:47,440 --> 00:12:48,460
だがこのスライドから

356
00:12:48,690 --> 00:12:49,540
読み取って欲しい一番のポイントは、

357
00:12:49,900 --> 00:12:50,870
やらなくてはいけない事は

358
00:12:51,070 --> 00:12:54,370
コスト関数とgradientを返す

359
00:12:55,500 --> 00:12:56,930
関数を書かなきゃいけないって事だ。

360
00:12:58,410 --> 00:12:59,750
だからこれをロジスティック回帰や

361
00:12:59,960 --> 00:13:01,410
線形回帰ですら、

362
00:13:02,100 --> 00:13:03,430
もし線形回帰をこれらのアルゴリズムを使って

363
00:13:03,560 --> 00:13:06,230
最適化したい場合は、

364
00:13:07,340 --> 00:13:08,350
やらなくてはならない事は

365
00:13:08,500 --> 00:13:09,960
ここにあるこれらを計算する

366
00:13:10,820 --> 00:13:12,280
適切なコードを代入するという事。

367
00:13:15,100 --> 00:13:17,910
さて、今やあなたはこれらのアドバンスドな最適化アルゴリズムの使い方を知った。

368
00:13:19,030 --> 00:13:21,170
これらのアルゴリズムを使うと

369
00:13:21,320 --> 00:13:22,660
これらはとても洗練された

370
00:13:22,870 --> 00:13:25,190
最適化ライブラリなので

371
00:13:25,690 --> 00:13:26,710
どうしてもちょっと、

372
00:13:26,940 --> 00:13:28,510
不透明で、

373
00:13:28,740 --> 00:13:30,390
だからちょっとだけデバッグが大変になっちゃう。

374
00:13:31,290 --> 00:13:32,660
でもこれらのアルゴリズムはしばしば

375
00:13:33,010 --> 00:13:34,370
最急降下法より早く走るので、

376
00:13:35,010 --> 00:13:36,760
だいたい普段は

377
00:13:37,060 --> 00:13:38,180
大規模な機械学習の問題があると私は

378
00:13:38,410 --> 00:13:39,500
最急降下法の代わりにこれらのアルゴリズムを

379
00:13:39,760 --> 00:13:42,110
使っている。

380
00:13:43,900 --> 00:13:45,070
そしてこれらのアイデアを知る事で

381
00:13:45,450 --> 00:13:46,710
あなたもロジスティック回帰や

382
00:13:47,350 --> 00:13:48,780
線形回帰も、もっと大規模な問題に

383
00:13:49,100 --> 00:13:51,410
適用出来るようになると思う。

384
00:13:51,830 --> 00:13:53,820
以上がアドバンスドな最適化、というコンセプトだ。

385
00:13:55,120 --> 00:13:56,170
次のビデオでは

386
00:13:56,320 --> 00:13:57,720
それはロジスティック回帰の最後のビデオとなるが、

387
00:13:58,550 --> 00:13:59,470
すでにご存知の

388
00:13:59,600 --> 00:14:00,990
ロジスティック回帰を使って、

389
00:14:01,520 --> 00:14:02,790
それをどうマルチクラスの分類問題に

390
00:14:02,990 --> 00:14:05,420
適用するかを議論する。