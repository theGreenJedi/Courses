В этом видео мы обсудим, как подбирать значения параметров тета в задачах логистической регрессии. В частности, я хочу определить целевую функцию оптимизации, функцию стоимости, которую мы будем использовать для подбора параметров. Вот наша задача обучения с учителем. Нам нужно найти подходящие параметры для модели логистической регрессии. У нас есть обучающий набор из m учебных примеров. Как обычно, каждый из наших примеров представляет собой вектор параметров размерности n+1. И, как обычно, х нулевое равно 1. Первая характеристика, то есть характеристика номер ноль, всегда равняется единице. И поскольку это задача классификации, для каждого примера из обучающего набора y равняется нулю или единице. Это гипотеза, а вектор ее параметров тета — здесь. И вопрос, который я хочу разобрать — как найти, как подобрать параметры тета исходя из этого обучающего набора. Когда мы строили модель линейной регрессии, наша функция затрат выглядела так. Я записал ее немного по другому: перенес 1/2 из единицы, деленной на 2m, под знак суммы. Это позволит мне представить функцию затрат иначе. Вместо этого квадрата ошибки я напишу «стоимость» от h(x) и y, а «стоимость» определю равной этому выражению. Тому самому квадрату ошибки, деленному пополам. В такой записи яснее видно, что функция затрат равна средней ошибке по обучающему набору, то есть 1/m умножить на сумму этих выражений для обучающих примеров. Чтобы упростить еще немного, я уберу верхние индексы. То есть «стоимость» от h(x) и y равна половине квадрата ошибки, и по сути эта величина — цена, которую я хочу, чтобы мой алгоритм «заплатил», если он выдает предсказание h(x) при том, что реальное значение равно y. Так что я просто зачеркну эти индексы.Так? Для линейной регрессии мы установили именно такую цену, то есть половину квадрата разницы между предсказанным и наблюдаемым значением y. Ну хорошо, такая функция затрат сработала для линейной регрессии, но мы - то имеем дело с логистической. Если бы мы могли минимизировать эту функцию затрат, она бы нам подошла. Но такая функция затрат для логистической регрессии оказывается невыпуклой функцией параметров тета. Вот что я имею в виду под невыпуклой. Попробуем изобразить нашу функцию затрат J от тета. В логистической регрессии функция h нелинейна, верно? Она равна единице, деленной на один плюс e в степени минус транспонированный вектор тета на x. Довольно сложная нелинейная функция. И если мы возьмем эту сигмоиду, подставим ее сюда, затем возьмем эту стоимость и подставим ее сюда, получится функция J, график которой может выглядеть как-то так. Со множеством локальных минимумов. Такие функции называются невыпуклыми. Сами видите, если применить градиентный спуск к такой функции, он необязательно придет в глобальный минимум. Мы же, напротив, хотели бы получить выпуклую функцию стоимости J от тета, график которой — одна чашеобразная кривая, чтобы градиентный спуск гарантированно сошелся в глобальном минимуме. Таким образом, проблема с использованием квадратичной ошибки в том, что из-за нелинейного элемента, сигмоиды, которая входит вот сюда, J от тета становится невыпуклой. Так что нам нужно найти другую функцию затрат, выпуклую, чтобы мы могли применять мощный алгоритм вроде градиентного спуска и быть уверены, что получим глобальный минимум. И вот какую функцию стоимости мы будем использовать для логистической регрессии. Мы положим цену или штраф, который алгоритм платит, предсказывая h(x) — какое-то число, например, 0,7, — когда в действительности значение метки равно y, равным минус логарифму h(x), если y равно 1, и минус логарифму от единицы минус h(x), если y равно 0. На вид это довольно замысловатая функция, но давайте построим ее график и попробуем понять, как она устроена. Начнем со случая, когда y = 1. Если y = 1, функция стоимости равна −log(h(x)), и ее график... Пусть наша ось абсцисс соответствует h(x). Значение гипотезы всегда лежит между 0 и 1. Так? Значит, h(x) лежит в пределах от 0 до 1. Итак, если вы построите график функции затрат, он окажется таким. Это можно понять, построив сначала график функции log(z), где z откладывается по оси абсцисс. Он выглядит так. Здесь он уходит к минус бесконечности. Вот как выглядит логарифмическая функция. Здесь ноль, здесь единица. А z, конечно же, играет роль h(x). Соответственно, −log(z) выглядит так: просто поменяем знак, получится минус логарифм. А поскольку нас интересуют значения функции только на интервале от 0 до 1, эту часть можно отбросить, и у нас останется вот эта часть кривой. Именно так выглядит график слева. И у этой функции затрат есть несколько интересных и полезных свойств. Во-первых, обратите внимание, что когда y = 1 и h(x) = 1, то есть если предсказанное гипотезой значение в точности равно известному нам y, то стоимость равна нулю. Так? На самом деле кривая здесь не становится горизонтальной, она пересекает ось. Итак, если h(x) = 1, если гипотеза предсказывает y = 1, и это в самом деле так, то стоимость равна нулю. Вот эта точка на графике. Так? Поскольку мы пока рассматриваем случай y = 1, когда h(x) равно 1, cтоимость равна нулю. А этого нам и надо. Потому что, если мы правильно предсказали y, стоимость должна быть нулевой. Во-вторых, обратите внимание, что когда h(x) приближается к нулю, когда предсказание гипотезы близко к нулю, стоимость резко возрастает и стремится к бесконечности. Это воплощает наше понимание гипотезы: если гипотеза предсказывает 0, то есть утверждает, что вероятность y = 1 равняется нулю, а это все равно что прийти к пациенту и сказать ему: «вероятность того, что ваша опухоль злокачественная, равняется нулю», то есть сказать, что это совершенно невероятно, но при этом на деле опухоль пациента злокачественная, то есть в действительности y = 1, хотя мы и сказали, что вероятность этого равна нулю, что опухоль никак не может быть злокачественной, — так вот, если мы с большой уверенностью предсказали, что y не равно 1, и оказались неправы, нам нужно наложить на алгоритм очень-очень большой штраф. И этому соответствует рост стоимости к бесконечности в случае, когда y = 1, а h(x) приближается к нулю. Мы рассмотрели случай y = 1, теперь посмотрим, как выглядит функция затрат при y = 0. Когда y = 0, стоимость определяется вот этим выражением. График функции −log(1 − z) выглядит так. Здесь у меня 0, здесь 1. Что-то вроде этого. Соответственно, так выглядит и график функции затрат в случае y = 0. Кривая теперь резко возрастает, уходит к бесконечности при приближении h(x) к единице. Поскольку это значит, что при y = 0 мы предсказали, что y = 1 почти наверняка, с вероятностью, близкой к единице, и тогда нам нужно заплатить очень большой штраф. И, наоборот, если h(x) = 0 притом, что y = 0, гипотеза попала в яблочко. Предсказанное значение — ноль, и y в действительности равно нулю, так что в этой точке стоимость нулевая. В этом видео мы рассмотрели функцию затрат для одного обучающего примера. Вопрос анализа выпуклости лежит за пределами этого курса, но можно показать, что при таком выборе функции затрат мы получим выпуклую задачу оптимизации, то есть полная функция затрат J от тета — выпуклая и не имеет локальных экстремумов. В следующем видео мы возьмем функцию затрат для одного обучающего примера и пойдем дальше, определив функцию затрат и для всего обучающего набора. Кроме того, мы найдем способ записать ее проще, чем до того. Основываясь на этой функции, мы сможем применить градиентный спуск и завершить создание алгоритма логистической регрессии.