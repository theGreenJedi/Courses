1
00:00:00,300 --> 00:00:01,680
В предыдущем видео мы обсудили общий вид гипотезы в

2
00:00:01,990 --> 00:00:03,920
затрат J от тета в задачах

3
00:00:04,440 --> 00:00:06,700
логистической регрессии.

4
00:00:07,800 --> 00:00:08,930
В этом

5
00:00:09,020 --> 00:00:10,250
более сложных алгоритмах оптимизации и

6
00:00:10,850 --> 00:00:12,340
приемах, которые они

7
00:00:12,670 --> 00:00:14,060
используют.

8
00:00:15,180 --> 00:00:16,480
Благодаря некоторым из этих приемов

9
00:00:16,630 --> 00:00:17,930
мы сможем заставить

10
00:00:19,010 --> 00:00:20,220
логистическую регрессию работать гораздо быстрее,

11
00:00:20,350 --> 00:00:21,970
чем при использовании градиентного спуска.

12
00:00:22,880 --> 00:00:24,190
Кроме того, алгоритмы будут намного лучше

13
00:00:24,320 --> 00:00:26,060
масштабироваться к очень большим задачам

14
00:00:26,670 --> 00:00:28,030
машинного обучения, например, задачам с

15
00:00:28,660 --> 00:00:30,950
очень большим количеством признаков.

16
00:00:31,850 --> 00:00:33,360
Вот альтернативный взгляд на то, как работает

17
00:00:33,750 --> 00:00:34,910
метод градиентного спуска.

18
00:00:35,590 --> 00:00:38,030
У нас есть некоторая функция затрат J и мы хотим ее минимизировать.

19
00:00:38,950 --> 00:00:39,980
Для этого нам нужно

20
00:00:40,340 --> 00:00:41,080
написать

21
00:00:41,330 --> 00:00:42,640
код, который принимает на

22
00:00:42,850 --> 00:00:44,980
вход параметры тета и

23
00:00:45,200 --> 00:00:46,470
может вычислить

24
00:00:46,700 --> 00:00:48,190
значение функции J от тета и значения вот таких частных

25
00:00:48,620 --> 00:00:50,280
производных для j от 0 до n. Используя код,

26
00:00:50,530 --> 00:00:51,820
который может

27
00:00:51,890 --> 00:00:53,700
вычислять такие значения, метод

28
00:00:53,830 --> 00:00:54,980
градиентного спуска будет многократно

29
00:00:55,160 --> 00:00:56,710
обновлять тета по следующей

30
00:00:56,790 --> 00:00:58,620
формуле.

31
00:00:59,100 --> 00:00:59,100
Да?

32
00:00:59,280 --> 00:01:00,500
То есть градиентный спуск использует код,

33
00:01:00,670 --> 00:01:01,750
вычисляющий эти частные производные,

34
00:01:02,090 --> 00:01:03,800
подставляет полученные значения сюда и таким

35
00:01:04,480 --> 00:01:07,330
образом обновляет параметры тета.

36
00:01:08,650 --> 00:01:09,590
И можно представить это так: градиентный спуск

37
00:01:09,910 --> 00:01:11,070
требует предоставить код для вычисления J

38
00:01:11,350 --> 00:01:12,670
от тета и частных производных, а

39
00:01:12,810 --> 00:01:14,050
затем, подставляя

40
00:01:14,230 --> 00:01:15,700
результаты в

41
00:01:15,900 --> 00:01:16,930
формулу, минимизирует для нас

42
00:01:17,370 --> 00:01:20,110
функцию стоимости.

43
00:01:20,970 --> 00:01:21,970
На деле, для самого градиентного спуска

44
00:01:22,480 --> 00:01:23,790
вычисление функции стоимости J от тета не

45
00:01:24,170 --> 00:01:26,520
требуется.

46
00:01:26,940 --> 00:01:28,980
Нужен только код для вычисления частных производных.

47
00:01:29,740 --> 00:01:30,480
Но для случая, когда

48
00:01:30,590 --> 00:01:32,300
ваша программа

49
00:01:33,000 --> 00:01:34,060
также некоторым образом следит за сходимостью,

50
00:01:34,190 --> 00:01:35,440
мы исходим из того,

51
00:01:35,530 --> 00:01:37,380
что должны предоставить и код

52
00:01:37,510 --> 00:01:38,530
для вычисления J от

53
00:01:38,890 --> 00:01:40,250
тета.

54
00:01:42,700 --> 00:01:44,130
Итак, написав код для этих вычислений, мы

55
00:01:44,280 --> 00:01:45,860
можем использовать, например, метод

56
00:01:46,090 --> 00:01:47,820
градиентного спуска.

57
00:01:48,910 --> 00:01:51,590
Но метод градиентного спуска - это не единственный алгоритм, который мы можем использовать.

58
00:01:52,280 --> 00:01:53,690
Есть и другие, более сложные, более совершенные

59
00:01:54,330 --> 00:01:55,930
алгоритмы, которые используют

60
00:01:56,720 --> 00:01:57,880
различные подходы для

61
00:01:58,400 --> 00:01:59,520
минимизации функции затрат, основанные

62
00:01:59,960 --> 00:02:01,550
на возможности

63
00:02:01,760 --> 00:02:03,040
вычислить эти

64
00:02:03,490 --> 00:02:04,790
величины.

65
00:02:05,110 --> 00:02:07,910
Метод сопряженных градиентов,

66
00:02:08,110 --> 00:02:09,240
алгоритм Бройдена — Флетчера — Гольдфарба — Шанно (BFGS) и

67
00:02:09,460 --> 00:02:11,490
его модификация с ограниченным использованием памяти

68
00:02:11,640 --> 00:02:12,610
(L-BFGS) - это примеры более

69
00:02:12,810 --> 00:02:13,670
сложных алгоритмов

70
00:02:13,750 --> 00:02:15,430
оптимизации, которые при

71
00:02:15,670 --> 00:02:16,940
наличии способа вычисления J

72
00:02:17,620 --> 00:02:19,880
от тета и производных могут для минимизации функции потерь применять более сложные стратегии, чем метод градиентного спуска.

73
00:02:21,260 --> 00:02:22,560
Подробное описание этих трех алгоритмов выходит

74
00:02:22,780 --> 00:02:25,920
далеко за рамки курса.

75
00:02:26,490 --> 00:02:28,200
На самом деле, вы можете провести

76
00:02:28,650 --> 00:02:30,570
много дней или даже несколько недель, изучая эти

77
00:02:31,060 --> 00:02:32,670
алгоритмы,

78
00:02:33,240 --> 00:02:35,840
например, в углубленном курсе по численным методам,

79
00:02:36,920 --> 00:02:38,200
так что я лишь немного расскажу об их свойствах.

80
00:02:40,080 --> 00:02:42,150
У этих трех алгоритмов есть ряд преимуществ.

81
00:02:42,900 --> 00:02:44,070
Первое состоит в том, что ни для одного из них вы

82
00:02:44,290 --> 00:02:45,850
не должны вручную подбирать

83
00:02:46,000 --> 00:02:48,970
скорость обучения альфа.

84
00:02:50,670 --> 00:02:51,450
Вы можете представить себе это так:

85
00:02:51,650 --> 00:02:53,630
умея некоторым образом вычислять

86
00:02:54,230 --> 00:02:56,900
производные и функцию затрат,

87
00:02:57,320 --> 00:02:59,740
они используют их в более хитром внутреннем цикле.

88
00:03:00,060 --> 00:03:00,680
И действительно, в них есть хитрый внутренний цикл,

89
00:03:01,810 --> 00:03:03,780
который называется

90
00:03:04,200 --> 00:03:05,840
алгоритмом линейного поиска.

91
00:03:06,520 --> 00:03:08,010
Он автоматически проверяет различные значения

92
00:03:08,080 --> 00:03:09,360
скорости обучения альфа и благодаря

93
00:03:10,010 --> 00:03:11,090
этому даже может подбирать

94
00:03:12,030 --> 00:03:12,900
различную скорость обучения для

95
00:03:13,130 --> 00:03:14,570
каждой итерации.

96
00:03:15,490 --> 00:03:18,230
Таким образом, вам не нужно самим ее выбирать.

97
00:03:21,430 --> 00:03:22,770
На самом деле эти алгоритмы выполняют

98
00:03:22,910 --> 00:03:24,260
более сложные действия, чем просто

99
00:03:24,470 --> 00:03:25,640
выбор хорошей скорости обучения и

100
00:03:25,800 --> 00:03:27,300
поэтому часто оказывается, что они

101
00:03:27,490 --> 00:03:30,320
сходятся гораздо быстрее, чем метод градиентного спуска.

102
00:03:32,470 --> 00:03:33,740
Но детальное

103
00:03:33,980 --> 00:03:35,160
обсуждение

104
00:03:35,360 --> 00:03:36,740
выбор хорошей скорости обучения и

105
00:03:36,880 --> 00:03:38,770
что именно

106
00:03:39,020 --> 00:03:40,840
они делают,

107
00:03:41,040 --> 00:03:42,230
выходит за рамки этого

108
00:03:42,710 --> 00:03:44,420
курса.

109
00:03:45,580 --> 00:03:47,060
Вообще-то я долгое время,

110
00:03:47,570 --> 00:03:49,020
наверное, лет десять,

111
00:03:49,170 --> 00:03:50,170
использовал эти алгоритмы, и

112
00:03:50,470 --> 00:03:53,070
довольно часто. И только

113
00:03:53,290 --> 00:03:54,410
несколько лет назад я действительно

114
00:03:54,510 --> 00:03:55,460
разобрался, в чем суть

115
00:03:56,150 --> 00:03:57,200
алгоритмов BFGS, O-BFGS и метода

116
00:03:57,780 --> 00:04:00,220
сопряженных градиентов.

117
00:04:00,980 --> 00:04:02,740
Так что вы вполне можете

118
00:04:03,560 --> 00:04:05,380
успешно применять эти алгоритмы и решать с их помощью различные задачи

119
00:04:05,480 --> 00:04:06,530
обучения, не разбираясь в

120
00:04:06,780 --> 00:04:08,490
тонкостях устройства

121
00:04:09,460 --> 00:04:11,140
внутреннего цикла.

122
00:04:12,270 --> 00:04:13,630
Если у этих алгоритмов и есть

123
00:04:14,200 --> 00:04:15,350
недостатки, я бы сказал, что

124
00:04:15,610 --> 00:04:16,970
основной недостаток состоит в том, что

125
00:04:17,110 --> 00:04:19,390
они намного сложнее, чем метод градиентного спуска.

126
00:04:20,180 --> 00:04:21,700
В частности, думаю, вам не следует пытаться самостоятельно запрограммировать эти

127
00:04:21,970 --> 00:04:23,290
алгоритмы — метод сопряженных градиентов, L-BGFS, BFGS — разве

128
00:04:23,850 --> 00:04:26,060
только если вы специалист по

129
00:04:26,360 --> 00:04:29,520
численным методам.

130
00:04:30,720 --> 00:04:32,320
Вместо этого я посоветовал бы вам

131
00:04:32,420 --> 00:04:33,640
использовать специальную библиотеку - по

132
00:04:33,850 --> 00:04:35,240
той же причине, по которой не

133
00:04:35,590 --> 00:04:36,660
стал бы рекомендовать вам написать свой

134
00:04:36,770 --> 00:04:39,010
код для вычисления

135
00:04:39,140 --> 00:04:40,600
квадратного корня для чисел или нахождения

136
00:04:40,710 --> 00:04:42,530
обратных матриц.

137
00:04:43,030 --> 00:04:43,770
Чтобы извлечь квадратный корень, мы

138
00:04:44,120 --> 00:04:44,940
используем уже написанную

139
00:04:45,150 --> 00:04:46,440
кем-то другим функцию,

140
00:04:47,080 --> 00:04:48,310
выполняющую это

141
00:04:48,530 --> 00:04:50,200
действие.

142
00:04:51,330 --> 00:04:53,530
К счастью, в Octave (и в тесно связанном с

143
00:04:53,760 --> 00:04:55,070
ним языке MATLAB), который мы

144
00:04:55,430 --> 00:04:57,110
будем использовать, есть

145
00:04:57,140 --> 00:04:58,370
очень хорошая...есть весьма

146
00:04:58,530 --> 00:05:02,410
неплохая библиотека, где некоторые из более сложных алгоритмов оптимизации уже реализованы.

147
00:05:03,380 --> 00:05:04,350
И если вы будете использовать встроенную библиотеку, то

148
00:05:04,600 --> 00:05:06,800
получите очень хорошие результаты.

149
00:05:08,010 --> 00:05:08,880
Хочу заметить, что между хорошими и плохими реализациями

150
00:05:09,370 --> 00:05:10,880
этих алгоритмов есть

151
00:05:11,230 --> 00:05:12,740
существенная разница.

152
00:05:13,690 --> 00:05:15,010
Так что если для решения задачи

153
00:05:15,120 --> 00:05:16,270
машинного обучения вы используете другой язык — C, C++,

154
00:05:16,470 --> 00:05:17,560
Java или еще какой-то, — вам,

155
00:05:18,190 --> 00:05:20,090
вероятно, стоит пересмотреть несколько

156
00:05:20,250 --> 00:05:24,060
библиотек, чтобы быть уверенным,

157
00:05:24,210 --> 00:05:24,710
что вы используете

158
00:05:24,730 --> 00:05:25,660
хорошую

159
00:05:25,740 --> 00:05:27,790
реализацию.

160
00:05:28,250 --> 00:05:29,410
Поскольку хорошая

161
00:05:29,480 --> 00:05:30,740
реализация метода сопряженных

162
00:05:31,680 --> 00:05:33,150
градиентов, или L-BFGS, может

163
00:05:33,530 --> 00:05:35,150
отличаться от менее удачной его

164
00:05:35,350 --> 00:05:37,680
реализации производительностью.

165
00:05:43,060 --> 00:05:44,310
Теперь я хочу объяснить, как эти алгоритмы

166
00:05:44,580 --> 00:05:47,080
используются, на примере.

167
00:05:48,970 --> 00:05:50,220
Допустим, у вас есть задача с

168
00:05:50,370 --> 00:05:51,620
двумя параметрами,

169
00:05:53,380 --> 00:05:55,580
тета 1 и тета 2.

170
00:05:56,410 --> 00:05:57,450
И, пусть ваша функция затрат J от

171
00:05:57,970 --> 00:05:59,210
тета равняется сумме

172
00:05:59,430 --> 00:06:01,540
квадратов тета 1 минус 5 и тета 2 минус 5.

173
00:06:02,630 --> 00:06:04,080
Для этой функции затрат

174
00:06:04,590 --> 00:06:06,960
искомые значения тета 1 и тета 2,

175
00:06:07,080 --> 00:06:09,590
значения, при которых J минимальна,

176
00:06:09,940 --> 00:06:10,910
это тета 1 равно

177
00:06:11,030 --> 00:06:12,040
5 и тета 2

178
00:06:12,420 --> 00:06:14,220
равно 5.

179
00:06:15,230 --> 00:06:16,620
Вы в разной мере знакомы с

180
00:06:16,950 --> 00:06:18,320
дифференциальным исчислением,

181
00:06:19,010 --> 00:06:20,770
поэтому я просто написал выражения

182
00:06:20,850 --> 00:06:23,420
для частных производных J.

183
00:06:24,270 --> 00:06:25,060
Я продифференцировал функцию.

184
00:06:26,260 --> 00:06:27,250
Итак, если вы

185
00:06:27,480 --> 00:06:29,220
хотите применить

186
00:06:29,810 --> 00:06:31,380
один из более совершенных

187
00:06:31,660 --> 00:06:32,630
алгоритмов для минимизации функции J — представьте, что мы не

188
00:06:32,880 --> 00:06:34,680
знаем, что

189
00:06:34,780 --> 00:06:36,140
минимум

190
00:06:36,240 --> 00:06:37,550
достигается при значениях 5 и 5, и

191
00:06:37,970 --> 00:06:39,840
хотим вычислить эти значения программно,

192
00:06:40,040 --> 00:06:41,560
желательно, используя что-то

193
00:06:41,730 --> 00:06:43,430
получше, чем градиентный

194
00:06:43,550 --> 00:06:45,010
спуск, — вам нужно будет

195
00:06:45,570 --> 00:06:46,690
написать в Octave

196
00:06:46,860 --> 00:06:48,190
примерно такую функцию.

197
00:06:49,210 --> 00:06:51,180
То есть мы зададим функцию

198
00:06:52,180 --> 00:06:53,250
costFunction(theta), как здесь, которая

199
00:06:53,380 --> 00:06:55,660
возвращает два значения. Первый, jVal — это значение самой

200
00:06:55,760 --> 00:06:57,780
функции J, поэтому

201
00:06:58,910 --> 00:07:00,020
я пишу jVal равно, ну,

202
00:07:00,680 --> 00:07:01,780
вы понимаете, тета 1

203
00:07:02,080 --> 00:07:03,210
минус 5 в квадрате

204
00:07:03,440 --> 00:07:04,630
плюс тета 2 минус 5 в

205
00:07:05,330 --> 00:07:06,230
квадрате.

206
00:07:06,540 --> 00:07:09,140
Просто выражение для вычисления функции затрат.

207
00:07:10,540 --> 00:07:12,040
А второе возвращаемое

208
00:07:12,260 --> 00:07:14,190
значение — это градиент.

209
00:07:14,840 --> 00:07:16,030
Градиент — это вектор 2х1, и

210
00:07:16,160 --> 00:07:17,320
два его элемента

211
00:07:18,870 --> 00:07:20,050
соответствуют двум частным

212
00:07:20,120 --> 00:07:22,100
производным,

213
00:07:22,800 --> 00:07:24,670
записанным вот здесь.

214
00:07:27,150 --> 00:07:28,570
Реализовав таким образом

215
00:07:29,580 --> 00:07:30,390
функцию потерь, вы сможете затем

216
00:07:31,510 --> 00:07:33,010
вызвать функцию усовершенствованной

217
00:07:34,270 --> 00:07:35,720
оптимизации, которая в Octave называется fminunc.

218
00:07:35,950 --> 00:07:36,900
Это аббревиатура для

219
00:07:37,610 --> 00:07:39,360
Function (функция) MINimization (минимизации) UNConstrained (без ограничений).

220
00:07:40,300 --> 00:07:41,520
Вызвать ее можно следующим образом.

221
00:07:41,790 --> 00:07:42,350
Вы задаете несколько параметров.

222
00:07:43,230 --> 00:07:43,580
Переменная options представляет структуру

223
00:07:44,330 --> 00:07:46,680
данных, где хранятся настройки алгоритма:

224
00:07:47,320 --> 00:07:48,960
'GradObj' и 'on' означают, что

225
00:07:49,160 --> 00:07:52,100
используется метод с использованием градиента,

226
00:07:52,270 --> 00:07:55,180
то есть что вы в самом деле предоставите алгоритму код для вычисления градиента.

227
00:07:56,150 --> 00:07:57,550
Максимальное количество итераций я задам

228
00:07:57,840 --> 00:07:59,280
равным, допустим, 100.

229
00:07:59,580 --> 00:08:02,230
Зададим также начальное значение для тета.

230
00:08:02,720 --> 00:08:03,680
Это вектор 2x1.

231
00:08:04,440 --> 00:08:06,860
Затем этой командой вы вызываете функцию fminunc.

232
00:08:07,530 --> 00:08:10,290
С помощью символа @, коммерческого at,

233
00:08:10,420 --> 00:08:11,810
передается указатель на функцию costFunction, которую мы

234
00:08:13,010 --> 00:08:14,320
только что определили выше.

235
00:08:15,060 --> 00:08:16,020
Когда вы запустите эту команду, вычисления будут

236
00:08:16,270 --> 00:08:18,290
выполнены с помощью одного из

237
00:08:18,620 --> 00:08:20,490
более совершенных алгоритмов оптимизации.

238
00:08:21,110 --> 00:08:23,350
Вы можете считать, что это то же, что и градиентный спуск,

239
00:08:23,690 --> 00:08:25,170
только скорость обучения альфа задается автоматически,

240
00:08:25,500 --> 00:08:27,290
вам не нужно выбирать его самим.

241
00:08:28,210 --> 00:08:29,880
Но на деле будет использован более

242
00:08:30,160 --> 00:08:32,000
сложный алгоритм оптимизации,

243
00:08:32,640 --> 00:08:33,770
можно сказать, «прокачанный» градиентный спуск,

244
00:08:34,400 --> 00:08:36,490
который постарается найти для вас оптимальное значение тета.

245
00:08:37,180 --> 00:08:39,040
Давайте я покажу вам, как это выглядит в Octave.

246
00:08:40,690 --> 00:08:42,460
Функция costFunction(theta) точно такая же, как

247
00:08:42,900 --> 00:08:46,440
на предыдущем слайде.

248
00:08:46,650 --> 00:08:49,070
Она вычисляет jVal — значение функции стоимости,

249
00:08:49,920 --> 00:08:51,810
и градиент, два элемента которого — частные

250
00:08:52,040 --> 00:08:53,050
производные функции затрат по каждому

251
00:08:53,450 --> 00:08:54,430
из двух

252
00:08:55,220 --> 00:08:56,200
параметров,

253
00:08:56,360 --> 00:08:57,910
тета 1 и тета 2.

254
00:08:59,040 --> 00:09:00,360
Давайте переключимся в окно Octave.

255
00:09:00,710 --> 00:09:02,900
Я введу команды, которые только что описывал.

256
00:09:03,470 --> 00:09:05,850
options равно optimset,

257
00:09:06,630 --> 00:09:08,510
так в options записываются

258
00:09:09,670 --> 00:09:11,190
параметры алгоритма

259
00:09:11,710 --> 00:09:13,850
оптимизации,

260
00:09:14,130 --> 00:09:17,600
GradObj — on, MaxIter — 100, то есть: не

261
00:09:18,310 --> 00:09:19,610
более сотни итераций и я предоставлю

262
00:09:19,730 --> 00:09:22,090
алгоритму градиент.

263
00:09:23,490 --> 00:09:27,190
И начальное значение initialTheta равно вектору 2х1, заполненному нулями.

264
00:09:27,980 --> 00:09:29,280
Это мое изначальное предположение для тета.

265
00:09:30,500 --> 00:09:31,390
Теперь получим

266
00:09:32,620 --> 00:09:35,100
значения [optTheta, functionVal, exitFlag]

267
00:09:37,610 --> 00:09:39,430
равны fminunc — f-min-unconstrained,

268
00:09:40,570 --> 00:09:41,600
передадим указатель на функцию затрат,

269
00:09:43,010 --> 00:09:44,700
первое предположение о тета,

270
00:09:46,090 --> 00:09:49,060
и параметры алгоритма, вот так.

271
00:09:49,820 --> 00:09:52,760
И если я нажму Enter, запустится алгоритм оптимизации,

272
00:09:53,940 --> 00:09:54,810
и мы очень быстро получим результат.

273
00:09:55,790 --> 00:09:57,040
Здесь код выглядит так

274
00:09:57,430 --> 00:09:58,430
странно, потому что он слишком

275
00:09:59,700 --> 00:10:00,290
длинный,

276
00:10:00,680 --> 00:10:02,540
то есть это обозначает, что начало команды

277
00:10:02,760 --> 00:10:04,890
скрыто, а видна только часть после переноса строки.

278
00:10:05,490 --> 00:10:06,290
А дальше написан, собственно, результат

279
00:10:06,550 --> 00:10:08,500
численного решения, алгоритм, так

280
00:10:08,670 --> 00:10:10,400
сказать, «прокачанный» градиентный спуск

281
00:10:10,440 --> 00:10:11,620
нашел оптимальное

282
00:10:11,760 --> 00:10:13,150
значение вектора тета: тета 1 равно 5, тета 2 равно 5,

283
00:10:13,400 --> 00:10:15,670
как мы и надеялись.

284
00:10:16,520 --> 00:10:18,760
Значение функции в экстремуме

285
00:10:18,840 --> 00:10:21,430
равно 10 в минус тридцатой степени,

286
00:10:21,670 --> 00:10:23,160
то есть, по сути, ноль, на что мы тоже

287
00:10:23,370 --> 00:10:24,760
рассчитывали.

288
00:10:24,840 --> 00:10:27,060
Флаг завершения, exitFlag, равен 1 — это

289
00:10:27,240 --> 00:10:29,080
показатель того, что

290
00:10:29,730 --> 00:10:31,400
алгоритм сошелся.

291
00:10:31,800 --> 00:10:33,010
Если хотите, запустите команду help fminunc и

292
00:10:33,150 --> 00:10:35,020
прочитайте,

293
00:10:35,130 --> 00:10:36,480
какие значения может

294
00:10:36,680 --> 00:10:38,650
принимать exitFlag и в каких случаях.

295
00:10:38,760 --> 00:10:41,600
Главное, что он позволяет узнать, считает ли алгоритм, что он сошелся.

296
00:10:43,960 --> 00:10:46,450
Вот так можно запустить эти алгоритмы в Octave.

297
00:10:47,480 --> 00:10:48,920
Должен сказать, что для использования алгоритмов,

298
00:10:48,940 --> 00:10:51,020
реализованных в Octave, ваш

299
00:10:51,640 --> 00:10:53,010
вектор параметров theta должен

300
00:10:53,370 --> 00:10:54,940
принадлежать d-мерному вещественному

301
00:10:55,280 --> 00:10:58,210
пространству, где d больше либо равно 2.

302
00:10:58,450 --> 00:11:00,330
Так что если theta — одно вещественное число,

303
00:11:00,770 --> 00:11:02,040
если размерность вектора меньше

304
00:11:02,160 --> 00:11:03,160
двух, fminunc может не

305
00:11:03,800 --> 00:11:04,860
сработать. Если вам

306
00:11:05,160 --> 00:11:06,840
нужно минимизировать функцию

307
00:11:07,560 --> 00:11:08,760
одного вещественного аргумента,

308
00:11:09,140 --> 00:11:10,310
почитайте документацию по

309
00:11:10,590 --> 00:11:11,590
fminunc в Octave и

310
00:11:11,830 --> 00:11:12,930
посмотрите,

311
00:11:13,100 --> 00:11:14,680
как это можно

312
00:11:14,950 --> 00:11:16,230
сделать.

313
00:11:18,230 --> 00:11:19,360
Так мы минимизировали взятую для

314
00:11:19,620 --> 00:11:21,640
примера простую квадратичную функцию

315
00:11:22,190 --> 00:11:23,810
затрат,

316
00:11:24,440 --> 00:11:26,520
но как нам применить этот алгоритм для логистической регрессии?

317
00:11:27,720 --> 00:11:29,270
В задаче логистической регрессии у нас есть

318
00:11:29,520 --> 00:11:31,290
вектор параметров тета... Я сейчас буду использовать смесь

319
00:11:31,430 --> 00:11:32,210
математической нотации и

320
00:11:32,620 --> 00:11:34,880
нотации Octave,

321
00:11:35,300 --> 00:11:36,400
но, надеюсь, объяснение

322
00:11:36,870 --> 00:11:38,050
будет

323
00:11:38,520 --> 00:11:40,360
понятным.

324
00:11:40,540 --> 00:11:41,780
Так вот, вектор тета содержит параметры от

325
00:11:42,210 --> 00:11:44,230
тета 0 до тета n, но Octave

326
00:11:46,090 --> 00:11:48,040
нумерует элементы векторов, начиная с 1, то

327
00:11:48,460 --> 00:11:49,640
есть тета нулевому в Octave будет

328
00:11:49,710 --> 00:11:51,190
соответствовать theta(1), тета

329
00:11:51,330 --> 00:11:53,290
первому —theta(2), и

330
00:11:53,930 --> 00:11:54,690
так

331
00:11:55,280 --> 00:11:56,180
до theta(n+1),

332
00:11:56,780 --> 00:11:58,430
понятно?

333
00:11:58,610 --> 00:12:00,650
Все потому, что Octave нумерует

334
00:12:01,320 --> 00:12:03,070
элементы векторов начиная с

335
00:12:03,430 --> 00:12:05,200
индекса 1, а не с 0.

336
00:12:06,920 --> 00:12:07,950
Теперь нам нужно написать

337
00:12:08,160 --> 00:12:09,670
функцию затрат

338
00:12:09,880 --> 00:12:12,070
costFunction,

339
00:12:12,710 --> 00:12:14,210
которая соответствует функции затрат для логистической регрессии.

340
00:12:15,170 --> 00:12:16,450
А именно, она должна возвращать

341
00:12:16,880 --> 00:12:18,310
jVal, то есть нам нужно

342
00:12:18,940 --> 00:12:20,430
написать код для вычисления

343
00:12:20,640 --> 00:12:22,440
затем, подставляя

344
00:12:22,710 --> 00:12:24,010
вычислить градиент.

345
00:12:24,540 --> 00:12:25,460
Так, для gradient(1) нам

346
00:12:25,920 --> 00:12:27,080
нужно

347
00:12:27,280 --> 00:12:29,100
написать код, вычисляющий значение частной производной по тета 0, затем — по

348
00:12:29,390 --> 00:12:31,250
тета 1, и так

349
00:12:31,600 --> 00:12:34,300
далее.

350
00:12:34,770 --> 00:12:36,260
Еще раз, это gradient(1),

351
00:12:37,500 --> 00:12:38,390
gradient(2) и так

352
00:12:39,030 --> 00:12:40,330
далее, а не

353
00:12:40,500 --> 00:12:42,730
gradient(0) и gradient(1), поскольку

354
00:12:43,460 --> 00:12:46,200
Octave индексирует векторы начиная с единицы, а не с нуля.

355
00:12:47,440 --> 00:12:48,460
Но основная идея, которую, как я

356
00:12:48,690 --> 00:12:49,540
надеюсь, вы вынесете

357
00:12:49,900 --> 00:12:50,870
из этого слайда, — нам

358
00:12:51,070 --> 00:12:54,370
нужно написать функцию, которая

359
00:12:55,500 --> 00:12:56,930
возвращает значения функции затрат и градиента.

360
00:12:58,410 --> 00:12:59,750
Таким образом, чтобы использовать

361
00:12:59,960 --> 00:13:01,410
сложные алгоритмы в задаче логистической

362
00:13:02,100 --> 00:13:03,430
регрессии — или даже в

363
00:13:03,560 --> 00:13:06,230
задаче линейной регресии, если вам это нужно, —

364
00:13:07,340 --> 00:13:08,350
вам необходимо

365
00:13:08,500 --> 00:13:09,960
написать здесь

366
00:13:10,820 --> 00:13:12,280
соответствующий код.

367
00:13:15,100 --> 00:13:17,910
Теперь вы знаете, как использовать более совершенные алгоритмы оптимизации.

368
00:13:19,030 --> 00:13:21,170
Из-за того, что для этих

369
00:13:21,320 --> 00:13:22,660
алгоритмов вы используете

370
00:13:22,870 --> 00:13:25,190
сложные оптимизационные библиотеки,

371
00:13:25,690 --> 00:13:26,710
отладка становится

372
00:13:26,940 --> 00:13:28,510
немного более непрозрачной и

373
00:13:28,740 --> 00:13:30,390
трудной.

374
00:13:31,290 --> 00:13:32,660
Поскольку зачастую они работают

375
00:13:33,010 --> 00:13:34,370
быстрее, чем градиентный

376
00:13:35,010 --> 00:13:36,760
спуск, для объемных

377
00:13:37,060 --> 00:13:38,180
задач машинного обучения, я

378
00:13:38,410 --> 00:13:39,500
скорее буду использовать именно

379
00:13:39,760 --> 00:13:42,110
их.

380
00:13:43,900 --> 00:13:45,070
Разобравшись с этими идеями, надеюсь, вы сможете

381
00:13:45,450 --> 00:13:46,710
заставить работать логистическую или

382
00:13:47,350 --> 00:13:48,780
линейную регрессию для

383
00:13:49,100 --> 00:13:51,410
решения много более масштабных задач.

384
00:13:51,830 --> 00:13:53,820
На этом я заканчиваю разговор о сложных алгоритмах оптимизации.

385
00:13:55,120 --> 00:13:56,170
А в следующем и последнем видео по

386
00:13:56,320 --> 00:13:57,720
логистической регрессии я расскажу, как применять

387
00:13:58,550 --> 00:13:59,470
алгоритм логистической регрессии,

388
00:13:59,600 --> 00:14:00,990
который вы теперь знаете, в

389
00:14:01,520 --> 00:14:02,790
задачах многоклассовой

390
00:14:02,990 --> 00:14:05,420
классификации.