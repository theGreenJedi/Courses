इस वीडियो में, हम बात करेंगे कि थीटा के पैरामीटर्ज़ फिट कैसे करने हैं लॉजिस्टिक रिग्रेशन में. ख़ासकर, मैं परिभाषित करूँगा ऑप्टिमायज़ेशन अब्जेक्टिव, या कॉस्ट फ़ंक्शन जो हम इस्तेमाल करेंगे फ़िट करने के लिए पेरमिटर्स. यहाँ है सूपर्वायज़्ड लर्निंग प्रॉब्लम फ़िट करने के लिए लजिस्टिक रेग्रेशन मॉडल. हमारे पास है एक ट्रेनिंग सेट एम ट्रेनिंग इग्ज़ाम्पल्ज़ का और हमेशा की तरह, प्रत्येक हमारा इग्ज़ाम्पल दर्शाया गया है एक फ़ीचर वेक्टर से जो है एक एन प्लस एक डिमेन्शनल, और हमेशा की तरह हमारे पास है एक्स 0 बराबर एक. पहला फ़ीचर या एक ज़ीरो फ़ीचर है हमेशा बराबर एक. और क्योंकि यह एक कॉम्प्यूटेशनल प्रॉब्लम है, हमारे ट्रेनिंग सेट की प्रॉपर्टी / गुणस्वभाव है प्रत्येक लेबल वाय या 0 है या 1. यह है एक हायपॉथिसस, और पेरमिटर्स इस हायपॉथिसस के हैं यह थीटा यहाँ पर. और प्रश्न जिसकी मैं बात करना चाहता हूँ है कि दिया होने पर यह ट्रेनिंग सेट, कैसे करते हैं हम चुनाव, या कैसे फ़िट करते हैं पेरमिटर्स थीटा? पहले जब हम बना रहे थे लिनीअर रेग्रेशन मॉडल, हमने इस्तेमाल किया था निम्न कॉस्ट फ़ंक्शन. मैं लिखा है इसे थोड़ा भिन्न ढंग से जहाँ बजाय लिखने के 1 ओवर 2एम, मैंने लिया एक आधा / वन-हाफ़ और डाला है इसे समेशन के अंदर. अब मैं इस्तेमाल करना चाहता हूँ एक वैकल्पिक ढंग लिखने का इस कॉस्ट फ़ंक्शन को. जो है कि लिखने के स्थान पर यह स्क्वेर टर्म यहाँ, चलिए लिखते हैं यहाँ कॉस्ट्स एच ऑफ़ एक्स, वाय और और मैं करूँगा परिभाषित वह कुल / टोटल कॉस्ट एच ऑफ़ एक्स, वाय बराबर इसके. सिर्फ़ बराबर इस स्क्वेर्ड एरर के आधे के. तो अब हम अधिक स्पष्ट रूप से देख सकते हैं कि कॉस्ट फ़ंक्शन है एक सम मेरे ट्रेनिंग सेट पर, जो है 1 ओवर एन टाइम्ज़ सम मेरे ट्रेनिंग सेट की इस कॉस्ट टर्म का यहाँ. और सरल करने के लिए इस इक्वेज़न को थोड़ा और, यह होगा सुविधाजनक हटा देना उन सूपरस्क्रिप्ट्स को. तो सिर्फ़ परिभाषित करें कॉस्ट एच ऑफ़ एक्स कॉमा वाय बराबर आधा इस स्क्वेर्ड एरर का. और अर्थ इस कॉस्ट फ़ंक्शन का है कि, यह है कॉस्ट जो मैं चाहता हूँ मेरा लर्निंग अल्गोरिद्म चुकाए / दे, यदि यह आउट्पुट करता है वह वैल्यू, यदि इसकी प्रिडिक्शन है एच ऑफ़ एक्स और असली लेबल था वाय. तो सिर्फ़ काट देता हैं सूपरस्क्रिप्ट्स, सही, और कोई आश्चर्य नहीं लिनीअर रेग्रेशन के लिए जो कॉस्ट हमने परिभाषित की है या इसकी कॉस्ट है यह वन-हाफ़ टाइम्ज़ स्क्वेर अंतर का बीच में मैंने प्रिडिक्ट किया है और जो असली वैल्यू है जो हमारे पास है 0 वाय के लिए. अब इस फ़ंक्शन ने सही काम किया लिनीअर रेग्रेशन के लिए. लेकिन यहाँ, हमारी रुचि है लजिस्टिक रेग्रेशन में. यदि हम मिनमायज़ कर सकते इस कॉस्ट फ़ंक्शन को जो प्लग किया है जे में यहाँ. यह सही काम करेगा. लेकिन ऐसा होता है कि यदि हम इस्तेमाल करते हैं यह विशेष कॉस्ट फ़ंक्शन, यह होगा एक नॉन-कान्वेक्स फ़ंक्शन पेरमिटर्स थीटा का. यहाँ है कि मेरा क्या मतलब है नॉन-कान्वेक्स से. यहाँ है मेरा कॉस्ट फ़ंक्शन जे ऑफ़ थीटा और लजिस्टिक रेग्रेशन के लिए, यह फ़ंक्शन एच यहाँ है नॉन लिनीअर जो है वन ओवर वन प्लस ई टु नेगेटिव थीटा ट्रान्स्पोज़. तो वह है एक काफ़ी पेचीदा नॉनलिनीअर फ़ंक्शन. और यदि आप लेते हैं फ़ंक्शन प्लग करते हैं इसे यहाँ. और तब लेते हैं यह कॉस्ट फ़ंक्शन और प्लग करते हैं इसे वहाँ और फिर प्लॉट करते हैं कि जे ऑफ़ थीटा कैसा दिखता है. आप पाते हैं कि जे ऑफ़ थीटा दिख सकता है एक फ़ंक्शन की तरह जो ऐसा है बहुत से लोकल मिनिमा के साथ. और औपचारिक टर्म उसके लिए हैं कि यह है एक नॉन-कान्वेक्स फ़ंक्शन. और आप एक तरह से बता सकते हैं, यदि आपको रन करना होता ग्रेडीयंट डिसेंट इस तरह के फ़ंक्शन पर यह नहीं है गारंटी कि यह कन्वर्ज होगा ग्लोबल मिनिमम पर. जबकि इसके विपरीत हम क्या चाहेंगे कि हो एक कॉस्ट फ़ंक्शन जे ऑफ़ थीटा जो है कान्वेक्स, जो है एक धनुष के आकार का फ़ंक्शन जो दिखता हैं ऐसा ताकि यदि रन करें ग्रेडीयंट डिसेंट हमें गारंटी होगी कि वह कन्वर्ज करेगा ग्लोबल मिनिमम पर. और समस्या इस स्क्वेर्ड कॉस्ट फ़ंक्शन को इस्तेमाल करने से है क्योंकि यह बहुत नॉन-लिनीअर फ़ंक्शन जो है यहाँ मध्य में, जे ऑफ़ थीटा हो जाता है एक नॉन-कान्वेक्स फ़ंक्शन यदि आपको परिभाषित करना होता इसे एक स्क्वेर कॉस्ट फ़ंक्शन. तो मैं क्या करना चाहूँगा कि, बजाय बनाने के एक भिन्न कॉस्ट फ़ंक्शन, जो कान्वेक्स है, और ताकि हम अप्लाई कर सकें एक बढ़िया अल्गोरिद्म, जैसे ग्रेडीयंट डिसेंट और हमें गारंटी हो जाए ग्लोबल मिनिमम मिलने की. यहाँ है कॉस्ट फ़ंक्शन जो हम इस्तेमाल करेंगे लजिस्टिक रेग्रेशन के लिए. हम कहेंगे कि कॉस्ट, या पेनल्टी / जुर्माना जो अल्गोरिद्म अदा करता है, यदि यह देता है वैल्यू एच(एक्स), तो यदि यह है कोई संख्या जैसे 0.7, यह प्रिडिक्ट करता है वैल्यू एच ऑफ़ एक्स. और असली कॉस्ट लेबल है वाय. कॉस्ट होगी -लॉग(एच(एक्स)) यदि वाय = 1 और -लॉग (1-एच(एक्स)) यदि वाय =0. यह दिखता है एक काफ़ी पेचीदा फ़ंक्शन, लेकिन चलिए प्लॉट करते हैं इस फ़ंक्शन को समझने के लिए कि यह क्या कर रहा है. चलिए शुरू करते हैं केस वाय = 1 से. यदि वाय = 1, तब यह कोस्ट फ़ंक्शन है -लॉग(एच(एक्स)). और यदि हम प्लॉट करे उसे, तो मान लो कि हॉरिज़ॉंटल ऐक्सिस है एच(एक्स). तो अब हम जानते हैं कि एक हायपॉथिसस आउट्पुट करेगी एक वैल्यू 0 और 1 के बीच में. सही, एच(एक्स), वह रहता है 0 और 1 के बीच. यदि आप प्लॉट करते हैं कि यह कॉस्ट फ़ंक्शन कैसा दिखता हैं, आप पाते हैं कि यह ऐसा दिखता है. एक नज़रिया देखने का कि यह क्यों ऐसा दिखता है क्योंकि यदि आपको प्लॉट करना होता लॉग ज़ी ज़ी हॉरिज़ॉंटल ऐक्सिस पर, तब वह ऐसा दिखता. और यह जाता है माइनस इन्फ़िनिटी, सही? तो ऐसा दिखता है लॉग फ़ंक्शन. और यह है 0, यह है 1. यहाँ, ज़ी निस्संदेह रोल कर रहा है एच ऑफ़ एक्स का. और इसलिए -लॉग ज़ी दिखेगा ऐसा. केवल बदल देने से साइन, माइनस लॉग ज़ी, और हमें रुचि है सिर्फ़ रेंज में कि कब यह फ़ंक्शन होता है ज़ीरो और एक के बीच, तो उसे छोड़ देते हैं. और इसलिए हमारे पास बचता है सिर्फ़, आप जानते हैं, यह हिस्सा कर्व का, और वैसा यह कर्व बाईं तरफ़ दिखता है. अब, इस कॉस्ट फ़ंक्शन के हैं कुछ दिलचस्प और वांछनीय गुणस्वभाव. पहला, आप ध्यान करो कि यदि वाय है बराबर 1 और एच(एक्स) है बराबर 1, दूसरे शब्दों में, यदि हायपॉथिसस ठीक-ठीक प्रिडिक्ट करती है एच बराबर 1 और वाय है वास्तव में बराबर उसके जो प्रिडिक्ट किया है, तब कॉस्ट = 0 सही? वह कॉरेस्पॉंड करता है कर्व को वाक़ई में दबाया हुआ नहीं है. कर्व अभी भी आगे जा रहा है. पहले, ध्यान दें कि यदि एच(एक्स)=1, यदि वह हायपॉथिसस प्रिडिक्ट करती है वाय =1 और असली में वाय=1 तब कॉस्ट=0. वह कॉरेस्पॉंड करता है इस पोईँट को नीचे यहाँ, सही? यदि एच(एक्स)=1 और हम केवल ले रहे है केस वाय =1 यह. लेकिन यदि एच(एक्स)=1 तब कॉस्ट है नीचे यहाँ, है बराबर 0. और वह है जहाँ हम चाहते हैं होना क्योंकि यदि हम सही प्रिडिक्ट करेंगे आउट्पुट वाय, तब कॉस्ट है 0. लेकिन अब ध्यान करें कि जैसे एच(एक्स) पहुँचता है 0, तो जैसे आउट्पुट एक हायपॉथिसस की पहुँचती है 0, कॉस्ट अत्यधिक बढ़ जाती है और जाती है इन्फ़िनिटी तक. और यह क्या करता है कि यह कैप्चर करता है कि यदि एक हायपॉथिसस 0 की, है ऐसा कहने जैसा कि हायपॉथिसस बता रही है कि वाय बराबर है 1 की सम्भावना है 0. यह एक प्रकार से है जाना हमारे रोगी के पास और कहना कि सम्भावना कि आपको एक मेलिगनेंट ट्यूमर है, सम्भावना कि वाय=1, ज़ीरो है. अत:, यह बिल्कुल सम्भावित नहीं है कि आपका ट्यूमर मेलिगनेंट है. लेकिन यदि ऐसा होता है कि ट्यूमर, रोगी का ट्यूमर, असल में मेलिगनेंट है, तो यदि वाय है बराबर एक, हमारे उसे बताने के बाद भी, कि सम्भावना वह होने की ज़ीरो है. अत:, यह बिल्कुल सम्भावित नहीं है कि यह मेलिगनेंट है. लेकिन यदि हम उन्हें बताते है इतनी दृढ़ता से और हम ग़लत सिद्ध हो जाते हैं तब हम दण्डित करते हैं लर्निंग अल्गोरिद्म को एक बहुत ही बड़ी कॉस्ट से. और वह कैप्चर होता है करने से इस कॉस्ट पहुँचाने से इन्फ़िनिटी तक यदि वाय बराबर है 1 और एच(एक्स) पहुँचता है 0. यह स्लाइड लेती है केस वाय बराबर 1 का. चलो देखते हैं कि यह कॉस्ट फ़ंक्शन कैसा दिखता हैं वाय बराबर 0 के लिए. यदि वाय है बराबर 0 के, तब कोस्ट फ़ंक्शन ऐसा दिखता है, यह दिखता है इस इक्स्प्रेशन के जैसे यहाँ पर, और यदि आप प्लॉट करते हैं फ़ंक्शन, -लॉग(1-ज़ी), आपको जो मिलता है वह है कॉस्ट फ़ंक्शन वास्तव में ऐसा दिखता है. तो यह जाता है 0 से 1, कुछ वैसे और इसलिए यदि आप प्लॉट करते हैं कॉस्ट फ़ंक्शन को वाय बराबर 0 के केस में, आप पाते हैं कि यह ऐसा दिखता है. और यह कर्व क्या करता है यह अब ऊपर जाता है और यह जाता है प्लस इन्फ़िनिटी पर जैसे एच ऑफ़ एक्स होता है 1 क्योंकि जैसे कि मैं कह रहा था, कि यदि वाय हो जाता है बराबर 0 के. लेकिन हमने प्रिडिक्ट किया था कि वाय है बराबर 1 लगभग निश्चिंतता के साथ, शायद 1, तब हमें देनी पड़ती है एक बहुत बड़ी कॉस्ट. और इसके विपरीत,
134
00:09:23,080 --> 000:09:28,080
यदि एच ऑफ़ एक्स है 0 और वाय बराबर है 0, तब हायपॉथिसस ने सही 
135
000:09:28,080 --> 000:09:32,080
सही प्रिडिक्ट किया है वाय बराबर 0, और वाय हुआ भी बराबर 0,
136
000:09:32,080 --> 000:09:37,080
तो इस पोईँट पर, कॉस्ट फ़ंक्शन होगा 0.
137
000:09:37,080 --> 000:09:42,080
इस विडीओ में हम परिभाषित करेंगे कॉस्ट फ़ंक्शन एक ट्रेनिंग इग्ज़ाम्पल के लिए.
138
000:09:42,080 --> 000:09:46,080
कन्वेक्सिटी अनालिसिस का विषय इस पाठ्य क्रम के विषय क्षेत्र से बाहर है, लेकिन 
139
000:09:46,080 --> 000:09:51,080
यह सम्भव है दिखा पाना कि एक ख़ास कॉस्ट फ़ंक्शन के साथ,
140
000:09:51,080 --> 000:09:55,080
यह देगा एक कान्वेक्स ऑप्टिमायज़ेशन प्रॉब्लम.
141
000:09:55,080 --> 000:10:00,080
ओवरॉल कॉस्ट फ़ंक्शन जे ऑफ़ थीटा होगा कान्वेक्स और लोकल ऑप्टिमा नहीं होंगे.
142
000:10:00,080 --> 000:10:05,080
अगले विडीओ में हम आगे ले जाएँगे इन कॉस्ट फ़ंक्शन के सुझावों को 
143
000:10:05,080 --> 000:10:10,080
एक ट्रेनिंग इग्ज़ाम्पल के और आगे डिवेलप करेंगे, और परिभाषित करेंगे कॉस्ट फ़ंक्शन 
144
000:10:10,080 --> 000:10:14,080
ट्रेनिंग सेट के लिए.
145
000:10:14,080 --> 000:10:18,080
और हम निकालेंगे एक आसान तरीक़ा इसे लिखने का बजाय जो हम प्रयोग कर रहे थे 
146
000:10:18,080 --> 000:10:20,080
अभी तक, और निर्भर करते हुए उस पर हम गणन करेंगे ग्रेडीयंट डिसेंट, और 
147
000:10:20,080 --> 000:10:22,080
जो हमें देगा हमारा लजिस्टिक रेग्रेशन अल्गोरिद्म.