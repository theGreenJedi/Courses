1
00:00:00,300 --> 00:00:01,680
در ویدیو ی قبلی 

2
00:00:01,990 --> 00:00:03,920
درباره کاربرد روش گرادیان کاهشی برای کمینه سازی 

3
00:00:04,440 --> 00:00:06,700
تابع هزینه J(theta) برای رگرسیون لاجیستیک صحبت کردیم.

4
00:00:07,800 --> 00:00:08,930
در این ویدئو می خواهم 

5
00:00:09,020 --> 00:00:10,250
به شما راجع به 

6
00:00:10,850 --> 00:00:12,340
الگوریتمهای بهینه سازی  و برخی

7
00:00:12,670 --> 00:00:14,060
موضوعات پیشرفته بهینه سازی صحبت کنم.

8
00:00:15,180 --> 00:00:16,480
با استفاده از برخی از این ایده ها، ما

9
00:00:16,630 --> 00:00:17,930
می توانیم رگرسیون لاجیستیک را

10
00:00:19,010 --> 00:00:20,220
با بسیار سریع تر از

11
00:00:20,350 --> 00:00:21,970
مقدار ممکن با گرادیان کاهشی اجرا کنیم.

12
00:00:22,880 --> 00:00:24,190
و این همچنین کمک می کند که

13
00:00:24,320 --> 00:00:26,060
الگوریتم ها  بسیار بهتر در مقیاس  

14
00:00:26,670 --> 00:00:28,030
مسایل یادگیری ماشین بسیار بزرگ،  قابل گسترش باشند.

15
00:00:28,660 --> 00:00:30,950
مثل حالتی که تعداد  خصیصه ها بسیار زیاد باشد.

16
00:00:31,850 --> 00:00:33,360
این دید متفاوتی از

17
00:00:33,750 --> 00:00:34,910
عملکرد گرادیان کاهشی است.

18
00:00:35,590 --> 00:00:38,030
ما یک تابع هزینه J داریم و می خواهیم آن را کمینه کنیم.

19
00:00:38,950 --> 00:00:39,980
پس چیزی که می خواهیم

20
00:00:40,340 --> 00:00:41,080
این است، باید 

21
00:00:41,330 --> 00:00:42,640
کدی بنویسیم که بتواند 

22
00:00:42,850 --> 00:00:44,980
به عنوان ورودی پارامتر theta را بگیرد و 

23
00:00:45,200 --> 00:00:46,470
و قادر باشد دو چیز را محاسبه کند: هزینه J

24
00:00:46,700 --> 00:00:48,190
در نقطه theta و این

25
00:00:48,620 --> 00:00:50,280
مشتقات جزئی  برای 

26
00:00:50,530 --> 00:00:51,820
J = 0 , J=1

27
00:00:51,890 --> 00:00:53,700
تا j=N.   با داشتن چنین کدی، کاری که 

28
00:00:53,830 --> 00:00:54,980
و می تواند این دو چیز را انجام دهد

29
00:00:55,160 --> 00:00:56,710
گرادیان کاهشی می کند این است که

30
00:00:56,790 --> 00:00:58,620
به تکرار این به روز رسانی را انجام می دهد.

31
00:00:59,100 --> 00:00:59,100
مگه نه؟

32
00:00:59,280 --> 00:01:00,500
خوب پس اگر کدی را که 

33
00:01:00,670 --> 00:01:01,750
که برای محاسبه این مشتقات 

34
00:01:02,090 --> 00:01:03,800
جزئی نوشتیم داشته باشیم، گرادیان کاهشی 

35
00:01:04,480 --> 00:01:07,330
در اینجا کارش می گذارد و برای به روز رسانی پارامتر ما theta  آن را به کار می برد.

36
00:01:08,650 --> 00:01:09,590
ا نگاه دیگر به

37
00:01:09,910 --> 00:01:11,070
گرادیان کاهشی این است که

38
00:01:11,350 --> 00:01:12,670
ما باید کدی را تامین کنیم که 

39
00:01:12,810 --> 00:01:14,050
J(theta) و این 

40
00:01:14,230 --> 00:01:15,700
مشتقات را  محاسبه کند و بعد

41
00:01:15,900 --> 00:01:16,930
اینها  داخل گرادیان

42
00:01:17,370 --> 00:01:20,110
کاهشی تعبیه می شود که به کمک آن بتواند تابع را برای ما کمینه کند

43
00:01:20,970 --> 00:01:21,970
فکر کنم، برای گرادیان کاهشی

44
00:01:22,480 --> 00:01:23,790
از لحاظ تئوری حتی به کدی که 

45
00:01:24,170 --> 00:01:26,520
تابع هزینه  J(theta) را  محاسبه کند احتیاج ندارید.

46
00:01:26,940 --> 00:01:28,980
تنها باید بتوانید عبارات مشتقی را محاسبه کنید.

47
00:01:29,740 --> 00:01:30,480
ولی اگر به کد خود

48
00:01:30,590 --> 00:01:32,300
 به عنوان یک ناظر بر همگرایی

49
00:01:33,000 --> 00:01:34,060
روش نگاه کنیم

50
00:01:34,190 --> 00:01:35,440
می بینیم که می خواهیم کدی

51
00:01:35,530 --> 00:01:37,380
 تامین کنیم که

52
00:01:37,510 --> 00:01:38,530
هم تابع هزینه و

53
00:01:38,890 --> 00:01:40,250
هم مشتقات جزئی را محاسبه کند.

54
00:01:42,700 --> 00:01:44,130
پس، حالا که کدی نوشته ایم که   

55
00:01:44,280 --> 00:01:45,860
این دو مورد را محاسبه می کند

56
00:01:46,090 --> 00:01:47,820
یکی از الگوریتمهایی که می توانیم استفاده کنیم گرادیان کاهشی است.

57
00:01:48,910 --> 00:01:51,590
ولی گرادیان کاهشی تنها الگوریتمی که می توانیم استفاده کنیم نیست.

58
00:01:52,280 --> 00:01:53,690
الگوریتم های دیگری هم هستند،

59
00:01:54,330 --> 00:01:55,930
خیلی پیچیده تر، خیلی پیشرفته تر،

60
00:01:56,720 --> 00:01:57,880
که اگر فقط ما برای آنها

61
00:01:58,400 --> 00:01:59,520
راهی برای محاسبه

62
00:01:59,960 --> 00:02:01,550
این دو مورد تامین کنیم، آن وقت آنها

63
00:02:01,760 --> 00:02:03,040
با رویکردهای متفاوتی 

64
00:02:03,490 --> 00:02:04,790
تابع هزینه را برای ما بهینه سازی می کنند.

65
00:02:05,110 --> 00:02:07,910
خوب، روش Congugate gradient BFGS 

66
00:02:08,110 --> 00:02:09,240
و LBFGS مثالهایی از روشهای

67
00:02:09,460 --> 00:02:11,490
بهینه سازی پیچیده تر هستند که

68
00:02:11,640 --> 00:02:12,610
به راهی برای محاسبه

69
00:02:12,810 --> 00:02:13,670
J(Theta) و را هی برای 

70
00:02:13,750 --> 00:02:15,430
مشتقات نیاز دارند و پس از آن می توانند

71
00:02:15,670 --> 00:02:16,940
روشهای پیچیده تری از 

72
00:02:17,620 --> 00:02:19,880
گرادیان کاهشی برای کمینه کردن تابع هزینه را به کار ببرند.

73
00:02:21,260 --> 00:02:22,560
جزئیات دقیق اینکه این 

74
00:02:22,780 --> 00:02:25,920
سه الگوریتم، چه میکنند، فراتر از حدود این درس است.

75
00:02:26,490 --> 00:02:28,200
و معمولا در نهایت کار به اینجا می کشد که

76
00:02:28,650 --> 00:02:30,570
شما چندین روز،

77
00:02:31,060 --> 00:02:32,670
یا هفته های کمی را به مطالعه این الگوریتمها بگذرانید

78
00:02:33,240 --> 00:02:35,840
اگر کلاسی در زمینه محاسبات عددی پیشرفته بگیرید.

79
00:02:36,920 --> 00:02:38,200
اما فقط بگذارید برخی از ویژگیهایشان را بگویم.

80
00:02:40,080 --> 00:02:42,150
این سه الگوریتم چند مزیت دارند.

81
00:02:42,900 --> 00:02:44,070
یکی اینکه، برای هیچ کدامشان

82
00:02:44,290 --> 00:02:45,850
معمولا لازم نیست نرخ یادگیری 

83
00:02:46,000 --> 00:02:48,970
آلفا را دستی انتخاب کنید.

84
00:02:50,670 --> 00:02:51,450
پس یک جور دیگر نگاه

85
00:02:51,650 --> 00:02:53,630
به این الگوریتمها، 

86
00:02:54,230 --> 00:02:56,900
روشی برای محاسبه مشتق و تابع هزینه است.

87
00:02:57,320 --> 00:02:59,740
می توانید تصور کنید که این الگوریتم ها یک حلقه هوشمند داخلی دارند.

88
00:03:00,060 --> 00:03:00,680
و در واقع حلقه داخلی هوشمند این الگوریتمها

89
00:03:01,810 --> 00:03:03,780
 جستجوی خط  نام گرفته است.

90
00:03:04,200 --> 00:03:05,840
که به طور خودکار

91
00:03:06,520 --> 00:03:08,010
مقادیر مختلفی را 

92
00:03:08,080 --> 00:03:09,360
برای نرخ یادگیری امتحان می کند و 

93
00:03:10,010 --> 00:03:11,090
یک نرخ یادگیری خوب را انتخاب می کند

94
00:03:12,030 --> 00:03:12,900
حتی می تواند برای هر مرحله

95
00:03:13,130 --> 00:03:14,570
نرخ یادگیری مختلفی انتخاب کند.

96
00:03:15,490 --> 00:03:18,230
در نتیجه شما لازم نیست خودتان این کار را بکنید.

97
00:03:21,430 --> 00:03:22,770
این الگوریتمها معمولا کارهای 

98
00:03:22,910 --> 00:03:24,260
پیچیده تری از انتخاب

99
00:03:24,470 --> 00:03:25,640
یک نرخ یادگیری خوب می کنند

100
00:03:25,800 --> 00:03:27,300
در نتیجه معولا خیلی سریعتر

101
00:03:27,490 --> 00:03:30,320
از گرادیان کاهشی همگرا می شوند.

102
00:03:32,470 --> 00:03:33,740
این الگوریتمها معمولا کارهای 

103
00:03:33,980 --> 00:03:35,160
پیچیده تری از انتخاب

104
00:03:35,360 --> 00:03:36,740
یک نرخ یادگیری خوب می کنند

105
00:03:36,880 --> 00:03:38,770
در نتیجه معولا خیلی سریعتر

106
00:03:39,020 --> 00:03:40,840
از گرادیان کاهشی همگرا می شوند.

107
00:03:41,040 --> 00:03:42,230
ولی جزئیات اینکه دقیقا

108
00:03:42,710 --> 00:03:44,420
چه کار می کنند فراتر از حیطه این درس است.

109
00:03:45,580 --> 00:03:47,060
من خودم قبلها

110
00:03:47,570 --> 00:03:49,020
این الگوریتم ها را برای مدت زیادی به فراوانی به کار برده ام

111
00:03:49,170 --> 00:03:50,170
مثلا شاید بیشتر از

112
00:03:50,470 --> 00:03:53,070
 یک دهه

113
00:03:53,290 --> 00:03:54,410
و می دونید فقط 

114
00:03:54,510 --> 00:03:55,460
چند سالی هست که واقعا

115
00:03:56,150 --> 00:03:57,200
جزئیات اینکه روشهای

116
00:03:57,780 --> 00:04:00,220
conjugate gradient، BFGS و O-BFGS چه کار می کنند را در آوردم.

117
00:04:00,980 --> 00:04:02,740
یعنی کاملا میشه که این 

118
00:04:03,560 --> 00:04:05,380
الگوریتمها را با موفقیت استفاده کرد و 

119
00:04:05,480 --> 00:04:06,530
و در بسیاری از مسائل یادگیری به کارشان برد

120
00:04:06,780 --> 00:04:08,490
بدون اینکه واقعا 

121
00:04:09,460 --> 00:04:11,140
حلقه داخلی آنها را بفهمی.

122
00:04:12,270 --> 00:04:13,630
اگر این الگوریتمها عیبی داشته باشند،

123
00:04:14,200 --> 00:04:15,350
اگر من باشم می گم عیب اصلیشان 

124
00:04:15,610 --> 00:04:16,970
این است که خیلی 

125
00:04:17,110 --> 00:04:19,390
پیچیده تر از گرادیان کاهشی هستند.

126
00:04:20,180 --> 00:04:21,700
و مخصوصا، شما نباید 

127
00:04:21,970 --> 00:04:23,290
این الگوریتمها را 

128
00:04:23,850 --> 00:04:26,060
گرادیان در هم آمیخته، LBFGS و BFGS.

129
00:04:26,360 --> 00:04:29,520
خودتان به شخصه پیاده سازی کنید مگر آنکه خبره محاسبات عددی باشید.

130
00:04:30,720 --> 00:04:32,320
همانطور که  توصیه نمی کنم 

131
00:04:32,420 --> 00:04:33,640
که خودتان کدی را بنویسید که

132
00:04:33,850 --> 00:04:35,240
که جذر اعداد را محاسبه کند 

133
00:04:35,590 --> 00:04:36,660
یا

134
00:04:36,770 --> 00:04:39,010
یا ماتریسها را معکوس کند.

135
00:04:39,140 --> 00:04:40,600
کاری که در مورد این الگوریتمها سفارش می کنم این است که

136
00:04:40,710 --> 00:04:42,530
که به سادگی یک کتابخانه نرم افزاری را به کار برید.

137
00:04:43,030 --> 00:04:43,770
می دونید، برای جذر گرفتن  کاری که

138
00:04:44,120 --> 00:04:44,940
که همه ما می کنیم این است که 

139
00:04:45,150 --> 00:04:46,440
تابعی را 

140
00:04:47,080 --> 00:04:48,310
که کس دیگری نوشته

141
00:04:48,530 --> 00:04:50,200
برای جذر گرفتن از اعداد خود استفاده می کنیم.

142
00:04:51,330 --> 00:04:53,530
خوشبختانه اکتاو و 

143
00:04:53,760 --> 00:04:55,070
زبان مرتبط به آن متلب

144
00:04:55,430 --> 00:04:57,110
ما  از آن استفاده خواهیم کرد.

145
00:04:57,140 --> 00:04:58,370
اکتاو کتابخانه قابل قبولی دارد

146
00:04:58,530 --> 00:05:02,410
که برخی از این الگوریتمهای پیچیده را پیاده سازی کرده است.

147
00:05:03,380 --> 00:05:04,350
و فقط اگر کتابخانه 

148
00:05:04,600 --> 00:05:06,800
تعبیه شده در آن را بکار ببرید جوابهای خوبی می گیرید.

149
00:05:08,010 --> 00:05:08,880
البته باید بگویم بین پیاده سازیهای

150
00:05:09,370 --> 00:05:10,880
خوب یا بد این الگوریتمها

151
00:05:11,230 --> 00:05:12,740
فرق هست.

152
00:05:13,690 --> 00:05:15,010
و در نتیجه اگر شما زبان 

153
00:05:15,120 --> 00:05:16,270
متفاوتی برای برنامه یادگیری ماشین

154
00:05:16,470 --> 00:05:17,560
خود به کار می برید 

155
00:05:18,190 --> 00:05:20,090
مثل C ،C++، جاوا 

156
00:05:20,250 --> 00:05:24,060
در آنصورت

157
00:05:24,210 --> 00:05:24,710
بهتر است چند تا کتابخانه مختلف را 

158
00:05:24,730 --> 00:05:25,660
امتحان کنید که 

159
00:05:25,740 --> 00:05:27,790
تا یک پیاده سازی خوبشان را پیدا کنید.

160
00:05:28,250 --> 00:05:29,410
چون در کارایی 

161
00:05:29,480 --> 00:05:30,740
یک پیاده سازی خوب 

162
00:05:31,680 --> 00:05:33,150
مثلا گرادیان کانتور یا  LBFGS 

163
00:05:33,530 --> 00:05:35,150
در مقابل یک پیاده سازی نه چندان خوب

164
00:05:35,350 --> 00:05:37,680
فرق هست.

165
00:05:43,060 --> 00:05:44,310
حالا بگذارید بگم چطور

166
00:05:44,580 --> 00:05:47,080
این الگوریتمها را بکار ببریم و این را با یک مثال نشان خواهم داد.

167
00:05:48,970 --> 00:05:50,220
فرض کنید مسئله ای 

168
00:05:50,370 --> 00:05:51,620
با دو پارامتر داریم با نامهای

169
00:05:53,380 --> 00:05:55,580
theta0, theta1

170
00:05:56,410 --> 00:05:57,450
و فرض کنید تابع هزینه شما

171
00:05:57,970 --> 00:05:59,210
J(theta)  برابر

172
00:05:59,430 --> 00:06:01,540
J(theta) = (theta1 - 5) ^2 + (theta2 -5)^2 

173
00:06:02,630 --> 00:06:04,080
خوب با این تابع هزینه

174
00:06:04,590 --> 00:06:06,960
مقدارهای کمینه کننده theta1 و theta2

175
00:06:07,080 --> 00:06:09,590
اگر بخواهید تابع هزینه را به عنوان تابعی از theta  کمینه کنید

176
00:06:09,940 --> 00:06:10,910
مقداری که آن را کمینه می کند

177
00:06:11,030 --> 00:06:12,040
خواهد بود theta1 برابر است با

178
00:06:12,420 --> 00:06:14,220
5 و theta2 برابر است با 5

179
00:06:15,230 --> 00:06:16,620
حالا، باز باید تاکید کنم که، میدانم بعضی از شما،

180
00:06:16,950 --> 00:06:18,320
بیشتر از بعضیهای دیگه حسابان بلدید.

181
00:06:19,010 --> 00:06:20,770
ولی مشتق تابع هزینه

182
00:06:20,850 --> 00:06:23,420
برابر خواهد بود با این دو عبارات پایین اینجا. 

183
00:06:24,270 --> 00:06:25,060
من حسابانش را انجام داده ام.

184
00:06:26,260 --> 00:06:27,250
پس اگر بخواهید تابع هزینه را داشته باشید

185
00:06:27,480 --> 00:06:29,220
یکی از الگوریتمهای بهینه سازی پیشرفته را

186
00:06:29,810 --> 00:06:31,380
برای کمینه کردن این تابع هزینه J به کار ببرید

187
00:06:31,660 --> 00:06:32,630
و اگر ما

188
00:06:32,880 --> 00:06:34,680
نمی دانستیم که مینیمم در 

189
00:06:34,780 --> 00:06:36,140
5و5 هستش ولی اگر بخواهید

190
00:06:36,240 --> 00:06:37,550
تابع هزینه را داشته باشید و کمینه آن را 

191
00:06:37,970 --> 00:06:39,840
به صورت عددی با استفاده از روشی مثل

192
00:06:40,040 --> 00:06:41,560
گرادیان کاهشی ولی ترجیحا

193
00:06:41,730 --> 00:06:43,430
پیشرفته تر از گرادیان کاهشی محاسبه کنید

194
00:06:43,550 --> 00:06:45,010
کاری که می کنید این است که یک تابع اکتیو

195
00:06:45,570 --> 00:06:46,690
شبیه به این پیاده سازی کنید، پس ما

196
00:06:46,860 --> 00:06:48,190
یک تابع هزینه پیاده سازی می کنیم

197
00:06:49,210 --> 00:06:51,180
یک تابع هزینه theta، یک تابع مانند آن،

198
00:06:52,180 --> 00:06:53,250
و کاری که انجام میدهد این است که

199
00:06:53,380 --> 00:06:55,660
دو آرگومان برمیگرداند، 

200
00:06:55,760 --> 00:06:57,780
اولی، به نام jVal 

201
00:06:58,910 --> 00:07:00,020
بیانگر این است که چطور مقدار تابع هزینه 

202
00:07:00,680 --> 00:07:01,780
J را محاسبه می کنیم، در نتیجه این می گوید مقدار J 

203
00:07:02,080 --> 00:07:03,210
برابر، میدونید، theta1

204
00:07:03,440 --> 00:07:04,630
منهای 5 به توان دو به اضافه

205
00:07:05,330 --> 00:07:06,230
theta2 منهای 5 به توان دو است.

206
00:07:06,540 --> 00:07:09,140
پس فقط اینجا فقط این تابع هزینه حساب میشود.

207
00:07:10,540 --> 00:07:12,040
و دومین آرگومانی که تابع برمیگرداند

208
00:07:12,260 --> 00:07:14,190
گرادیان است.

209
00:07:14,840 --> 00:07:16,030
پس گرادیان باید یک بردار

210
00:07:16,160 --> 00:07:17,320
دو در یک باشد،

211
00:07:18,870 --> 00:07:20,050
و دو مولفه 

212
00:07:20,120 --> 00:07:22,100
بردار گرادیان متناظر با 

213
00:07:22,800 --> 00:07:24,670
دو عبارت مشتق جزئی اینجاست.

214
00:07:27,150 --> 00:07:28,570
با پیاده سازی این تابع هزینه،

215
00:07:29,580 --> 00:07:30,390
شما بعدش می توانید

216
00:07:31,510 --> 00:07:33,010
تابع بهینه سازی پیشرفته را فراخوانی کنید

217
00:07:34,270 --> 00:07:35,720
fminunc را فراخوانی کنید.

218
00:07:35,950 --> 00:07:36,900
که مخفف

219
00:07:37,610 --> 00:07:39,360
کمینه سازی تابعی بدون قید در اکتیو هست.

220
00:07:40,300 --> 00:07:41,520
و طریقه فراخوانی آن به صورت زیر است.

221
00:07:41,790 --> 00:07:42,350
یک تعدادی گزینه را تعیین می کنید.

222
00:07:43,230 --> 00:07:43,580
options ساختار داده ای است که 

223
00:07:44,330 --> 00:07:46,680
گزینه هایی را که شما می خواهید ذخیره می کند.

224
00:07:47,320 --> 00:07:48,960
آن را فعال کنید

225
00:07:49,160 --> 00:07:52,100
این پارامتر هدف گرادیان را on میکند.

226
00:07:52,270 --> 00:07:55,180
معنی آن این است که واقعا شما قرار است برای این الگوریتم گرادیان فراهم کنید.

227
00:07:56,150 --> 00:07:57,550
من می خواهم بیشترین تعداد مراحل را 

228
00:07:57,840 --> 00:07:59,280
بگذارم، مثلا صد.

229
00:07:59,580 --> 00:08:02,230
ما یک حدس اولیه از مقدار theta به آن می دهیم.

230
00:08:02,720 --> 00:08:03,680
که یک بردار 2 در 1 است.

231
00:08:04,440 --> 00:08:06,860
و سپس این دستور fminunc را فراخوانی میکند.

232
00:08:07,530 --> 00:08:10,290
این علامت @ یک 

233
00:08:10,420 --> 00:08:11,810
اشاره گر به تابع هزینه را نشان می دهد که

234
00:08:13,010 --> 00:08:14,320
بالاتر تعریف کردیم.

235
00:08:15,060 --> 00:08:16,020
و اگر این را فراخوانی کنید، 

236
00:08:16,270 --> 00:08:18,290
می دونید، یکی از الگوریتمهای 

237
00:08:18,620 --> 00:08:20,490
پیچیده تر بهینه سازی را استفاده می کند.

238
00:08:21,110 --> 00:08:23,350
و اگر می خواهید می توانید آن را درست شبیه گرادیان کاهشی ببینید 

239
00:08:23,690 --> 00:08:25,170
ولی نرخ یادگیری به طور خودکار انتخاب میشود

240
00:08:25,500 --> 00:08:27,290
و شما مجبور نیستید خودتان آن را تعیین کنید.

241
00:08:28,210 --> 00:08:29,880
ولی بعدش  میخواهد

242
00:08:30,160 --> 00:08:32,000
یکی از الگوریتمهای بهینه سازی پیشرفته را استفاده کند

243
00:08:32,640 --> 00:08:33,770
مثل حالت خیلی قوی گرادیان کاهشی 

244
00:08:34,400 --> 00:08:36,490
تا مقدار بهینه theta را برای شما پیدا کند.

245
00:08:37,180 --> 00:08:39,040
بگذارید نشانتان بدهم این در اکتیو چه شکلی است.

246
00:08:40,690 --> 00:08:42,460
خوب. من این تابع هزینه را نوشته ام.

247
00:08:42,900 --> 00:08:46,440
یک تابع از theta، دقیقا به صورتی که قبلا آن را داشتیم.

248
00:08:46,650 --> 00:08:49,070
مقدار J را محاسبه می کند که تابع هزینه است.

249
00:08:49,920 --> 00:08:51,810
و گرادیان را که دو 

250
00:08:52,040 --> 00:08:53,050
مولفه دارد که 

251
00:08:53,450 --> 00:08:54,430
که مشتقات جزئی تابع هزینه  هستند

252
00:08:55,220 --> 00:08:56,200
بر حسب

253
00:08:56,360 --> 00:08:57,910
دو پارامتر، theta1 و theta2.

254
00:08:59,040 --> 00:09:00,360
حالا بیایید به پنجره اکتیو من برویم.

255
00:09:00,710 --> 00:09:02,900
من این دو تابعی را که دارم تایپ میکنم.

256
00:09:03,470 --> 00:09:05,850
پس options برابر است با optimset.

257
00:09:06,630 --> 00:09:08,510
نمادگزاری که برای انتصاب

258
00:09:09,670 --> 00:09:11,190
پارامترها در انتخابهای من 

259
00:09:11,710 --> 00:09:13,850
در الگوریتم بهینه سازی من بکار می رود. option را فعال کنید و maxIter را مقدار 100 بدهید

260
00:09:14,130 --> 00:09:17,600
که اعلام کند تا 100 

261
00:09:18,310 --> 00:09:19,610
مرحله پیش می رویم و 

262
00:09:19,730 --> 00:09:22,090
من گرادیان را برای الگوریتم خود فراهم می کنم.

263
00:09:23,490 --> 00:09:27,190
فرض کنیم theta اولیه برابر بردار صفر دو در یک باشد.

264
00:09:27,980 --> 00:09:29,280
پس این مقدار حدس اولیه من برای theta است.

265
00:09:30,500 --> 00:09:31,390
حالا دارم optTheta

266
00:09:32,620 --> 00:09:35,100
functionVal   exitFlag

267
00:09:37,610 --> 00:09:39,430
مساوی است با fmin بدون قید،

268
00:09:40,570 --> 00:09:41,600
یک اشاره گر به تابع هزینه،

269
00:09:43,010 --> 00:09:44,700
و حدس اولیه ام را ارائه میدهم.

270
00:09:46,090 --> 00:09:49,060
و  انتخابها.

271
00:09:49,820 --> 00:09:52,760
و اگر enter بزنم،  این الگوریتم بهینه سازی را اجرا می کند.

272
00:09:53,940 --> 00:09:54,810
و خیلی هم سریع کارش تمام می شود.

273
00:09:55,790 --> 00:09:57,040
این شکل خنده دار

274
00:09:57,430 --> 00:09:58,430
خط من به خاطر ایت است که

275
00:09:59,700 --> 00:10:00,290
که کد چرخیده خط بعد.

276
00:10:00,680 --> 00:10:02,540
این چیز خنده دار

277
00:10:02,760 --> 00:10:04,890
فقط به خاطر آن است که خط دستور من چرخیده است

278
00:10:05,490 --> 00:10:06,290
ولی چیزی که می گوید این است

279
00:10:06,550 --> 00:10:08,500
عددی می سازد، می دونید، از آن 

280
00:10:08,670 --> 00:10:10,400
به دید گرادیان کاهشی 

281
00:10:10,440 --> 00:10:11,620
ّبهبود یافته نگاه کنید که مقدار بهینه که پیدا کرده است

282
00:10:11,760 --> 00:10:13,150
theta1 برابر 

283
00:10:13,400 --> 00:10:15,670
5 و theta2 ّبرابر 5 است، دقیفا همانطور که انتظار داشتیم.

284
00:10:16,520 --> 00:10:18,760
مقدار تابع در 

285
00:10:18,840 --> 00:10:21,430
نقطه بهینه 10 به توان منفی 30 است.

286
00:10:21,670 --> 00:10:23,160
پس اساسا صفر است که 

287
00:10:23,370 --> 00:10:24,760
آن هم همان مقداریست که منتظرش بودیم.

288
00:10:24,840 --> 00:10:27,060
و پرچم خروج 

289
00:10:27,240 --> 00:10:29,080
1 است و این نشان می دهد که

290
00:10:29,730 --> 00:10:31,400
که وضعیت همگرایی چیست.

291
00:10:31,800 --> 00:10:33,010
و اگر بخواهید میتوانید 

292
00:10:33,150 --> 00:10:35,020
مستندات fminunc را بخوانید

293
00:10:35,130 --> 00:10:36,480
که توضیح میدهد

294
00:10:36,680 --> 00:10:38,650
چطور پرچم خروج را تعبیر کنید.

295
00:10:38,760 --> 00:10:41,600
ولی درکل پرچم خروج کمک می کند که مطمئن شویم که آیا الگوریتم همگرا شده است یا نه.

296
00:10:43,960 --> 00:10:46,450
خوب این نحوه اجرای این الگوریتمها در اکتیو است.

297
00:10:47,480 --> 00:10:48,920
راستی باید بگویم که

298
00:10:48,940 --> 00:10:51,020
برای پیاده سازی اکتیو 

299
00:10:51,640 --> 00:10:53,010
این مقدار theta، یعنی بردار پارامتر thetaی شما 

300
00:10:53,370 --> 00:10:54,940
باید در 

301
00:10:55,280 --> 00:10:58,210
R^d باشد که در آن d بزرگتر یا مساوی 2 است.

302
00:10:58,450 --> 00:11:00,330
حالا اگر theta  فقط یک عدد حقیقی باشد

303
00:11:00,770 --> 00:11:02,040
یعنی حداقل یک

304
00:11:02,160 --> 00:11:03,160
بردار دو بعدی نباشد

305
00:11:03,800 --> 00:11:04,860
یا برداری با بعد بیشتر از دو نباشد،

306
00:11:05,160 --> 00:11:06,840
این fminunc  

307
00:11:07,560 --> 00:11:08,760
ممکن است کار نکند و در نتیجه 

308
00:11:09,140 --> 00:11:10,310
در صورتی که یک تابع یک بعدی دارید که می خواهید

309
00:11:10,590 --> 00:11:11,590
بهینه سازی کنید،

310
00:11:11,830 --> 00:11:12,930
 باید به مستندات

311
00:11:13,100 --> 00:11:14,680
اکتیو برای fminunc مراجعه کنید 

312
00:11:14,950 --> 00:11:16,230
برای جزئیات بیشتر.

313
00:11:18,230 --> 00:11:19,360
خوب... این روشی است که ما  برای بهینه سازی 

314
00:11:19,620 --> 00:11:21,640
مثال غیرواقعی بازیگونه  خود با

315
00:11:22,190 --> 00:11:23,810
تابع هزینه درجه دوم  ساده خود بکار میبریم.

316
00:11:24,440 --> 00:11:26,520
چطور این را درمورد  به طور مثال رگرسیون لاجیستیک به کار می بریم؟

317
00:11:27,720 --> 00:11:29,270
در رگرسیون لاجیستیک ما یک 

318
00:11:29,520 --> 00:11:31,290
بردار پارامتر theta داریم و

319
00:11:31,430 --> 00:11:32,210
من مخلوطی از نمادگذاری

320
00:11:32,620 --> 00:11:34,880
اکتیو و نوعی نمادگذاری ریاضی به کار می بریم.

321
00:11:35,300 --> 00:11:36,400
امیدوارم این توضیح 

322
00:11:36,870 --> 00:11:38,050
واضح باشد 

323
00:11:38,520 --> 00:11:40,360
بردار theta  تشکیل شده از این پارامترها:

324
00:11:40,540 --> 00:11:41,780
theta0 تا theta n 

325
00:11:42,210 --> 00:11:44,230
به خاطر اینکه اکتیو  اندیس گذاری بردارها را

326
00:11:46,090 --> 00:11:48,040
از یک آغاز می کند

327
00:11:48,460 --> 00:11:49,640
در نتیجه  اندیس صفرم theta 

328
00:11:49,710 --> 00:11:51,190
theta 1  نوشته می شود

329
00:11:51,330 --> 00:11:53,290
در اکتیو،  اندیس اول نوشته خواهد شد

330
00:11:53,930 --> 00:11:54,690
theta 2

331
00:11:55,280 --> 00:11:56,180
و این نوشته خواهد شد 

332
00:11:56,780 --> 00:11:58,430
theta n+1  درسته؟

333
00:11:58,610 --> 00:12:00,650
و این بخاطر این است که اکتیو اندیس گذاری

334
00:12:01,320 --> 00:12:03,070
بردارها را  با شروع از اندیس 1 انجام می دهد.

335
00:12:03,430 --> 00:12:05,200
به جای اندیس 0.

336
00:12:06,920 --> 00:12:07,950
پس چیزی که ما احتیاج داریم

337
00:12:08,160 --> 00:12:09,670
نوشتن یک تابع هزینه است

338
00:12:09,880 --> 00:12:12,070
که هزینه

339
00:12:12,710 --> 00:12:14,210
رگرسیون لاجیستیک را در بر بگیرد.

340
00:12:15,170 --> 00:12:16,450
مشخصا، تابع هزینه 

341
00:12:16,880 --> 00:12:18,310
باید JVal را برگرداند، می دانید

342
00:12:18,940 --> 00:12:20,430
همانطور که کدهایی لازم دارید که

343
00:12:20,640 --> 00:12:22,440
که J(theta) را محاسبه کند

344
00:12:22,710 --> 00:12:24,010
همینطور لازم است به آن گرادیان را بدهیم.

345
00:12:24,540 --> 00:12:25,460
پس گرادیان 1 

346
00:12:25,920 --> 00:12:27,080
کدی خواهد بود 

347
00:12:27,280 --> 00:12:29,100
که مشتق جزئی نسبت به 

348
00:12:29,390 --> 00:12:31,250
را نسیت به theta0  مشتق پاره ای بعدی 

349
00:12:31,600 --> 00:12:34,300
نسبت به theta 1  و بقیه را حساب می کند.

350
00:12:34,770 --> 00:12:36,260
باید مجددا تاکید کنم که این 

351
00:12:37,500 --> 00:12:38,390
گرادیان 1، گرادیان 2 و غیره است،

352
00:12:39,030 --> 00:12:40,330
نه گرادیان 0 و گرادیان 1 

353
00:12:40,500 --> 00:12:42,730
بخاطر اینکه اندیس گذاری اکتیو 

354
00:12:43,460 --> 00:12:46,200
از یک شروع می شود به جای 0.

355
00:12:47,440 --> 00:12:48,460
ولی مفهوم اصلی که امیدوارم از این

356
00:12:48,690 --> 00:12:49,540
اسلاید به همراه ببرید این است که

357
00:12:49,900 --> 00:12:50,870
کاری که شما لازم است انجام دهید

358
00:12:51,070 --> 00:12:54,370
نوشتن تابعی است که 

359
00:12:55,500 --> 00:12:56,930
تابع هزینه و گرادیان را برمیگدراند.

360
00:12:58,410 --> 00:12:59,750
در نتیجه  برای  

361
00:12:59,960 --> 00:13:01,410
اعمال این به رگرسیون لاجستیک 

362
00:13:02,100 --> 00:13:03,430
یا حتی به رگرسیون خطی، اگر

363
00:13:03,560 --> 00:13:06,230
اگر می خواهید که این الگوریتمهای بهینه سازی را  برای رگرسیون خطی به کار ببرید

364
00:13:07,340 --> 00:13:08,350
کاری که لازم است انجام دهید این است که 

365
00:13:08,500 --> 00:13:09,960
کد مناسب برای محاسبه 

366
00:13:10,820 --> 00:13:12,280
این چیزهای این بالا را وارد کنید.

367
00:13:15,100 --> 00:13:17,910
در نتیجه، الان شما میدانید که چطور این الگوریتمهای پیشرفته بهینه سازی را بکاربرید.

368
00:13:19,030 --> 00:13:21,170
بخاطر اینکه برای 

369
00:13:21,320 --> 00:13:22,660
این الگوریتمها شما یک 

370
00:13:22,870 --> 00:13:25,190
کتابخانه بهینه سازی پیچیده را بکار می برید، 

371
00:13:25,690 --> 00:13:26,710
این مسئله کد را کمی

372
00:13:26,940 --> 00:13:28,510
پنهانتر میکند و در نتیجه ممکن است

373
00:13:28,740 --> 00:13:30,390
کمی مشکل زدایی آن دشوارتر باشد.

374
00:13:31,290 --> 00:13:32,660
ولی بخاطر اینکه این الگوریتمها معمولا

375
00:13:33,010 --> 00:13:34,370
بسیار سریعتر از گرادیان کاهشی اجرا میشوند

376
00:13:35,010 --> 00:13:36,760
اکثر اوقات وقتی من 

377
00:13:37,060 --> 00:13:38,180
یک مسئله یادگیری ماشین بزرگ دارم

378
00:13:38,410 --> 00:13:39,500
من 

379
00:13:39,760 --> 00:13:42,110
این الگوریتمها را به جای گرادیان کاهشی به کار می برم.

380
00:13:43,900 --> 00:13:45,070
و با این ایده ها امیدوارم که 

381
00:13:45,450 --> 00:13:46,710
که شما بتوانید رگرسیون لاجیستیک 

382
00:13:47,350 --> 00:13:48,780
و همینطور رگرسیون خطی را 

383
00:13:49,100 --> 00:13:51,410
روی مسایل بزرگتری به کار بیندازید.

384
00:13:51,830 --> 00:13:53,820
خوب، مفاهیم بهینه سازی پیشرفته ما همینها بود.

385
00:13:55,120 --> 00:13:56,170
و در ویدیوی بعد و 

386
00:13:56,320 --> 00:13:57,720
آخری در مورد رگرسیون لاجیستیک،

387
00:13:58,550 --> 00:13:59,470
می خواهم به شما بگویم

388
00:13:59,600 --> 00:14:00,990
که جطور الگوریتم رگرسیون لاجیستیکی 

389
00:14:01,520 --> 00:14:02,790
که راجع به آن میدانید را بردارید  و آن را برای مسایل 

390
00:14:02,990 --> 00:14:05,420
طبقه بندی چند کلاسه به کار برید.