このビデオでは、 ロジスティック回帰において パラメータのシータをどうフィッティングするかを扱います。 その中でも特に 最適化の目的関数、言い換えると コスト関数を定義したいと思う、 これが教師有り学習で ロジスティック回帰にフィッティングする問題だ。 M個のトレーニング手本の トレーニングセットがある。 そしていつもどおり 我らの手本データは N+1次元のフィーチャーのベクトルで表される。 そしていつも通り x0=1とする。 我らの最初のフィーチャー、またの名をゼロ番目のフィーチャーは いつも1だ。 そしてこれは分類問題なので 我らのトレーニングセットは 各ラベルのyが 0か1のどちらかという性質を持つ。 これが仮説で そのパラメータは ここのシータとなる。 そして我らの議論したい 問題とは、 これらのトレーニングセットが与えられた時に パラメータのシータをどう選ぶか、言い換えるとどうフィッティングするかという事だ。 線形回帰のモデルを開発してた時に戻ると、 こんなコスト関数を使っていた。 ここでは以前とはちょっと違う書き方をした、 1/2mと書く代わりに 1/2を和の中に入れてみた。 今回はこのコスト関数を 別の書き方で 書いてみたい、 それはこんな風に二次の項で 書く代わりに、 こんな風に書いてみよう、 costの、 hのxに、カンマ、yと。 そしてcostのhのxカンマyの項を こう、 定義する。 それは単に誤差の二乗の半分。 つまり今や、我らはより明白に コスト関数は トレーニングセットに渡って 足し合わせる、、、より正確に言うと 1/mかける、 トレーニングセットに渡ってこのコスト項を足し合わせた物、である事が分かる。 そしてこの等式をさらに もうちょっとだけ単純化すると これらの上付き添字を取り除いてしまうと便利だ。 すると、単にcostの hのxカンマy は イコール、 1/2 誤差の二乗。 そしてこのコスト関数の解釈は これは私の学習アルゴリズムに 支払って欲しい コストだ、 もし学習アルゴリズムが 出力した値が、 この予測、hのxだった時で、 そして実際の値のラベルがy だった時に。 つまり、単にこれらの上付き添字を取り除いた。 そして線形回帰の場合、 当然定義したコストは、 これのコストは 1/2掛ける 予測した値と 実際に観測された値との 差の二乗だ。 今、このコスト関数は 線形回帰には問題無いけど、 今興味があるのはロジスティック回帰だ。 もしJのここに代入される コスト関数を最小化出来たら 問題無いだろう。 でもこの場合のコスト関数は パラメータのシータに対して、 非凸である事が知られている。 非凸とはこういう意味だ。 我らはあるコスト関数Jのシータがあり ロジスティック回帰の場合 この関数hは 非線形だ。でしょ？ それは、 1足すeの マイナスシータの転置にx  分の1だった。 つまりそれは、とても複雑な非線形の関数だ。 そしてsigmoid関数を これに代入して、 そしてこのコスト関数を ここに代入して そしてJのシータがどうなるかを プロットしてみると、 Jのシータはこんな感じの 関数となる事が分かる。 見ての通りたくさんの局所最適があり、 それを表す正式な用語が 非凸関数、という言葉。 そしてこんな類の 関数に最急降下法を 適用しても、 グローバルな極小に収束するという 保証は無い。 一方、 こんなコスト関数Jのシータ、 これは凸関数で、 一つの弓形の関数でこんな見た目の物、 こういう物に対して 最急降下法を実行すれば グローバル極小に 収束する事が 保証される訳だ。 そして二乗のコスト関数を 使う問題点としては、 この真ん中の sigmoid関数が、 まさに非線形なので Jのシータは非凸の関数になってしまう、 もしそれをコスト関数の 二乗で定義すれば、だ。 だから我らがやりたいのは、 その代わりに 異なるコスト関数で 凸関数で つまり最急降下法のような 素晴らしいアルゴリズムを適用して グローバル極小を見つける事を保証出来るようにする事だ。 これがロジスティック回帰で使うコスト関数だ。 アルゴリズムが hのxという値を出力した時に 支払うコストは こんなだと言ってる訳だ。 つまりこれはある数、0.7とかで、 それはhのxという値を予測している。 そしてコストは 実際の値がラベルyとなるとすると、 コストは、 y=1の時は -log h(x)として、 そしてy=0の時は -log(1-h(x))とする。 これは一見、とても複雑な関数に見える。 だがこの関数が何なのか、 直感を得る為にプロットしてみよう。 y=1のケースから始めよう。 もしy=1なら コスト関数は -log h(x) だから、 プロットすると、 横軸を h(x)とすると、 この仮説は0から1までの 値のどれかを出力する事を 知っている。 でしょ？ つまりh(x)は 0から1の間を変化する。 このコスト関数がどんな感じかプロットしたとすると、 こんな感じになるだろう。 どうしてプロットがこんな感じになるかを 理解する一つの方法としては、 zを横軸にlog zを プロットすると こんな感じとなる。 それはマイナス無限に近づいていく。 これがlog関数がどんな感じかだ。 これが0、これが1。 ここでこのzはもちろん h(x)の役割を果たしている。 だから -log zは こんな感じ。 単に符号を反転しただけ。 -log z。 そして我らはこの関数のうち、 0と1の間の領域にだけ 関心がある。 だからそこを取り除いて 残ったのは 曲線のこの部分。 そしてこれこそが左の曲線となる。 ここでこのコスト関数は 幾つか興味深い、都合の良い性質を持っている。 まず、もしy=1で h(x)=1の時は、 つまり、仮説が正確だとすると、 つまりh=1を予言して yが厳密に 予測した値と等しい。 するとコストはイコール0となる。 いい？ これは曲線が実際には水平に漸近してる訳では、、、 やり直し！
まず、気づくこととして h(x)=1で 仮説がy=1を 予言していて、 そして実際にy=1の時、 コストはイコール0となる。 それはこの点に対応する。 でしょ？ もしh(x)=1で y =1の場合だけに ここでは関心がある。 そしてh(x)=1。 するとコストはこの下の、イコール0となる。 そしてこれは、我らが望む事でもある。 何故なら、もし我らが yの結果を正しく予測したら、コストは0であるべきだ。 だがここで、もう一つ気づく事としては、 h(x)が0に近づくと、 つまりこのh、 仮説の出力が0に近づくと コストは急増して、無限大に近づく。 これがやってる事は 以下のような直感を捉えている訳だ、それは 仮説が0を出力した時、 つまりそれというのは仮説が y=1となる可能性は0だ、と 言っている訳だが、 それはつまり患者が 病院に行った時、 「あなたが悪性の腫瘍を持ってる可能性は y=1となる可能性は0だ」 と言われるような物だ。 つまりあなたの腫瘍が悪性である可能性は 絶対にありえない、という事。 でもそこでもし患者の 腫瘍が実際には悪性だと判明したとする。 つまりy=1となってしまったという事だ、 我らがそうなる確率は0だ！と 宣言した後にも関わらず。 つまりそれが悪性である可能性は完璧に0だ、と言ったという事なのにだ。 それだけの確信を持って 患者に言ったのにも関わらず、 我らが誤っている、という事が判明したのだ、 それはこのアルゴリズムに とても高いペナルティのコストを与えるべきだろう、 その直感が、このy =1で h(x)が0に近づくと 無限大へと行く事で 捉える事が出来ている。 これはy=1の時だが、 では次にy=0の時の コスト関数がどんな形か見てみよう。 y=0の時には、コストは こんな式となる。 そして関数、 -log(1-z) をプロットすると、 コスト関数はこんな 見た目となる。 そして0から1に行く。 こんな感じ。 たからy=0の時の コスト関数をプロットすると、 こんな感じになり、 そしてこの曲線が どんなかというと、 今回はh(x)が1に近づくと 無限大に向かって急上昇する。 何故ならそれは yが0だと判明したが、 我らは y=1と、確信を持って 確率1として予言した という事に対応するから。 だから、とても大きなコストを払う事になる。 ではy=0の時の コスト関数をプロットしてみよう。 y=0の時はこれが我らのコスト関数となる。 この式を見てみると、 そしてプロットしてみると、 -log(1-z)なので こんな見た目、 こんな図になる。 0から1までの範囲で 横軸が z軸。 つまりもしこのコスト関数を とって、y=0の時を プロットすると、 その結果は こんな見た目のコスト関数となる。 このコスト関数はどういう物かというと、 h(x)が1に近づくと 急上昇する、言い換えると 正の無限大に 行く。 これは仮説が、つまりh(x)が 1を、確かさ、つまり 確率1で予言し、 絶対にy =1だと 予言したのに、 y=0と 判明した場合に、 仮説、つまりは学習アルゴリズムに とても大きなコストを課すのは 合理的だ、という直感を取り込んだ物と言える。 逆に h(x)=0で y =0なら、 仮説は的中した事になる。 yの予測値は0で yが結局 0となった だからこの点では コスト関数は0に行く。 このビデオでは 単体のトレーニング手本について コスト関数を定義した。 凸解析がらみのトピックはこのコースのスコープを越えてしまう。 だが我らの選んだ コスト関数が 凸型の最適化問題に 帰着する事を証明する事が出来て、 全体のコスト関数 Jのシータは 凸となり、局所最適が存在しない事が分かる。 次のビデオでは これら単体のトレーニング手本に対しての コスト関数というアイデアを使って さらに開発を進めて トレーニングセット全体の コスト関数を定義する。 そしてそれを、 これまでやってきたよりも よりシンプルに書く方法も見つける。 そしてそれを元に 最急降下法を実行して それがロジスティック回帰のアルゴリズムとなる。