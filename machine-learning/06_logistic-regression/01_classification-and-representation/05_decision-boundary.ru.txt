В предыдущем видео мы обсудили общий вид гипотезы в задачах логической регрессии. Теперь я хочу рассказать о так называемой границе решений. Это поможет нам лучше понять, что вычисляет функция-гипотеза для логической регрессии. Вкратце напомню, к чему мы пришли в прошлый раз. Гипотеза h(x) равна g от произведения транспонированного вектора тета на x, где функция g — сигмоида — выглядит так: медленно растет от нуля и асимптотически приближается к единице. Давайте попробуем понять, когда такая гипотеза будет прогнозировать результат y, равный единице, а когда — нулю, и как выглядит эта функция, особенно в случае многих характеристик. Напомню, что значение гипотезы — это вероятность того, что y = 1 при заданных x и тета, так что если мы хотим получить предсказание «1» или предсказание «0», вот что можно сделать. Когда вычисленная гипотезой вероятность больше или равна 0,5, это значит, что y = 1 более вероятно, чем y = 0, и мы предсказываем y = 1. Напротив, если прогнозируемая вероятность того, что y = 1, меньше 0,5, мы предсказываем y = 0. Я выбрал знак «больше или равно» здесь и знак «меньше» здесь. Если значение h(x) в точности равно 0,5, мы можем предсказать как положительный, так и отрицательный результат. Это не слишком существенно, и я просто решил, что буду предсказывать положительный результат при получении h(x) = 0,5. Это незначительная деталь. Теперь я хочу понять, когда именно h(x) больше или равняется 0,5, то есть в каком случае мы предскажем y = 1. По графику сигмоиды можно понять, что g(z) больше или равняется 0,5, если z больше или равняется нулю. То есть в этой половине графика g принимает значения от 0,5 и выше. Вот эта отметка — это 0,5. То есть при неотрицательных z сигмоида g(z) принимает значение 0,5 или больше. Гипотеза логистической регрессии, h(x) равна g от транспонированного тета на x. Это выражение будет больше или равно 0,5, если произведение транспонированного тета на x будет больше или равно нулю. Потому что здесь это произведение соответствует z. Итак, наша гипотеза будет предсказывать y = 1, когда произведение транспонированного вектора тета на x будет неотрицательным. Обратимся ко второму случаю, когда гипотеза предсказывает y = 0. Следуя тому же рассуждению, h(x) будет меньше 0,5, если g(z) меньше 0,5; а область значений z, при которых g(z) меньше 0,5 — отрицательные z. То есть, когда g(z) меньше 0,5, наша гипотеза будет предсказывать y = 0, и, снова повторяя рассуждение, h(x) это g от произведения транспонированного тета на x, значит, мы будем предсказывать y = 0, когда это произведение меньше нуля. Еще раз сформулирую результат наших выкладок. Мы предсказываем y = 1 или y = 0 исходя из прогнозируемой вероятности: положительный ответ при вероятности больше или равной 0,5 и отрицательный в ином случае. Это эквивалентно тому, чтобы предсказывать y = 1, если произведение транспонированного вектора тета на вектор x больше или равно нулю, и y = 0, если оно меньше нуля. Попробуем теперь вникнуть в то, как гипотеза логистической регрессии выдает эти предсказания. Допустим, у нас есть обучающий набор, как на этом слайде, и наша гипотеза h(x) равна g от суммы: тета нулевое плюс тета первое на x1 плюс тета второе на x2. Мы пока не обсуждали, как подбирать параметры этой модели. Об этом речь пойдет в следующем видео. Пока предположим, что, применив еще не описанную процедуру, мы получили следующие значения параметров: тета нулевое равно, скажем, минус трем, тета первое и тета второе — единице. То есть вектор параметров тета будет равен [-3; 1; 1]. Теперь, зная эти параметры гипотезы, давайте попробуем выяснить, где гипотеза будет предсказывать y = 1, а где — y = 0. По формулам с предыдущего слайда мы знаем, что y более вероятно равняется единице, то есть вероятность того, что y = 1, больше или равна 0,5, в случае, если произведение транспонированного тета на x больше или равно нулю. И вот это выражение, которое я подчеркнул, — минус три плюс x1 плюс x2 — собственно, и есть это произведение, при условии, что вектор тета равен параметрам, которые мы только что выбрали. То есть для любого примера, характеристики x1 и x2 которого удовлетворяют этому неравенству: −3 + x1 + x2 ≥ 0, наша гипотеза будет считать более вероятным, что y = 1 и предскажет, что y = 1. Перенесем минус три в другую часть, и получим x1 + x2 ≥ 3. Соответственно, гипотеза предскажет, что y = 1, когда сумма x1 и x2 будет не меньше трех. Давайте посмотрим, что это означает, на графике. Если записать равенство, x1 + x2 = 3, оно определит прямую. Вот эта прямая, она проходит через точку 3 на обеих осях, x1 и x2. И часть пространства характеристик, часть плоскости x1-x2, которая соответствует неравенству x1 + x2 ≥ 3, это верхняя ее часть: все, что выше, то есть выше и правее этой пурпурной прямой линии, которую я только что нарисовал. Таким образом, область значений характеристик, для которых наша гипотеза предскажет y = 1, это вся вот эта часть, вся полуплоскость вверху справа. Давайте я это запишу. Я обозначу это множество точек областью y = 1; вторая область значений, где сумма x1 и x2 меньше трех, и мы будем предсказывать y = 0, — тоже полуплоскость. Вот эта область слева — область, где гипотеза предсказывает y = 0. Я хочу дать название пурпурной прямой, которую я нарисовал. Термин для этой линии — граница решений. Строго говоря, эта прямая, x1 + x2 = 3, соответствует множеству точек, где значение h(x) в точности равно 0,5. То есть граница решений — линия, разделяющая область, где гипотеза предсказывает y = 1, и область, где гипотеза предсказывает y = 0. Уточню для ясности: граница решений — свойство функции-гипотезы при заданных значениях параметров тета нулевое, тета первое и тета второе. Я нарисовал обучающий набор, некоторые обучающие данные, просто чтобы облегчить понимание графика. Даже если мы уберем данные, граница решений и области, где мы предсказываем y = 0 или y = 1, останутся, потому что они определяются гипотезой и ее параметрами и не зависят от обучающего набора. Позже мы поговорим о том, как определить подходящие параметры, и тогда, для вычисления значения параметров мы, конечно, будем использовать обучающие данные. Но если конкретные тета нулевое, тета первое и тета второе уже установлены, они однозначно определяют границу решений, и чтобы нарисовать ее на графике, обучающий набор нам не нужен. Рассмотрим более сложный набор данных. Как обычно, крестиками отмечены положительные примеры, а ноликами — отрицательные. Как логистическая регрессия может подобрать гипотезу к таким данным? Когда мы говорили о многомерной линейной регрессии, мы обсуждали добавление членов более высокого порядка к набору характеристик. То же самое можно сделать в задаче логистической регрессии. А именно, допустим, что моя гипотеза выглядит так: я добавил две характеристики, x1 в квадрате и x2 в квадрате. Таким образом, теперь у меня пять параметров, от тета нулевого до тета четвертого. Мы снова отложим до следующего видео вопрос о том, как алгоритмически выбрать значения параметров. Предположим, что при помощи некого алгоритма я уже определил, что тета нулевое равно −1, тета первое и тета второе равны нулю, а тета третье и тета четвертое — единице. То есть мой вектор параметров тета равен [−1; 0; 0; 1; 1]. Согласно тем же рассуждениям, что и прежде, гипотеза будет предсказывать, что y = 1, если минус один плюс x1 в квадрате плюс x2 в квадрате будет больше или равно нулю. То есть когда произведение транспонированного вектора тета на x будет больше или равно нулю. Я перенесу −1 в правую часть и получу, что гипотеза предсказывает положительный результат, если сумма квадратов x1 и x2 больше или равна единице. Как же выглядит граница решений? Если мы захотим построить график кривой «x1 в квадрате плюс x2 в квадрате равно 1», многие из вас узнают уравнение окружности единичного радиуса с центром в начале координат. Это и будет моя граница решений. Все, что находится вне этого круга, соответствует предсказанию y = 1. То есть здесь, снаружи, моя область y = 1. Для этих входных данных я буду предсказывать y = 1. А внутри окружности я буду предсказывать y = 0. Итак добавляя более сложные характеристики, характеристики более высокой степени, я могу отделять положительные и отрицательные результаты не только прямой линией, я могу получать и менее тривиальные границы решений, например, здесь граница решений — окружность. Повторюсь: граница решений не связана с обучающим набором, это свойство гипотезы и ее параметров. Как только мы получаем вектор параметров тета, мы можем определить границу решений, в нашем случае — окружность. Но обучающий набор не участвует в определении границы решений. Обучающий набор может использоваться для поиска подходящих параметров, позже мы разберемся, как это сделать, но если у вас уже есть вектор параметров тета, именно он задает границу решений. Я верну данные на график, чтобы он выглядел более понятно. Наконец, давайте рассмотрим еще более сложный пример. Можем ли мы получить еще более сложные границы решений, чем эта? Если использовать одночлены более высокой степени, к примеру, x1 в квадрате, x1 в квадрате на x2, x1 в квадрате на x2 в квадрате и так далее, возможно получить и более сложно устроенные границы решений. С помощью логистической регрессии можно получить границу решений в виде эллипса, а с каким-нибудь еще набором параметров может выйти граница решений какого-нибудь забавного вида, скажем, такого, а в особенно сложных случаях границы решений могут быть еще более замысловатой формы, например, такой. И все, что находится внутри, соответствует предсказанию y = 1, а все, что снаружи — y = 0. Многочленам высокой степени могут отвечать границы решений весьма сложной конфигурации. Итак, я надеюсь, что вы смогли наглядно представить себе, какими могут быть функции-гипотезы в модели логистической регресии. Теперь, понимая, как устроена h(x), в следующем видео мы поговорим о том, как автоматически подбирать значения параметров тета, как, основываясь на обучающих данных, алгоритмически получать подходящие параметры.