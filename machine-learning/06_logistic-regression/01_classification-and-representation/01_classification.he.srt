1
00:00:00,500 --> 00:00:04,650
בסרטון הזה ובסרטונים הקרובים, אני רוצה להתחיל לדבר על בעיות

2
00:00:04,650 --> 00:00:09,510
סיווג, שבהן המשתנה y שאתה רוצה לחזות הוא בוליאני.

3
00:00:09,510 --> 00:00:12,651
נפתח אלגוריתם שנקרא רגרסיה לוגיסטית,

4
00:00:12,651 --> 00:00:16,931
שהוא אחד מאלגוריתמי הלמידה הפופולריים ביותר והנפוצים ביותר בשימוש כיום.

5
00:00:19,473 --> 00:00:23,270
הנה כמה דוגמאות של בעיות סיווג.

6
00:00:23,270 --> 00:00:26,530
מוקדם יותר דיברנו על סיווג דוא"ל זבל

7
00:00:26,530 --> 00:00:29,390
כדוגמה לבעיית סיווג.

8
00:00:29,390 --> 00:00:33,110
דוגמה נוספת היא איך לסווג עסקאות מקוונות.

9
00:00:33,110 --> 00:00:35,540
אם יש לך אתר אינטרנט שמוכר דברים

10
00:00:35,540 --> 00:00:39,390
ואתה רוצה לדעת אם עסקה מסוימת נועדה להונות אותך או לא,

11
00:00:39,390 --> 00:00:44,590
האם הקונה משתמש בכרטיס אשראי גנוב או אולי גנב את סיסמת המשתמש.

12
00:00:44,590 --> 00:00:46,830
גם זו בעיית סיווג.

13
00:00:46,830 --> 00:00:50,840
וקודם דיברנו גם על דוגמה של סיווג

14
00:00:50,840 --> 00:00:53,960
גידולים, גידולים סרטניים, כממאירים או כשפירים.

15
00:00:55,120 --> 00:00:59,450
בכל הבעיות הללו המשתנה y שאנחנו מנסים לחזות הוא משתנה

16
00:00:59,450 --> 00:01:04,160
שאפשר לחשוב עליו כמקבל אחד משני ערכים, או אפס או אחת,

17
00:01:04,160 --> 00:01:09,080
או דואר זבל או לא דואר זבל, או הונאה או לא הונאה, גידול ממאיר או שפיר.

18
00:01:10,500 --> 00:01:15,670
שם נוסף לקבוצה שאנחנו מסמנים באפס הוא הקבוצה השלילית,

19
00:01:15,670 --> 00:01:20,020
ושם אחר עבור הקבוצה המקבלת את הערך אחת הוא הקבוצה החיובית.

20
00:01:20,020 --> 00:01:23,930
אז נסמן באפס את הגידולים השפירים, ובאחת,

21
00:01:23,930 --> 00:01:27,110
הקבוצה החיובית, את הגידולים הממאירים.

22
00:01:27,110 --> 00:01:31,370
או הסיווג לשתי קבוצות, דואר זבל או לא דואר זבל, וכן הלאה.

23
00:01:31,370 --> 00:01:35,120
ההחלטה על שתי הקבוצות, החיובית והשלילית, איזו אפס

24
00:01:35,120 --> 00:01:37,290
ואיזו אחת, היא שרירותית במקצת

25
00:01:37,290 --> 00:01:42,220
ולא ממש חשובה, אבל הרבה פעמים יש לנו הרגשה שקבוצה היא שלילית כי

26
00:01:42,220 --> 00:01:46,590
חסר לה משהו כמו היעדרו של גידול ממאיר.

27
00:01:46,590 --> 00:01:51,460
ואילו אחת בקבוצה החיובית מסמלת שיש לה התכונה

28
00:01:51,460 --> 00:01:55,170
שאנחנו מחפשים, אבל ההחלטה מי שלילית

29
00:01:55,170 --> 00:01:58,790
ומי חיובית היא שרירותית ולא חשובה כל כך.

30
00:02:00,150 --> 00:02:03,080
כרגע אנחנו נתחיל עם בעיות סיווג שבהן

31
00:02:03,080 --> 00:02:05,510
יש רק שתי אפשרויות, אפס ואחת.

32
00:02:05,510 --> 00:02:09,320
מאוחר יותר נדבר גם על בעיות רבות-מחלקות שבהן

33
00:02:09,320 --> 00:02:14,250
y עשוי לקבל נניח ארבעה ערכים אפס, אחת, שתיים, שלוש.

34
00:02:14,250 --> 00:02:17,720
זו נקראת בעיית סיווג רבת-מחלקות.

35
00:02:17,720 --> 00:02:22,140
אבל במשך מספר קטעי הווידאו הבאים, בואו נתחיל עם חלוקה לשתי קבוצות, או בעית סיווג

36
00:02:22,140 --> 00:02:25,978
בינארית ורק יותר מאוחר נטפל במצב של מחלקות מרובות.

37
00:02:25,978 --> 00:02:30,580
אז איך אנחנו מפתחים אלגוריתם לסיווג?

38
00:02:30,580 --> 00:02:34,770
הנה דוגמא של סדרת אימון עבור משימת סיווג

39
00:02:34,770 --> 00:02:37,410
לסיווג גידולים לממאירים או שפירים.

40
00:02:37,410 --> 00:02:44,570
ושימו לב שלממאירות יש רק שני ערכים, אפס דהיינו לא, אחת דהיינו כן.

41
00:02:44,570 --> 00:02:47,520
אז דבר אחד שאנחנו יכולים לעשות בהינתן סדרת האימון הזו

42
00:02:47,520 --> 00:02:50,309
הוא להחיל את האלגוריתם שאנחנו כבר מכירים,

43
00:02:51,410 --> 00:02:53,410
את הרגרסיה הלינארית, ביחס למערך הנתונים הזה,

44
00:02:53,410 --> 00:02:56,320
ופשוט לנסות להתאים את הקו הישר לנתונים.

45
00:02:56,320 --> 00:02:59,840
אז אם ניקח את סדרת האימון הזו ונתאים לה קו ישר,

46
00:02:59,840 --> 00:03:03,730
נקבל השערה שנראית אולי ככה, כן.

47
00:03:03,730 --> 00:03:05,695
אז זוהי ההשערה שלי.

48
00:03:05,695 --> 00:03:09,100
(h(x שווה תטא משוחלפת כפול x.

49
00:03:09,100 --> 00:03:14,650
אם אנחנו רוצים לעשות תחזית, אז דבר אחד שאפשר לנסות לעשות הוא לשים סף

50
00:03:14,650 --> 00:03:19,165
של תוצאות הסיווג ברמה של 0.5, זאת אומרת לפי

51
00:03:19,165 --> 00:03:23,985
הציר האנכי ב-0.5 ואם פונקצית ההשערה

52
00:03:23,985 --> 00:03:27,235
מוציאה ערך גדול או שווה ל-0.5 אז נחליט ש-y = 1.

53
00:03:27,235 --> 00:03:29,955
ואם התוצאה קטנה מ-0.5 אז נחליט ש-y = 0.

54
00:03:29,955 --> 00:03:32,775
הבה נראה מה קורה אם עשינו את זה.

55
00:03:32,775 --> 00:03:36,359
אז 0.5 הוא המקום שבו נמצא הסף

56
00:03:36,359 --> 00:03:39,990
וכך משתמשים כאן ברגרסיה ליניארית.

57
00:03:39,990 --> 00:03:43,520
כל מה שנמצא מימין לנקודה הזו אנחנו

58
00:03:43,520 --> 00:03:44,710
נשייך לקבוצה החיובית.

59
00:03:44,710 --> 00:03:50,360
כי ערך התוצאה שווה או גדול מ-0.5 על הציר האנכי, וכל

60
00:03:50,360 --> 00:03:54,629
מה שנמצא משמאל לנקודה הזו, אנחנו נעריך כערך שלילי.

61
00:03:55,720 --> 00:03:57,620
בדוגמא הספציפית הזו,

62
00:03:57,620 --> 00:04:01,630
נראה כאילו שהרגרסיה הליניארית בעצם עושה משהו הגיוני.

63
00:04:01,630 --> 00:04:05,420
למרות שזו בעיית סיווג.

64
00:04:05,420 --> 00:04:08,120
אבל עכשיו הבה ננסה לשנות קצת את הבעיה.

65
00:04:08,120 --> 00:04:11,530
הרשו לי להאריך קצת את הציר האופקי,

66
00:04:11,530 --> 00:04:15,263
ובואו נניח שיש לנו עוד נקודה אחת בסדרת האימון כאן הרחק מימין.

67
00:04:15,263 --> 00:04:18,900
שימו לב שדוגמת האימון הנוספת כאן

68
00:04:18,900 --> 00:04:21,960
לא באמת משנה שום דבר, כן.

69
00:04:21,960 --> 00:04:26,200
כאשר מסתכלים על סדרת האימון אז זה עדיין די ברור מהי השערה טובה.

70
00:04:26,200 --> 00:04:28,970
היא איזו נקודה כאן שכל מה שמימינה,

71
00:04:28,970 --> 00:04:31,010
בצד ימין שלה, אנחנו צופים שהיא חיובית.

72
00:04:31,010 --> 00:04:34,930
וכל מה שמשמאלה אנחנו כנראה צריכים לחזות כשלילית, כי

73
00:04:34,930 --> 00:04:39,620
מסדרת האימון הזו נראה כאילו כל הגידולים הגדולים יותר מאשר ערך מסוים כאן

74
00:04:39,620 --> 00:04:44,210
הם ממאירים, וכל הגידולים הקטנים מזה אינם ממאירים, לפחות

75
00:04:44,210 --> 00:04:44,820
עבור סדרת האימון הזו.

76
00:04:46,200 --> 00:04:50,730
אבל ברגע שהוספנו את התוספת כאן, אם נריץ עכשיו

77
00:04:50,730 --> 00:04:54,480
רגרסיה ליניארית, במקום לקבל קו ישר שמתאים לנתונים.

78
00:04:54,480 --> 00:04:56,090
הקו עשוי אולי להיראות כך.

79
00:04:57,600 --> 00:05:02,890
ואם עכשיו השערת הסף שלנו היא 0.5,

80
00:05:02,890 --> 00:05:06,350
אז אנחנו מגיעים לסף הזה בנקודה הזאת, אז

81
00:05:06,350 --> 00:05:09,750
כל מה שמימין לנקודה הזו אנחנו נחזה כחיובי,

82
00:05:09,750 --> 00:05:12,070
וכל מה שמשמאל לנקודה הזו אנחנו חוזים כשלילי.

83
00:05:14,590 --> 00:05:18,820
וזה נראה די רע לרגרסיה הליניארית, נכון,

84
00:05:18,820 --> 00:05:23,110
כי אתם יכולים לראות שאלה הם הדוגמאות החיוביות ואלה הם הדוגמות השליליות.

85
00:05:23,110 --> 00:05:28,090
וזה די ברור שאנחנו באמת צריכים לעשות את החיתוך בין שתי הקבוצות איפשהו בסביבות כאן,

86
00:05:28,090 --> 00:05:31,260
אבל איכשהו על ידי הוספה של דוגמא אחת כאן רחוק מימין,

87
00:05:31,260 --> 00:05:34,300
דוגמא שבאמת לא מוסיפה לנו שום מידע,

88
00:05:34,300 --> 00:05:37,050
הרי זו לא אמורה להיות הפתעה לאלגוריתם הלמידה

89
00:05:37,050 --> 00:05:40,260
שהדוגמא הרחוקה הזו תתברר כממאירה.

90
00:05:40,260 --> 00:05:45,210
אבל איכשהו המצאותה של הדוגמה הרחוקה הזו גורמת לרגרסיה הליניארית לשנות

91
00:05:45,210 --> 00:05:50,880
את ההתאמה של מודל הקו ישר לנתונים מהקו הזה כאן בצבע סגול

92
00:05:50,880 --> 00:05:55,670
אל הקו הכחול הזה כאן, מה שגורם לה לתת לנו השערה יותר גרועה.

93
00:05:56,900 --> 00:06:01,120
ולכן החלת רגרסיה ליניארית לבעית סיווג

94
00:06:01,120 --> 00:06:04,470
היא לעתים קרובות לא הרעיון הכי טוב.

95
00:06:04,470 --> 00:06:09,870
בדוגמא הראשונה לפני שהוספתי את דוגמת האימון הנוספת,

96
00:06:09,870 --> 00:06:14,760
לרגרסיה הליניארית פשוט היה מזל ויצאה לנו במקרה השערה

97
00:06:14,760 --> 00:06:19,940
שעבדה היטב בדוגמא המסוימת ההיא, אבל בדרך כלל הפעלת

98
00:06:19,940 --> 00:06:24,760
רגרסיה ליניארית על סדרת נתונים, אולי יהיה לכם מזל, אבל בדרך כלל זה לא הרעיון הכי טוב.

99
00:06:24,760 --> 00:06:28,350
אני לא הייתי משתמש ברגרסיה ליניארית עבור בעיות סיווג.

100
00:06:29,700 --> 00:06:33,830
הנה עוד דבר מצחיק שהיה קורה לו השתמשנו

101
00:06:33,830 --> 00:06:36,740
ברגרסיה ליניארית עבור בעית סיווג.

102
00:06:36,740 --> 00:06:40,630
בסיווג אנחנו יודעים ש-y הוא או אפס או אחת.

103
00:06:40,630 --> 00:06:44,250
אבל אם נשתמש ברגרסיה ליניארית, פונקציית ההשערה

104
00:06:44,250 --> 00:06:48,380
יכולה להוציא ערכי פלט שהם גדולים מאחת או קטנים מאפס,

105
00:06:48,380 --> 00:06:52,570
אפילו אם לכל דוגמאות האימון יש תוויות y שווה לאפס או אחת.

106
00:06:53,920 --> 00:06:56,739
וזה נראה קצת מוזר שלמרות

107
00:06:56,739 --> 00:07:00,786
שאנו יודעים כי התוויות חייבות להיות אפס או אחת זה נראה קצת מוזר

108
00:07:00,786 --> 00:07:05,661
אם האלגוריתם יכול להוציא פלט גדול בהרבה מאחת או קטן בהרבה מאפס.

109
00:07:09,135 --> 00:07:13,795
אז מה שנעשה בהרצאות הקרובות הוא לפתח אלגוריתם שנקרא רגרסיה

110
00:07:13,795 --> 00:07:17,044
לוגיסטית, שהמאפיין שלו הוא שהפלט,

111
00:07:17,044 --> 00:07:21,635
התחזית של רגרסיה לוגיסטית הוא תמיד בין אפס ואחת,

112
00:07:21,635 --> 00:07:25,114
ואף פעם לא עולה מעבר לאחת ולא יורד מתחת לאפס.

113
00:07:26,250 --> 00:07:29,260
ודרך אגב, השם רגרסיה לוגיסטית,

114
00:07:29,260 --> 00:07:33,370
ואנו נשתמש בו כאלגוריתם סיווג, הוא אולי

115
00:07:33,370 --> 00:07:38,230
לפעמים קצת מבלבל כי מופיע בה המונח רגרסיה,

116
00:07:38,230 --> 00:07:42,150
למרות שרגרסיה לוגיסטית היא למעשה אלגוריתם סיווג.

117
00:07:42,150 --> 00:07:44,720
אבל זהו פשוט השם שניתן לו מסיבות הסטוריות.

118
00:07:44,720 --> 00:07:49,210
אז אל תתנו לזה לבלבל אתכם, רגרסיה לוגיסטית הוא למעשה אלגוריתם

119
00:07:49,210 --> 00:07:54,542
סיווג שאנו מיישמים במצבים בהם ערך הפלט y הוא דיסקרטי,

120
00:07:54,542 --> 00:07:56,610
במקרה שלנו אפס או אחת.

121
00:07:56,610 --> 00:08:01,000
אז אני מקווה שאתם מבינים עכשיו למה רגרסיה ליניארית

122
00:08:01,000 --> 00:08:03,640
איננה מתאימה לבעיות סיווג.

123
00:08:03,640 --> 00:08:04,500
בסרטון הבא,

124
00:08:04,500 --> 00:08:08,080
נתחיל לעבוד על הפרטים של אלגוריתם הרגרסיה הלוגיסטית.