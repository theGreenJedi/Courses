前回のビデオでは ロジスティック回帰における仮説の表現について議論した。 今回行いたいのは 決定境界と呼ばれる物について 議論したい。 そしてこれを通して、 ロジスティック回帰の仮説関数が 何を計算してるかを、より良く理解出来るだろう。 復習として、 これが前回書いた物。 前回言ったのは、 仮説、hのxは、 イコール gの シータ転置 xで、 ここでgは この関数で、sigmoid関数と 呼ばれていて、それはこんな感じだった。 それは0から1まで ゆっくりと増加し、1に漸近する。 今やりたいのは、 この仮説がyを 1と予測するのはどんな時で、 それに対して yを0と予測するのはどんな時なのか より良く理解する事を試みて それを通して この仮説の関数がどんな物か より深く理解する事です。
特にフィーチャーが一つより多い場合に。 具体的には、この仮説は あるxとパラメータ シータが与えられた時に、 yが1となる確率を推計する。 だから、もし yが1なのか、0なのか 予測したければ こんな感じとなるでしょう。 仮説がy=1となる確率を 0.5以上と言う時はいつでも つまり、 これは y が 0 よりも 1 により近い ことを意味するので y=1 であると予測しよう。 そしてまた、 y が 1 である推定確率が 0.5 より小さいときは、 y=0 であると予測しよう。 これが以上で これが未満 もし h(x) が ちょうど 0.5 なら、 陽性と予測しても 陰性と予測しても構わない。 だがここでは大なりイコールとした。 つまりデフォルトでは、 h(x)が0.5なら 陽性だとした。 だがそれはどっちでも良い仔細な話だ。 私がやりたいのは、正確に いつh(x)が 0.5以上となり しいてはy=1を 予測するかを より深く理解したい、という事。 もしこのsigmoid関数のプロットを見たら sigmoid関数g(z)は、 zがゼロ以上の時は いつでも 0.5以上と なる事に 気付くでしょう。 つまりこの図の この半分では gは0.5またはそれ以上の値をとる。 0.5をとるかどうかは曖昧だ。 だからzが 正の時は、gのz、 つまりsigmoid関数は、0.5以上、つまり0.5を含むとする。 ロジスティック回帰の 仮説、hのxは gの (シータ転置 x) なので、 これはつまり、 0.5以上と 0.5を含む事になる。 (シータ転置 x)が 0以上の時はいつでも。 ここまで見てきたのは、、、ところで、 ここまではいいですかね。この シータ転置 xが、 zの代わりとなるからです。 で、ここまで見てきたのは、 我らの仮説は シータ転置 xが0以上の時はいつでも y=1と 予測するという事。 では反対の、 仮説がy=0を 予測するケースも見てみよう。 同様の議論により hのxは、 gのzがゼロ未満の時は いつでも0.5未満となる。 何故ならzの範囲が 0.5未満になるのに 必要な範囲は zが負の時という事だから。 gのzが0.5未満の時は 我らの仮説はy=0を 予測する事になる。 さっきと同様の議論で hのxは、イコール gの シータ転置 xなので、 だから、 この値、 シータ転置 xがゼロ未満の時はいつでも y=0と予測する事となる。 ここまでやった事を要約すると、 y=1と予測するか y=0と予測するかを 推計された確率が 0.5以上か または0.5未満かに従って 決定するとするなら、 その時は それは シータ転置 xが0以上の時は いつでも y=1と予測すると 言っているに等しい。 そしてシータ転置xが0未満なら y=0と 予測する。 この事を用いて、 ロジスティック回帰の仮説が どのようにそれらの予想を行うかを、より深く理解していこう。 今、このスライドに示しているような トレーニングセットが あるとしよう。 そして仮説h(x)は gのシータ0足す シータ1 x1 足す シータ2 x2 と仮定する。 このモデルにどうパラメータをフィッティングするかは まだ話していないね。 それは次のビデオで話すよ。 だがここではこの値を なんとかして決めたと仮定して、 以下の値をパラメータとして選んだとする。 シータ0を-3、 シータ1を1、 シータ2を1と。 つまり、パラメーターベクトルは シータ= -3、1、1。 そして、この仮説のパラメータを 与えられたとして、 仮説が結果として どこはy=1と予測し、 どこはy = 0と予測するかを 明らかにしていこう。 前のスライドにあった 式を用いると、 y=1の方が より有りそうなのは、 言い換えるとy=1となる確率が 0.5以降か0.5以上、 と言える。 それはシータ転置xが 0より大きい時はいつでもそうだ。 そしてこの式、 今下線を引いたこの、 -3+x1+x2は、 もちろん、シータが この値と等しいパラメータの時は シータ転置 xであり、 このパラメータはまさに我らが決めた物だった。 どのような例、 -3+x1+x2 大なりイコール 0を満たす どんなx1、x2も、 その条件をみたした 物なら 必ず、 我らの仮説は y=1が、より有りえそう、と 判断する、言い換えると、y=1を予測する。 この-3を 右辺にうつす事も出来て、 x1+x2 大なりイコール 3と 書き直す事が出来る。 すると、同じ事だが、 この仮説は、 x1+x2が3以上である限り、 y=1を予測する、とも解釈出来る。 図の上ではどう解釈出来るか見てみよう。 式を書き下すと、 x1+x2=3 で、 これは直線の方程式となる。 そしてその直線を 書いてみると、 以下のような直線となり、 それはx1軸とx2軸の 3と3を通る。 そして入力空間の部分、、、 x1+x2大なりイコール3 に対応する x1とx2の領域は、 それはこの右半分の平面となる。 つまりこの上、、、 このマゼンダ色の線より 右上の部分は全部。 だから、我らの仮説が、 y=1を予言する範囲は この領域。 このとても 巨大な範囲。 この右上の半空間。 その事を明示的に書いてみよう。 これをy=1の領域と 呼ぼう。 対照的に、 x1+x2＜3 となる領域、 そこはy=0を 予言する事になる領域。 それはこの領域に対応する。 この半平面。 この左側の半平面が 我らの仮説がy=0を予言する領域。 この線、 このマゼンダ色の線に名前をつけよう。 この線は決定境界（decision boundary）と 呼ばれる。 具体的にはこの直線、 x1+x2=3、 それに対応する点の集合は h(x)=0.5に ちょうど一致する 点に 対応する。 そして決定境界は、 それはこの直線の事だが、 それは仮説がy=1を 予言する範囲と、 y=0を予言する範囲を 分離する物だ。 確認しておくと、 決定境界は パラメータのシータ0、シータ1、シータ2を含む 仮説の性質だ。 図の中ではトレーニングセットも書いたが、 それは可視化を助ける為に書いたに過ぎない。 だがたとえデータセットを 取り除こうと、 決定境界は決定境界で、 y=1を予言する範囲と y =0を予言する範囲は変わらない。 それは仮説の性質、 そして仮説のパラメータの 性質だから。 データセットの性質では無い。 この後に、もちろん、 パラメータをどうフィッティングするかについて 議論していくが、 そこでは結局はトレーニングセット、つまり データを使って、パラメータの値を決定していくが、 一旦パラメータ、シータ0、シータ1、シータ2を 決めた後には、 その値が完全に 決定境界を定義し、 決定境界をプロットする為に トレーニングセットをプロットする必要は まったく無い。 もっと複雑な例も 見てみよう。 いつも通り、陽性の手本を クロス記号で示し、 陰性の手本をOで示す。 こんなトレーニングセットが与えられた時、 この種のデータにフィットする ロジスティック回帰は、どうやったら出来るか？ 前に、多項式回帰について話した時、 または線形回帰の時、 どうやって追加の高次の項を フィーチャーに足すのかについて 議論した。 ロジスティック回帰でもまったく同じ事が出来る。 具体的には、仮説がこんな形の場合を考えよう。 2つのフィーチャー、x1の二乗とx2の二乗を フィーチャーに追加した。 だから今や、5つのパラメータがある。 シータ0からシータ4まで。 以前と同じように、 どうやってシータ0からシータ4を 自動で選ぶかについては、 次のビデオまで説明は保留する事にする。 だが、ある手続きにより、 その値が決定されて、 結果は シータ0= -1 シータ1=0 シータ2=0 シータ3=1 シータ4=1 となったとする。 その意味する所は、 このパラメータの場合、 パラメータベクトルは こんな感じに、-1、0、0、1、1となる。 以前の議論と同様に、 仮説は-1+x1^2+x2^2が、 0以上の時はいつでも、 y =1を 予言する。 これはつまり、シータ転置掛ける、、、 シータ転置掛けるフィーチャーが 0以上の時はいつでも、って事だ。 そしてこの-1を 右辺へ持っていくと、 私の言ってる事はつまり、 仮説がy =1を 予言するのは、 x1^2+x2^2が 1より大きい時はいつでも、という事だ。 では、決定境界はどんな形だろうか？ x1 の自乗足す x2 の自乗イコール 1 の 曲線をプロットすると 分かってる人も何人かいると思うが、 これは 半径 1 で中心が 原点である円の方程式だ。 これがこの決定境界だ。 そして円の外側は すべて y=1 と 予測する。 このへんが y=1 の領域。 ここを y=1 と予測する。 そして円の内側は y=0 と予測する。 つまり、これらのより複雑な、 またはこれらの多項式をフィーチャーに追加する事で より複雑な決定境界を 得る事が出来、 陽性と陰性の手本を、単に直線で分けようとしなくて済む。 この例で得られた 決定境界は、円だ。 もう一度言うが、決定境界は トレーニングセットの 性質ではなく、仮説とパラメータの性質だ。 パラメータベクトルのシータが 与えられていさえすれば、 決定境界は定義され、 それはこの場合は円となる。 トレーニングセットが決定境界を決めるのでは無い。 トレーニングセットはパラメータのシータをフィッティングするのに使うかもしれない物だ。 それをどうやるかは後で議論する。 だがひとたびパラメータのシータが 手に入ってしまえば、それこそが決定境界を決める。 可視化の便宜の為、 トレーニングセットを戻そう。 最後に、もっと複雑な例を見てみよう。 これより複雑な 例が思い付くだろうか？ より高次の 多項式、 例えば x1^2, x1^2 * x2, x2^2, 等々だ。 とても高次の 多項式の場合、 より複雑な 決定境界を見ることができ、 ロジスティック回帰により 例えばこんな決定境界を 見つけることができるだろう。 こんな楕円や 異なる設定の パラメータによっては 代わりにこんな決定境界さえ 得られるだろう。 こんな奇妙な形。 またさらに複雑な例としては こんな決定境界も 得られるだろう。 より複雑なこんな形。 内部はすべて y=1 と予測し、 外部はすべて y=0 と予測する。 こういった高次の多項式により 非常に複雑な決定境界が得られるわけだ。 さて、これらの可視化によって、 ロジスティック回帰に使われる 表現形式で表現できる 仮説関数の範囲について 感じがつかめたと思う。 h(x) が表現するものについてわかったので、 次のビデオで私がしたいのは どのようにして自動的に パラメータ θ を選ぶかについてお話することだ。 トレーニングセットがあるとき、 パラメータを自動的にデータにフィットさせられるように