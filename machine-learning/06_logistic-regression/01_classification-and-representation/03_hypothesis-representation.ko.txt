로지스틱 회귀분석을 시작하면서 여러분들에게 표현 모델을 보여주고 싶습니다. 그것은, 우리가 분류문제를 가지고 있을 때에<br />우리의 가설을 표현하기 위해 사용할 함수입니다. 이전에, 우리는 분류문제의 결과값이 0에서 1 사이가 되도록 한다고 말했습니다. 그래서 우리는 이러한 결과값을 만족하는가설 즉, 예측값이 0에서 1사이인 가설을 만들고 싶습니다. 선형 회귀분석을 사용할 때에는, 이것이 가설 형식이었습니다. h(x)가 Θ의 전치행렬 * x입니다. 로지스틱 회귀분석에서는, 전 이것을 조금 변경하여 g(Θ의 전치행렬 * x)으로 다음과 같은 g함수를 사용하여 변경합니다. G(z) 중, z는 실수이며, 1 더하기 -z 지수의 e 분의 1로 정합니다. 이것은 시그모이드 함수 혹은 로지스틱 함수이며 로지스틱 함수라는 용어는 로지스틱 회귀분석이라는 이름의 만들었습니다. 이 외에도, 시그모이드라는 용어나 로지스틱 함수라는 용어는 기본적으로<br />동의어이며 같은 뜻을 의미합니다. 그래서 두 용어는 상호간 변경이 가능하며, 그리고 둘 다 함수 g를 나타낼 수 있습니다. 그리고 만일 우리가 이 두 방정식을 합치면 이것이 제 가설의 형태를 쓸 수 있는 다른 방법입니다. h(x)가 1 더하기 -Θ의 전치행렬 * x 지수의 e분의 1이며 제가 한 것은 변수 z를 가져온 것 뿐입니다. z는 실수이며, Θ의 전치행렬 * x와 연결됩니다. 그로므로, 전 z의 위치에 Θ의 전치행렬 * x를 위치하고 끝냅니다. 마침내, 여러분들에게 시그모이드 함수를 보여드리겠습니다. 우리는 이것을 여기에 이렇게 그립니다. 시그모이드 함수이자, g(z), 로지스틱 함수는 이렇게 생겼습니다. 0 근처에서 시작하여, 원점위의 0.5를 지날 때까지 오릅니다. 그 후 이렇게 납작해집니다. 그 결과 시그모이드 함수는 이렇게 생겼습니다. 여러분들께서도 아실 수 있다시피, 시그모이드 함수는 1과 0에 수렴하며, z값의 수평축을 기준으로 z가 음의 무한대로 향하면, g(z)는 0로 수렴합니다. 그리고 z가 무한대로 갈수록, g(Z)는 1에 수렴합니다. 그래서 g(z)는 0과 1사이의 양의 값을 가지며, 우리는 h(x)또한 0과 1사이임을 알 수 있습니다. 마침내, 이러한 가설 모형을 통해서, 우리가 할 것은 이전처럼 파라미터들을 우리의 데이터에 맞추는 것입니다. 주어진 훈련 예시들로부터, 우리는 매개변수Θ의 값을 정하고 이 가설들을 통해 우리는 예측을 할 수 있습니다. 우리는 매개변수Θ를 맞추는 학습<br />알고리즘에 대해 나중에 이야기하겠습니다. 먼저 이 모델의 해석에 대해 이야기해보죠. 제 가설 h(x)의 결과값에 대해 해석해보겠습니다. 제 가설의 결과들이 몇개의 수일 때에, 전 그 숫자들을 추가로 추입된 x에서 y가 1일 수 있는 가능성으로 여길 것입니다. 말하자면 이러한 예시와 같습니다. 우리가 종양 분류 예시를 사용한다고 해보죠. 우리들은 특징들인 x벡터들이 있고, x 밑첨자 0은 언제나 그렇듯 1입니다. 그리고 하나의 특징은 종양의 크기입니다. 특정 크기의 종양을 가진 환자들이 추가되었다고 가정하고, 전 그들의 특징 벡터인 x를 제 가설에 추가합니다. 그리고 제 가설의 결과값은 0.7이 나왔습니다. 저는 제 가설을 다음과 같이 해석할 것입니다. 전 가설이 말해주는 것은 특징 x를 가진 환자 사례에서, y가 1일 확률은 0.7이라는 것입니다. 다르게 말하면, 전 제 환자에게 말하기를 비극적이게도, 악성 종양일 확률이 70%라는 것입니다. 이것을 조금 더 잘 짜여지고, 수학적으로 도출하기 위해 전 제 가설의 결과값을 Θ에서 정해진 변수 x에서 y=1일 가능성으로 해석합니다. 여러분들 중에 확률과 친숙하신 분들은<br />이러한 방정식이 이해가 되실 것입니다. 만일 확률과 친숙하지 않은 분들을 위해 이것이 제가 이 표현을 읽는 방식입니다. 이것이 y가 1일 가능성이며 주어진 x, 즉 제 환자가 특징x를 가지고 있을 때에 다시 말하자면 제 환자가 제 특징들인 x로 나타내지는 특정 종양 크기를 가질 때에 이것이 Θ에 매개된 가능성입니다. 그래서 전 기본적으로 제 가설을 저에게 y가 1일 가능성의 측정값을 준다고 여깁니다. 이제, 이것은 분류 문제이기에 우리는 y가 0 혹은 1일 것이라는 걸 알고 있습니다. 그렇죠? 그 2개의 값들은 y가 가질 수 있는 유일한 값들입니다. 훈련 예시들이건, 제 가상 병원으로 걸어 들어오는 새로운 환자이건 말이죠. 주어진 h(x)에서 우리는 y가 0의 값을 가지는 가능성도 계산할 수 있습니다. 왜냐하면 y는 0이거나 1이기 때문입니다. 우리는 y가 0인 가능성과 y가 1인 가능성을 합하면 1이여야 하는 것을 압니다. 이 첫번째 방정식은 조금 더 복잡해 보일 수 있습니다. 이것은 기본적으로 특징 x와 정해진<br />매개변수Θ를 가진 특정한 환자의 경우에서 y가 0일 가능성입니다. 이에 특징 x와 정해진 매개변수를 가지는 같은 환자의 경우에서 y가 1일 확률을 더하면 1이 될 것입니다. 만일 이러한 방정식이 조금 복잡하다면 x와 Θ를 제외하고, 이것이 그냥 y가 0일 확률과 1일 확률을 더한 것이라고 생각하셔도 됩니다. 그리고 우리는 y가 0이거나 1인 것을 알기에 참인 것을 알며, y가 0일 확률과 1일 확률들은 합이 반드시 1이 됩니다. 그리고 만일 여러분이 이 변수를 오른쪽으로 옮기면, 여러분들은 이 방정식을 끝낼 수 있습니다. 이것은 y가 0일 가능성은 1 빼기 y가 1일 가능성과 같다는 것을 나타냅니다. 그러므로 만일 x에 대한 가설의 그 값(y=1일 가능성)이 주어진다면 여러분들은 매우 쉽게 y=0일 가능성이나 측정 예측값을 계산할 수 있습니다. 이제 여러분들은 로지스틱 회귀분석의 가설 모형이 어떤 건지 알게되었고, 또한 수학적인 공식을 살펴보았으며, 로지스틱 회귀분석의 가설을 정의해봤습니다. 다음 강의에서는, 여러분들이 가설 함수의 모습에 대해 더 나은 직관을 가지도록 하고 싶습니다. 그리고 여러분들에게 결정 경계(decision boundary)라는 것에 대해 말하고 시각화된 것들을 살펴 봄으로써, 로지스틱 회귀분석의 가설 함수가 어떻게 생겼는가에 대해 더 살펴보겠습니다.