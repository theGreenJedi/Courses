在上一节视频里 我们讨论了 核函数这个想法 以及怎样利用它去 实现支持向量机的一些新特性 在这一节视频中 我将 补充一些缺失的细节 并简单的介绍一下 怎么在实际中使用应用这些想法 例如 怎么处理 支持向量机中的偏差方差折中 在上一节课中 我谈到过选择标记点 例如 l(1) l(2) l(3) 这些点使我们能够定义 相似度函数 也称之为核函数 在这个例子里 我们的相似度函数为高斯核函数 这使我们能够 构造一个这样的假设函数 但是 我们从哪里得到这些标记点？ 我们从哪里得到l(1) l(2) l(3)？ 而且 在一些复杂的学习问题中 也许我们需要 更多的标记点 而不是我们手选的这三个 因此 在实际应用时 怎么选取标记点 是机器学习中必须解决的问题 这是我们的数据集 有一些正样本和一些负样本 我们的想法是 我们将选取样本点 我们拥有的 每一个样本点 我们只需要直接使用它们 我们直接 将训练样本 作为标记点 如果我有一个 训练样本x(1) 那么 我将把第一个标记点 就放在跟我的第一个训练样本点 完全重合的地方 如果我有另一个 训练样本x(2) 那么 我将把第二个标记点选在 与第二个样本点重合的位置上 在右边的这幅图上 我用红点和蓝点 来阐述 这幅图以及这些点的颜色 可能并不显眼 但是利用 这个方法 最终能得到 m个标记点 l(1) l(2) 直到 l(m) 即每一个标记点 的位置都与 每一个样本点 的位置精确对应 这个过程很棒 这说明特征函数基本上 是在描述 每一个样本距离 样本集中其他样本的距离 我们具体的列出 这个过程的大纲 给定m个训练样本 我将选取与 m个训练样本精确一致 的位置作为我的标记点 当输入样本x 样本x可以 属于训练集 也可以属于交叉验证集 也可以属于测试集 给定样本x 我们可以计算 这些特征 即f1 f2 等等 这里 l(1) 等于 x(1) 剩下标记点的以此类推 最终我们能到一个特征向量 我将特征向量记为f 我将f1 f2等等 构造为 特征向量 一直写到fm 此外 按照惯例 如果我们需要的话 可以添加额外的特征f0 f0的值始终为1 它与我们之前讨论过的 截距x0的作用相似 举个例子 假设我们有训练样本(x(i), y(i)) 这个样本对应的 特征向量可以 这样计算 给定x(i) 我们可以通过相似度函数 将其映射到f1(i) 在这里 我将整个单词similarity(相似度) 简记为sim 简记为sim f2(i)等于x(i)与l(2) 之间的相似度 以此类推 最后有fm(i) 等于x(i)与l(m)之间的相似度 在这一列中间的 某个位置 即第i个元素 有一个特征 为fi(i) 为fi(i) 这是x(i)和l(i)之间的 相似度 这里l(i)就等于 x(i) 所以 fi(i)衡量的是 x(i)与其自身的相似度 如果你使用高斯核函数的话 这一项等于 exp(-0/(2*sigma^2)) 等于1 所以 对于这个样本来说 其中的某一个特征等于1 接下来 类似于我们之前的过程 我将这m个特征 合并为一个特征向量 于是 相比之前用x(i)来描述样本 x(i)为n维或者n+1维空间 的向量 取决于你的具体项数 可能为n维向量空间 也可能为n+1维向量空间 我们现在可以用 这个特征向量f 来描述我的特征向量 我将合并f(i) 将所有这些项 合并为一个向量 即从f1(i) 到fm(i) 如果有需要的话 我们通常也会加上 f0(i)这一项 f0(i)等于1 那么 这个向量 就是 我们用于描述训练样本的 特征向量 当给定核函数 和相似度函数后 我们按照这个方法来使用支持向量机 如果你已经得到参数 θ 并且想对样本x做出预测 我们先要计算 特征向量f f是m+1维特征向量 这里之所以有m 是因为我们有m个训练样本 于是就有m个标记点 我们在 θ 的转置乘以f 大于或等于0时 预测y=1 对吧 θ 的转置乘以f 等于θ0×f0加上θ1×f1 加上点点点 直到θm×fm 所以 参数向量θ 在这里为 m+1维向量 这里有m是因为 标记点的个数等于 训练点的个数 m就是训练集的大小 所以 参数向量θ为m+1维 以上就是当已知参数θ时 怎么做出预测的过程 怎样得到参数θ呢？ 你在使用 SVM学习算法时 具体来说就是要求解这个最小化问题 你需要求出能使这个式子取最小值的参数θ 式子为C乘以这个我们之前见过的代价函数 只是在这里 相比之前使用 θ的转置乘以x(i) 即我们的原始特征 做出预测 我们将替换 特征向量x(i) 并使用这个新的特征向量 我们使用θ的转置 乘以f(i)来对第i个训练样本 做出预测 我们可以看到 这两个地方(都要做出替换) 通过解决这个最小化问题 我们就能得到支持向量机的参数 最后一个细节是 对于这个优化问题 我们有 n=m个特征 就在这里 我们拥有的特征个数 显然 有效的特征个数 应该等于f的维数 所以 n其实就等于m 如果愿意的话 你也可以认为这是一个求和 这确实就是 j从1到m的累和 可以这么来看这个问题 你可以想象 n就等于m 因为如果f 不是新的特征向量 那么我们有m+1个特征 额外的1是因为截距的关系 因此这里 我们仍要j从1累加到n 与我们之前 视频中讲过的正则化类似 我们仍然不对θ0 做正则化处理 这就是 j从1累加到m 而不是从0累加到m的原因 以上 就是支持向量机的学习算法 我在这里 还要讲到 一个数学细节 在支持向量机 实现的过程中 这最后一项与这里写的有细微差别 其实在实现支持向量机时 你并不需要知道 这个细节 事实上这写下的这个式子 已经给你提供了 全部需要的原理 但是在支持向量机实现的过程中 这一项 θj从1到m的平方和 这一项可以被重写为 θ的转置 乘以θ 如果我们忽略θ0的话 考虑θ1直到θm 并忽略theta_0 那么 θj的平方和 可以被重写为 θ 的转置乘以 θ 大多数支持向量机 在实现的时候 其实是替换掉 θ 的转置乘以 θ 用 θ 的转置乘以 某个矩阵 这依赖于你采用的核函数 再乘以 θ 这其实是另一种略有区别的距离度量方法 我们用一种略有变化的 度量来取代 不直接用 θ 的模的平方进行最小化 而是最小化了另一种类似的度量 这是参数向量θ的变尺度形式 这种变化和核函数相关 这个数学细节 使得支持向量机 能够更有效率的运行 支持向量机做这种修改的 理由是 这么做可以适应 超大的训练集 例如 当你的训练集有10000个样本时 根据我们之前定义标记点的方法 我们最终有10000个标记点 θ也随之是10000维的向量 或许这时这么做还可行 但是 当m变得非常非常大时 那么求解 这么多参数 如果m为50,000或者100,000 此时 利用支持向量机软件包 来解决我写在这里的最小化问题 求解这些参数的成本 会非常高 这些都是数学细节 事实上你没有必要了解这些 它实际上 细微的修改了最后一项 使得最终的优化目标 与直接最小化θ的模的平方略有区别 如果愿意的话 你可以直接认为 这个具体的实现细节 尽管略微的改变了 优化目标 但是它主要是为了计算效率 所以 你不必要对此有太多担心 顺便说一下 你可能会想为什么我们不将 核函数这个想法 应用到其他算法 比如逻辑回归上 事实证明 如果愿意的话 确实可以将核函数 这个想法用于定义特征向量 将标记点之类的技术用于逻辑回归算法 但是用于支持向量机 的计算技巧 不能较好的推广到其他算法诸如逻辑回归上 所以 将核函数用于 逻辑回归时 会变得非常的慢 相比之下 这些计算技巧 比如具体化技术 对这些细节的修改 以及支持向量软件的实现细节 使得支持向量机 可以和核函数相得益彰 而逻辑回归和核函数 则运行得十分缓慢 更何况它们还不能 使用那些高级优化技巧 因为这些技巧 是人们专门为 使用核函数的支持向量机开发的 但是这些问题只有 在你亲自实现最小化函数 才会遇到 我将在下一节视频中 进一步讨论这些问题 但是 你并不需要知道 怎么去写一个软件 来最小化代价函数 你能找到很好的成熟软件来做这些 就像我一直 不建议自己写矩阵求逆函数 或者平方根函数 的道理一样 我也不建议亲自写 最小化代价函数的代码 而应该使用 人们开发的 成熟的软件包 这些软件包 已经包含了那些数值优化技巧 所以你不必担心这些东西 但是另外一个 值得说明的问题是 在你使用支持向量机时 怎么选择 支持向量机中的参数？ 在本节视频的末尾 我想稍微说明一下 在使用支持向量机时的 偏差-方差折中 在使用支持向量机时 其中一个要选择的事情是 目标函数中的 参数C 回忆一下 C的作用与1/λ相似 λ是逻辑回归算法中 的正则化参数 所以 大的C对应着 我们以前在逻辑回归 问题中的小的λ 这意味着不使用正则化 如果你这么做 就有可能得到一个低偏差但高方差的模型 如果你使用了 较小的C 这对应着 在逻辑回归问题中 使用较大的 λ 对应着一个高偏差 但是低方差的模型 所以 使用较大C值的模型 为高方差 更倾向于过拟合 而使用较小C值的模型为高偏差 更倾向于欠拟合 C只是我们要选择的其中一个参数 另外一个要选择的参数是 高斯核函数中的σ^2 当高斯核函数中的 σ^2偏大时 那么对应的相似度函数 为exp(-||x-l(i)||^2/(2*σ^2)) exp(-||x-l(i)||^2/(2*σ^2)) exp(-||x-l(i)||^2/(2*σ^2)) 在这个例子中 如果我们只有一个特征x1 我们在这个位置 有一个标记点 如果σ^2较大 那么高斯核函数 倾向于变得相对平滑 这可能是我的特征fi 所以 由于函数平滑 且变化的比较平缓 这会给你的模型 带来较高的偏差和较低的方差 由于高斯核函数变得平缓 就更倾向于得到一个 随着输入x 变化得缓慢的模型 反之 如果σ^2很小 这是我的标记点 利用其给出特征x1 那么 高斯核函数 即相似度函数会变化的很剧烈 我们标记出这两种情况下1的位置 在σ^2较小的情况下 特征的变化会变得不平滑 会有较大的斜率 和较大的导数 在这种情况下 最终得到的模型会 是低偏差和高方差 看到这条曲线 本周的编程作业 你就能亲自实现这些想法 并亲眼看到这些效果 这就是利用核函数的支持向量机算法 希望这些关于 偏差和方差的讨论 能给你一些 对于算法结果预期的直观印象