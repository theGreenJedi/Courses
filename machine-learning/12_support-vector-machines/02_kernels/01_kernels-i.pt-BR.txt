Neste vídeo, eu gostaria de iniciar adaptando a
máquina de vetor de suporte para desenvolver classificadores não-lineares complexos. A técnica prinicipal para fazer isso é algo chamado Kernels. Vamos ver o que são Kernels e como usá-los. Se você tem um conjunto de treino que se parece com isso, e você quer encontrar uma fronteira de decisão não-linear para distinguir os exemplos negativos e positivos, talvez uma fronteira de decisão que se pareça com isso. Uma forma de fazer isso é propor um conjunto de variáveis polinomiais complexas, certo?
Assim, um conjunto de variáveis que se pareça com isso, então você acaba com uma hipótese X que prevê 1 se você sabe que Teta 0 e mais Teta 1 X1 mais ponto ponto ponto,
todas essas variáveis polinomiais são maiores que 0, e que prevê 0, do contrário. E outra forma de escrever isso, para introduzir um nível de uma nova notação que eu irei usar posteriomente, é que nós podemos pensar numa hipótese como calcular uma fronteira de decisão usando isso. Então, Teta 0 mais Teta 1 f1 mais Teta 2, f2 mais Teta 3, f3 mais e assim por diante. Onde eu irei usar essa nova denotação f1, f2, f3 e assim por diante para denotar esse novo tipo de variáveis que eu estou calculando, então f1 é apenas X1, f2 é igual a X2, f3 é igual a este um aqui. Então, X1 X2. Assim, f4 é igual a X1 ao quadrado onde f5 vai ser x2 ao quadrado e assim sucessivamente e nós vimos anteriormente que propor esses polinômios de alta ordem é uma forma de obter mais variáveis, mas a questão é, se existe uma escolha diferente de variáveis ou se existe um tipo melhor de variáveis do que esses polinômios de alta ordem,
pois você sabe, não está claro que este polinômio de alta ordem é o que nós queremos, e o que nós falamos sobre visão computacional sobre quando a entrada é uma imagem com vários pixels. Nós também vimos que usar
polinômios de alta ordem acaba tornando-se muito caro
computacionalmente porque existem vários desses termos de polinômios de alta ordem. Então, existe uma escolha diferente ou uma opção melhor de variáveis que nós podemos usar nessa espécie de forma hipotética. Então, aqui está uma idéia de como definir novas variáveis f1, f2, f3. Nessa linha eu vou definir apenas três novas variáveis, mas para problemas reais nós podemos definir um número muito maior. Mais aqui está o que eu vou fazer nesta fase de variáveis X1, X2 e Eu vou deixar X0 de fora, o interceptor X0, mas nessa fase X1 X2,
eu vou apenas, escolher manualmente alguns pontos,
e então chamar esses pontos l1, nós vamos escolher um ponto diferente,
vamos chamar ele de l2 e vamos escolher o terceiro e chamar este de l3,
e por enquanto, vamos apenas dizer que eu vou escolher estes três pontos manualmente. Eu vou chamar estes três pontos de "pontos de referência",
então referência um, dois, três. O que vou fazer é definir minhas novas variáveis conforme segue, dado um exemplo X, deixe-me definir minha primeira variável f1 como sendo uma medida da similaridade entre meus exemplos de treinamento X e meu primeiro ponto de referência, e essa fórmula específica que eu vou usar para medir a similaridade vai ser este E para menos o comprimento de X menos l1, ao quadrado, dividido por dois sigma ao quadrado. Então, dependendo de você ter assistido ou não o vídeo opcional anterior, essa notação , sabe, é o comprimento do vetor W. E assim, essa coisa aqui, esse X menos l1, esse é na  verdade, a distância euclidiana ao quadrado, é a distância euclidiana entre o ponto x e o marco l1. Veremos mais sobre isso posteriormente. Mas essa é a minha primeira variável, e minha segunda variável f2 será você sabe, função de similaridade que mede o quão similar X é para l2 e novamente
será definido como a função a seguir. Este é E, menos para o quadrado da distância
euclidiana negativa, entre X e o segundo ponto de referência, isso é o que o enumerador é, e então, dividido por 2 sigma ao quadrado e similarmente f3 é, você sabe, similaridade entre X e l3, que é igual a, novamente, a fórmula similar. E o que essa função similaridade é, o termo matemático para isso, é que isso será uma função de Kernel. E esse Kernel específico que eu estou usando aqui, é na verdade, chamado de Kernel Gaussiano. E essa fórmula, essa escolha particular de função de similaridade é chamada de Kernel Gaussiano. Mas a forma da terminologia é que, no abstrato, essas diferentes funções de similaridade são chamadas de Kernels e nós podemos ter diferentes funções de similaridade e o exemplo específico que eu estou dando
aqui é chamado de Kernel Gaussiano. Nós veremos outros exemplos de outros Kernels. Mas, apenas por hora,
pense nelas como funções de similaridade. Assim, ao invés de escrever similaridade entre X e l, as vezes nós também escrevemos isso com a notação kernel, denotado como k, em caixa baixa,
entre x e um dos pontos de referência. Vamos ver o que Kernels realmente fazem e porque esse tipo de função similaridade, porque essas expressões podem fazer sentido. Então vamos pegar o meu primeiro ponto de referência. Meu ponto de referência l1, que é um dos pontos que eu
escolhi na minha figura agora. Então a similaridade do Kernel entre x e l1 é dada por essa expressão. Apenas para ter certeza, de que nós estamos na "mesma página" sobre o que o termo numerador é, o numerador também pode ser escrito como uma soma de J igual a 1, até N nesta distância. Então este é o componente de distância entre o vetor "x" e o vetor "l". E novamente, para o propósito desses slides eu estou ignorando X0. Apenas ignorando o termo interceptador X0,
que é sempre igual a 1. Então, você vê, é assim que você calcula o Kernel com similaridade
entre X e um ponto de referência. Vejamos o que esta função faz. Suponha que X está próximo de um dos
pontos de referência. Então esta fórmula de distância euclidiana e o numerador vão ser próximos de 0. Então, este termo aqui, a distância ao quadrado, a distância usando X e 0 serão próximas de zero, e assim f1, é uma variável simples, será aproximadamente E menos 0 e então, o numerador ao quadrado sobre 2 sigma ao quadrado de forma que E para 0, e E para menos 0, E para 0 será próximo de um. Eu vou colocar o símbolo de aproximação aqui porque a distância talvez não seja exatamente 0, mas se X é próximo do ponto de referência, esse termo será próximo de 0, e assim f1 será próximo de 1. Por outro lado, se X está longe de l1, então essa primeira variável f1 será E menos de algum grande número ao quadrado, dividido por dois sigma ao quadrado e E negativo de um grande número será próximo de 0. Então o que essas variáveis fazem é medir quão similar X é de um dos seus pontos de referência,
e a variável f será próxima de 1 quando X está próximo do seu ponto de referência e será 0 ou próximo de zero quando X estiver distante do seu ponto de referência. Cada uma dessas referências, na linha anterior, eu desenhei três pontos de referência, l1, l2, l3. Cada um desses pontos de referência, definem uma nova variável f1, f2 e f3. Isto é, dado o exemplo de treinamento X, agora nós podemos calcular três novas variáveis: f1, f2 e f3, dados os três pontos de referência que eu escrevi agora. Mas primeiro, vamos olhar essa função de exponenciação, vamos para essa função de similaridade e plotar algumas figuras e entender melhor como isso realmente fica. Para este exemplo,
vamos dizer que eu tenho duas variáveis, X1 e X2 E vamos dizer que meu primeiro ponto de referência, l1, está na localização , 3 5. Vamos dizer que eu defino sigma ao quadrado igual a um, por enquanto. Se eu plotar como essa variável se parece, vou obter como retorno esta figura. Então o eixo vertical,  a altura da superfície é o valor de f1 e aqui em baixo no eixo horizontal, se eu tenho um exemplo de treino, e existe x1 e existe x2. Dado um certo exemplo de treino, o exemplo de treinamento aqui, que mostra o valor de x1 e x2 numa altura acima da superfície, mostra o valor correspondente de f1, e  aqui embaixo está a mesma figura que eu mostrei, usando uma plotagem quantificável, com x1 no eixo horizontal, x2 no eixo horizontal, essa figura no fundo é apenas uma curva de nível da superfície 3D. Você nota que quando X é igual a 3 5 exatamente, então, o f1 recebe o valor 1, porque ele está no seu máximo, e conforme X se distancia, essa variável recebe os valores que estão próximos de 0. E assim, isto é realmente uma variável, mede o quão próximo X está do primeiro X do ponto de referência e se entre 0 e 1, dependendo de quão próximo X está do primeiro ponto de referência l1. Agora, outra coisa feita nesse slide é mostrar os efeitos da variação do parâmetro sigma ao quadrado. Então, sigma ao quadrado é o parâmetro E conforme você varia ele,
você obtêm efeitos diferentes. Vamos definir sigma ao quadrado para ser igual a 0.5 e ver o que nós obtemos.
Nós obtemos sigma ao quadrado para 0.5, o que você vai encontrar é que o Kernel parece similar, exceto pela largura do gráfico se torna mais estreita. Os contornos encolhem um pouco também. Assim, se sigma ao quadrado é igual a 0.5 então conforme você inicia de X igual a 3 5 e conforme você se distancia, então a variável f1 cai para zero muito mais rapidamente, e da mesma forma, se você aumenta sigma ao quadrado para três, nesse caso conforme eu me afasto de "l". Então, esse ponto aqui é realmente l, certo, isto é l1 na localização 3 5, exibido aqui em cima. Se sigma ao quadrado é grande, então conforme você se afasta de l1, o valor da variável cai muito mais devagar. Então, dada essa definição das variáveis, vamos ver qual origem da hipótese nós podemos aprender. Dado o exemplo de treinamento X, vamos calcular essas variáveis f1, f2, f3 e uma hipótese irá predizer uma quando teta 0 mais Teta 1 f1 mais Teta 2 f2, e assim por diante, for maior ou igual a 0. Para este exemplo em particular, vamos dizer que eu já encontrei um algoritmo de aprendizado e vamos dizer que, de alguma forma eu obti estes valores do parâmetro. Então se Teta 0 é igual a menos 0.5 , Teta 1 é igual a 1, Teta 2 igual a 1 e, Teta 3 é igual a 0, E o que eu quero fazer é considerar o que acontece se nós temos um exemplo de treino que recebe como localização este ponto em magenta, exatamente onde eu acabei de desenhar o ponto. Então vamos dizer que eu tenho um exemplo de treino X, o que a minha hipótese irá prever? Bem, se eu ver essa fórmula, pelo fato do meu treino de exemplo X estar próximo de l1, temos que f1 estará próximo de 1, porque o meu exemplo de treino X está distante de l2 e l3. f2 será próximo de 0 e f3 será próximo de 0. Então, se eu observar aquela fórmula, eu tenho Teta 0 mais Teta 1 vezes 1 mais Teta 2 vezes algum valor. Não exatamente 0, mas vamos dizer que próximo de 0, Então mais Teta 3 vezes algo próximo de 0. E isto será igual a estes valores agora. Então, isso resulta em menos 0.5 mais 1 vezes 1 que é 1 e assim por diante. Qual é igual a 0.5, que é maior que ou igual a 0. Então, nesse ponto, nós vamos estimar Y igual 1, porque isso é maior que ou igual a zero. Agora, vamos pegar um outro ponto. Vamos dizer que eu pego um ponto diferente.
Eu vou desenhar este aqui numa cor diferente, digamos ciano, para um ponto ali fora, se isso fosse meu exemplo de treinamento X, então se você computar de maneira semelhante, você verá que f1, f2 e f3 vão todos ser próximos de 0. E assim, nós temos Teta 0 mais Teta 1, f1, e assim por diante, e isso será igual a menos 0.5 , porque Teta 0 é menos 0.5 e f1, f2 e f3 são todos zero. Então isso será menos 0.5, isso é menor que zero. Então, nesse ponto fora dali, nós vamos estimar que Y é igual a zero. Se você fizer isso, isso, você mesmo, para uma faixa de pontos diferentes, certifique-se de convencer você mesmo que se você possui um exemplo de treinamento que é próximo de l2, por exemplo, então nesse ponto nós também vamos estimar Y igual a 1. E de fato, o que você acaba fazendo, se você observar ao redor dessa borda esse espaço, o que nós vamos ver é que para pontos próximos de l1 e l2, nós acabamos estimando positivo. E para pontos distantes de l1 e l2, isto é, para pontos distantes destes dois pontos de referência, nós acabamos estimando que a classe é igual a 0. O que nós acabamos fazendo, é que a fronteira de decisão dessa hipótese iria acabar parecendo com algo assim, onde dentro da fronteira de decisão vermelha, nós estimamos Y igual a 1 e fora nós estimamos Y igual a 0. Então é assim com esta definição dos pontos de referência e da função Kernel. Nós podemos aprender Fronteiras
de Decisão não-lineares bem complexas, como o que eu desenhei onde nós estimamos positivamente, quando nós estamos próximos de um
dos dois pontos de referência. E nós estimamos negativo quando nós estivermos muito distantes de qualquer um dos pontos de referência. E então, isto é parte da idéia dos Kernels e de como nós os utilizamos com a máquina de vetores de suporte, que é o que nós definimos estas variáveis extras usando referências e funções de similaridade para aprender classificadores não-lineares mais complexos. Então, espero que isso dê a você uma idéia de Kernels e de como nós podemos usá-los para definir novas variáveis para a máquina de vetores de suporte. Mas existem algumas questões que ainda não foram respondidas. Uma delas é, como nós obtemos esses
pontos de referência? Como nós escolhemos esses pontos? E outra é, quais outras funções de similaridade,
se existe alguma, nós podemos usar além daquela que nós falamos a respeito, que é chamada de Kernel Gaussiano. No próximo vídeo nós responderemos essas questões e vamos juntar tudo para mostrar como as máquinas de suporte de vetores com Kernels podem ser uma forma eficaz de aprender funções não-lineares complexas.