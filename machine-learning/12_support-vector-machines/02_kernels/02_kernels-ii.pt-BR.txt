No vídeo passado, começamos a falar sobre a ideia de kernels e como ela pode ser usada para definir novos recursos para a máquina de vetores de suporte. Neste vídeo, eu quero preencher alguns detalhes que faltaram, e, também, falar um pouco sobre como utilizar essas ideias na prática. Por exemplo, como elas se relacionam com a inclinação e a variância em máquinas de vetores de suporte. No último vídeo, falei sobre o processo de escolher referências, ou marcações. Aqueles l⁽¹⁾, l⁽²⁾, l⁽³⁾, l⁽⁴⁾ nos permitiram definir a função de similaridade, também chamada kernel, ou, como neste exemplo, se você tem esta função de similaridade, isto é um kernel gaussiano. E isso nos permitiu construir esse formato de função hipótese. Mas de onde conseguimos essas referências? De onde conseguimos l⁽¹⁾, l⁽²⁾, l⁽³⁾, l⁽⁴⁾? E parace, também, que para problemas de aprendizagem complexos, talvez vamos querer muito mais referências do que as três que escolhemos à mão. Na prática, esta é a forma com que as referências são escolhidas. Dado o problema de aprendizado de máquina, temos algum conjunto de dados constituído por exemplos positivos e negativos. Bom, a ideia aqui é pegar os exemplos, e, para cada exemplo de treinamento que temos, vamos apenas chamá-las. Vamos simplesmente colocar referências exatamente no mesmo lugar que os exemplos de treinamento. Assim, se eu tenho um exemplo de treinamento x⁽¹⁾, vou escolher minha primeira referência coincidindo exatamente com o mesmo lugar do meu primeiro exemplo de treinamento. E, se tenho um exemplo de treinamento diferente x⁽²⁾, vou colocar a segunda referência no lugar do segundo exemplo de treinamento. Na figura à direita, usei pontos vermelhos e azuis somente como ilustração, as cores nesta figura, as cores dos pontos na figura à direita não é importante. Mas o que vou acabar fazendo com este método é criar m referências l⁽¹⁾, l⁽²⁾, até l⁽ᵐ⁾ se eu tiver m exemplos de treinamento com uma referência por localização de cada um dos meus exemplos de treinamento. Isso é bom, porque isto diz que meus recursos vão medir, basicamente, o quão próximo de um exemplo é aquilo que encontrei no conjunto de treinamento. Ou seja, explicando novamente, dados m exemplos de treinamento, escolherei a localização das minhas referências para coincidir exatamente nas localizações dos m exemplos de treinamento. Quando você recebe um exemplo x, e nesse exemplo x pode ser algo dentro do conjunto de treinamento, no conjunto de validação cruzada ou no conjunto de teste. Dado o exemplo x, vamos computar esses recursos encontrando f₁, f₂, e por aí vai. Aqui, l⁽¹⁾ é igual a x⁽¹⁾, e assim também com os demais. E estes, então, nos dão um vetor de recursos. Deixe-me escrever f como um vetor de recursos. Tomarei esses f₁, f₂, e assim por diante, e os agruparei em um vetor de recursos. Todos eles, até f⁽ᵐ⁾ Além disso, somente por convenção, se quisermos, podemos adicionar um recurso extra f₀, que é sempre igual a 1. Ele tem um papel parecido com o que vimos antes para x₀, que era o intercepto. Por exemplo, se  nós temos um exemplo de treinamento (x⁽ⁱ⁾, y⁽ⁱ⁾), os recursos que calcularíamos para esse exemplo de treinamento seriam como os seguintes: dado x⁽ⁱ⁾, vamos então, mapeá-lo para f1(i), que é a similaridade. Vou abreviar como 'SIM' em vez de escrever a palavra similaridade, OK? e f₂⁽ⁱ⁾ é igual à similaridade entre x⁽ⁱ⁾ e l⁽²⁾, e similarmente com os próximos, até fm(i) sendo a similaridade entre x⁽ⁱ⁾ e l⁽ᵐ⁾. E, em algum lugar por aí no meio, em algum lugar nessa lista, no i-ésimo componente, teremos realmente um recurso que é fᵢ⁽ⁱ⁾ que será a similaridade entre x⁽ⁱ⁾ e l⁽ⁱ⁾, onde l⁽ⁱ⁾ é igual a x⁽ⁱ⁾, e assim fᵢ⁽ⁱ⁾ será simplesmente a similaridade entre x⁽ⁱ⁾ e ele mesmo. E, se você estiver usando o kernel gaussiano, isso é e elevado a 0 sobre 2 sigma ao quadrado, que resulta em 1. Assim, um dos meus recursos para esse exemplo de treinamento será igual a 1. E, da mesma forma que fiz acima, posso tomar todos esses m recursos e agrupá-los em um vetor de recursos. Assim, em vez de representar meu exemplo usando x⁽ⁱ⁾, que é um vetor em ℝⁿ ou ℝⁿ⁺¹. Dependendo de como você considera, ele estará em ℝⁿ ou ℝⁿ⁺¹. Podemos, agora, representar meu exemplo de treinamento usando esse vetor de recursos f. Vou escrever isso como f⁽ⁱ⁾. Ele é o resultado de agrupar tudo isso e juntar em um vetor. Assim, f₁⁽ⁱ⁾ até fₘ⁽ⁱ⁾, e, se você quiser também, adicionamos esse f₀⁽ⁱ⁾, onde f₀⁽ⁱ⁾ é igual a 1. Assim, esse vetor é meu novo vetor de recursos com o qual represento meu exemplo de treinamento. Assim, dados esses kernels e funções de similaridade, aqui está como podemos usar uma máquina de vetores de suporte simples. Se você já tem parâmetros θ aprendidos, e recebe um valor de x com o qual quer fazer uma previsão, o que fazemos é computar os recursos f, que é um vetor em ℝᵐ⁺¹. E a dimensão depende de m porque temos m exemplos de treinamento e portanto m referências, e estimamos y = 1 se θ' · f -3 + x₁ + x₂  ≥ 0, Correto. Aqui, θ' · f é igual a θ₀ · f₀ mais θ₁ · f₁, somando até θₘ · fₘ. Assim, meu vetor de parâmetros θ também será um vetor de dimensão m + 1. Novamente, isso depende de m porque o número de referências é igual ao tamanho do conjunto de treinamento. Então, m é o tamanho do conjunto de treinamento e o vetor de parâmetros θ será de dimensão m + 1. É assim que você faz uma estimativa se você já conhece os parâmetros θ. E como você obtém os parâmetros θ? Você os calcula usando o algoritmo de aprendizagem da SVM, ou, especificamente, resolve este problema de minimização. Você obteve os parâmetros que minimizam C vezes aquela função de custo que tínhamos anteriormente. Mas agora, em vez de fazer estimativas usando θ' · x⁽ⁱ⁾ usando os recursos originais, x⁽ⁱ⁾, tomamos os recursos x⁽ⁱ⁾ e os substituímos com novos recursos de modo que usamos θ' · f⁽ⁱ⁾ para fazer uma estimativa do i-ésimo exemplo de treinamento, nesses dois lugares aqui, e é resolvendo esse problema de minimização que você obtém os parâmetros para a máquina de vetores de suporte. Só mais um detalhe, por causa desse problema de otimização nós temos n = m recursos. Ou seja, aqui, o número de recursos que temos é, na verdade, a dimensão de f. Assim, n será igual a m. Se você preferir, pode pensar nisso como uma soma, essa é uma soma de j = 1 até m. E uma maneira de pensar sobre isso, você pode pensar que n é igual a m, porque se f é o vetor de recursos, temos m + 1 recursos, sendo o + 1 proveniente do intercepto. Aqui, ainda fazemos a soma de j = 1 até n, porque, assim como nossos vídeos anteriores sobre regularização, nós não regularizamos o parâmetro θ₀, que é a razão porque a soma vai de j = 1 até m em vez de j = 0 até m. Esse é o algoritmo de aprendizagem da máquina de vetores de suporte. Faltou ainda um detalhe matemático que eu deveria mencionar, que é que, da forma que a máquina de vetores de suporte é implementada, esse último termo é calculado um pouco diferentemente. Mas você não precisa saber esse detalhe para poder usar máquinas de vetores de suporte, e, na verdade, as equações que foram escritas aqui devem fornecer toda a intuição de que você precisará. Aliás, na implementação da máquina de vetores de suporte, lembra daquele termo, a somatória em j de θⱼ²? Outra forma de escrever esse termo é θ' · θ, se ignorarmos o parâmetro θ₀. Ou seja, de θ₁ a θₘ, ignorando θ₀. Assim, essa soma em j de θⱼ² pode também ser escrita como θ' · θ. Na verdade, a maioria das implementações de máquinas de vetores de suporte substituem esse θ' · θ, e calculam na verdade θ' vezes alguma matriz no meio, que depende do kernel sendo usado, vezes θ. Isso nos fornece uma métrica de distância um pouco diferente. Usaremos uma medida um pouco diferente em vez de minimizar exatamente a norma de θ ao quadrado, minimizamos algo parecido com ela. Essa é uma versão modificada do vetor de parâmetros θ que depende do kernel. Mas isso é um detalhe matemático. Ele possibilita à máquina de vetores de suporte executar muito mais eficientemente. A razão porque a máquina de vetores de suporte faz isso com essa modificação é que ela possibilita escalar a conjuntos de treinamento muito maiores. Por exemplo, se você tiver um conjunto de treinamento com 10.000 exemplos, da forma que definimos as referências, acabamos com 10.000 referências. Assim, θ será de dimensão 10.000. Talvez isso ainda funcione, mas quando m se torna muito, muito grande, achar uma solução para todos esses parâmetros, se m for 50.000 ou 100.000 uma solução para todos esses parâmetros pode se tornar custosa para o software de máquina de vetores de suporte, minimizando esse problema de otimização que desenhei aqui. Isso é meio que um detalhe matemático, que, novamente, você não precisa saber muito sobre. Ele modifica aquele último termo um pouquinho para otimizar algo um pouco diferente que simplesmente minimizar a norma de θ ao quadrado. Mas, se você preferir, sinta-se à vontade para pensar nisso como um detalhe de implementação que muda o objetivo um pouquinho, mas é feito principalmente por questões de eficiência computacional. Portanto, você não deve ter que se preocupar muito com isso. Aliás, se você estiver pensando por que não aplicamos a ideia de kernel a outros algoritmos também, como regressão logística, se você quiser, pode aplicar a ideia de kernel e definir a fonte dos recursos utilizando referências e todo o resto na regressão logística. Mas os truques computacionais que valem para máquinas de vetores de suporte não generalizam bem a outros algoritmos, como regressão logística. Assim, utilizar kernel s com regressão logística resultará em um algoritmo lento, enquanto, por causa desses truques computacionais, como esse mostrado, e como eles modificam isso, juntamente com os detalhes de como máquinas de vetores de suporte são implementadas, máquinas de vetores de suporte e kernels tendem a se dar bem juntos, enquanto regressão logística com kernels, por mais que você consiga fazer, vai executar muito lentamente. E não conseguirá tirar proveito das técnicas avançadas de otimização que pessoas já descobriram para o caso particular de executar uma máquina de suporte de vetores com kernels. Mas tudo isso diz respeito somente a como você efetivamente implementa software para minimizar a função de custo. Falarei mais sobre isso no próximo vídeo, mas você não precisa saber como escrever software para minimizar essa função de custo porque você consegue achar software pronto muito bom para fazer isso. Da mesma forma que eu não recomendaria fazer um programa para inverter uma matriz ou calcular uma raiz quadrada, eu não recomendo escrever software para minimizar essa função de custo por conta própria mas sim utilizar pacotes de software prontos que já foram desenvolvidos, e esses pacotes de software já incorporam esses truques de otimização numérica, então você não precisa se preocupar com eles. Mas outra coisa que é bom saber é: quando você usa uma máquina de vetores de suporte, como você escolhe os seus parâmetros? A última coisa que quero fazer neste vídeo é falar um pouquinho sobre viés e variância no contexto de uma máquina de vetores de suporte. Quando usar uma SVM, uma das coisas que você precisa escolher é o parâmetro C, que estava no objetivo de otimização. Lembre-se que C tinha um papel similar a 1/λ, onde λ é o parâmetro de regularização que tínhamos na regressão logística. Assim, se você tiver um valor alto para C, isso corresponde ao que temos na regressão logística, em que um valor pequeno de λ significa não usar muita regularização. Se você o fizer, irá tenderá a encontrar uma hipótese com viés baixo e variância alta. Agora, se você usar um valor menor de C, isso corresponde a usar regressão logística com um valor mais alto de λ, e isso corresponde a uma hipótese com viés alto e variância baixa. Assim, hipóteses com C grande tem maior variância, e é mais suscetível a sobreajuste, enquanto hipóteses com C pequeno têm viés maior e são, portanto, mais suscetíveis a subajuste. Assim, esse parâmetro C é um dos que precisamos escolher. O outro é o parâmetro σ², que apareceu no kernel gaussiano. Se o σ² no kernel gaussiano é grande, a função de similaridade mudará, aquela igual a e elevado a -||x - l⁽ⁱ⁾||² sobre 2σ². Neste exemplo, se eu tiver somente um recurso, x₁, terei uma referência exatamente naquele local, se σ² é grande, o kernel gaussiano diminuirá de valor relativamente devagar, então este seria meu recurso fᵢ, e essa função seria mais plana, variando mais lentamente, o que te dará uma hipótese com viés alto e variância baixa, porque o kernel gaussiano cai lentamente. Você tenderá a conseguir uma hipótese que varia lentamente à medida que mudamos a entrada x. Em contraste, se σ² for pequeno e esta é minha referência, dada pelo único recurso x₁, o kernel gaussiano, minha função de similaridade, variará mais abruptamente. Nos dois casos o pico tem valor 1, e se σ² é pequeno, meus recursos irão variar mais lentamente. Aqui as inclinações são maiores, as derivadas são maiores. Usando isso, você ajustará hipóteses de viés baixo, mas pode ter variância alta. E se olhando essa curva, no exercício de programação desta semana, você vai poder brincar com algumas dessas ideias por conta própria, e ver esses efeitos por conta própria. Bom, esse foi o algoritmo SVM com kernels. Espero que esta discussão sobre viés e variância te dará uma noção de como você pode esperar que esse algoritmo se comporte também.