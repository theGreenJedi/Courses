このビデオでは、 複雑な非線形の分類器を構築する為に サポートベクタマシンを用いる事を開始しよう。 これを行う主要なテクニックは、カーネルと呼ばれる物だ。 カーネルとは何で、それをどう使うのかを見ていこう。 こんなトレーニングセットが あったとして、 陽性と陰性の手本を区別する 非線型な決定境界を 見つけたいとする。 たとえばこんな感じの決定境界だ。 これを行う方法として 一つ考えられるのは、 複雑な多項式のフィーチャーを用いる事だ。 つまり、例えばこんな感じのフィーチャー群を用いて、 結果として、 仮説のxを、 シータ0 + シータ1 x1 + ...と これらの多項式フィーチャーに渡って計算して それが0より大きければ 1を予測し、 そしてそれ以外では0を予測する。 これを書くもう一つの 方法として、後で使う事になる ちょっとしたノーテーションを 導入しておくと、 仮説を、これを用いて 決定境界を計算する物と 考える事が出来る、 つまり、シータ0 + シータ1 f1 + シータ2 f2 + シータ3 f3、 などなど。 ここで私は この新しいノーテーション f1, f2, f3などを、 今計算している、 ある種の新しいフィーチャーを示すのに用いていて、 つまりf1は単にx1で、f2はイコールx2で、 f3は これに等しい。 つまり x1x2。 f4はイコール x1の二乗で、 f5はx2の二乗、 などとなり、 そして以前に見たように、 これらの高次の多項式を 含めていくのは、 もっと多くのフィーチャーにする為の一つの方法だ。 ここで疑問に思うのは、 この高次の多項式以外の フィーチャーの選択が無いものか？という事。 何故なら、 これらの高次の多項式は、 我らの欲しい物かいまいち分からないし、 コンピュータビジョンの、 入力が画像のピクセルの場合などを 議論したが、 そこでは高次の多項式を用いるのは 計算量的にとても高くつく事を見た、 何故ならその場合は、 大量の高次の多項式が存在する事になるから。 だから、もっと別の、 もっと良いフィーチャーの選択肢で、 この種の仮説の形に 代入出来るような物は 無いものか？ そこでこれが、新しいフィーチャーf1, f2, f3を定義する 一つのやり方だ。 このスライドでは、 三つのフィーチャーだけを定義するが、 現実の問題では、もっと多くの数の フィーチャーを定義する事になる。 だがここでは、 フィーチャーx1とx2の 張る空間に対し、 ここでx0、 切片項のx0は 省略する事にするが、 このx1 x2の空間の中で、 幾つかの点を手動で選ぶ、 そしてこれらの点を、これをl(1)と呼び、 さらに別の点を 選んで、これをl(2)と呼ぶ、 そして三番目を 選んで、 これをl(3)と呼ぶ事にする。 ここでは、これら三つの点は 手動で選ぶ事にする。 そしてこれらの点をランドマークと呼ぶ事にする。つまりランドマークの1, 2, 3。 そして新たなフィーチャーを 以下のように定義する。 ある手本xが与えられた時に、 最初のフィーチャーf1を なんらかの 類似度の指標、 トレーニング手本xと 最初のランドマークとの類似度を定義する、 そしてここでは具体的に以下のような 類似度の指標を用いる、 eの指数乗の マイナスの x-l1の距離を二乗して、 2 シグマ二乗 で割る。 前回のオプショナルのビデオを あなたが見ているかどうかは分からないが、 このノーテーション、これは ベクトルwの長さを表す。 だからこの、ここのこれは、 このx-l(1)は、 これは実体は単なる ユークリッド距離の二乗で、 点xとランドマークl1との ユークリッド距離だ。 この事は後でもうちょっと詳しく見る事にする。 だがとにかく、これが最初のフィーチャーだ。 二番目のフィーチャーf2は l(2)とxが どれだけ類似しているかを測る similarity関数で、これは以下のような関数で 定義される。 それはeの、以下による指数乗で、その指数は マイナスの xと二番目のランドマークとの間の距離の二乗を分子として、 割ることの2 シグマ二乗 が指数となる。 同様にf3は xとl3の間の類似度で、 それはイコール また同様の式となる。 そしてこのsimilarity関数とは 数学的な用語では、 カーネル関数と 呼ばれる物だ。 そしてここで具体的にカーネルとして使っている関数は 実際にはガウスカーネルと呼ばれる。 つまり、この式、この具体的な similarity関数の選択を、ガウスカーネルと呼ぶ。 だが用語的には、 これらの様々なsimilarity関数を抽象的に カーネルと呼び、 similarity関数には様々な物がありうる。 そしてこの例で私が与えた物は、ガウスカーネルと呼ばれる。 我らは後に、別のカーネルの例を見る事になるだろう。 だが現時点では、これらは単なる類似度の関数と思っておけば良い。 こんな事情により、xとlの間のsimilarity(類似度)と 書く代わりに、 時々これを、カーネルを表す 小文字のkを用いて、xとランドマークの間のカーネル、と記述する事もある。 ではカーネルが実際に 何をやるかを見てみよう、 そして何故この種の類似度関数が、 これらの式が筋が通っているのかを見てみよう。 そこでまず最初のランドマーク、 ランドマークl(1)に対して、 これはさっきの図で私が選んだ点の一つだが、 xとl(1)の間のカーネルの類似度は、この式で与えられる。 この分子が 実際にどうなるのかを 一応書いておくと、 分子はこのように、 j=1からnまでの、ある種の距離の 和となる。 つまりこれは、xとベクトルlの 各要素に渡って取った距離だ。 そしてここでも、 これらのスライド上では、 x0を無視する。 つまり切片項のx0を、これはいつも=1だが、 単に無視する事にする。 すると、これがxとランドマークの類似度を用いて カーネルを計算する方法だ。 ではこの関数が何をするかを見ていこう。 xがランドマークの一つと近いとしてみよう。 すると、この分子にある ユークリッド距離の式は 0に近い値となる。 つまり、この項、 このxとl(1)の距離、 この距離は 0に近い値となる。 するとf1、フィーチャーは、 だいたい近似的に eの指数乗である所の -0の二乗が分子で、2 シグマ二乗が分母。 つまり eの0乗で、 eの-0乗で、 eの0乗はだいたい1。 そしてここに近似の記号を用いたのは 距離は厳密に0では 無いかもしれないからだが、 しかしxがランドマークに近ければ、 この項は0に近くなり、 f1は1に近くなる。 逆に、xがl(1)から 遠く離れている時には、 この最初のフィーチャーf1は eの指数乗である所の 何らかの大きな数字の二乗に、 割ることの2 シグマ二乗で、 つまりeの マイナスの、大きな数による累乗は 0に近い値となる。 つまりこれらのフィーチャーが 行う事は、xとランドマークの一つとが どれだけ似ているかを 測っているのだ。 そしてフィーチャーfは xがランドマークに近い時に 1に近い値となり、 xがランドマークから遥か離れていると、 0に近い 値となる。 前のスライドにあった これらのランドマークそれぞれに対し、 私は三つのランドマーク、l(1)、l(2)、l(3)を描いたのだが、 これらのランドマークそれぞれに対し、新しいフィーチャー f1, f2, f3を定義する。 つまり、ある所与の トレーニング手本xに対し 三つのフィーチャー f1, f2, f3を計算出来る、 さっき書いた三つのランドマークが 所与の時には。 だがまず、 この指数関数を最初に見よう。 まずこの類似度(similarity)関数から見ていこう。 そしてそれをプロットしてみて、 それが実際にどんな形かをもっと良く理解していこう。 この例では、二つのフィーチャーx1とx2があるとする。 そして最初のランドマークのl(1)は、 地点3, 5に あるとする。 そしてシグマ二乗はここでは1としよう。 もしこのフィーチャーをプロットすると、 この図が得られる。 この垂直の軸、 表面の高さは、 f1の値だ。 この下の、水平軸達は、 あるトレーニング手本があった時に、 それはx1とx2を持っている訳だが、 あるトレーニング手本が与えられた時に、 このx1とx2の値の トレーニング手本、 この点の上の曲面までの高さが、 対応するf1の値を示している、 そしてこの下に、 同じ図を等高線プロットを用いて 描いた物を示しておいた。 x1が横軸で x2が縦軸。 つまりこの下の図は この三次元曲面の 等高線プロットだ。 見て分かるように、 xがぴったり3, 5の時には、 f1の値は 1を取る、 何故ならそこが 最大となる点だからだ。 そしてxが離れていくと、 xが離れていくにつれて、 このフィーチャーは 0に近い値を取る。 だからこれは実際に、 f1はxが最初のランドマークと どれだけ近いかを測る指標、フィーチャーで、 それは0と1の間の値を xが最初のランドマークl(1)が どれだけ近いかに応じて 取る。 さて、もう一つこのスライドで 見せたい事は、 このパラメータ、シグマ二乗を変えた時の効果だ。 シグマ二乗はガウスカーネルのパラメータで、 それを変えていく事で、ちょっと違った効果が得られる。 シグマ二乗を イコール0.5にセットしよう。 そしてどうなるか見てみよう。シグマ二乗に0.5をセットすると、 カーネルは基本的には 似たような形だが、こぶの幅が 狭くなる。 等高線もちょっと縮む。 つまりシグマ二乗がイコール0.5なら、 xイコール3 5から 始まって、 そこから離れていくにつれて、 フィーチャーf1は0に、 より急速に落ちていく。 逆に、 シグマ2乗を3に 増加させると、 その場合は、 点lから離れていくと、 この点がlだが、 これはl(1)で、これは 座標3 5だ。それはここ。 そしてシグマ二乗が大きくなると、 するとl(1)から 離れていくに連れて フィーチャーの値は よりゆっくりと、低下していく。 このフィーチャーの定義が 与えられたとして、 どんな仮説が学習出来るか見てみよう。 ある手本xが与えられたとして、 これらのフィーチャーf1, f2, f3を 計算する、 そして仮説は シータ0 + シータ1 f1 + シータ2 f2...が 0以上の時に 1を予測する。 この具体例だと、 既に学習アルゴリズムを見つけていて、 どうにかして既に これらのパラメータの値を 得ていたとする。 そして シータ0 = -0.5、 シータ1 = 1, シータ2 = 1, シータ3 = 0だったとする。 そして私がやりたい事は、 以下のようなマゼンダの点の位置の 手本の時には、 何が起こるか？を 見てみる事だ。 たった今描いたここの点。 トレーニング手本xがあったら、 仮説は何を予測するだろうか？ この式を見てみると、 トレーニング手本xは l(1)の近くなので、 f1は 1に近い値となる、 トレーニング手本xは l(2)とl(3)からは遠く離れているから、 f2は0に近い値で、 f3も0に近い値だから、 この式を見ると、 シータ0 + シータ1 掛ける1 + シータ2掛ける何かしらの値で、その値は 完全に0では無いが、0に近いとしよう。 そして足すことのシータ3掛ける何かしら0に近い値。 これはイコール、これらの値を代入すると、 すると-0.5+ 1*1でこれは1、などと以下同様に、 これはイコール0.5となり、これは0以上だ。 つまり、この点は y=1と予測する事になる、 何故ならこれが0以上となったから。 ここで、別の点をとってみよう。 今度は別の点、 これを別の色で シアン色で 描く事にしよう、それを ここの点とする。 これがトレーニング手本xだとすると、 同様の計算を行っていくと、 f1, f2, f3は 全て0に近い値となる、 すると、シータ0+ シータ1 f1 + ...と続いていき、 これはイコール -0.5となる。 何故ならシータ0は-0.5で、 f1, f2, f3は全てゼロだから。 だからこれは0.5となり、これは0未満だ。 つまり、ここの点は、 y=0と 予測する事になる。 そしてもしあなたが自分で、 さまざまな点で 計算してみると、 l(2)に近い点の トレーニング手本は、 これもまたy=1を予測する事になる、と 納得出来ると思う。 そして結局、 最終的にあなたがやってるのは、 この空間を見回して、 l(1)とl(2)に 近い点なら、 陽性と予測する事になる。 そしてl(1)とl(2)から 遠い点は、 これら二つのランドマークの両方から遠い点は、 そのクラスはイコール0だと 予測する事になる。 だから最終的にやってる事は、 この仮説の決定境界は こんな感じの 物となり、 この赤い決定境界の内側を y=1と予測し、 外側を y=0と予測する。 つまり以上が、 ランドマークとカーネル関数を 定義する事で、 我らは極めて複雑な、非線型の決定境界を 学習させる事が出来る、 例えば私がさっき描いたような、 二つのランドマークに近い時に陽性と予想し、 どのランドマークからも遠く離れている時には 陰性と 予想するような。 以上がカーネルという 考え方と、 それをサポートベクターマシンで 使う方法だ、 それはランドマークを用いて、 これらの新しいフィーチャーを定義し、 類似度関数を用いてより複雑な非線型の分類器を学習する。 以上でカーネルと それを用いて サポートベクターマシンで新しいフィーチャーを定義する方法が 分かっただろうか。 だが、まだ幾つか答えてない疑問がある。 一つ目は、どうやってこれらのランドマークを得たらいいだろうか？ これらのランドマークを、どうやって選んだらいいだろう？ そしてもう一つは、 別の類似度関数として、 我らが話してきた物、ガウスカーネル以外には どんな物が使えるだろうか？ 次のビデオではこれらの問いに対する 解答を与えていく。 そして全てを組み合わせて サポートベクターマシンがカーネルとあわせて どのように複雑な非線形の関数を 学習する事が出来るかを見ていく。