En este video me gustaría empezar a adaptar las máquinas de soporte vectorial para desarrollar clasificadores no lineales complejos. La técnica principal para hacer esto es una técnica llamada kernels o núcleos. Veamos qué son estos kernels y cómo podemos utilizarlos. Si tienes un conjunto de entrenamiento que luce así y quieres encontrar una frontera de decisión no lineal para distinguir entre los ejemplos negativos y los positivos, quizá una frontera parecida a esta, una manera de lograrlo es obtener un conjunto de variables polinómicas complejas. ¿Sí? Es decir, un conjunto de variables como este. Entonces tendremos una hipótesis de “X” que predice 1 si «theta» 0 más «theta» 1”x”1 más el resto de las variables polinómicas es mayor que 0 y, de lo contrario, predice 0. Otra manera de escribir esto, con la que introduciré una nueva notación que utilizaré después, es que podemos pensar en una hipótesis que calcula una frontera de decisión utilizando lo siguiente: «theta» 0 más «theta» 1, “f”1 más «theta» 2, “f”2 más «theta»3, “f”3 etc. donde esta nueva notación “f”1, “f”2, “f”3, etc., denota las nuevas variables que estoy calculando. Ahora, “f”1 es igual a “X”1, “f”2 es igual a “X”2 y “f”3 es igual al tercer término “X”1 ”X”2, “f”4 es igual a “X”1 cuadrada y f5 es igual a “X”2 cuadrada, etc. Ya vimos antes que obtener estos polinomios de alto orden es una manera de obtener muchas variables. La cuestión es si existe una elección distinta de variables o una elección de variables mejor que estos polinomios de alto orden, porque no tenemos claro si estos polinomios de alto orden son lo que queremos. Cuando hablamos acerca de la visión computacional hablamos de cuando la imagen de entrada tiene muchos pixeles, también vimos cómo utilizar polinomios de alto orden se vuelve computacionalmente caro por todos estos términos de polinomios de alto orden. Entonces ¿existe una elección de variables diferente o mejor que podamos utilizar en este tipo de formulación de hipótesis? Aquí hay una idea de cómo podemos definir las nuevas variables “f”1, “f”2, “f”3, etc. En esta línea definiré sólo tres nuevas variables, pero en problemas reales podemos definir muchas más. Lo que haré en esta fase de variables tendremos “X”1, y “X”2 y dejaremos “X”0 fuera de la ecuación, y elegiré algunos puntos manualmente. A este puntos les llamaré “L”1 y luego elegiré otro punto y lo llamaré “L”2 y luego un tercer punto y lo llamaré “L”3. Por ahora digamos que sólo elegiré estos tres punto manualmente. Llamaré estos tres puntos “puntos de referencia”. Tengo el punto de referencia uno, dos y tres. Lo que haré es definir mis variables como sigue: con un ejemplo “X” definiré mi primera variable, “f”1, como la medida de la similaridad entre el ejemplo de entrenamiento “X” y el primer punto de referencia. La fórmula específica que utilizaré para medir la similaridad es “E” elevada a menos la longitud de “X” menos “L”1 cuadrada, dividida entre 2 sigma al cuadrado. Dependiendo de si viste el video opcional anterior, entenderás, o no, que esta notación es la longitud del vector “W”. Esta “X” menos “L”1 es, de hecho, la distancia euclidiana cuadrada; es decir, la distancia euclidiana entre el punto “X” y el punto de referencia “L”1. Hablaremos de esto más adelante. Esta fue mi primera variable. Mi segunda variable, “f”2 será una variable de similaridad que mide qué tan similares son “X” y “L”2. Definiremos esto con la siguiente variable: “E” elevado a menos la raíz cuadrada de la distancia euclidiana entre “X” y el segundo punto de referencia que se expresa en este numerador, dividido entre 2 sigma cuadrada. De igual manera, “f”3 es la similaridad entre “X” y “L”3 que es igual a una fórmula similar. El término matemático para esta variable de similaridad será función de kernel. El kernel específico que utilizaré aquí se llama kernel Gaussiano. Esta fórmula o esta elección particular de la variable de similaridad se llama kernel Gaussiano. A nivel abstracto, estas variables de similaridad se llaman kernels y podemos tener diferentes variables de similaridad. El kernel específico que estoy explicando ahora es el kernel Gaussiano. Veremos otros ejemplos de otros kernels pero, por ahora, pensaremos en estos como variables de similaridad. Entonces, en vez de escribir la similaridad entre “X” y “L”, a veces también escribiremos esto como kernel, que denotaremos con una “k” minúscula, sobre “x” y uno de mis puntos de referencia “L”. Veamos qué hacen los kernels en realidad y por qué este tipo de variables de similaridad o este tipo de expresiones tienen sentido. Tomaré mi primer punto de referencia;  el punto de referencia “L”1 que es uno de los puntos que elegí en de mi figura hace un momento. La similaridad del kernel entre “X” y “L”1 se obtiene con esta expresión. Para asegurarme de que estamos en el mismo canal acerca de cuál es término del numerador, les recuerdo que también se puede escribir como la sumatoria de “J” igual a 1 a la “n”. Esta es la distancia de los componentes entre el vector “X” y el vector “L”. En esta diapositiva, les reitero que ignoraré “X”0. Estoy ignorando el término “X”0 porque siempre es igual a 1. Porque, como sabes, esto es se calcula el kernel con la similaridad entre “X” y el punto de referencia. Ahora veamos qué hace esta variable. Supongamos que “X” es casi igual a uno de los puntos de referencia. La fórmula euclidiana para la distancia del numerador será casi igual a 0; es decir, la distancia entre “X” y “L” será casi igual a 0 y “f”1 será aproximadamente “e” a la menos 0 cuadrada en el numerador sobre 2 sigma cuadrada. Ahora, “e” a la menos 0 será casi igual a 1. Pondré el signo de aproximación aquí porque la distancia puede variar del cero, pero si “X” es casi igual al punto de referencia, entonces este término será casi igual a 0 y, por lo tanto, “f”1 será casi igual a 1. Por el contrario, si “X” está lejos de “L1”, esta primera variable “f1” será “e” a la menos un valor alto al cuadrado dividido entre dos «sigma» cuadrada y “e” a la menos un número alto será casi igual a 0. Lo que hacen estas variables es medir qué tan similar es “X” a uno de los puntos de referencia y la variable “f” será casi igual a 1 cuando “X” sea casi igual al punto de referencia y 0 o casi igual a 0 cuando “X” esté lejos del punto de referencia. Cada uno de estos puntos de referencia que escribí en la diapositiva anterior, “L1”, “L2” y “L3” cada una, define una nueva variable “f1”, “f2” y “f3”. Con el ejemplo de entrenamiento “X”, podemos calcular tres nuevas variable: “f1”, “f2” y “f3” con los tres puntos de referencia que acabo de escribir. Primero, veamos esta función exponencial o esta variable de similaridad y tracemos algunas figuras para entender mejor cómo se representan. Para este ejemplo, digamos que tengo dos variables “x1 y “x2” y digamos que mi primer punto de referencia, “L1” está en la ubicación 3,5. Ahora, supongamos que fijo sigma cuadrada igual a 1, por el momento. Si trazo esto, lo que obtendré será esta figura. En el eje vertical, la altura de la superficie es el valor “f1” y abajo, en el eje horizontal están los ejemplos de entrenamiento como “x1” y “x2”. Con un ejemplo de entrenamiento dado, como este ejemplo de entrenamiento que muestra los valores de “x1 y “x2”, la altura sobre la superficie muestra el valor correspondiente de “f1”. Abajo tenemos la figura que había mostrado antes llamada gráfica de superficie donde “x1” está en el eje vertical y “x2” está en el eje horizontal. Esta figura de abajo es sólo la gráfica de superficie de la superficie 3D. Puedes ver que cuando “x” es igual a 3,5 exactamente, “f1” toma el valor de 1, porque es su máximo y a medida que se aleja “x” esta variables asume valores cercanos al 0. Concluimos que esta variable “f1” mide qué tan cercana es “x” al primer punto de referencia y varía entre 0 y 1 dependiendo de qué tan cerca esté “x” al punto de referencia “L1”. Lo siguiente que me gustaría hacer en esta diapositiva es mostrar los efectos que se provocan al variar el parámetro sigma cuadrada. Sigma cuadrada es el parámetro del kernel Gaussiano y, a medida que varía, arroja resultados ligeramente distintos. Voy a fijar sigma cuadrada como 0.5 y ver qué obtengo. Cuando fijamos sigma cuadrada en 0.5 lo que encontraremos es que el kernel se ve similar, excepto por que el ancho del cono se hace más delgado. Los contornos también se hacen más pequeños. Iniciamos con sigma cuadrada igual a 0.5 y si inicias con “x” igual a 3, 5 pero, a medida que nos alejamos, la variable “f1” cae a 0 mucho más rápido. Por el contrario, si aumentamos sigma cuadrada y la fijamos como sigma cuadrada igual a 3, conforme me alejo de “L”, este punto de aquí o  Ahora, este punto es realmente “i”, que es “L1” está en en la ubicación 3,5. Así que se muestra aquí. Y si sigma cuadrada es grande, entonces te alejas de "L"1 el valor de la variable sigma al cuadrado caerá mucho más lentamente. Ahora, dada esta definición de variables, veamos qué fuentes de hipótesis podemos aprender. Con el ejemplo de entrenamiento “X”, calcularemos estas variables “f1”, “f2” y “f3” y nuestra hipótesis predecirá 1 cuando «theta» 0 más «theta»1, “f1” más «theta» 2, “f2” etc. es mayor que o igual a 0. Para este ejemplo en particular, digamos que ya apliqué el algoritmo de aprendizaje y terminé con estos valores para los parámetros: «theta» 0 es igual a menos 0.5, «theta» 1 es igual a 1, «theta» 2 s igual a 1 y «theta» 3 es igual a 0. Me gustaría considerar qué sucede cuando tengo un ejemplo de entrenamiento que tiene su ubicación en este punto magenta, justo aquí arriba. Supongamos que tenemos el ejemplo de entrenamiento “x”, ¿qué predecirá mi hipótesis? Analicemos esta fórmula: Si mi ejemplo de entrenamiento “x” es casi igual a “L1”, “f1” también será casi igual a 1 y si mi ejemplo de entrenamiento “x” está lejos de “L2” y “L3” “f2” será casi igual a 0 y “f3” también será igual a 0. Así que, si miro esa fórmula, tengo «theta» 0 más «theta» 1 por 1 más «theta» 2 por algún valor que quizá no sea exactamente 0, pero que está cerca, más «theta» 3 por algún valor cercano a 0. Esto será igual a estos valores y esto nos da menos 0.5 más 1 por 1 que es 1, etc., que es igual a 0.5, que es mayor o igual a 0. En este punto predeciré que “y” es igual a 1, porque esto es mayor que o igual a 0. Ahora, tomemos un punto diferente. Digamos que tomo este punto y lo dibujo en un color diferente. De color cian. Si este fuera mi ejemplo de entrenamiento “x”, e hiciera un cálculo similar, encontraré que “f1”, “f2 y “f3” estarán cerca de 0. Tenemos «theta» 0 más «theta» 1, “f1” más todo lo de más, que será igual a menos 0.5, porque «theta» 0 es menos 0.5 y “f1, “f2” y f3” serán cero. Esto será menos 0.5 y esto menor que 0. En este punto, predeciremos que “y” es igual a cero. Y si hace esto, esto tú mismo para un rango de puntos, asegúrate de entender que si tienes un ejemplo de entrenamiento casi igual a “L2”. A este punto también prediremos que “y” es igual a 1. De hecho, lo que terminarás haciendo es, al ver esta frontera, este espacio, encontraremos que para los puntos cercanos a “L1” y “L2” tendremos una predicción positiva y para los puntos que están lejos de los puntos de referencia “L1” y “L2”, terminaremos prediciendo que la clase es igual a 0. Al final la frontera de decisión de esta hipótesis terminará siendo algo así, en donde dentro de la frontera de decisión predeciremos que “y” es igual a 1 y fuera de ella predeciremos que “y” es igual a 0. Y entonces esto es de los puntos de referencia y de la variable de kernel podemos aprender fronteras de decisiones no lineares muy complejas, como las que acabo de trazar aquí, con las que predecimos un resultado positivo cuando estamos cerca de alguno de los puntos de referencia y predecimos un resultado negativo cuando estamos lejos de los puntos de referencia. Esto es parte del concepto de kernels y de cómo podemos utilizarlos con la máquina de soporte vectorial para definir las variables adicionales utilizando puntos de referencia y variables de similaridad para aprender clasificadores no lineales más complejos. Espero que esto te dé un sentido o una idea de los kernels y de cómo los podemos utilizar para definir nuevas variables para la máquina de soporte vectorial. Pero todavía hay algunas preguntas que no hemos respondido. Una es ¿cómo obtenemos estos puntos de referencia? ¿cómo los elegimos? Otra es, ¿qué otras variables de similaridad, si las hay, pueden utilizarse además del kernel Gaussiano, que es de la que hablamos? En el video siguiente responderemos estas preguntas y pondremos todo junto para mostrar cómo pueden ser eficientes las máquinas de soporte vectorial con los kernels para aprender variables complejas no lineales.