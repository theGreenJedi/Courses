1
00:00:00,530 --> 00:00:01,550
En el video anterior

2
00:00:01,950 --> 00:00:03,230
hablamos de los

3
00:00:03,710 --> 00:00:04,590
kernels y de cómo los podemos utilizar

4
00:00:04,860 --> 00:00:07,900
para definir nuevas variables para la máquina de soporte vectorial.

5
00:00:08,100 --> 00:00:08,910
En este video me gustaría

6
00:00:09,230 --> 00:00:10,670
explicar algunos detalles faltantes y

7
00:00:11,020 --> 00:00:12,070
decir unas palabras

8
00:00:12,270 --> 00:00:14,100
acerca de cómo utilizar estas ideas en la práctica, tales

9
00:00:14,650 --> 00:00:15,850
como lo concerniente a la compensación del

10
00:00:16,340 --> 00:00:20,120
oscilación y la varianza en la máquina de soporte vectorial.

11
00:00:22,690 --> 00:00:23,680
En el video anterior también

12
00:00:24,000 --> 00:00:25,970
hablé acerca el proceso de elegir puntos de referencia como

13
00:00:26,660 --> 00:00:28,890
“L1”, “L2” y “L3” y de cómo esto nos

14
00:00:29,150 --> 00:00:30,220
ayudarían a definir la

15
00:00:30,300 --> 00:00:31,900
variable de similaridad también

16
00:00:32,200 --> 00:00:33,500
llamada kernel. En este ejemplo en particular, esta

17
00:00:33,690 --> 00:00:34,830
variable

18
00:00:35,070 --> 00:00:37,410
de similaridad es un kernel Gaussiano

19
00:00:38,610 --> 00:00:40,370
Lo anterior nos ayudó a construir esta

20
00:00:40,660 --> 00:00:42,070
formulación de una variable de hipótesis.

21
00:00:43,180 --> 00:00:44,880
Pero, ¿de dónde sacamos estos puntos de referencia?

22
00:00:45,150 --> 00:00:45,670
¿De dónde sacamos “L1”, “L2” y “L3”?

23
00:00:45,690 --> 00:00:49,080
Parece que para los problemas de aprendizaje

24
00:00:49,610 --> 00:00:50,830
complejos necesitaremos más

25
00:00:50,920 --> 00:00:53,060
puntos de referencia que estos tres que consideramos por elección manual.

26
00:00:55,160 --> 00:00:56,450
En la práctica los puntos

27
00:00:56,580 --> 00:00:57,730
de referencia se eligen de la siguiente manera:

28
00:00:57,830 --> 00:00:59,910
En un problema de aprendizaje

29
00:01:00,150 --> 00:01:01,110
automático dado, tenemos un conjunto de

30
00:01:01,370 --> 00:01:02,230
datos con ejemplos positivos

31
00:01:02,710 --> 00:01:04,460
y negativos. Cuando tomo

32
00:01:05,310 --> 00:01:06,270
estos ejemplos,

33
00:01:06,630 --> 00:01:08,200
por cada

34
00:01:08,470 --> 00:01:09,780
ejemplo de entrenamiento que tenga,

35
00:01:10,490 --> 00:01:11,430
pondré un punto de referencia

36
00:01:11,980 --> 00:01:13,270
en la misma

37
00:01:13,440 --> 00:01:14,850
posición o la

38
00:01:15,490 --> 00:01:17,600
misma ubicación que los ejemplos de entrenamiento.

39
00:01:18,930 --> 00:01:20,360
Si tengo un ejemplo de

40
00:01:20,680 --> 00:01:21,880
entrenamiento “x1”, entonces

41
00:01:22,120 --> 00:01:23,460
elegiré mi

42
00:01:23,670 --> 00:01:24,550
primer punto de referencia

43
00:01:25,100 --> 00:01:26,470
en la misma ubicación

44
00:01:27,250 --> 00:01:28,170
que mi primer ejemplo de entrenamiento.

45
00:01:29,260 --> 00:01:30,180
Y si tengo otro ejemplo de

46
00:01:30,470 --> 00:01:32,340
entrenamiento “x2”, pondré el segundo

47
00:01:32,500 --> 00:01:33,980
punto de referencia

48
00:01:35,060 --> 00:01:37,300
en la ubicación de mi segundo ejemplo de entrenamiento.

49
00:01:38,480 --> 00:01:39,320
En la figura de la derecha utilicé

50
00:01:39,480 --> 00:01:40,480
puntos rojos y azules

51
00:01:40,820 --> 00:01:41,930
como representación. El color

52
00:01:42,420 --> 00:01:44,320
de los puntos

53
00:01:44,370 --> 00:01:46,030
en la figura de la derecha no es importante.

54
00:01:47,120 --> 00:01:47,930
Entonces, utilizando este método

55
00:01:48,110 --> 00:01:49,660
terminaré con un número “m”

56
00:01:49,790 --> 00:01:51,450
de puntos de referencia

57
00:01:52,160 --> 00:01:53,690
“L1”, “L2”, etc.,

58
00:01:54,950 --> 00:01:56,320
hasta “L(m)”, si tengo

59
00:01:56,380 --> 00:01:58,180
“m” ejemplos de entrenamiento

60
00:01:58,420 --> 00:02:00,500
con un punto de referencia por ubicación

61
00:02:00,810 --> 00:02:02,680
de cada uno

62
00:02:02,860 --> 00:02:04,810
de mis ejemplos de entrenamiento. Esto es bueno

63
00:02:04,950 --> 00:02:05,920
porque indica que mis

64
00:02:06,120 --> 00:02:07,630
variables medirán

65
00:02:07,700 --> 00:02:09,300
qué tan cerca

66
00:02:09,380 --> 00:02:10,800
está un ejemplo de

67
00:02:10,970 --> 00:02:13,150
uno de los puntos que dibujé en mi conjunto de entrenamiento.

68
00:02:13,440 --> 00:02:14,180
Ahora, escribiré este esbozo

69
00:02:14,350 --> 00:02:16,270
un poco mejor tomando mis ejemplos

70
00:02:16,470 --> 00:02:17,870
de entrenamiento como referencia y eligiendo

71
00:02:18,050 --> 00:02:19,100
la ubicación de

72
00:02:19,310 --> 00:02:20,430
mis puntos de referencia para que sea

73
00:02:21,190 --> 00:02:23,920
exactamente cercana a mis “m” ejemplos de entrenamiento.

74
00:02:25,430 --> 00:02:26,600
Cuando tienes un ejemplo “x”, (que

75
00:02:26,920 --> 00:02:28,090
en este caso puede ser algo

76
00:02:28,230 --> 00:02:29,260
del conjunto de entrenamiento, en el conjunto de

77
00:02:29,570 --> 00:02:30,800
validación cruzada

78
00:02:31,490 --> 00:02:32,470
o en el de prueba)

79
00:02:33,320 --> 00:02:34,090
con este ejemplo “x”

80
00:02:34,320 --> 00:02:35,470
calcularemos las

81
00:02:35,750 --> 00:02:37,220
variables “f1”,

82
00:02:37,560 --> 00:02:39,180
“f2”, etc.,

83
00:02:39,580 --> 00:02:41,120
donde “L1” es igual a

84
00:02:41,490 --> 00:02:42,850
“x1”, etc., y estas

85
00:02:43,570 --> 00:02:46,080
me darán un vector de variables que

86
00:02:46,840 --> 00:02:49,540
denotaré con una “f”.

87
00:02:50,270 --> 00:02:52,090
Tomaré estas “f1”, “f2”, etc., y

88
00:02:52,290 --> 00:02:53,370
las agruparé en un

89
00:02:53,580 --> 00:02:55,330
vector de variables que incluya

90
00:02:56,330 --> 00:02:58,000
todas estas, hasta “fm”.

91
00:02:59,320 --> 00:03:01,080
Sólo por costumbre,

92
00:03:01,610 --> 00:03:02,870
si queremos podemos añadir una

93
00:03:02,990 --> 00:03:06,250
variables adicional “f0” que siempre es igual a 1.

94
00:03:06,450 --> 00:03:08,530
Esta juega un rol muy similar al que teníamos anteriormente para

95
00:03:09,480 --> 00:03:11,200
“x0”, que era nuestro interceptor.

96
00:03:13,200 --> 00:03:14,450
Entonces, por ejemplo, si

97
00:03:14,580 --> 00:03:16,550
tenemos un ejemplo de entrenamiento “x(i), y(i)”,

98
00:03:18,270 --> 00:03:19,300
la variable que calcularíamos

99
00:03:20,080 --> 00:03:21,330
para este ejemplo de entrenamiento serán

100
00:03:21,440 --> 00:03:23,440
las que siguen: mapearemos “x(i)”

101
00:03:23,640 --> 00:03:26,560
a “f1(i)”, que es

102
00:03:27,980 --> 00:03:29,670
la similaridad y la abreviaré SIM en vez

103
00:03:29,960 --> 00:03:31,980
de escribir toda

104
00:03:32,090 --> 00:03:33,380
la palabra

105
00:03:35,540 --> 00:03:35,540
similaridad, ¿sí?

106
00:03:37,050 --> 00:03:39,180
Y,“f2(i)” es igual a la

107
00:03:40,090 --> 00:03:42,780
similaridad entre “x(i)” y “L2”,

108
00:03:43,140 --> 00:03:45,050
etc. hasta

109
00:03:45,230 --> 00:03:48,370
“fm(i)” igual a la

110
00:03:49,600 --> 00:03:54,480
similaridad de “x(i)” y “L(m)”.

111
00:03:55,700 --> 00:03:58,700
En algún lugar a lo

112
00:03:59,160 --> 00:04:01,320
largo de esta lista; es decir, en el

113
00:04:01,480 --> 00:04:03,930
componente i,

114
00:04:04,230 --> 00:04:05,740
tendré un componente

115
00:04:06,150 --> 00:04:07,590
característico “f” subíndice

116
00:04:08,170 --> 00:04:09,930
“i(i)” que

117
00:04:10,050 --> 00:04:11,180
será la similaridad

118
00:04:13,080 --> 00:04:14,550
entre “x” y “L(i)”

119
00:04:15,680 --> 00:04:16,990
en la que “L(i)” es

120
00:04:17,190 --> 00:04:18,560
igual a “x(i)” .

121
00:04:19,140 --> 00:04:20,320
“fi(i)” será

122
00:04:20,410 --> 00:04:22,250
la similaridad entre x y “fi(i)” misma.

123
00:04:23,960 --> 00:04:25,380
Si estás utilizando un kernel Gaussiano,

124
00:04:25,620 --> 00:04:26,720
esto es “e” a la menos 0

125
00:04:27,170 --> 00:04:29,440
sobre 2 «sigma» cuadrada, por lo que será igual a 1, y es correcto.

126
00:04:29,790 --> 00:04:31,060
Una de mis variables para este ejemplo de

127
00:04:31,370 --> 00:04:32,940
entrenamiento será igual a 1.

128
00:04:34,290 --> 00:04:35,570
De manera similar a lo que tenemos arriba,

129
00:04:35,990 --> 00:04:36,940
puedo tomar todas estas

130
00:04:37,870 --> 00:04:39,910
variables “m” y agruparlas en un vector de variables.

131
00:04:40,340 --> 00:04:41,730
En vez de representar mi ejemplo

132
00:04:42,710 --> 00:04:44,200
utilizando “x(i)”, que es

133
00:04:44,430 --> 00:04:46,970
este vector R(n) más R(n)1 vector dimensional.

134
00:04:48,290 --> 00:04:49,590
Dependiendo de cómo hayas llamado

135
00:04:49,990 --> 00:04:51,120
tu interceptor, será R(n) o

136
00:04:52,070 --> 00:04:52,750
R(n) más 1.

137
00:04:53,440 --> 00:04:55,140
Ahora, podemos representar el

138
00:04:55,300 --> 00:04:56,700
conjunto de entrenamiento utilizando, en cambio,

139
00:04:56,980 --> 00:04:58,810
el vector de variables “f”. Escribiré

140
00:04:58,920 --> 00:05:01,240
“f” superíndice “i”  que tomará

141
00:05:01,400 --> 00:05:03,060
todas estas cosas y las

142
00:05:03,300 --> 00:05:06,010
agrupará en un vector.

143
00:05:06,540 --> 00:05:09,180
“f1(i)” hasta

144
00:05:09,430 --> 00:05:12,740
·fm(i)” y si quieres

145
00:05:13,030 --> 00:05:15,160
añadir este

146
00:05:15,420 --> 00:05:16,990
“f0(i)” donde

147
00:05:17,130 --> 00:05:19,370
“f0(i)” es igual a 1.

148
00:05:19,370 --> 00:05:20,970
Este vector de

149
00:05:21,300 --> 00:05:23,260
aquí me dará mi nuevo

150
00:05:23,430 --> 00:05:25,180
vector de variables con el que

151
00:05:25,480 --> 00:05:28,310
representaré mi ejemplo de entrenamiento.

152
00:05:29,040 --> 00:05:30,980
A continuación explicaré cómo utilizar

153
00:05:31,530 --> 00:05:33,160
una máquina de soporte vectorial con

154
00:05:33,400 --> 00:05:35,030
estos kernels y variables de similaridad.

155
00:05:35,720 --> 00:05:37,100
Si ya tienes un conjunto de

156
00:05:37,300 --> 00:05:39,040
parámetros teta aprendidos o si ya le has dado un valor a “x” y quieres hacer una predicción,

157
00:05:41,680 --> 00:05:42,850
lo que harás es calcular las variables “f”,

158
00:05:43,060 --> 00:05:44,170
que son ahora un vector

159
00:05:44,450 --> 00:05:46,920
“R(m)” más 1 vector dimensional.

160
00:05:49,040 --> 00:05:50,640
Tenemos “m” aquí

161
00:05:51,610 --> 00:05:53,190
porque tenemos “m” ejemplos de entrenamiento y, por lo tanto

162
00:05:53,570 --> 00:05:56,370
“m” puntos de referencia. Lo que

163
00:05:57,330 --> 00:05:58,310
haremos será predecir

164
00:05:58,600 --> 00:06:00,180
1 si teta transpuesta de “f”

165
00:06:00,780 --> 00:06:01,860
es mayor que o igual a 0.

166
00:06:02,230 --> 00:06:02,430
¿Correcto?

167
00:06:02,640 --> 00:06:03,770
Entonces teta traspuesta de “f” es

168
00:06:04,090 --> 00:06:07,200
igual a teta 0, “f0” más teta 1,

169
00:06:07,900 --> 00:06:08,990
“f1” más todo

170
00:06:09,120 --> 00:06:11,200
esto más teta “m”,

171
00:06:12,170 --> 00:06:13,900
“f(m)”. Ahora, mi

172
00:06:14,050 --> 00:06:15,720
parámetro vector teta

173
00:06:16,170 --> 00:06:17,730
también será un vector

174
00:06:17,990 --> 00:06:21,260
“m” más 1 vector dimensional.

175
00:06:21,780 --> 00:06:23,100
Tenemos “m” aquí porque

176
00:06:23,260 --> 00:06:25,030
el número de puntos de referencia es

177
00:06:25,450 --> 00:06:26,600
igual al tamaño del conjunto de entrenamiento.

178
00:06:26,910 --> 00:06:28,190
De manera que “m” es el tamaño del conjunto de entrenamiento y

179
00:06:29,100 --> 00:06:31,950
ahora, el parámetro vectorial teta será “m” más 1 dimensional.

180
00:06:32,990 --> 00:06:33,990
Así es como se hace una predicción si

181
00:06:34,360 --> 00:06:36,870
ya tienes un valor fijo para el parámetro teta.

182
00:06:37,840 --> 00:06:39,160
¿Cómo obtienes el parámetro teta?

183
00:06:39,680 --> 00:06:40,650
Lo puedes hacer utilizando el

184
00:06:40,920 --> 00:06:43,040
algoritmo de aprendizaje de las SVM. Específicamente,

185
00:06:43,850 --> 00:06:46,460
lo que puedes hacer es resolver este problema de minimización

186
00:06:46,690 --> 00:06:48,170
minimizando el parámetro teta de C por

187
00:06:48,540 --> 00:06:51,630
esta función de costo que teníamos antes.

188
00:06:52,430 --> 00:06:54,770
Ahora, en vez de

189
00:06:55,040 --> 00:06:56,650
hacer predicciones

190
00:06:56,970 --> 00:06:59,300
utilizando teta transpuesta de

191
00:07:00,020 --> 00:07:01,410
“x(i)” utilizando nuestras

192
00:07:01,720 --> 00:07:03,320
variables originales, “x(i)” tomamos

193
00:07:03,520 --> 00:07:04,840
las variables “x(i)” y las

194
00:07:05,090 --> 00:07:06,260
remplazamos por nuevas variables

195
00:07:07,270 --> 00:07:09,080
para utilizar teta transpuesta de

196
00:07:09,380 --> 00:07:10,840
“f(i)” para hacer una

197
00:07:11,130 --> 00:07:12,480
predicción en los ejemplos de

198
00:07:12,860 --> 00:07:13,860
entrenamiento “i” y vemos

199
00:07:14,230 --> 00:07:16,580
que en ambos casos podemos

200
00:07:16,700 --> 00:07:18,270
obtener los parámetros de la

201
00:07:18,760 --> 00:07:22,130
máquina de soporte vectorial resolviendo este problema de minimización.

202
00:07:23,240 --> 00:07:24,640
Un último detalle es

203
00:07:24,870 --> 00:07:26,880
que para este problema

204
00:07:27,510 --> 00:07:29,580
de optimización tenemos

205
00:07:30,570 --> 00:07:32,300
“n” igual a “m” variables;

206
00:07:32,860 --> 00:07:33,650
es decir el número efectivo de

207
00:07:34,520 --> 00:07:36,010
variables que tenemos,

208
00:07:37,100 --> 00:07:38,240
que es

209
00:07:38,410 --> 00:07:39,390
la dimensión

210
00:07:39,670 --> 00:07:41,020
de “f”. Entonces, “n”

211
00:07:41,730 --> 00:07:42,690
será igual

212
00:07:42,900 --> 00:07:44,470
a “m”. Si gustas, puedes

213
00:07:44,610 --> 00:07:45,530
pensar en esto como una

214
00:07:46,340 --> 00:07:47,280
suma. Realmente es una

215
00:07:47,590 --> 00:07:48,680
suma de “J” igual a 1

216
00:07:49,490 --> 00:07:50,390
a “m”. Una manera de pensar en

217
00:07:50,470 --> 00:07:51,500
“n” es verla

218
00:07:51,620 --> 00:07:53,250
como igual a

219
00:07:53,550 --> 00:07:55,060
“m”, porque si

220
00:07:55,570 --> 00:07:57,320
“f” no es una variable nueva, entonces

221
00:07:57,970 --> 00:07:59,650
tendremos “m” más 1

222
00:08:00,120 --> 00:08:02,920
variables donde más 1 viene del interceptor.

223
00:08:05,090 --> 00:08:06,760
Aquí, seguimos haciendo la suma

224
00:08:06,990 --> 00:08:08,110
de “J” igual a 1 a “n”, porque, al igual

225
00:08:08,440 --> 00:08:10,070
que en nuestros videos anteriores

226
00:08:10,380 --> 00:08:11,700
de la regularización,

227
00:08:12,580 --> 00:08:14,110
aún no hemos regularizado

228
00:08:14,180 --> 00:08:15,650
el parámetro teta cero, que

229
00:08:15,780 --> 00:08:16,560
es la razón de la suma de “J”

230
00:08:16,740 --> 00:08:17,930
igual a 1 a “m”

231
00:08:18,880 --> 00:08:19,840
en vez de “j” igual a cero

232
00:08:20,000 --> 00:08:22,200
a “m”.  Esto que expliqué fue

233
00:08:22,580 --> 00:08:23,760
la el algoritmo de aprendizaje de la máquina de soporte vectorial.

234
00:08:24,660 --> 00:08:26,260
Hay un detalle matemático

235
00:08:27,160 --> 00:08:28,310
adicional que

236
00:08:28,440 --> 00:08:29,840
debería mencionar:

237
00:08:29,930 --> 00:08:30,780
La manera en la que se implementa

238
00:08:31,310 --> 00:08:33,020
una máquina de soporte vectorial

239
00:08:33,320 --> 00:08:34,750
este último término se desarrolla de manera distinta.

240
00:08:35,680 --> 00:08:36,730
No necesitas saber

241
00:08:36,770 --> 00:08:38,080
este último detalle para utilizar

242
00:08:38,190 --> 00:08:39,190
máquinas de soporte vectorial.

243
00:08:39,700 --> 00:08:41,330
De hecho, las ecuaciones que están

244
00:08:41,450 --> 00:08:42,500
escritas aquí deberían darte

245
00:08:42,620 --> 00:08:45,160
todo el conocimiento

246
00:08:45,310 --> 00:08:46,190
que necesitas.

247
00:08:46,450 --> 00:08:48,450
Pero en la manera en la que se implementa una máquina de soporte vectorial,

248
00:08:48,570 --> 00:08:50,960
otra manera de escribir el término, la suma de “j”

249
00:08:53,110 --> 00:08:54,780
de teta “j” cuadrada es como

250
00:08:55,580 --> 00:08:57,660
teta transpuesta

251
00:08:58,500 --> 00:08:59,530
de teta si ignoramos

252
00:09:00,120 --> 00:09:02,730
el parámetro teta 0.

253
00:09:03,570 --> 00:09:05,640
Así que tenemos teta 1 hasta

254
00:09:05,800 --> 00:09:10,090
teta “m” ignorando  teta 0.

255
00:09:11,130 --> 00:09:13,790
La suma de

256
00:09:14,510 --> 00:09:15,900
“J” de teta “j” cuadrada también se puede

257
00:09:16,040 --> 00:09:18,870
escribir como teta transpuesta de teta.

258
00:09:19,930 --> 00:09:21,520
Lo que hacen la mayoría de las implementaciones

259
00:09:21,730 --> 00:09:23,380
de máquinas de soporte vectorial es

260
00:09:23,720 --> 00:09:25,520
remplazar esta teta transpuesta de teta

261
00:09:26,280 --> 00:09:28,270
con teta transpuesta por

262
00:09:28,590 --> 00:09:30,140
una matriz, que

263
00:09:30,820 --> 00:09:33,930
depende del número de kernel, por teta. Esto resulta en una medición ligeramente distinta.

264
00:09:34,160 --> 00:09:35,500
Esto nos da una métrica de la distancia un poco distinta.

265
00:09:36,140 --> 00:09:37,770
En vez de

266
00:09:38,070 --> 00:09:40,050
minimizar la norma

267
00:09:41,320 --> 00:09:43,250
de teta cuadrada o

268
00:09:43,790 --> 00:09:45,990
minimizamos algo muy similar,

269
00:09:46,140 --> 00:09:47,610
que es como la versión a escala

270
00:09:47,770 --> 00:09:50,150
del parámetro vector teta que depende del kernel.

271
00:09:50,950 --> 00:09:52,440
Este es un detalle matemático que

272
00:09:53,210 --> 00:09:54,360
le permite a la máquina

273
00:09:54,650 --> 00:09:56,350
de soporte vectorial ejecutarse con más eficiencia.

274
00:09:58,300 --> 00:09:59,410
La razón por la que la máquina de soporte vectorial hace esto es porque

275
00:09:59,700 --> 00:10:01,500
es esta modificación le

276
00:10:02,020 --> 00:10:03,250
permite

277
00:10:03,300 --> 00:10:05,740
escalarla a conjuntos de entrenamiento más grandes.

278
00:10:06,370 --> 00:10:07,800
Por ejemplo, si tienes

279
00:10:07,970 --> 00:10:11,530
un conjunto de entrenamiento con 10,000 ejemplos de entrenamiento,

280
00:10:12,590 --> 00:10:13,560
entonces, deberemos terminar con

281
00:10:13,950 --> 00:10:15,750
10,000 puntos de referencia, por lo que

282
00:10:16,780 --> 00:10:18,060
teta se vuelve 10,000 dimensional.

283
00:10:18,490 --> 00:10:20,450
Tal vez funcione. Pero cuando “m”

284
00:10:20,450 --> 00:10:21,710
se hace muy

285
00:10:22,470 --> 00:10:24,020
muy grande para resolver

286
00:10:24,150 --> 00:10:25,480
todos estos parámetros, quizá

287
00:10:25,590 --> 00:10:26,590
50,000 o 100,000, resolverlos

288
00:10:26,880 --> 00:10:28,170
puede

289
00:10:28,340 --> 00:10:29,660
resultar

290
00:10:29,890 --> 00:10:31,240
caro para el software de

291
00:10:31,420 --> 00:10:33,690
optimización de la máquina de vector de soporte que, por lo tanto,

292
00:10:33,870 --> 00:10:35,750
resolverá el problema que dibujé aquí abajo.

293
00:10:36,490 --> 00:10:37,570
Como detalle matemático,

294
00:10:37,860 --> 00:10:39,580
que realmente no es necesario que conozcas,

295
00:10:41,000 --> 00:10:43,070
de hecho modifica

296
00:10:43,350 --> 00:10:44,380
un poco el último término

297
00:10:44,500 --> 00:10:45,940
para optimizar algo ligeramente distinto

298
00:10:46,080 --> 00:10:48,560
que sólo minimizar la norma cuadrada de teta cuadrada de teta.

299
00:10:49,370 --> 00:10:50,600
Pero si gustas, puedes

300
00:10:51,080 --> 00:10:52,450
pesar esto como

301
00:10:52,710 --> 00:10:54,880
un detalle de implementación

302
00:10:55,340 --> 00:10:56,750
que cambia el objetivo un poco

303
00:10:56,880 --> 00:10:58,260
pero se realiza por

304
00:10:58,930 --> 00:11:01,590
eficiencia computacional.

305
00:11:02,260 --> 00:11:04,390
Usualmente no tienes que preocuparte por esto.

306
00:11:07,640 --> 00:11:09,460
Por cierto, en caso de que te estés preguntando

307
00:11:09,560 --> 00:11:10,730
por qué no aplicamos la

308
00:11:11,100 --> 00:11:12,210
idea del kernel a otras

309
00:11:12,570 --> 00:11:13,690
regresiones

310
00:11:14,040 --> 00:11:15,450
logísticas resulta que de hecho,

311
00:11:15,670 --> 00:11:16,770
si quieres, puedes aplicar

312
00:11:16,900 --> 00:11:18,120
el kernel y

313
00:11:18,550 --> 00:11:19,850
definir la fuente de las variables

314
00:11:19,990 --> 00:11:22,920
utilizando puntos de referencia para la regresión logística.

315
00:11:23,880 --> 00:11:25,860
Pero los trucos computacionales

316
00:11:26,440 --> 00:11:28,110
que aplican para la máquina de soporte vectorial no

317
00:11:28,430 --> 00:11:30,700
se generalizarán bien en otros algoritmos como la regresión logística.

318
00:11:31,310 --> 00:11:33,110
Utilizar kernels con

319
00:11:33,260 --> 00:11:34,390
la regresión logística puede ser lento

320
00:11:34,580 --> 00:11:36,330
pero debido a los

321
00:11:36,440 --> 00:11:37,940
trucos computacionales que

322
00:11:38,150 --> 00:11:39,490
modifican este término

323
00:11:39,900 --> 00:11:41,130
y los detalles de cómo se implementa el software de la

324
00:11:41,320 --> 00:11:43,140
máquina de soporte vectorial,

325
00:11:43,240 --> 00:11:44,990
estas máquinas y los

326
00:11:45,300 --> 00:11:47,090
kernels combinan muy bien juntos, mientras

327
00:11:47,930 --> 00:11:49,450
que la regresión logística y los kernels no

328
00:11:50,250 --> 00:11:51,990
son tan compatibles, se ejecutarían muy lentamente y

329
00:11:52,890 --> 00:11:53,670
no serán capaces de tomar

330
00:11:53,750 --> 00:11:55,420
las ventajas de las técnicas de optimización

331
00:11:56,040 --> 00:11:57,360
avanzadas que se han

332
00:11:57,530 --> 00:11:58,530
descubierto para el caso

333
00:11:59,140 --> 00:12:00,950
particular de ejecutar una máquina de soporte vectorial con un kernel.

334
00:12:01,540 --> 00:12:03,340
Todo esto se refiere a

335
00:12:03,710 --> 00:12:04,850
cómo puedes implementar software para

336
00:12:05,230 --> 00:12:06,900
minimizar la función de costos.

337
00:12:07,870 --> 00:12:08,940
Hablaré más al respecto en

338
00:12:09,040 --> 00:12:09,950
el siguiente video aunque realmente no necesitas

339
00:12:10,150 --> 00:12:11,530
tener conocimiento de cómo construir

340
00:12:12,200 --> 00:12:13,520
software para minimizar esta función de

341
00:12:13,670 --> 00:12:14,890
costos, porque puedes encontrar software

342
00:12:15,170 --> 00:12:17,560
ya desarrollado que resulta excelente para esto.

343
00:12:18,670 --> 00:12:19,890
Yo no recomendaría escribir

344
00:12:20,140 --> 00:12:21,340
código para invertir

345
00:12:21,850 --> 00:12:22,960
la matriz o para calcular la

346
00:12:23,150 --> 00:12:24,490
raíz cuadrara ni

347
00:12:24,660 --> 00:12:26,420
para minimizar

348
00:12:26,560 --> 00:12:27,750
la función de costo por ti mismo.

349
00:12:28,240 --> 00:12:29,610
En vez de ello, recomendaría utilizar

350
00:12:29,780 --> 00:12:31,490
paquetes de software ya desarrollados

351
00:12:31,740 --> 00:12:33,240
por otros que incluyen

352
00:12:33,540 --> 00:12:35,140
estos trucos de optimización

353
00:12:35,790 --> 00:12:37,720
numérica para que

354
00:12:39,540 --> 00:12:41,770
tú no te preocupes por ello.

355
00:12:41,950 --> 00:12:42,920
Otra cosa que vale la pena saber

356
00:12:43,180 --> 00:12:45,200
cuando aplicas una

357
00:12:45,350 --> 00:12:46,400
máquina de soporte vectorial es

358
00:12:46,640 --> 00:12:47,730
cómo elegir los parámetros para la

359
00:12:47,820 --> 00:12:50,220
máquina de soporte vectorial.

360
00:12:51,520 --> 00:12:52,300
Lo último que quiero hacer en este video es

361
00:12:52,400 --> 00:12:53,290
decir unas palabras acerca

362
00:12:53,450 --> 00:12:54,680
de la compensación entre el sesgo

363
00:12:54,840 --> 00:12:57,070
y la varianza cuando utilizamos una máquina de soporte vectorial.

364
00:12:57,900 --> 00:12:59,230
Cuando utilizamos una SVM, una de

365
00:12:59,390 --> 00:13:00,670
las cosas que debes elegir es el

366
00:13:00,960 --> 00:13:03,850
parámetro “C”, que teníamos en el

367
00:13:04,090 --> 00:13:05,880
objetivo de optimización. Recordemos que

368
00:13:05,980 --> 00:13:07,690
“C” jugaba un papel similar

369
00:13:07,770 --> 00:13:09,800
a 1 sobre

370
00:13:10,050 --> 00:13:11,750
«lambda», donde «lambda» era el parámetro de regularización

371
00:13:12,520 --> 00:13:13,970
para la regresión logística.

372
00:13:15,360 --> 00:13:16,760
Si tienes un gran valor de

373
00:13:16,930 --> 00:13:18,760
“C”, corresponde a lo que teníamos en la

374
00:13:19,520 --> 00:13:20,560
regresión logística: un valor pequeño

375
00:13:21,270 --> 00:13:22,260
de «lambda» quiere decir que no se está

376
00:13:22,670 --> 00:13:25,080
usando mucha regularización.

377
00:13:25,980 --> 00:13:26,960
Haciendo esto tenderemos

378
00:13:27,050 --> 00:13:29,330
a obtener una hipótesis con un sesgo bajo y una varianza más alta, mientras

379
00:13:30,570 --> 00:13:31,420
que si utilizamos un valor

380
00:13:31,630 --> 00:13:33,050
de “C” más bajo y estamos

381
00:13:33,240 --> 00:13:34,510
utilizando la regresión

382
00:13:34,660 --> 00:13:36,450
logística con un valor alto de «lambda»,

383
00:13:36,620 --> 00:13:38,090
obtendremos una

384
00:13:38,690 --> 00:13:40,180
hipótesis con un sesgo más alto y una

385
00:13:40,470 --> 00:13:41,760
varianza más baja.

386
00:13:42,580 --> 00:13:44,520
La hipótesis

387
00:13:45,000 --> 00:13:46,870
con la “C” mayor tendrá una

388
00:13:47,450 --> 00:13:48,380
varianza más alta y será más

389
00:13:48,580 --> 00:13:50,290
propensa al sobreajuste, mientras que la

390
00:13:50,450 --> 00:13:52,820
hipótesis con la “C” menor tendrá un sesgo mayor y será

391
00:13:52,910 --> 00:13:54,900
más propensa al subajuste.

392
00:13:56,710 --> 00:13:59,870
Este parámetro “C” es uno de los parámetros que debemos elegir.

393
00:14:00,210 --> 00:14:01,280
El otro parámetro es

394
00:14:02,280 --> 00:14:04,580
«sigma» cuadrada que apareció en el kernel Gaussiano.

395
00:14:05,760 --> 00:14:07,080
Si en el kernel Gaussiano

396
00:14:07,750 --> 00:14:09,370
tenemos un valor alto para «sigma» cuadrada,

397
00:14:09,640 --> 00:14:11,350
en la variable de similaridad,

398
00:14:11,530 --> 00:14:12,710
que es “e” a la

399
00:14:13,390 --> 00:14:14,710
menos “x” menos punto de referencia

400
00:14:16,280 --> 00:14:17,950
cuadrado sobre 2 «sigma» cuadrada,

401
00:14:20,130 --> 00:14:21,290
y si tengo sólo

402
00:14:21,480 --> 00:14:23,330
una variable “x1”

403
00:14:23,570 --> 00:14:25,390
y un punto de referencia

404
00:14:25,490 --> 00:14:27,710
en esa ubicación y si el valor de «sigma»

405
00:14:27,960 --> 00:14:29,230
cuadrado es alto,

406
00:14:29,480 --> 00:14:30,600
el kernel Gaussiano tenderá a

407
00:14:30,690 --> 00:14:32,940
decrecer relativamente lento

408
00:14:33,960 --> 00:14:34,740
Esta de aquí sería mi variable

409
00:14:35,210 --> 00:14:36,690
“f(i)”, y representa una

410
00:14:36,880 --> 00:14:38,970
variable más suave, que varía más

411
00:14:39,060 --> 00:14:40,640
suavemente y que nos

412
00:14:40,760 --> 00:14:42,750
dará hipótesis con

413
00:14:43,030 --> 00:14:44,170
altas oscilaciones y varianzas bajas. Debido

414
00:14:44,550 --> 00:14:46,000
a que el kernel Gaussiano decrece suavemente,

415
00:14:46,840 --> 00:14:48,240
tiende a generar hipótesis que varían

416
00:14:48,520 --> 00:14:50,060
lentamente o suavemente

417
00:14:50,130 --> 00:14:51,860
a medida que cambiamos

418
00:14:52,050 --> 00:14:53,680
el valor de entrada “x”. En contraste, si «sigma»

419
00:14:54,030 --> 00:14:55,330
cuadrada tenía un valor bajo y si mi

420
00:14:55,660 --> 00:14:57,430
punto de referencia dado o mi variable

421
00:14:57,540 --> 00:14:58,830
“f1” mi kernel Gaussiano,

422
00:14:58,960 --> 00:15:01,440
o mi variable de similaridad

423
00:15:01,820 --> 00:15:04,630
tendrán una variación más abrupta.

424
00:15:05,310 --> 00:15:07,520
En ambos casos

425
00:15:07,580 --> 00:15:08,550
elegiría uno. Si «sigma»

426
00:15:08,870 --> 00:15:11,730
cuadrada es pequeña, entonces mis variables variarán menos suavemente así

427
00:15:12,190 --> 00:15:13,740
que tendremos pendientes más

428
00:15:14,250 --> 00:15:15,300
altas o derivadas más altas.

429
00:15:16,020 --> 00:15:17,170
Utilizando esto terminarás

430
00:15:17,330 --> 00:15:19,620
por ajustar una hipótesis de sesgo

431
00:15:19,840 --> 00:15:21,870
bajo y podrás tener una varianza más alta.

432
00:15:23,030 --> 00:15:24,460
Si observas esta curva

433
00:15:24,680 --> 00:15:26,240
de esta semana, podrás

434
00:15:26,450 --> 00:15:27,230
jugar con algunas de estas ideas

435
00:15:27,330 --> 00:15:29,480
y ver sus efectos por ti mismo.

436
00:15:31,590 --> 00:15:34,430
Eso es todo acerca de la máquina de soporte vectorial con algoritmos de kernel.

437
00:15:35,320 --> 00:15:36,450
Espero que esta discusión del sesgo y la

438
00:15:37,090 --> 00:15:39,170
varianza te hayan dado un mejor

439
00:15:39,310 --> 00:15:40,380
sentido de cómo puedes esperar que

440
00:15:40,460 --> 00:15:42,600
se comporte un algoritmo.