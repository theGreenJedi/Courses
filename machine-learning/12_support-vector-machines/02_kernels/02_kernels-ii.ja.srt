1
00:00:00,530 --> 00:00:01,550
前回のビデオでは

2
00:00:01,950 --> 00:00:03,230
カーネルというアイデアと、

3
00:00:03,710 --> 00:00:04,590
サポートベクタマシンにおいて、それを使う事で

4
00:00:04,860 --> 00:00:07,900
どうやって新しいフィーチャーを定義するかの議論を始めた。

5
00:00:08,100 --> 00:00:08,910
このビデオでは、いくつかの

6
00:00:09,230 --> 00:00:10,670
欠けている詳細を埋めるのと、

7
00:00:11,020 --> 00:00:12,070
これらのアイデアを実際にどう使うのかについて

8
00:00:12,270 --> 00:00:14,100
幾つか助言もしたいと思う。

9
00:00:14,650 --> 00:00:15,850
例えば、バイアス-バリアンス間のトレードオフを

10
00:00:16,340 --> 00:00:20,120
サポートベクタマシンでどう取るのか、とか。

11
00:00:22,690 --> 00:00:23,680
前回のビデオでは

12
00:00:24,000 --> 00:00:25,970
いくつかのランドマーク、l(1)、l(2)、l(3)を

13
00:00:26,660 --> 00:00:28,890
選ぶプロセスについて言及した。

14
00:00:29,150 --> 00:00:30,220
そしてそれで類似度関数を

15
00:00:30,300 --> 00:00:31,900
定義することが出来た。

16
00:00:32,200 --> 00:00:33,500
それはカーネル関数とも呼ばれるのだった。

17
00:00:33,690 --> 00:00:34,830
そしてこの例の場合、また

18
00:00:35,070 --> 00:00:37,410
この類似度関数はガウスカーネルとも呼ばれるのだった。

19
00:00:38,610 --> 00:00:40,370
そしてそれによって、

20
00:00:40,660 --> 00:00:42,070
この形の仮説関数を定義出来るのだった。

21
00:00:43,180 --> 00:00:44,880
だが、これらのランドマークをどこから得たらいいのだろうか？

22
00:00:45,150 --> 00:00:45,670
どこから l(1)、l(2)、l(3)を得る事が出来るだろうか？

23
00:00:45,690 --> 00:00:49,080
そしてまた、複雑な学習問題においては、

24
00:00:49,610 --> 00:00:50,830
手動で選ぶたった三つだけよりも、

25
00:00:50,920 --> 00:00:53,060
もっとたくさんのランドマークが欲しい事もあるだろう。

26
00:00:55,160 --> 00:00:56,450
さて、実際には、こんな感じに

27
00:00:56,580 --> 00:00:57,730
ランドマークは選ばれる。

28
00:00:57,830 --> 00:00:59,910
機械学習の問題が

29
00:01:00,150 --> 00:01:01,110
与えられた時に、

30
00:01:01,370 --> 00:01:02,230
ある陽性のデータセットと

31
00:01:02,710 --> 00:01:04,460
陰性のデータセットがある訳だが、その時に、これがアイデアだ。

32
00:01:05,310 --> 00:01:06,270
我らは手本を取って、

33
00:01:06,630 --> 00:01:08,200
そして各手本に対し

34
00:01:08,470 --> 00:01:09,780
我らは単にそれを

35
00:01:10,490 --> 00:01:11,430
ランドマークと呼ぶ事に、

36
00:01:11,980 --> 00:01:13,270
つまりランドマークを

37
00:01:13,440 --> 00:01:14,850
そのトレーニング手本と

38
00:01:15,490 --> 00:01:17,600
全く同じ場所に置く事にする。

39
00:01:18,930 --> 00:01:20,360
だからもしあるトレーニング手本を

40
00:01:20,680 --> 00:01:21,880
取ったら、それがx1だったとすると、

41
00:01:22,120 --> 00:01:23,460
その時に、私は最初のランドマークを

42
00:01:23,670 --> 00:01:24,550
これと全く同じ場所に

43
00:01:25,100 --> 00:01:26,470
最初のトレーニング手本と全く同じ場所に

44
00:01:27,250 --> 00:01:28,170
選ぶ。

45
00:01:29,260 --> 00:01:30,180
そして別のトレーニング手本x2が

46
00:01:30,470 --> 00:01:32,340
あったとすると、

47
00:01:32,500 --> 00:01:33,980
二番目のランドマークには、

48
00:01:35,060 --> 00:01:37,300
この二番目の手本の場所を選ぶ。

49
00:01:38,480 --> 00:01:39,320
この右側の図では、

50
00:01:39,480 --> 00:01:40,480
私は赤と青の点を

51
00:01:40,820 --> 00:01:41,930
イラストで示す目的の為に用いたが、

52
00:01:42,420 --> 00:01:44,320
この図の色は、この右側の図の

53
00:01:44,370 --> 00:01:46,030
点の色は、重要では無い。

54
00:01:47,120 --> 00:01:47,930
だがこの手法を用いる事で

55
00:01:48,110 --> 00:01:49,660
結局の所、

56
00:01:49,790 --> 00:01:51,450
結局の所、m個のランドマーク

57
00:01:52,160 --> 00:01:53,690
l(1)、l(2)、、、、と

58
00:01:54,950 --> 00:01:56,320
l(m)まで、

59
00:01:56,380 --> 00:01:58,180
もし私がトレーニング手本をm個持っていて、

60
00:01:58,420 --> 00:02:00,500
各位置につき一つのランドマーク、

61
00:02:00,810 --> 00:02:02,680
各トレーニング手本の位置につき

62
00:02:02,860 --> 00:02:04,810
一つのランドマークを持つ事になる。

63
00:02:04,950 --> 00:02:05,920
そしてこれはナイスだ。何故なら、それはようするに

64
00:02:06,120 --> 00:02:07,630
私のフィーチャー達は、基本的には

65
00:02:07,700 --> 00:02:09,300
トレーニングセットに見られる物の一つに

66
00:02:09,380 --> 00:02:10,800
サンプルがどれだけ近いのか、を

67
00:02:10,970 --> 00:02:13,150
測っているという事だからだ。

68
00:02:13,440 --> 00:02:14,180
このアウトラインをもうちょっと

69
00:02:14,350 --> 00:02:16,270
具体的に書くと、

70
00:02:16,470 --> 00:02:17,870
m個のトレーニング手本が与えられた時に、

71
00:02:18,050 --> 00:02:19,100
ランドマーク達の場所を

72
00:02:19,310 --> 00:02:20,430
トレーニング手本の場所達と

73
00:02:21,190 --> 00:02:23,920
完全に同じになるように選ぶ。

74
00:02:25,430 --> 00:02:26,600
手本xが与えられた時に、

75
00:02:26,920 --> 00:02:28,090
この手本xは

76
00:02:28,230 --> 00:02:29,260
トレーニングセットの場合もあり得るし、

77
00:02:29,570 --> 00:02:30,800
クロスバリデーションセットの場合もあり得るし、

78
00:02:31,490 --> 00:02:32,470
テストセットの場合もあり得るが、

79
00:02:33,320 --> 00:02:34,090
ある手本xが与えられた時に、

80
00:02:34,320 --> 00:02:35,470
我らはこれらのフィーチャー、

81
00:02:35,750 --> 00:02:37,220
f1, f2, などなどを

82
00:02:37,560 --> 00:02:39,180
計算する事になる。

83
00:02:39,580 --> 00:02:41,120
ここでl(1)は実際には

84
00:02:41,490 --> 00:02:42,850
x1に等しい、などなど。

85
00:02:43,570 --> 00:02:46,080
そしてこの結果が、フィーチャーベクトルを与える。

86
00:02:46,840 --> 00:02:49,540
フィーチャーベクトルをfと書く事にしよう。

87
00:02:50,270 --> 00:02:52,090
これらのf1, f2, ...を持ってきて、

88
00:02:52,290 --> 00:02:53,370
これを単にフィーチャーベクトルとして

89
00:02:53,580 --> 00:02:55,330
グループ化する。

90
00:02:56,330 --> 00:02:58,000
これらをf(m)まで持ってくる。

91
00:02:59,320 --> 00:03:01,080
そして、慣例により、

92
00:03:01,610 --> 00:03:02,870
足したければ追加のフィーチャー、f(0)を、

93
00:03:02,990 --> 00:03:06,250
それはいつもイコール1だが、それを足しても良い。

94
00:03:06,450 --> 00:03:08,530
これは以前我らが持っていた物と、似たような働きをする。

95
00:03:09,480 --> 00:03:11,200
x0は、切片項だった。

96
00:03:13,200 --> 00:03:14,450
例えば、トレーニング手本x(i), y(i)が

97
00:03:14,580 --> 00:03:16,550
あったとすると、

98
00:03:18,270 --> 00:03:19,300
このトレーニング手本に対して

99
00:03:20,080 --> 00:03:21,330
我らが計算する事になるフィーチャーは、

100
00:03:21,440 --> 00:03:23,440
以下のようになる： 所与のx(i)に対し、

101
00:03:23,640 --> 00:03:26,560
それを以下のようにマップする、まずf1(i)、

102
00:03:27,980 --> 00:03:29,670
これはsimilarity（類似度）だが、

103
00:03:29,960 --> 00:03:31,980
全部のスペル、similarityと書く代わりに

104
00:03:32,090 --> 00:03:33,380
SIMと省略して

105
00:03:35,540 --> 00:03:35,540
書く事にする。

106
00:03:37,050 --> 00:03:39,180
そしてf2(i)はイコール、

107
00:03:40,090 --> 00:03:42,780
x(i)とl(2)との類似度（similarity）だ。

108
00:03:43,140 --> 00:03:45,050
以下同様に

109
00:03:45,230 --> 00:03:48,370
降りていって、fm(i)=

110
00:03:49,600 --> 00:03:54,480
x(i)とl(m)のsimilarity。

111
00:03:55,700 --> 00:03:58,700
そして間のどこかで、

112
00:03:59,160 --> 00:04:01,320
このリストの途中のどこかで、

113
00:04:01,480 --> 00:04:03,930
i番目の要素があり、

114
00:04:04,230 --> 00:04:05,740
fの下付き添字iの(i)という

115
00:04:06,150 --> 00:04:07,590
フィーチャーの要素がある、

116
00:04:08,170 --> 00:04:09,930
それはxとl(i)との

117
00:04:10,050 --> 00:04:11,180
類似度

118
00:04:13,080 --> 00:04:14,550
となる。

119
00:04:15,680 --> 00:04:16,990
ここでl(i)はイコールx(i)だから、

120
00:04:17,190 --> 00:04:18,560
つまり、

121
00:04:19,140 --> 00:04:20,320
fi(i)は単に

122
00:04:20,410 --> 00:04:22,250
x同士の類似度、つまり自分自身との類似度となる。

123
00:04:23,960 --> 00:04:25,380
もしガウスカーネルを使ってるなら、

124
00:04:25,620 --> 00:04:26,720
これは実際に、eの指数乗の

125
00:04:27,170 --> 00:04:29,440
-0 割ることの 2シグマ二乗 となるから、これは1となる。これは問題無い。

126
00:04:29,790 --> 00:04:31,060
つまり、このトレーニング手本の

127
00:04:31,370 --> 00:04:32,940
フィーチャーの一つは、イコール1となる。

128
00:04:34,290 --> 00:04:35,570
そして上でやったのと同様にして、

129
00:04:35,990 --> 00:04:36,940
これらのm個のフィーチャーに対して、

130
00:04:37,870 --> 00:04:39,910
フィーチャーベクトルにグループ化出来る。

131
00:04:40,340 --> 00:04:41,730
つまり、手本をx(i)を用いて

132
00:04:42,710 --> 00:04:44,200
表現する代わりに、ここでx(i)は

133
00:04:44,430 --> 00:04:46,970
R(n)かR(n+1)次元のベクトル、

134
00:04:48,290 --> 00:04:49,590
これは切片項を数えるかどうかによって

135
00:04:49,990 --> 00:04:51,120
R(n)となるか

136
00:04:52,070 --> 00:04:52,750
R(n+1)となる。

137
00:04:53,440 --> 00:04:55,140
ここで我らは、トレーニング手本を

138
00:04:55,300 --> 00:04:56,700
代わりにこの新しいフィーチャーベクトルfを

139
00:04:56,980 --> 00:04:58,810
表現とする事が出来る。

140
00:04:58,920 --> 00:05:01,240
私はこれをfの上付き添字iと書く事にする、

141
00:05:01,400 --> 00:05:03,060
それはこれら全てを

142
00:05:03,300 --> 00:05:06,010
積み上げてベクトルにした物だ。

143
00:05:06,540 --> 00:05:09,180
つまり、このf1(i)から

144
00:05:09,430 --> 00:05:12,740
fm(i)まで。そしてもしお望みなら、

145
00:05:13,030 --> 00:05:15,160
そして普通はそうするか、f0(i)を足しても良い。

146
00:05:15,420 --> 00:05:16,990
ここでf0(i)は

147
00:05:17,130 --> 00:05:19,370
イコール1。

148
00:05:19,370 --> 00:05:20,970
そしてこのここのベクトル、

149
00:05:21,300 --> 00:05:23,260
これは新しいフィーチャーベクトルを

150
00:05:23,430 --> 00:05:25,180
与えてくれる、それを用いて

151
00:05:25,480 --> 00:05:28,310
トレーニング手本の表現とする事が出来る。

152
00:05:29,040 --> 00:05:30,980
つまり、これらのカーネルと

153
00:05:31,530 --> 00:05:33,160
similarity(類似度)関数が与えられた時に、

154
00:05:33,400 --> 00:05:35,030
これがサポートベクターマシンの使い方だ。

155
00:05:35,720 --> 00:05:37,100
もしあなたが既に、学習したパラメータセットのシータを

156
00:05:37,300 --> 00:05:39,040
持っていたとして、所与のxの値に対して予測を行いたいとすると、

157
00:05:41,680 --> 00:05:42,850
やるべき事は、フィーチャーfを計算して、

158
00:05:43,060 --> 00:05:44,170
それはここではR(m+1)次元の

159
00:05:44,450 --> 00:05:46,920
フィーチャーベクトルとなるが、

160
00:05:49,040 --> 00:05:50,640
ここでmとなるのは、

161
00:05:51,610 --> 00:05:53,190
我らがトレーニング手本としてm個を持ち、

162
00:05:53,570 --> 00:05:56,370
つまりはランドマークもm個だからだが、

163
00:05:57,330 --> 00:05:58,310
そして我らがやる事は、

164
00:05:58,600 --> 00:06:00,180
シータ転置fが0以上なら

165
00:06:00,780 --> 00:06:01,860
必ず、

166
00:06:02,230 --> 00:06:02,430
でしょ？

167
00:06:02,640 --> 00:06:03,770
つまり、シータ転置f は、もちろん、

168
00:06:04,090 --> 00:06:07,200
単なる シータ0 f0 + シータ1 f1

169
00:06:07,900 --> 00:06:08,990
+ ....

170
00:06:09,120 --> 00:06:11,200
+シータm f(m) となるが、

171
00:06:12,170 --> 00:06:13,900
つまりパラメータベクトルシータも

172
00:06:14,050 --> 00:06:15,720
ここではm+1次元の

173
00:06:16,170 --> 00:06:17,730
ベクトルと

174
00:06:17,990 --> 00:06:21,260
なる。

175
00:06:21,780 --> 00:06:23,100
そしてここでmとなるのは、

176
00:06:23,260 --> 00:06:25,030
ランドマークの総数は

177
00:06:25,450 --> 00:06:26,600
トレーニングセットの数と等しいから。

178
00:06:26,910 --> 00:06:28,190
つまり、mはトレーニングセットの数であり、

179
00:06:29,100 --> 00:06:31,950
そしてここではパラメータベクトルシータもm+1次元となる。

180
00:06:32,990 --> 00:06:33,990
以上が、予測を行う方法だ、

181
00:06:34,360 --> 00:06:36,870
既にあなたがパラメータシータを得ている、という場合での。

182
00:06:37,840 --> 00:06:39,160
どうやってパラメータシータを得たらいいだろうか？

183
00:06:39,680 --> 00:06:40,650
あなたはSVMの学習アルゴリズムを使って

184
00:06:40,920 --> 00:06:43,040
それを行う事が出来る。具体的には、

185
00:06:43,850 --> 00:06:46,460
あなたはこの最小化問題を解く事になる。

186
00:06:46,690 --> 00:06:48,170
パラメータのシータを

187
00:06:48,540 --> 00:06:51,630
C掛けるこのコスト関数 を最小化するようにシータを選ぶ。

188
00:06:52,430 --> 00:06:54,770
ただしここでは、ここを見ると、

189
00:06:55,040 --> 00:06:56,650
シータ転置 x(i) を用いて

190
00:06:56,970 --> 00:06:59,300
予測を行う代わりに、

191
00:07:00,020 --> 00:07:01,410
元のフィーチャーx(i)を使う代わりに、

192
00:07:01,720 --> 00:07:03,320
その代わりにフィーチャーx(i)の所を

193
00:07:03,520 --> 00:07:04,840
新しいフィーチャーで

194
00:07:05,090 --> 00:07:06,260
置き換える。

195
00:07:07,270 --> 00:07:09,080
我らはシータ転置 f(i) を用いる、

196
00:07:09,380 --> 00:07:10,840
i番目のトレーニング手本の

197
00:07:11,130 --> 00:07:12,480
予測を行う為に。

198
00:07:12,860 --> 00:07:13,860
これは両方にあるのを

199
00:07:14,230 --> 00:07:16,580
見てくれ。

200
00:07:16,700 --> 00:07:18,270
そしてこの最小化問題を解く事で、

201
00:07:18,760 --> 00:07:22,130
サポートベクターマシンのパラメータが得られる。

202
00:07:23,240 --> 00:07:24,640
そして最後に一つ細かい話を。

203
00:07:24,870 --> 00:07:26,880
この最適化問題では、

204
00:07:27,510 --> 00:07:29,580
我らは実際にはn=mのフィーチャーを

205
00:07:30,570 --> 00:07:32,300
持っているから、

206
00:07:32,860 --> 00:07:33,650
つまりここで、

207
00:07:34,520 --> 00:07:36,010
我らが持ってるフィーチャーの総数は、

208
00:07:37,100 --> 00:07:38,240
実際に効力のあるフィーチャーの数は、

209
00:07:38,410 --> 00:07:39,390
fの次元となる。

210
00:07:39,670 --> 00:07:41,020
つまり、nは実際に

211
00:07:41,730 --> 00:07:42,690
イコールmとなる。

212
00:07:42,900 --> 00:07:44,470
だからもしお望みなら、

213
00:07:44,610 --> 00:07:45,530
これを、和と、、、

214
00:07:46,340 --> 00:07:47,280
j=1からmまでの和と

215
00:07:47,590 --> 00:07:48,680
考える事が出来る。

216
00:07:49,490 --> 00:07:50,390
それを考える一つの方法としては、

217
00:07:50,470 --> 00:07:51,500
それを、

218
00:07:51,620 --> 00:07:53,250
n=mと考える、という

219
00:07:53,550 --> 00:07:55,060
方法がある。

220
00:07:55,570 --> 00:07:57,320
何故なら、fを新しいフィーチャーと考えると、

221
00:07:57,970 --> 00:07:59,650
m+1個のフィーチャーとなるが、

222
00:08:00,120 --> 00:08:02,920
ここで+1は切片項から来るが、

223
00:08:05,090 --> 00:08:06,760
ここで我らは和を

224
00:08:06,990 --> 00:08:08,110
j=1からmに渡って取っている、

225
00:08:08,440 --> 00:08:10,070
何故なら以前正規化のビデオで

226
00:08:10,380 --> 00:08:11,700
やったのと同様に、

227
00:08:12,580 --> 00:08:14,110
我らはここでもパラメータのシータ0を

228
00:08:14,180 --> 00:08:15,650
正規化しない。

229
00:08:15,780 --> 00:08:16,560
だから和はj=1からmに

230
00:08:16,740 --> 00:08:17,930
渡って取るのであって、

231
00:08:18,880 --> 00:08:19,840
j=0からmまででは無い理由だ。

232
00:08:20,000 --> 00:08:22,200
以上がサポートベクタマシンの

233
00:08:22,580 --> 00:08:23,760
学習アルゴリズムだ。

234
00:08:24,660 --> 00:08:26,260
最後に一つ、

235
00:08:27,160 --> 00:08:28,310
数学的な脇道の細かい話を

236
00:08:28,440 --> 00:08:29,840
言及しておく。

237
00:08:29,930 --> 00:08:30,780
それはサポートベクタマシンの実装を

238
00:08:31,310 --> 00:08:33,020
する時には、この最後の項は、

239
00:08:33,320 --> 00:08:34,750
ちょっとだけ違った感じにする。

240
00:08:35,680 --> 00:08:36,730
あなたは本当に

241
00:08:36,770 --> 00:08:38,080
サポートベクターマシンを使うに際し

242
00:08:38,190 --> 00:08:39,190
この最後の詳細を知っている必要は無い。

243
00:08:39,700 --> 00:08:41,330
そして実際には、ここに書いた

244
00:08:41,450 --> 00:08:42,500
方程式は、あたなが必要な直感的な理解の

245
00:08:42,620 --> 00:08:45,160
全てを提供するはずだ。

246
00:08:45,310 --> 00:08:46,190
だが、サポートベクターマシンを

247
00:08:46,450 --> 00:08:48,450
実装する方法としては、この項、

248
00:08:48,570 --> 00:08:50,960
シータjの二乗の和、

249
00:08:53,110 --> 00:08:54,780
別の書き方をすると、

250
00:08:55,580 --> 00:08:57,660
シータ転置 シータとなる、

251
00:08:58,500 --> 00:08:59,530
もしパラメータ、シータ0を

252
00:09:00,120 --> 00:09:02,730
無視すれば。

253
00:09:03,570 --> 00:09:05,640
するとシータ1からシータmまで、

254
00:09:05,800 --> 00:09:10,090
シータ0は無視。

255
00:09:11,130 --> 00:09:13,790
するとこのjに渡って和を取ることの

256
00:09:14,510 --> 00:09:15,900
シータjの二乗は、

257
00:09:16,040 --> 00:09:18,870
シータ転置 シータ と書く事も出来る訳だ。

258
00:09:19,930 --> 00:09:21,520
そして多くのサポートベクターマシンの実装が

259
00:09:21,730 --> 00:09:23,380
やってる事としては、実際にはこの

260
00:09:23,720 --> 00:09:25,520
シータ転置 シータを置き換えて、

261
00:09:26,280 --> 00:09:28,270
その代わりに、シータ転置に掛ける事の

262
00:09:28,590 --> 00:09:30,140
なんからの行列を中に入れる、ここでこの行列は

263
00:09:30,820 --> 00:09:33,930
あなたの用いるカーネルによって決まる物で、それに掛けるシータ。

264
00:09:34,160 --> 00:09:35,500
つまりこれは、ちょっとだけ違った距離の計量を与える。

265
00:09:36,140 --> 00:09:37,770
我らは最小化する対象として

266
00:09:38,070 --> 00:09:40,050
ちょっと別の指標を用いるのだ、

267
00:09:41,320 --> 00:09:43,250
厳密なシータ二乗のノルムの代わりに。

268
00:09:43,790 --> 00:09:45,990
つまり、何かしらそれに似た物を最小化している。

269
00:09:46,140 --> 00:09:47,610
それはリスケールしたバージョンの

270
00:09:47,770 --> 00:09:50,150
パラメータベクトルシータ、みたいな物で、それはカーネルに依存している。

271
00:09:50,950 --> 00:09:52,440
だが、これはある種、数学的な詳細に属する話だ、

272
00:09:53,210 --> 00:09:54,360
そうする事で、サポートベクターマシンのソフトウェアが

273
00:09:54,650 --> 00:09:56,350
もっと効率的に実行出来るようになる。

274
00:09:58,300 --> 00:09:59,410
そしてサポートベクターマシンがこれを行う理由は、

275
00:09:59,700 --> 00:10:01,500
この修正を行う事で、

276
00:10:02,020 --> 00:10:03,250
もっと大きなトレーニングセットに

277
00:10:03,300 --> 00:10:05,740
スケール出来るようになるからだ。

278
00:10:06,370 --> 00:10:07,800
何故なら、例えばもしあなたが

279
00:10:07,970 --> 00:10:11,530
1万個のトレーニング手本によるトレーニングセットを持っていたとすると、

280
00:10:12,590 --> 00:10:13,560
我らのランドマークの定義の仕方では、

281
00:10:13,950 --> 00:10:15,750
結果としては1万個のランドマークを得る事になる。

282
00:10:16,780 --> 00:10:18,060
だからシータは1万次元になる。

283
00:10:18,490 --> 00:10:20,450
この位ならなんとかなるかもしれないが、

284
00:10:20,450 --> 00:10:21,710
mがもっと、もっと大きくなっていくと、

285
00:10:22,470 --> 00:10:24,020
これらのパラメータ全てに対して

286
00:10:24,150 --> 00:10:25,480
解く、というのは、

287
00:10:25,590 --> 00:10:26,590
mが5万とか10万とかだとすると、

288
00:10:26,880 --> 00:10:28,170
これらのパラメータ全てに関して

289
00:10:28,340 --> 00:10:29,660
解いていく、というのは

290
00:10:29,890 --> 00:10:31,240
サポートベクタマシンの最適化ソフトウェアにとって

291
00:10:31,420 --> 00:10:33,690
高くつく事態となりうる。

292
00:10:33,870 --> 00:10:35,750
かくして、ここに書いた最小化問題を解くのに、

293
00:10:36,490 --> 00:10:37,570
これはある種の数学的な詳細であって、

294
00:10:37,860 --> 00:10:39,580
繰り返しになるがあなたは本当に知ってる必要の無い事だが、

295
00:10:41,000 --> 00:10:43,070
それはこの最後の項を

296
00:10:43,350 --> 00:10:44,380
ちょっとだけ修正して、

297
00:10:44,500 --> 00:10:45,940
単にシータのノルムの二乗を最小化するのとは、

298
00:10:46,080 --> 00:10:48,560
ちょっとだけ違った物を最適化する。

299
00:10:49,370 --> 00:10:50,600
だがお望みなら、

300
00:10:51,080 --> 00:10:52,450
この事を、ある種の実装の詳細に過ぎない話で、

301
00:10:52,710 --> 00:10:54,880
目的関数をちょっとだけ変更する、という物で、

302
00:10:55,340 --> 00:10:56,750
その理由は主に

303
00:10:56,880 --> 00:10:58,260
計算的な効率性を求めて、だ、と、

304
00:10:58,930 --> 00:11:01,590
考えていただいて構わない。

305
00:11:02,260 --> 00:11:04,390
だから通常は、この事に思い悩む必要は無い。

306
00:11:07,640 --> 00:11:09,460
ところで、もしあなたが、

307
00:11:09,560 --> 00:11:10,730
その他の学習アルゴリズムでも

308
00:11:11,100 --> 00:11:12,210
カーネルのアイデアを用いないのは何故かしら？と

309
00:11:12,570 --> 00:11:13,690
不思議に思うなら、例えばロジスティック回帰とか、

310
00:11:14,040 --> 00:11:15,450
実のところ、

311
00:11:15,670 --> 00:11:16,770
もしお望みなら、

312
00:11:16,900 --> 00:11:18,120
カーネルの考え方を適用する事は出来て、

313
00:11:18,550 --> 00:11:19,850
ランドマークを用いたフィーチャーを

314
00:11:19,990 --> 00:11:22,920
定義したりなどを、ロジスティック回帰に対して行う事は出来る。

315
00:11:23,880 --> 00:11:25,860
だがサポートベクターマシンに使えた

316
00:11:26,440 --> 00:11:28,110
計算的なトリックは

317
00:11:28,430 --> 00:11:30,700
その他のアルゴリズム、ロジスティック回帰みたいなのとかには、あまりうまく一般化出来ない。

318
00:11:31,310 --> 00:11:33,110
つまり、カーネルをロジスティック回帰と共に用いると

319
00:11:33,260 --> 00:11:34,390
とても遅くなりすぎるだろう、

320
00:11:34,580 --> 00:11:36,330
一方で、

321
00:11:36,440 --> 00:11:37,940
計算的なトリックのおかげで、

322
00:11:38,150 --> 00:11:39,490
それはサポートベクタマシンのソフトウェアの

323
00:11:39,900 --> 00:11:41,130
詳細に埋め込まれた、これをどう変形するか、

324
00:11:41,320 --> 00:11:43,140
などのトリックのおかげで、

325
00:11:43,240 --> 00:11:44,990
サポートベクターマシンとカーネルは

326
00:11:45,300 --> 00:11:47,090
とりわけいい感じに、一緒に使えるのだ。

327
00:11:47,930 --> 00:11:49,450
一方でロジスティック回帰とカーネルは、

328
00:11:50,250 --> 00:11:51,990
使える事は使えるんだが、とても実行速度が遅い。

329
00:11:52,890 --> 00:11:53,670
そしてロジスティック回帰では、

330
00:11:53,750 --> 00:11:55,420
サポートベクタマシンとカーネルの組み合わせで

331
00:11:56,040 --> 00:11:57,360
実行する時だけに使える人々が編みだした

332
00:11:57,530 --> 00:11:58,530
アドバンスドな最適化のテクニックの

333
00:11:59,140 --> 00:12:00,950
恩恵が得られない。

334
00:12:01,540 --> 00:12:03,340
だがこれは全て、

335
00:12:03,710 --> 00:12:04,850
コスト関数を最小化するソフトウェアを

336
00:12:05,230 --> 00:12:06,900
実際にどう実装するかに依存した話だ。

337
00:12:07,870 --> 00:12:08,940
私は次のビデオでもうちょっとこの話をするが、

338
00:12:09,040 --> 00:12:09,950
だがあなたは、本当に

339
00:12:10,150 --> 00:12:11,530
コスト関数を最小化するソフトウェアを

340
00:12:12,200 --> 00:12:13,520
どうやって書くかを

341
00:12:13,670 --> 00:12:14,890
知る必要は無い。

342
00:12:15,170 --> 00:12:17,560
何故ならあなたは、とても良い既製品のソフトウェアでこれを行ってくれるものを、見つける事が出来るからだ。

343
00:12:18,670 --> 00:12:19,890
そしてまた、知っての通り、

344
00:12:20,140 --> 00:12:21,340
私は行列の逆行列を計算するコードを

345
00:12:21,850 --> 00:12:22,960
自分で書く事はオススメしないし、

346
00:12:23,150 --> 00:12:24,490
ルートを計算するコードを自分で書く事もオススメしない。

347
00:12:24,660 --> 00:12:26,420
コスト関数を最小化する関数を計算するソフトウェアを

348
00:12:26,560 --> 00:12:27,750
自分で書く事も推奨しない。

349
00:12:28,240 --> 00:12:29,610
代わりに既製品の

350
00:12:29,780 --> 00:12:31,490
ソフトウェアパッケージを使うべきだ、

351
00:12:31,740 --> 00:12:33,240
人々が既に開発してくれている、

352
00:12:33,540 --> 00:12:35,140
そしてそれらのソフトウェアパッケージには

353
00:12:35,790 --> 00:12:37,720
既にこれらの数値計算の最適化のトリックが埋めこまれているので、

354
00:12:39,540 --> 00:12:41,770
それについてあなたが心配する必要は無い。

355
00:12:41,950 --> 00:12:42,920
だが、サポートベクタマシンを使う時に

356
00:12:43,180 --> 00:12:45,200
もう一つ知っておく

357
00:12:45,350 --> 00:12:46,400
価値のある事として、

358
00:12:46,640 --> 00:12:47,730
サポートベクタマシンのパラメータを

359
00:12:47,820 --> 00:12:50,220
どうやって選ぶのか？という事がある。

360
00:12:51,520 --> 00:12:52,300
そしてこのビデオで最後に私が

361
00:12:52,400 --> 00:12:53,290
やりたい事は、

362
00:12:53,450 --> 00:12:54,680
サポートベクターマシンを使う時の

363
00:12:54,840 --> 00:12:57,070
バイアス-バリアンスのトレードオフを取る事について、ちょっと言っておく事だ。

364
00:12:57,900 --> 00:12:59,230
SVMを使う時には、

365
00:12:59,390 --> 00:13:00,670
あなたが決める必要のある事の一つに、

366
00:13:00,960 --> 00:13:03,850
パラメータCがある、

367
00:13:04,090 --> 00:13:05,880
それはこの最適化の目的関数内にあった物で、

368
00:13:05,980 --> 00:13:07,690
そしてCは 1/ラムダ に似たような

369
00:13:07,770 --> 00:13:09,800
役割を果たす事を、思い出してくれ、

370
00:13:10,050 --> 00:13:11,750
ここでラムダはロジスティック回帰にあった

371
00:13:12,520 --> 00:13:13,970
正規化パラメータ。

372
00:13:15,360 --> 00:13:16,760
さて、もしあなたが大きな値の

373
00:13:16,930 --> 00:13:18,760
Cを持つ時は、これは

374
00:13:19,520 --> 00:13:20,560
ロジスティック回帰において

375
00:13:21,270 --> 00:13:22,260
小さなラムダの値をとる事に対応し、

376
00:13:22,670 --> 00:13:25,080
それはつまり、あまり正規化しない、という事を意味する。

377
00:13:25,980 --> 00:13:26,960
そうしておけば、

378
00:13:27,050 --> 00:13:29,330
仮説を得る傾向にある。

379
00:13:30,570 --> 00:13:31,420
一方でもしあなたがもっと小さいCの値を

380
00:13:31,630 --> 00:13:33,050
使うとすると、その時は

381
00:13:33,240 --> 00:13:34,510
ロジスティック回帰において

382
00:13:34,660 --> 00:13:36,450
大きなラムダの値を使う事に

383
00:13:36,620 --> 00:13:38,090
対応するのだから、

384
00:13:38,690 --> 00:13:40,180
これは高バイアス、低バリアンスの

385
00:13:40,470 --> 00:13:41,760
仮説に対応する。

386
00:13:42,580 --> 00:13:44,520
つまり、大きなCによる仮説は、

387
00:13:45,000 --> 00:13:46,870
より高いバリアンスとなり、

388
00:13:47,450 --> 00:13:48,380
そしてよりオーバーフィットしがちとなる。

389
00:13:48,580 --> 00:13:50,290
一方で小さなCの仮説は、

390
00:13:50,450 --> 00:13:52,820
より高いバイアスとなり、

391
00:13:52,910 --> 00:13:54,900
そしてよりアンダーフィットしがちとなる。

392
00:13:56,710 --> 00:13:59,870
つまり、このパラメータCが、我らが選ぶ必要がある一つ、という事だ。

393
00:14:00,210 --> 00:14:01,280
別の我らが選ぶ必要がある物としては、

394
00:14:02,280 --> 00:14:04,580
パラメータ、シグマ二乗がある。これはガウスカーネルに現れる物だ。

395
00:14:05,760 --> 00:14:07,080
さて、もしガウスカーネルにおいて

396
00:14:07,750 --> 00:14:09,370
シグマ二乗が大きければ、

397
00:14:09,640 --> 00:14:11,350
その時は、similarity関数の中で、

398
00:14:11,530 --> 00:14:12,710
eの指数乗の

399
00:14:13,390 --> 00:14:14,710
マイナスの x引くランドマークi

400
00:14:16,280 --> 00:14:17,950
の二乗、を2シグマ二乗で割った物、

401
00:14:20,130 --> 00:14:21,290
ここでフィーチャーが

402
00:14:21,480 --> 00:14:23,330
たった一つ、x1しか無かったとすると、

403
00:14:23,570 --> 00:14:25,390
そしてランドマークがここに

404
00:14:25,490 --> 00:14:27,710
あったとすると、

405
00:14:27,960 --> 00:14:29,230
もしシグマ二乗が大きければ、その時は

406
00:14:29,480 --> 00:14:30,600
ガウスカーネルは相対的に

407
00:14:30,690 --> 00:14:32,940
ゆっくりと落ちていく、

408
00:14:33,960 --> 00:14:34,740
するとこれは、私のフィーチャーf(i)を

409
00:14:35,210 --> 00:14:36,690
つまりこれは、

410
00:14:36,880 --> 00:14:38,970
よりスムースな関数となり、よりゆっくり変化する。

411
00:14:39,060 --> 00:14:40,640
つまりこれは、

412
00:14:40,760 --> 00:14:42,750
より高いバイアス、より低いバリアンスの仮説を

413
00:14:43,030 --> 00:14:44,170
あなたに与える。

414
00:14:44,550 --> 00:14:46,000
何故ならガウスカーネルはよりスムースに落ちていくから。

415
00:14:46,840 --> 00:14:48,240
それはゆっくり変化する傾向の仮説を得る事になり、

416
00:14:48,520 --> 00:14:50,060
あるいは入力xを変化させると

417
00:14:50,130 --> 00:14:51,860
よりスムースに変化していく仮説となる。

418
00:14:52,050 --> 00:14:53,680
一方で、対照的に、

419
00:14:54,030 --> 00:14:55,330
もしシグマ二乗が

420
00:14:55,660 --> 00:14:57,430
小さくて、そして

421
00:14:57,540 --> 00:14:58,830
これがランドマークとして、

422
00:14:58,960 --> 00:15:01,440
所与の一つのフィーチャーx1に対して、

423
00:15:01,820 --> 00:15:04,630
ガウスカーネル、類似度関数は、とても急速に変化する。

424
00:15:05,310 --> 00:15:07,520
どちらのケースでも丘の頂点は1だ。

425
00:15:07,580 --> 00:15:08,550
つまりシグマ二乗が小さければ、

426
00:15:08,870 --> 00:15:11,730
私のフィーチャーはよりスムースで無く変化する。

427
00:15:12,190 --> 00:15:13,740
つまりもっと高い傾き、あるいは

428
00:15:14,250 --> 00:15:15,300
より高い導関数となる。

429
00:15:16,020 --> 00:15:17,170
そしてこれを用いる事で、

430
00:15:17,330 --> 00:15:19,620
仮説のフィッティングを、

431
00:15:19,840 --> 00:15:21,870
低バイアス、高バリアンスに行う事が出来る。

432
00:15:23,030 --> 00:15:24,460
そしてこのカーブを見てみると、

433
00:15:24,680 --> 00:15:26,240
実際にあなたは

434
00:15:26,450 --> 00:15:27,230
これらのアイデアの幾つかを自分自身で

435
00:15:27,330 --> 00:15:29,480
試してみる事が出来て、その効果を自分自身で見てみる事が出来る。

436
00:15:31,590 --> 00:15:34,430
以上がカーネルとサポートベクターマシンのアルゴリズムだ。

437
00:15:35,320 --> 00:15:36,450
そしてこのバイアスとバリアンスの議論で

438
00:15:37,090 --> 00:15:39,170
あなたが、このアルゴリズムが

439
00:15:39,310 --> 00:15:40,380
どんな風に振舞うと期待したらいいか

440
00:15:40,460 --> 00:15:42,600
いくらかつかめたらいいな。