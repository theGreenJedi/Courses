1
00:00:00,530 --> 00:00:01,550
在上一节视频里 我们讨论了

2
00:00:01,950 --> 00:00:03,230
核函数这个想法

3
00:00:03,710 --> 00:00:04,590
以及怎样利用它去

4
00:00:04,860 --> 00:00:07,900
实现支持向量机的一些新特性

5
00:00:08,100 --> 00:00:08,910
在这一节视频中 我将

6
00:00:09,230 --> 00:00:10,670
补充一些缺失的细节

7
00:00:11,020 --> 00:00:12,070
并简单的介绍一下

8
00:00:12,270 --> 00:00:14,100
怎么在实际中使用应用这些想法

9
00:00:14,650 --> 00:00:15,850
例如 怎么处理

10
00:00:16,340 --> 00:00:20,120
支持向量机中的偏差方差折中

11
00:00:22,690 --> 00:00:23,680
在上一节课中

12
00:00:24,000 --> 00:00:25,970
我谈到过选择标记点

13
00:00:26,660 --> 00:00:28,890
例如 l(1) l(2) l(3)

14
00:00:29,150 --> 00:00:30,220
这些点使我们能够定义

15
00:00:30,300 --> 00:00:31,900
相似度函数

16
00:00:32,200 --> 00:00:33,500
也称之为核函数

17
00:00:33,690 --> 00:00:34,830
在这个例子里

18
00:00:35,070 --> 00:00:37,410
我们的相似度函数为高斯核函数

19
00:00:38,610 --> 00:00:40,370
这使我们能够

20
00:00:40,660 --> 00:00:42,070
构造一个这样的假设函数

21
00:00:43,180 --> 00:00:44,880
但是 我们从哪里得到这些标记点？

22
00:00:45,150 --> 00:00:45,670
我们从哪里得到l(1) l(2) l(3)？

23
00:00:45,690 --> 00:00:49,080
而且 在一些复杂的学习问题中

24
00:00:49,610 --> 00:00:50,830
也许我们需要

25
00:00:50,920 --> 00:00:53,060
更多的标记点 而不是我们手选的这三个

26
00:00:55,160 --> 00:00:56,450
因此 在实际应用时

27
00:00:56,580 --> 00:00:57,730
怎么选取标记点

28
00:00:57,830 --> 00:00:59,910
是机器学习中必须解决的问题

29
00:01:00,150 --> 00:01:01,110
这是我们的数据集

30
00:01:01,370 --> 00:01:02,230
有一些正样本和一些负样本

31
00:01:02,710 --> 00:01:04,460
我们的想法是

32
00:01:05,310 --> 00:01:06,270
我们将选取样本点

33
00:01:06,630 --> 00:01:08,200
我们拥有的

34
00:01:08,470 --> 00:01:09,780
每一个样本点

35
00:01:10,490 --> 00:01:11,430
我们只需要直接使用它们

36
00:01:11,980 --> 00:01:13,270
我们直接

37
00:01:13,440 --> 00:01:14,850
将训练样本

38
00:01:15,490 --> 00:01:17,600
作为标记点

39
00:01:18,930 --> 00:01:20,360
如果我有一个

40
00:01:20,680 --> 00:01:21,880
训练样本x(1)

41
00:01:22,120 --> 00:01:23,460
那么

42
00:01:23,670 --> 00:01:24,550
我将把第一个标记点

43
00:01:25,100 --> 00:01:26,470
就放在跟我的第一个训练样本点

44
00:01:27,250 --> 00:01:28,170
完全重合的地方

45
00:01:29,260 --> 00:01:30,180
如果我有另一个

46
00:01:30,470 --> 00:01:32,340
训练样本x(2)

47
00:01:32,500 --> 00:01:33,980
那么 我将把第二个标记点选在

48
00:01:35,060 --> 00:01:37,300
与第二个样本点重合的位置上

49
00:01:38,480 --> 00:01:39,320
在右边的这幅图上

50
00:01:39,480 --> 00:01:40,480
我用红点和蓝点

51
00:01:40,820 --> 00:01:41,930
来阐述

52
00:01:42,420 --> 00:01:44,320
这幅图以及这些点的颜色

53
00:01:44,370 --> 00:01:46,030
可能并不显眼

54
00:01:47,120 --> 00:01:47,930
但是利用

55
00:01:48,110 --> 00:01:49,660
这个方法

56
00:01:49,790 --> 00:01:51,450
最终能得到

57
00:01:52,160 --> 00:01:53,690
m个标记点 l(1) l(2)

58
00:01:54,950 --> 00:01:56,320
直到 l(m)

59
00:01:56,380 --> 00:01:58,180
即每一个标记点

60
00:01:58,420 --> 00:02:00,500
的位置都与

61
00:02:00,810 --> 00:02:02,680
每一个样本点

62
00:02:02,860 --> 00:02:04,810
的位置精确对应

63
00:02:04,950 --> 00:02:05,920
这个过程很棒

64
00:02:06,120 --> 00:02:07,630
这说明特征函数基本上

65
00:02:07,700 --> 00:02:09,300
是在描述

66
00:02:09,380 --> 00:02:10,800
每一个样本距离

67
00:02:10,970 --> 00:02:13,150
样本集中其他样本的距离

68
00:02:13,440 --> 00:02:14,180
我们具体的列出

69
00:02:14,350 --> 00:02:16,270
这个过程的大纲

70
00:02:16,470 --> 00:02:17,870
给定m个训练样本

71
00:02:18,050 --> 00:02:19,100
我将选取与

72
00:02:19,310 --> 00:02:20,430
m个训练样本精确一致

73
00:02:21,190 --> 00:02:23,920
的位置作为我的标记点

74
00:02:25,430 --> 00:02:26,600
当输入样本x

75
00:02:26,920 --> 00:02:28,090
样本x可以

76
00:02:28,230 --> 00:02:29,260
属于训练集

77
00:02:29,570 --> 00:02:30,800
也可以属于交叉验证集

78
00:02:31,490 --> 00:02:32,470
也可以属于测试集

79
00:02:33,320 --> 00:02:34,090
给定样本x

80
00:02:34,320 --> 00:02:35,470
我们可以计算

81
00:02:35,750 --> 00:02:37,220
这些特征 即f1

82
00:02:37,560 --> 00:02:39,180
f2 等等

83
00:02:39,580 --> 00:02:41,120
这里 l(1) 等于 x(1)

84
00:02:41,490 --> 00:02:42,850
剩下标记点的以此类推

85
00:02:43,570 --> 00:02:46,080
最终我们能到一个特征向量

86
00:02:46,840 --> 00:02:49,540
我将特征向量记为f

87
00:02:50,270 --> 00:02:52,090
我将f1 f2等等

88
00:02:52,290 --> 00:02:53,370
构造为

89
00:02:53,580 --> 00:02:55,330
特征向量

90
00:02:56,330 --> 00:02:58,000
一直写到fm

91
00:02:59,320 --> 00:03:01,080
此外 按照惯例

92
00:03:01,610 --> 00:03:02,870
如果我们需要的话

93
00:03:02,990 --> 00:03:06,250
可以添加额外的特征f0 f0的值始终为1

94
00:03:06,450 --> 00:03:08,530
它与我们之前讨论过的

95
00:03:09,480 --> 00:03:11,200
截距x0的作用相似

96
00:03:13,200 --> 00:03:14,450
举个例子 

97
00:03:14,580 --> 00:03:16,550
假设我们有训练样本(x(i), y(i))

98
00:03:18,270 --> 00:03:19,300
这个样本对应的

99
00:03:20,080 --> 00:03:21,330
特征向量可以

100
00:03:21,440 --> 00:03:23,440
这样计算 给定x(i)

101
00:03:23,640 --> 00:03:26,560
我们可以通过相似度函数

102
00:03:27,980 --> 00:03:29,670
将其映射到f1(i)

103
00:03:29,960 --> 00:03:31,980
在这里 我将整个单词similarity(相似度)

104
00:03:32,090 --> 00:03:33,380
简记为sim

105
00:03:35,540 --> 00:03:35,540
简记为sim

106
00:03:37,050 --> 00:03:39,180
f2(i)等于x(i)与l(2)

107
00:03:40,090 --> 00:03:42,780
之间的相似度

108
00:03:43,140 --> 00:03:45,050
以此类推

109
00:03:45,230 --> 00:03:48,370
最后有fm(i)

110
00:03:49,600 --> 00:03:54,480
等于x(i)与l(m)之间的相似度

111
00:03:55,700 --> 00:03:58,700
在这一列中间的

112
00:03:59,160 --> 00:04:01,320
某个位置

113
00:04:01,480 --> 00:04:03,930
即第i个元素

114
00:04:04,230 --> 00:04:05,740
有一个特征

115
00:04:06,150 --> 00:04:07,590
为fi(i)

116
00:04:08,170 --> 00:04:09,930
为fi(i)

117
00:04:10,050 --> 00:04:11,180
这是x(i)和l(i)之间的

118
00:04:13,080 --> 00:04:14,550
相似度

119
00:04:15,680 --> 00:04:16,990
这里l(i)就等于

120
00:04:17,190 --> 00:04:18,560
x(i) 所以

121
00:04:19,140 --> 00:04:20,320
fi(i)衡量的是

122
00:04:20,410 --> 00:04:22,250
x(i)与其自身的相似度

123
00:04:23,960 --> 00:04:25,380
如果你使用高斯核函数的话

124
00:04:25,620 --> 00:04:26,720
这一项等于

125
00:04:27,170 --> 00:04:29,440
exp(-0/(2*sigma^2)) 等于1

126
00:04:29,790 --> 00:04:31,060
所以 对于这个样本来说

127
00:04:31,370 --> 00:04:32,940
其中的某一个特征等于1

128
00:04:34,290 --> 00:04:35,570
接下来 类似于我们之前的过程

129
00:04:35,990 --> 00:04:36,940
我将这m个特征

130
00:04:37,870 --> 00:04:39,910
合并为一个特征向量

131
00:04:40,340 --> 00:04:41,730
于是 相比之前用x(i)来描述样本

132
00:04:42,710 --> 00:04:44,200
x(i)为n维或者n+1维空间

133
00:04:44,430 --> 00:04:46,970
的向量

134
00:04:48,290 --> 00:04:49,590
取决于你的具体项数

135
00:04:49,990 --> 00:04:51,120
可能为n维向量空间

136
00:04:52,070 --> 00:04:52,750
也可能为n+1维向量空间

137
00:04:53,440 --> 00:04:55,140
我们现在可以用

138
00:04:55,300 --> 00:04:56,700
这个特征向量f

139
00:04:56,980 --> 00:04:58,810
来描述我的特征向量

140
00:04:58,920 --> 00:05:01,240
我将合并f(i)

141
00:05:01,400 --> 00:05:03,060
将所有这些项

142
00:05:03,300 --> 00:05:06,010
合并为一个向量

143
00:05:06,540 --> 00:05:09,180
即从f1(i)

144
00:05:09,430 --> 00:05:12,740
到fm(i) 如果有需要的话

145
00:05:13,030 --> 00:05:15,160
我们通常也会加上

146
00:05:15,420 --> 00:05:16,990
f0(i)这一项

147
00:05:17,130 --> 00:05:19,370
f0(i)等于1

148
00:05:19,370 --> 00:05:20,970
那么 这个向量

149
00:05:21,300 --> 00:05:23,260
就是

150
00:05:23,430 --> 00:05:25,180
我们用于描述训练样本的

151
00:05:25,480 --> 00:05:28,310
特征向量

152
00:05:29,040 --> 00:05:30,980
当给定核函数

153
00:05:31,530 --> 00:05:33,160
和相似度函数后

154
00:05:33,400 --> 00:05:35,030
我们按照这个方法来使用支持向量机

155
00:05:35,720 --> 00:05:37,100
如果你已经得到参数 θ

156
00:05:37,300 --> 00:05:39,040
并且想对样本x做出预测

157
00:05:41,680 --> 00:05:42,850
我们先要计算

158
00:05:43,060 --> 00:05:44,170
特征向量f

159
00:05:44,450 --> 00:05:46,920
f是m+1维特征向量

160
00:05:49,040 --> 00:05:50,640
这里之所以有m

161
00:05:51,610 --> 00:05:53,190
是因为我们有m个训练样本

162
00:05:53,570 --> 00:05:56,370
于是就有m个标记点

163
00:05:57,330 --> 00:05:58,310
我们在 θ 的转置乘以f

164
00:05:58,600 --> 00:06:00,180
大于或等于0时

165
00:06:00,780 --> 00:06:01,860
预测y=1

166
00:06:02,230 --> 00:06:02,430
对吧

167
00:06:02,640 --> 00:06:03,770
θ 的转置乘以f

168
00:06:04,090 --> 00:06:07,200
等于θ0×f0加上θ1×f1

169
00:06:07,900 --> 00:06:08,990
加上点点点

170
00:06:09,120 --> 00:06:11,200
直到θm×fm

171
00:06:12,170 --> 00:06:13,900
所以

172
00:06:14,050 --> 00:06:15,720
参数向量θ

173
00:06:16,170 --> 00:06:17,730
在这里为

174
00:06:17,990 --> 00:06:21,260
m+1维向量

175
00:06:21,780 --> 00:06:23,100
这里有m是因为

176
00:06:23,260 --> 00:06:25,030
标记点的个数等于

177
00:06:25,450 --> 00:06:26,600
训练点的个数

178
00:06:26,910 --> 00:06:28,190
m就是训练集的大小

179
00:06:29,100 --> 00:06:31,950
所以 参数向量θ为m+1维

180
00:06:32,990 --> 00:06:33,990
以上就是当已知参数θ时

181
00:06:34,360 --> 00:06:36,870
怎么做出预测的过程

182
00:06:37,840 --> 00:06:39,160
怎样得到参数θ呢？

183
00:06:39,680 --> 00:06:40,650
你在使用

184
00:06:40,920 --> 00:06:43,040
SVM学习算法时

185
00:06:43,850 --> 00:06:46,460
具体来说就是要求解这个最小化问题

186
00:06:46,690 --> 00:06:48,170
你需要求出能使这个式子取最小值的参数θ

187
00:06:48,540 --> 00:06:51,630
式子为C乘以这个我们之前见过的代价函数

188
00:06:52,430 --> 00:06:54,770
只是在这里

189
00:06:55,040 --> 00:06:56,650
相比之前使用

190
00:06:56,970 --> 00:06:59,300
θ的转置乘以x(i) 即我们的原始特征

191
00:07:00,020 --> 00:07:01,410
做出预测

192
00:07:01,720 --> 00:07:03,320
我们将替换

193
00:07:03,520 --> 00:07:04,840
特征向量x(i)

194
00:07:05,090 --> 00:07:06,260
并使用这个新的特征向量

195
00:07:07,270 --> 00:07:09,080
我们使用θ的转置

196
00:07:09,380 --> 00:07:10,840
乘以f(i)来对第i个训练样本

197
00:07:11,130 --> 00:07:12,480
做出预测

198
00:07:12,860 --> 00:07:13,860
我们可以看到

199
00:07:14,230 --> 00:07:16,580
这两个地方(都要做出替换)

200
00:07:16,700 --> 00:07:18,270
通过解决这个最小化问题

201
00:07:18,760 --> 00:07:22,130
我们就能得到支持向量机的参数

202
00:07:23,240 --> 00:07:24,640
最后一个细节是

203
00:07:24,870 --> 00:07:26,880
对于这个优化问题

204
00:07:27,510 --> 00:07:29,580
我们有

205
00:07:30,570 --> 00:07:32,300
n=m个特征

206
00:07:32,860 --> 00:07:33,650
就在这里

207
00:07:34,520 --> 00:07:36,010
我们拥有的特征个数

208
00:07:37,100 --> 00:07:38,240
显然 有效的特征个数

209
00:07:38,410 --> 00:07:39,390
应该等于f的维数

210
00:07:39,670 --> 00:07:41,020
所以

211
00:07:41,730 --> 00:07:42,690
n其实就等于m

212
00:07:42,900 --> 00:07:44,470
如果愿意的话

213
00:07:44,610 --> 00:07:45,530
你也可以认为这是一个求和

214
00:07:46,340 --> 00:07:47,280
这确实就是

215
00:07:47,590 --> 00:07:48,680
j从1到m的累和

216
00:07:49,490 --> 00:07:50,390
可以这么来看这个问题

217
00:07:50,470 --> 00:07:51,500
你可以想象

218
00:07:51,620 --> 00:07:53,250
n就等于m

219
00:07:53,550 --> 00:07:55,060
因为如果f

220
00:07:55,570 --> 00:07:57,320
不是新的特征向量

221
00:07:57,970 --> 00:07:59,650
那么我们有m+1个特征

222
00:08:00,120 --> 00:08:02,920
额外的1是因为截距的关系

223
00:08:05,090 --> 00:08:06,760
因此这里

224
00:08:06,990 --> 00:08:08,110
我们仍要j从1累加到n

225
00:08:08,440 --> 00:08:10,070
与我们之前

226
00:08:10,380 --> 00:08:11,700
视频中讲过的正则化类似

227
00:08:12,580 --> 00:08:14,110
我们仍然不对θ0

228
00:08:14,180 --> 00:08:15,650
做正则化处理

229
00:08:15,780 --> 00:08:16,560
这就是

230
00:08:16,740 --> 00:08:17,930
j从1累加到m

231
00:08:18,880 --> 00:08:19,840
而不是从0累加到m的原因

232
00:08:20,000 --> 00:08:22,200
以上

233
00:08:22,580 --> 00:08:23,760
就是支持向量机的学习算法

234
00:08:24,660 --> 00:08:26,260
我在这里

235
00:08:27,160 --> 00:08:28,310
还要讲到

236
00:08:28,440 --> 00:08:29,840
一个数学细节

237
00:08:29,930 --> 00:08:30,780
在支持向量机

238
00:08:31,310 --> 00:08:33,020
实现的过程中

239
00:08:33,320 --> 00:08:34,750
这最后一项与这里写的有细微差别

240
00:08:35,680 --> 00:08:36,730
其实在实现支持向量机时

241
00:08:36,770 --> 00:08:38,080
你并不需要知道

242
00:08:38,190 --> 00:08:39,190
这个细节

243
00:08:39,700 --> 00:08:41,330
事实上这写下的这个式子

244
00:08:41,450 --> 00:08:42,500
已经给你提供了

245
00:08:42,620 --> 00:08:45,160
全部需要的原理

246
00:08:45,310 --> 00:08:46,190
但是在支持向量机实现的过程中

247
00:08:46,450 --> 00:08:48,450
这一项

248
00:08:48,570 --> 00:08:50,960
θj从1到m的平方和

249
00:08:53,110 --> 00:08:54,780
这一项可以被重写为

250
00:08:55,580 --> 00:08:57,660
θ的转置

251
00:08:58,500 --> 00:08:59,530
乘以θ

252
00:09:00,120 --> 00:09:02,730
如果我们忽略θ0的话

253
00:09:03,570 --> 00:09:05,640
考虑θ1直到θm

254
00:09:05,800 --> 00:09:10,090
并忽略theta_0

255
00:09:11,130 --> 00:09:13,790
那么

256
00:09:14,510 --> 00:09:15,900
θj的平方和

257
00:09:16,040 --> 00:09:18,870
可以被重写为 θ 的转置乘以 θ

258
00:09:19,930 --> 00:09:21,520
大多数支持向量机

259
00:09:21,730 --> 00:09:23,380
在实现的时候

260
00:09:23,720 --> 00:09:25,520
其实是替换掉 θ 的转置乘以 θ

261
00:09:26,280 --> 00:09:28,270
用 θ 的转置乘以

262
00:09:28,590 --> 00:09:30,140
某个矩阵 这依赖于你采用的核函数

263
00:09:30,820 --> 00:09:33,930
再乘以 θ 

264
00:09:34,160 --> 00:09:35,500
这其实是另一种略有区别的距离度量方法

265
00:09:36,140 --> 00:09:37,770
我们用一种略有变化的

266
00:09:38,070 --> 00:09:40,050
度量来取代

267
00:09:41,320 --> 00:09:43,250
不直接用 θ 的模的平方进行最小化

268
00:09:43,790 --> 00:09:45,990
而是最小化了另一种类似的度量

269
00:09:46,140 --> 00:09:47,610
这是参数向量θ的变尺度形式

270
00:09:47,770 --> 00:09:50,150
这种变化和核函数相关

271
00:09:50,950 --> 00:09:52,440
这个数学细节

272
00:09:53,210 --> 00:09:54,360
使得支持向量机

273
00:09:54,650 --> 00:09:56,350
能够更有效率的运行

274
00:09:58,300 --> 00:09:59,410
支持向量机做这种修改的

275
00:09:59,700 --> 00:10:01,500
理由是

276
00:10:02,020 --> 00:10:03,250
这么做可以适应

277
00:10:03,300 --> 00:10:05,740
超大的训练集

278
00:10:06,370 --> 00:10:07,800
例如

279
00:10:07,970 --> 00:10:11,530
当你的训练集有10000个样本时

280
00:10:12,590 --> 00:10:13,560
根据我们之前定义标记点的方法

281
00:10:13,950 --> 00:10:15,750
我们最终有10000个标记点

282
00:10:16,780 --> 00:10:18,060
θ也随之是10000维的向量

283
00:10:18,490 --> 00:10:20,450
或许这时这么做还可行

284
00:10:20,450 --> 00:10:21,710
但是 当m变得非常非常大时

285
00:10:22,470 --> 00:10:24,020
那么求解

286
00:10:24,150 --> 00:10:25,480
这么多参数

287
00:10:25,590 --> 00:10:26,590
如果m为50,000或者100,000

288
00:10:26,880 --> 00:10:28,170
此时

289
00:10:28,340 --> 00:10:29,660
利用支持向量机软件包

290
00:10:29,890 --> 00:10:31,240
来解决我写在这里的最小化问题

291
00:10:31,420 --> 00:10:33,690
求解这些参数的成本

292
00:10:33,870 --> 00:10:35,750
会非常高

293
00:10:36,490 --> 00:10:37,570
这些都是数学细节

294
00:10:37,860 --> 00:10:39,580
事实上你没有必要了解这些

295
00:10:41,000 --> 00:10:43,070
它实际上

296
00:10:43,350 --> 00:10:44,380
细微的修改了最后一项

297
00:10:44,500 --> 00:10:45,940
使得最终的优化目标

298
00:10:46,080 --> 00:10:48,560
与直接最小化θ的模的平方略有区别

299
00:10:49,370 --> 00:10:50,600
如果愿意的话

300
00:10:51,080 --> 00:10:52,450
你可以直接认为

301
00:10:52,710 --> 00:10:54,880
这个具体的实现细节

302
00:10:55,340 --> 00:10:56,750
尽管略微的改变了

303
00:10:56,880 --> 00:10:58,260
优化目标

304
00:10:58,930 --> 00:11:01,590
但是它主要是为了计算效率

305
00:11:02,260 --> 00:11:04,390
所以 你不必要对此有太多担心

306
00:11:07,640 --> 00:11:09,460
顺便说一下

307
00:11:09,560 --> 00:11:10,730
你可能会想为什么我们不将

308
00:11:11,100 --> 00:11:12,210
核函数这个想法

309
00:11:12,570 --> 00:11:13,690
应用到其他算法 比如逻辑回归上

310
00:11:14,040 --> 00:11:15,450
事实证明

311
00:11:15,670 --> 00:11:16,770
如果愿意的话

312
00:11:16,900 --> 00:11:18,120
确实可以将核函数

313
00:11:18,550 --> 00:11:19,850
这个想法用于定义特征向量

314
00:11:19,990 --> 00:11:22,920
将标记点之类的技术用于逻辑回归算法

315
00:11:23,880 --> 00:11:25,860
但是用于支持向量机

316
00:11:26,440 --> 00:11:28,110
的计算技巧

317
00:11:28,430 --> 00:11:30,700
不能较好的推广到其他算法诸如逻辑回归上

318
00:11:31,310 --> 00:11:33,110
所以 将核函数用于

319
00:11:33,260 --> 00:11:34,390
逻辑回归时

320
00:11:34,580 --> 00:11:36,330
会变得非常的慢

321
00:11:36,440 --> 00:11:37,940
相比之下 这些计算技巧

322
00:11:38,150 --> 00:11:39,490
比如具体化技术

323
00:11:39,900 --> 00:11:41,130
对这些细节的修改

324
00:11:41,320 --> 00:11:43,140
以及支持向量软件的实现细节

325
00:11:43,240 --> 00:11:44,990
使得支持向量机

326
00:11:45,300 --> 00:11:47,090
可以和核函数相得益彰

327
00:11:47,930 --> 00:11:49,450
而逻辑回归和核函数

328
00:11:50,250 --> 00:11:51,990
则运行得十分缓慢

329
00:11:52,890 --> 00:11:53,670
更何况它们还不能

330
00:11:53,750 --> 00:11:55,420
使用那些高级优化技巧

331
00:11:56,040 --> 00:11:57,360
因为这些技巧

332
00:11:57,530 --> 00:11:58,530
是人们专门为

333
00:11:59,140 --> 00:12:00,950
使用核函数的支持向量机开发的

334
00:12:01,540 --> 00:12:03,340
但是这些问题只有

335
00:12:03,710 --> 00:12:04,850
在你亲自实现最小化函数

336
00:12:05,230 --> 00:12:06,900
才会遇到

337
00:12:07,870 --> 00:12:08,940
我将在下一节视频中

338
00:12:09,040 --> 00:12:09,950
进一步讨论这些问题

339
00:12:10,150 --> 00:12:11,530
但是 你并不需要知道

340
00:12:12,200 --> 00:12:13,520
怎么去写一个软件

341
00:12:13,670 --> 00:12:14,890
来最小化代价函数

342
00:12:15,170 --> 00:12:17,560
你能找到很好的成熟软件来做这些

343
00:12:18,670 --> 00:12:19,890
就像我一直

344
00:12:20,140 --> 00:12:21,340
不建议自己写矩阵求逆函数

345
00:12:21,850 --> 00:12:22,960
或者平方根函数

346
00:12:23,150 --> 00:12:24,490
的道理一样

347
00:12:24,660 --> 00:12:26,420
我也不建议亲自写

348
00:12:26,560 --> 00:12:27,750
最小化代价函数的代码

349
00:12:28,240 --> 00:12:29,610
而应该使用

350
00:12:29,780 --> 00:12:31,490
人们开发的

351
00:12:31,740 --> 00:12:33,240
成熟的软件包

352
00:12:33,540 --> 00:12:35,140
这些软件包

353
00:12:35,790 --> 00:12:37,720
已经包含了那些数值优化技巧

354
00:12:39,540 --> 00:12:41,770
所以你不必担心这些东西

355
00:12:41,950 --> 00:12:42,920
但是另外一个

356
00:12:43,180 --> 00:12:45,200
值得说明的问题是

357
00:12:45,350 --> 00:12:46,400
在你使用支持向量机时

358
00:12:46,640 --> 00:12:47,730
怎么选择

359
00:12:47,820 --> 00:12:50,220
支持向量机中的参数？

360
00:12:51,520 --> 00:12:52,300
在本节视频的末尾

361
00:12:52,400 --> 00:12:53,290
我想稍微说明一下

362
00:12:53,450 --> 00:12:54,680
在使用支持向量机时的

363
00:12:54,840 --> 00:12:57,070
偏差-方差折中

364
00:12:57,900 --> 00:12:59,230
在使用支持向量机时

365
00:12:59,390 --> 00:13:00,670
其中一个要选择的事情是

366
00:13:00,960 --> 00:13:03,850
目标函数中的

367
00:13:04,090 --> 00:13:05,880
参数C

368
00:13:05,980 --> 00:13:07,690
回忆一下

369
00:13:07,770 --> 00:13:09,800
C的作用与1/λ相似

370
00:13:10,050 --> 00:13:11,750
λ是逻辑回归算法中

371
00:13:12,520 --> 00:13:13,970
的正则化参数

372
00:13:15,360 --> 00:13:16,760
所以

373
00:13:16,930 --> 00:13:18,760
大的C对应着

374
00:13:19,520 --> 00:13:20,560
我们以前在逻辑回归

375
00:13:21,270 --> 00:13:22,260
问题中的小的λ

376
00:13:22,670 --> 00:13:25,080
这意味着不使用正则化

377
00:13:25,980 --> 00:13:26,960
如果你这么做

378
00:13:27,050 --> 00:13:29,330
就有可能得到一个低偏差但高方差的模型

379
00:13:30,570 --> 00:13:31,420
如果你使用了

380
00:13:31,630 --> 00:13:33,050
较小的C

381
00:13:33,240 --> 00:13:34,510
这对应着

382
00:13:34,660 --> 00:13:36,450
在逻辑回归问题中

383
00:13:36,620 --> 00:13:38,090
使用较大的 λ

384
00:13:38,690 --> 00:13:40,180
对应着一个高偏差

385
00:13:40,470 --> 00:13:41,760
但是低方差的模型

386
00:13:42,580 --> 00:13:44,520
所以

387
00:13:45,000 --> 00:13:46,870
使用较大C值的模型

388
00:13:47,450 --> 00:13:48,380
为高方差

389
00:13:48,580 --> 00:13:50,290
更倾向于过拟合

390
00:13:50,450 --> 00:13:52,820
而使用较小C值的模型为高偏差

391
00:13:52,910 --> 00:13:54,900
更倾向于欠拟合

392
00:13:56,710 --> 00:13:59,870
C只是我们要选择的其中一个参数

393
00:14:00,210 --> 00:14:01,280
另外一个要选择的参数是

394
00:14:02,280 --> 00:14:04,580
高斯核函数中的σ^2 

395
00:14:05,760 --> 00:14:07,080
当高斯核函数中的

396
00:14:07,750 --> 00:14:09,370
σ^2偏大时

397
00:14:09,640 --> 00:14:11,350
那么对应的相似度函数

398
00:14:11,530 --> 00:14:12,710
为exp(-||x-l(i)||^2/(2*σ^2))

399
00:14:13,390 --> 00:14:14,710
exp(-||x-l(i)||^2/(2*σ^2))

400
00:14:16,280 --> 00:14:17,950
exp(-||x-l(i)||^2/(2*σ^2))

401
00:14:20,130 --> 00:14:21,290
在这个例子中

402
00:14:21,480 --> 00:14:23,330
如果我们只有一个特征x1

403
00:14:23,570 --> 00:14:25,390
我们在这个位置

404
00:14:25,490 --> 00:14:27,710
有一个标记点

405
00:14:27,960 --> 00:14:29,230
如果σ^2较大

406
00:14:29,480 --> 00:14:30,600
那么高斯核函数

407
00:14:30,690 --> 00:14:32,940
倾向于变得相对平滑

408
00:14:33,960 --> 00:14:34,740
这可能是我的特征fi

409
00:14:35,210 --> 00:14:36,690
所以

410
00:14:36,880 --> 00:14:38,970
由于函数平滑

411
00:14:39,060 --> 00:14:40,640
且变化的比较平缓

412
00:14:40,760 --> 00:14:42,750
这会给你的模型

413
00:14:43,030 --> 00:14:44,170
带来较高的偏差和较低的方差

414
00:14:44,550 --> 00:14:46,000
由于高斯核函数变得平缓

415
00:14:46,840 --> 00:14:48,240
就更倾向于得到一个

416
00:14:48,520 --> 00:14:50,060
随着输入x

417
00:14:50,130 --> 00:14:51,860
变化得缓慢的模型

418
00:14:52,050 --> 00:14:53,680
反之

419
00:14:54,030 --> 00:14:55,330
如果σ^2很小

420
00:14:55,660 --> 00:14:57,430
这是我的标记点

421
00:14:57,540 --> 00:14:58,830
利用其给出特征x1

422
00:14:58,960 --> 00:15:01,440
那么

423
00:15:01,820 --> 00:15:04,630
高斯核函数 即相似度函数会变化的很剧烈

424
00:15:05,310 --> 00:15:07,520
我们标记出这两种情况下1的位置

425
00:15:07,580 --> 00:15:08,550
在σ^2较小的情况下

426
00:15:08,870 --> 00:15:11,730
特征的变化会变得不平滑

427
00:15:12,190 --> 00:15:13,740
会有较大的斜率

428
00:15:14,250 --> 00:15:15,300
和较大的导数

429
00:15:16,020 --> 00:15:17,170
在这种情况下

430
00:15:17,330 --> 00:15:19,620
最终得到的模型会

431
00:15:19,840 --> 00:15:21,870
是低偏差和高方差

432
00:15:23,030 --> 00:15:24,460
看到这条曲线

433
00:15:24,680 --> 00:15:26,240
本周的编程作业

434
00:15:26,450 --> 00:15:27,230
你就能亲自实现这些想法

435
00:15:27,330 --> 00:15:29,480
并亲眼看到这些效果

436
00:15:31,590 --> 00:15:34,430
这就是利用核函数的支持向量机算法

437
00:15:35,320 --> 00:15:36,450
希望这些关于

438
00:15:37,090 --> 00:15:39,170
偏差和方差的讨论

439
00:15:39,310 --> 00:15:40,380
能给你一些

440
00:15:40,460 --> 00:15:42,600
对于算法结果预期的直观印象