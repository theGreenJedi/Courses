Às vezes se fala de máquinas de vetores de suporte como classificadores de margem larga. Neste vídeo, gostaria de dizer o que isso significa, o que também nos dará uma boa noção do que é uma hipótese de máquina de vetores de suporte (SVM, em inglês). Esta é minha função de custo para a máquina de vetores de suporte, onde à esquerda tracei minha função cost₁(z), que usei para exemplos positivos, e à direita tracei a função cost₀(z), onde 'z' está no eixo horizontal. Agora, vamos pensar sobre o que é necessário para que essas funções assumam um valor pequeno. Se você tiver um exemplo positivo, ou seja, se y = 1, então cost₁(x) é zero somente quando z ≥ 1. Em outras palavras, se você tiver um exemplo positivo, queremos que θ' · x seja maior ou igual a 1, e, da mesma forma, se y = 0, veja nesta função cost₀(z), é só nesta região, onde z é menor ou igual a -1, onde a função cost₀(z) é zero, e esta é uma propriedade interessante da máquina de vetores de suporte, pois, se tivermos um exemplo positivo, ou seja, y = 1, então tudo o que precisamos é que θ' · x ≥ 0. E isso significaria que classificamos corretamente, pois se θ' · x é maior que 0, nossa hipótese estimará zero. Da mesma forma, se você tiver um exemplo negativo, tudo o que você precisa é que θ' · x seja menor que 0, e já saberemos que acertamos nesse exemplo. Mas a máquina de vetores de suporte quer um pouco mais que isso. Ela te pede não só que você acerte o exemplo. Ou seja, não faça com que seja só um pouco maior que zero. O que realmente queremos é que isso seja um tanto maior que zero, por exemplo, um pouco maior ou igual 1, e queremos que isto seja muito menor que 0. Quero, na verdade, que ou igual a -1. Isso dá um novo fator de segurança, ou margem de segurança, para a máquina de vetores de suporte. Regressão logística faz algo similar também, claro, mas vamos ver o que acontece, quais são as consequências disso, no contexto da máquina de vetores de suporte. Na verdade, o que quero fazer a seguir é considerar um caso em que colocamos um valor muito alto para esta constante C, digamos que C tem um valor muito alto, talvez cem mil, um número gigante. Vamos ver o que a máquina de vetores de suporte vai fazer. Se C for muito, muito grande, então quando minimizarmos este objetivo de otimização, seremos muito motivados a escolher um valor que torne este primeiro termo igual a zero. Então, vamos tentar entender o problema de otimização no contexto do que seria necessário para fazer este primeiro termo do objetivo igual a zero, porque é próximo do que aconteceria se C for uma constante gigantesca, e esperamos que isso nos dê uma noção melhor sobre que tipo de hipótese uma máquina de vetores de suporte aprende. Nós já vimos que, sempre que temos um exemplo de treinamento indicado por y = 1, se você quer zerar esse primeiro termo, o que você precisa é encontrar um valor de θ tal que θ' · x ≥ 1. Da mesma forma, sempre que tivermos um exemplo com y = 0, para fazer com que o custo, cost₀(z), para garantir que esse custo seja 0, necessitamos que θ' · x⁽ⁱ⁾ seja menor ou igual a -1. Assim, se pensarmos no nosso problema de otimização como uma escolha de parâmetros, e com este primeiro termo igual a zero, o que resta é o problema de otimização a seguir. Vamos minimizar o primeiro termo, que será 0, então, como vamos escolher parâmetros que tornam esse termo igual a 0, ou seja, C · 0 mais 1/2 vezes este segundo termo fica C · 0, que podemos cortar porque sabemos que vai ser igual a 0. E isso será sujeito à restrição de que θ' · x⁽ⁱ⁾ se "h_θ(x) ≥ 0.5" um, quando y⁽ⁱ⁾ é igual a um, e θ' · x⁽ⁱ⁾ é menor ou igual a -1 quando o exemplo é negativo, e o que acontece quando você soluciona esse problema de otimização, quando você minimiza essa função dos parâmetros θ você encontra uma fronteira de decisão muito interessante. Se você olhara para um conjunto de dados como este, com exemplos positivos e negativos, os dados são linearmente separáveis. Ou seja, existe pelo menos uma linha reta, ou podem existir várias delas, que conseguem separar os exemplos positivos e negativos perfeitamente. Por exemplo, aqui está uma fronteira de decisão que separa os exemplos positivos e negativos, mas ela não parece ser muito natural. Podemos desenhar uma ainda pior, por exemplo, esta é outra fronteira de decisão que separa os exemplos positivos e negativos, mas por pouco. Mas nenhuma dessas parecem ser escolhas muito boas. A máquina de vetores de suporte vai escolher esta fronteira de decisão que estou desenhando em preto. Essa parece ser uma fronteira de decisão muito melhor que qualquer uma das outras, as que desenhei em magenta e verde. Esta linha preta parece ser um separador mais robusto, ela separa melhor os exemplos positivos e negativos. E, matematicamente, o que diferencia a fronteira de decisão em preto é que ela tem uma distância maior. Essa distância é chamada margem, quando eu desenho essas duas retas azuis a mais, vemos que a fronteira de decisão em preto tem uma distância mínima maior até qualquer dos exemplos de treinamento, enquanto as retas magenta e verde ficam muito perto dos exemplos de treinamento. Isso parece separar os exemplos positivos e negativos de uma maneira pior que a a linha preta. Então, essa é a distância em linha reta entre duas cidades. essa distância é chamada a margem da máquina de vetores de suporte, o que dá à SVM uma certa robustez, porque ela tenta separar os dados com a maior margem possível. Assim, a máquina de vetores de suporte é também chamada um classificador de margem larga, o que é uma consequência do problema de otimização que escrevemos no slide anterior. Eu sei que você deve estar imaginando como é que o problema de otimização que escrevi no slide anterior leva a esse classificador de margem larga. Eu sei que ainda não expliquei isso. No próximo vídeo vou tentar dar um pouco da intuição sobre a razão por que esse problema de otimização nos dá esse classificador de margem larga. Mas essa é uma característica útil para ter em mente se você está tentando entender que tipo de hipótese uma SVM escolherá. Ou seja, tentar separar os exemplos positivos e negativos com a maior margem possível. Eu quero dizer uma última coisa sobre classificadores de margem larga e essa intuição, então vamos ver o que esse classificador de margem larga faz no caso em que C, essa constante de regularização, for muito grande, acho que coloquei o valor de 100000 ou algo assim. Dado um conjunto de dados como este, talvez escolheremos a fronteira de decisão que separa exemplos positivos e negativos com margem larga. Na verdade, a SVM é um pouco mais sofisticada que essa visão de margem larga pode induzir a pensar. Em particular, se tudo o que você está fazendo é usar um classificador de margem larga, seus algoritmos de aprendizagem podem ser sensíveis a outliers, então vamos só adicionar um exemplo positivo como este mostrado na tela. Se adicionamos esse exemplo, parece que para podermos separar os dados com uma margem larga, vou acabar aprendendo uma fronteira de decisão como esta, certo? A fronteira é esta linha magenta, e Não é nada claro que, baseados em um único outlier, baseados em um único exemplo, não está nem um pouco claro que é uma boa ideia mudar a fronteira de decisão da linha preta para a magenta. Portanto, se C, o parâmetro de regularização, for muito grande, isso é exatamente o que a SVM fará, ela mudará a fronteira de decisão da linha preta para a linha magenta, mas se C for razoavelmente pequeno, se utilizássemos um valor de C que não fosse tão grande assim, você encontraria esta fronteira de decisão em preto. E, é claro, se os dados não forem linearmente separáveis, se você tiver alguns exemplos positivos aqui, ou alguns exemplos negativos aqui, a SVM também fará o que é certo. Assim, essa ideia do classificador de margem larga é, na verdade, a ideia que dá uma boa intuição somente para os casos em que o parâmetro de regularização C é muito largo. Só para lembrar, essa constante C tem um papel semelhante ao 1/λ, onde λ é o parâmetro de regularização que tínhamos visto anteriormente. Assim, somente quando 1/λ é muito grande, ou, equivalentemente, quando λ é muito pequeno, que aprendemos coisas como esta fronteira de decisão em magenta, mas, na prática, quando aplicamos máquinas de vetores de suporte, onde C não é tão, tão grande como aquilo, elas conseguem fazer um trabalho melhor em ignorar os poucos outliers como esses. Elas também se dão bem, são coerentes, mesmo que seus dados não sejam linearmente separáveis. Mas quando pensamos em viés e variância no contexto de máquinas de vetores de suporte, o que faremos um pouco mais tarde, espero que todos esses efeitos que envolvem o parâmetro de regularização se tornarão mais claros quando falarmos sobre isso. Espero que o vídeo tenha te dado alguma intuição sobre como a máquina de vetores de suporte funciona como um classificador de margem larga, que tenta separar os dados com uma margem larga, tecnicamente essa interpretação é verdadeira somente quando o parâmetro C é muito grande, mas é uma forma útil de pensar sobre máquinas de vetores de suporte. Ainda temos um passo que não expliquei neste vídeo, que é por que o problema de otimização que escrevi nesses slides acaba levando a um classificador de margem larga. Eu não fiz isso neste vídeo, mas no próximo vou explicar um pouco mais da matemática por trás disso, explicarei esse raciocínio sobre como o problema de otimização que vimos resulta em um classificador de margem larga.