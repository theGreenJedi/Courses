1
00:00:00,750 --> 00:00:02,160
人们有时将支持向量机

2
00:00:02,520 --> 00:00:04,380
看做是大间距分类器

3
00:00:04,990 --> 00:00:06,950
在这一部分

4
00:00:07,080 --> 00:00:08,030
我将介绍其中的含义

5
00:00:08,410 --> 00:00:09,500
这有助于我们

6
00:00:09,780 --> 00:00:10,520
直观理解

7
00:00:11,030 --> 00:00:12,780
SVM模型的

8
00:00:13,020 --> 00:00:17,460
假设是什么样的

9
00:00:18,070 --> 00:00:19,290
这是我的支持向量机模型的代价函数

10
00:00:21,310 --> 00:00:22,290
在左边这里

11
00:00:22,790 --> 00:00:24,300
我画出了关于 z 的代价函数 cost1(z)

12
00:00:24,560 --> 00:00:28,100
此函数用于正样本 而在右边这里我画出了

13
00:00:30,080 --> 00:00:31,510
关于 z 的代价函数 cost0(z)

14
00:00:31,950 --> 00:00:33,850
横轴表示 z

15
00:00:34,380 --> 00:00:35,520
现在让我们考虑一下

16
00:00:35,650 --> 00:00:38,380
最小化这些代价函数的必要条件是什么

17
00:00:39,660 --> 00:00:40,970
如果你有一个正样本

18
00:00:41,950 --> 00:00:43,170
y等于1

19
00:00:43,490 --> 00:00:45,060
则只有在 z 大于等于1时

20
00:00:45,200 --> 00:00:46,750
代价函数 cost1(z)

21
00:00:47,700 --> 00:00:50,070
才等于0

22
00:00:50,180 --> 00:00:51,370
换句话说

23
00:00:51,510 --> 00:00:52,860
如果你有一个正样本

24
00:00:53,110 --> 00:00:54,550
我们会希望 θ 转置乘以 x

25
00:00:54,870 --> 00:00:55,760
大于等于1

26
00:00:56,450 --> 00:00:58,030
反之

27
00:00:58,150 --> 00:00:59,300
如果 y 是等于0的

28
00:00:59,510 --> 00:01:00,490
我们观察一下

29
00:01:01,560 --> 00:01:03,000
函数cost0(z)

30
00:01:03,200 --> 00:01:04,310
它只有在

31
00:01:04,460 --> 00:01:05,810
z小于等于1

32
00:01:06,150 --> 00:01:07,320
的区间里

33
00:01:07,610 --> 00:01:10,150
函数值为0

34
00:01:10,640 --> 00:01:12,270
这是支持向量机的

35
00:01:12,560 --> 00:01:13,630
一个有趣性质 不是么

36
00:01:13,800 --> 00:01:15,060
事实上

37
00:01:15,440 --> 00:01:17,650
如果你有一个正样本 y等于1

38
00:01:18,370 --> 00:01:19,250
则其实我们仅仅要求

39
00:01:19,550 --> 00:01:21,950
θ 转置乘以 x 大于等于0

40
00:01:22,970 --> 00:01:25,270
就能将该样本恰当分出

41
00:01:25,860 --> 00:01:26,950
这是因为如果 θ 转置乘以 x 比0大的话

42
00:01:27,510 --> 00:01:28,980
我们的模型代价函数值为0

43
00:01:29,840 --> 00:01:30,710
类似地 如果你有一个负样本

44
00:01:31,340 --> 00:01:34,090
则仅需要 θ 转置乘以x

45
00:01:34,850 --> 00:01:37,290
小于等于0 就会将负例正确分离

46
00:01:37,670 --> 00:01:40,230
但是 支持向量机的要求更高

47
00:01:40,580 --> 00:01:43,360
不仅仅要能正确分开输入的样本

48
00:01:44,320 --> 00:01:45,990
即不仅仅

49
00:01:46,240 --> 00:01:47,580
要求 θ 转置乘以 x 大于0

50
00:01:47,890 --> 00:01:48,870
我们需要的是

51
00:01:49,060 --> 00:01:50,370
比0值大很多

52
00:01:50,490 --> 00:01:51,430
比如

53
00:01:51,680 --> 00:01:52,530
大于等于1

54
00:01:52,870 --> 00:01:54,400
我也想这个比0小很多

55
00:01:54,800 --> 00:01:55,970
比如我希望它

56
00:01:56,230 --> 00:01:58,140
小于等于-1

57
00:01:58,830 --> 00:02:00,000
这就相当于在支持向量机中嵌入了

58
00:02:00,120 --> 00:02:01,660
一个额外的安全因子

59
00:02:02,070 --> 00:02:03,630
或者说安全的间距因子

60
00:02:04,030 --> 00:02:05,700
当然 逻辑回归

61
00:02:06,340 --> 00:02:07,620
做了类似的事情

62
00:02:07,820 --> 00:02:08,900
但是让我们看一下

63
00:02:09,110 --> 00:02:10,350
在支持向量机中

64
00:02:10,460 --> 00:02:11,290
这个因子会

65
00:02:11,360 --> 00:02:13,180
导致什么结果

66
00:02:14,830 --> 00:02:15,740
具体而言 我接下来

67
00:02:16,010 --> 00:02:17,760
会考虑一个特例

68
00:02:17,900 --> 00:02:19,130
我们将这个常数 C

69
00:02:19,460 --> 00:02:21,240
设置成

70
00:02:21,400 --> 00:02:23,340
一个非常大的值

71
00:02:23,530 --> 00:02:24,700
比如我们假设

72
00:02:24,820 --> 00:02:28,080
C的值为100000 或者其它非常大的数

73
00:02:29,370 --> 00:02:31,290
然后来观察支持向量机会给出什么结果

74
00:02:31,580 --> 00:02:33,510
如果 C 非常大

75
00:02:33,820 --> 00:02:35,340
则最小化代价函数的时候

76
00:02:36,350 --> 00:02:38,080
我们将会很希望

77
00:02:38,300 --> 00:02:39,640
找到一个

78
00:02:39,950 --> 00:02:41,240
使第一项为0的

79
00:02:41,380 --> 00:02:43,180
最优解

80
00:02:44,810 --> 00:02:46,250
因此

81
00:02:46,670 --> 00:02:48,320
让我们

82
00:02:48,430 --> 00:02:49,820
尝试在

83
00:02:50,050 --> 00:02:51,520
代价项的第一项

84
00:02:51,880 --> 00:02:53,060
为0的情形下理解

85
00:02:53,470 --> 00:02:54,890
该优化问题

86
00:02:55,000 --> 00:02:56,100
比如我们可以把 C 

87
00:02:56,250 --> 00:02:59,420
设置成了非常大的常数

88
00:02:59,590 --> 00:03:00,780
这将给我们

89
00:03:01,300 --> 00:03:02,920
一些关于支持向量机

90
00:03:03,110 --> 00:03:05,520
模型的直观感受

91
00:03:06,440 --> 00:03:07,720
我们已经看到

92
00:03:08,140 --> 00:03:09,260
输入一个训练样本

93
00:03:09,480 --> 00:03:11,350
标签为

94
00:03:11,690 --> 00:03:13,850
y=1

95
00:03:13,950 --> 00:03:15,050
你想令第一项为0

96
00:03:15,240 --> 00:03:16,280
你需要做的是

97
00:03:16,450 --> 00:03:17,680
找到一个 θ

98
00:03:17,990 --> 00:03:20,380
使得 θ 转置乘以 x

99
00:03:20,690 --> 00:03:22,800
大于等于1

100
00:03:23,220 --> 00:03:24,250
类似地 对于一个训练样本

101
00:03:24,960 --> 00:03:26,910
标签为 y=0

102
00:03:27,240 --> 00:03:28,060
为了使

103
00:03:29,000 --> 00:03:30,520
cost0(z) 函数

104
00:03:30,610 --> 00:03:31,530
这个函数

105
00:03:31,790 --> 00:03:33,250
值为0 我们需要 θ 转置

106
00:03:33,810 --> 00:03:36,180
乘以x 的值

107
00:03:37,900 --> 00:03:38,740
小于等于-1

108
00:03:39,510 --> 00:03:40,770
因此

109
00:03:41,050 --> 00:03:43,030
现在考虑我们的优化问题

110
00:03:43,360 --> 00:03:45,000
选择参数

111
00:03:45,710 --> 00:03:46,750
使得第一项

112
00:03:47,020 --> 00:03:48,170
等于0

113
00:03:49,130 --> 00:03:50,230
就会导致下面的

114
00:03:50,330 --> 00:03:51,670
优化问题

115
00:03:52,050 --> 00:03:53,720
因为我们将

116
00:03:53,980 --> 00:03:55,360
选择参数使第一项为0

117
00:03:55,590 --> 00:03:56,710
因此这个函数的第一项为0

118
00:03:56,870 --> 00:03:58,040
因此是 C 乘以0

119
00:03:58,150 --> 00:03:59,710
加上二分之一

120
00:04:00,330 --> 00:04:01,330
乘以第二项

121
00:04:01,460 --> 00:04:05,440
这里第一项

122
00:04:05,620 --> 00:04:06,880
是C乘以0

123
00:04:07,160 --> 00:04:08,020
因此可以将其删去

124
00:04:08,130 --> 00:04:11,210
因为我知道它是0

125
00:04:11,380 --> 00:04:12,570
这将遵从以下的约束

126
00:04:13,400 --> 00:04:15,410
θ 转置乘以 x(i)

127
00:04:16,390 --> 00:04:17,560
大于或等于0.5

128
00:04:18,700 --> 00:04:20,930
如果 y (i)

129
00:04:22,180 --> 00:04:24,150
是等于1 的

130
00:04:24,940 --> 00:04:26,560
θ 转置乘以x(i)

131
00:04:26,690 --> 00:04:28,060
小于等于-1

132
00:04:29,030 --> 00:04:31,680
如果样本i是

133
00:04:32,110 --> 00:04:34,460
一个负样本

134
00:04:34,540 --> 00:04:35,520
这样 当你

135
00:04:35,660 --> 00:04:37,930
求解这个优化问题的时候

136
00:04:38,070 --> 00:04:39,440
当你最小化这个关于变量 θ 的函数的时候

137
00:04:40,710 --> 00:04:42,090
你会得到一个非常有趣的决策边界

138
00:04:42,590 --> 00:04:44,870
具体而言

139
00:04:45,010 --> 00:04:46,470
如果你考察

140
00:04:46,750 --> 00:04:49,660
这样一个数据集 其中有正样本 也有负样本

141
00:04:50,920 --> 00:04:52,430
可以看到 这个数据集是线性可分的

142
00:04:52,710 --> 00:04:54,960
我的意思是 存在一条直线把正负样本分开

143
00:04:55,530 --> 00:04:56,830
当然有多条不同的直线

144
00:04:56,920 --> 00:04:57,810
可以把

145
00:04:58,720 --> 00:05:01,060
正样本和负样本完全分开

146
00:05:01,560 --> 00:05:02,710
比如 这就是一个决策边界

147
00:05:04,270 --> 00:05:05,430
可以把正样本

148
00:05:05,570 --> 00:05:06,840
和负样本分开

149
00:05:07,030 --> 00:05:07,810
但是多多少少这个

150
00:05:07,900 --> 00:05:09,680
看起来并不是非常自然 是么?

151
00:05:09,810 --> 00:05:11,050
或者我们可以画一条更差的决策界

152
00:05:11,230 --> 00:05:13,540
这是另一条决策边界

153
00:05:13,710 --> 00:05:14,830
可以将正样本和负样本分开

154
00:05:14,900 --> 00:05:15,960
但仅仅是勉强分开

155
00:05:16,120 --> 00:05:18,530
这些决策边界看起来都不是特别好的选择

156
00:05:20,420 --> 00:05:22,880
支持向量机将会选择

157
00:05:23,140 --> 00:05:26,450
这个黑色的决策边界

158
00:05:29,010 --> 00:05:30,030
相较于之前

159
00:05:30,760 --> 00:05:32,310
我用粉色或者绿色

160
00:05:32,420 --> 00:05:34,450
画的决策界 这条黑色的看起来好得多

161
00:05:34,750 --> 00:05:35,790
黑线看起来

162
00:05:36,050 --> 00:05:37,840
是更稳健的决策界

163
00:05:38,610 --> 00:05:39,710
在分离正样本和负样本上它显得的更好

164
00:05:39,800 --> 00:05:42,830
数学上来讲 这是什么意思呢

165
00:05:43,530 --> 00:05:45,680
这条黑线有更大的距离

166
00:05:49,160 --> 00:05:50,580
这个距离叫做间距 (margin)

167
00:05:50,760 --> 00:05:51,790
当画出这两条

168
00:05:52,380 --> 00:05:54,320
额外的蓝线

169
00:05:54,540 --> 00:05:56,010
我们看到黑色的决策界

170
00:05:56,240 --> 00:05:59,990
和训练样本之间有更大的最短距离

171
00:06:00,120 --> 00:06:01,350
然而粉线和蓝线

172
00:06:01,580 --> 00:06:02,600
离训练样本就非常近

173
00:06:04,640 --> 00:06:06,100
在分离样本的时候就会

174
00:06:06,500 --> 00:06:08,910
比黑线表现差

175
00:06:09,850 --> 00:06:11,500
因此这张图片本身就有一定的误导性

176
00:06:11,800 --> 00:06:13,600
这个距离叫做

177
00:06:13,960 --> 00:06:16,500
支持向量机的

178
00:06:16,600 --> 00:06:21,300
间距

179
00:06:21,500 --> 00:06:22,480
而这是支持向量机

180
00:06:22,940 --> 00:06:24,010
具有鲁棒性的原因

181
00:06:24,360 --> 00:06:25,530
因为它努力

182
00:06:25,700 --> 00:06:27,440
用一个最大间距来分离样本

183
00:06:29,210 --> 00:06:30,250
因此支持向量机

184
00:06:30,380 --> 00:06:31,650
有时被称为

185
00:06:31,830 --> 00:06:33,930
大间距分类器

186
00:06:34,170 --> 00:06:36,180
而这其实是

187
00:06:36,430 --> 00:06:39,370
求解上一页幻灯片上优化问题的结果

188
00:06:40,140 --> 00:06:40,950
我知道你也许

189
00:06:41,100 --> 00:06:42,250
想知道

190
00:06:42,400 --> 00:06:43,900
求解上一页幻灯片中的优化问题

191
00:06:44,070 --> 00:06:45,080
为什么会产生这个结果

192
00:06:45,280 --> 00:06:47,270
它是如何产生这个大间距分类器的呢

193
00:06:48,350 --> 00:06:49,700
我知道我还没有解释这一点

194
00:06:50,520 --> 00:06:51,570
在下一节视频中

195
00:06:51,810 --> 00:06:53,340
我将会从直观上

196
00:06:53,500 --> 00:06:55,180
略述 为什么

197
00:06:55,430 --> 00:06:57,080
这个优化问题

198
00:06:57,570 --> 00:06:59,630
会产生大间距分类器

199
00:06:59,790 --> 00:07:00,860
总之这个图示

200
00:07:00,970 --> 00:07:01,780
有助于你

201
00:07:01,920 --> 00:07:03,150
理解

202
00:07:03,290 --> 00:07:05,600
支持向量机模型的做法

203
00:07:06,140 --> 00:07:07,200
即努力将正样本和负样本

204
00:07:07,270 --> 00:07:10,310
用最大的间距分开

205
00:07:12,890 --> 00:07:13,950
在本节课中

206
00:07:14,180 --> 00:07:15,930
关于大间距分类器

207
00:07:16,070 --> 00:07:17,900
我想讲最后一点

208
00:07:18,030 --> 00:07:19,340
我们将这个大间距分类器

209
00:07:20,010 --> 00:07:21,040
中的正则化因子

210
00:07:21,420 --> 00:07:23,640
常数C

211
00:07:24,160 --> 00:07:25,190
设置的非常大

212
00:07:25,390 --> 00:07:27,750
我记得我将其设置为了100000

213
00:07:28,310 --> 00:07:29,760
因此对这样的一个数据集

214
00:07:30,110 --> 00:07:31,630
也许我们将选择

215
00:07:32,110 --> 00:07:34,000
这样的决策界从而最大间距地

216
00:07:34,140 --> 00:07:36,210
分离开正样本和负样本

217
00:07:37,370 --> 00:07:39,020
事实上 支持向量机现在

218
00:07:39,370 --> 00:07:41,120
要比这个大间距分类器所体现的

219
00:07:41,440 --> 00:07:42,920
更成熟

220
00:07:43,630 --> 00:07:45,130
尤其是当你使用

221
00:07:45,310 --> 00:07:46,490
大间距分类器的时候

222
00:07:46,680 --> 00:07:48,850
你的学习算法

223
00:07:49,020 --> 00:07:50,270
会受异常点 (outlier) 的影响

224
00:07:50,920 --> 00:07:52,260
比如我们加入

225
00:07:52,450 --> 00:07:53,990
一个额外的正样本

226
00:07:54,520 --> 00:07:56,540
在这里

227
00:07:57,230 --> 00:07:58,830
如果你加了这个样本

228
00:07:58,950 --> 00:08:00,060
为了将样本

229
00:08:00,300 --> 00:08:01,410
用最大间距分开

230
00:08:02,680 --> 00:08:04,300
也许我最终

231
00:08:05,270 --> 00:08:07,260
会得到一条类似这样的决策界 对么?

232
00:08:07,540 --> 00:08:09,130
就是这条粉色的线

233
00:08:09,180 --> 00:08:10,210
仅仅基于

234
00:08:10,440 --> 00:08:11,950
一个异常值

235
00:08:12,180 --> 00:08:13,560
仅仅基于一个样本

236
00:08:13,790 --> 00:08:14,720
就将

237
00:08:14,890 --> 00:08:16,460
我的决策界

238
00:08:17,060 --> 00:08:17,980
从这条黑线变到这条粉线

239
00:08:18,290 --> 00:08:19,960
这实在是不明智的

240
00:08:20,980 --> 00:08:23,430
而如果正则化参数 C

241
00:08:23,640 --> 00:08:25,740
设置的非常大

242
00:08:25,970 --> 00:08:27,110
这事实上正是

243
00:08:27,300 --> 00:08:28,130
支持向量机将会做的

244
00:08:28,360 --> 00:08:29,820
它将决策界

245
00:08:30,270 --> 00:08:31,530
从黑线

246
00:08:31,650 --> 00:08:33,650
变到了粉线

247
00:08:33,810 --> 00:08:35,390
但是如果 C 设置的小一点

248
00:08:35,550 --> 00:08:36,720
如果你将 C

249
00:08:37,320 --> 00:08:39,090
设置的不要太大

250
00:08:39,260 --> 00:08:40,400
则你最终会得到

251
00:08:40,610 --> 00:08:44,500
这条黑线

252
00:08:44,830 --> 00:08:46,880
当然数据如果不是线性可分的

253
00:08:47,250 --> 00:08:48,790
如果你在这里

254
00:08:49,170 --> 00:08:50,440
有一些正样本 或者

255
00:08:50,980 --> 00:08:52,300
你在这里有一些负样本

256
00:08:52,570 --> 00:08:53,830
则支持向量机也会将它们恰当分开

257
00:08:54,260 --> 00:08:55,710
因此

258
00:08:56,060 --> 00:08:57,770
大间距分类器的描述

259
00:08:58,090 --> 00:08:59,410
真的

260
00:08:59,530 --> 00:09:01,720
仅仅是从直观上给出了

261
00:09:01,970 --> 00:09:03,440
正则化参数 C

262
00:09:03,560 --> 00:09:05,050
非常大的情形

263
00:09:05,190 --> 00:09:07,170
同时

264
00:09:07,420 --> 00:09:08,810
要提醒你 C 的作用

265
00:09:09,650 --> 00:09:11,300
类似于

266
00:09:11,850 --> 00:09:13,600
λ 分之一 λ 是

267
00:09:13,930 --> 00:09:15,950
我们之前使用过

268
00:09:16,110 --> 00:09:17,970
的正则化参数

269
00:09:18,080 --> 00:09:18,880
这只是C非常大的情形

270
00:09:19,080 --> 00:09:21,060
或者等价地

271
00:09:21,280 --> 00:09:23,110
λ 非常小的情形

272
00:09:23,560 --> 00:09:24,640
你最终会得到

273
00:09:24,850 --> 00:09:27,600
类似粉线这样的决策界

274
00:09:28,870 --> 00:09:29,560
但是实际上 应用支持向量机的时候

275
00:09:30,190 --> 00:09:31,620
当 C 不是

276
00:09:31,910 --> 00:09:33,180
非常非常大的时候

277
00:09:34,840 --> 00:09:36,390
它可以忽略掉一些异常点的影响

278
00:09:36,980 --> 00:09:38,590
得到更好的决策界

279
00:09:39,150 --> 00:09:40,320
甚至当你的数据不是线性可分的时候

280
00:09:40,620 --> 00:09:44,400
支持向量机也可以给出好的结果

281
00:09:44,690 --> 00:09:46,810
我们稍后会介绍一点

282
00:09:46,980 --> 00:09:47,990
支持向量机的偏差和方差

283
00:09:48,170 --> 00:09:50,170
希望在那时候

284
00:09:50,410 --> 00:09:51,990
关于如何处理参数的这种平衡会变得

285
00:09:52,410 --> 00:09:53,710
更加清晰

286
00:09:53,830 --> 00:09:55,280
我希望

287
00:09:55,580 --> 00:09:57,290
这节课给出了一些

288
00:09:57,600 --> 00:09:59,680
关于为什么支持向量机

289
00:09:59,850 --> 00:10:01,810
被看做大间距分类器的直观理解

290
00:10:01,950 --> 00:10:03,040
它用最大间距将样本区分开

291
00:10:03,610 --> 00:10:05,210
尽管从技术上讲

292
00:10:06,140 --> 00:10:07,160
这只有当

293
00:10:07,460 --> 00:10:08,710
参数C是非常大的时候是真的

294
00:10:10,230 --> 00:10:11,720
但是它对于理解支持向量机是有益的

295
00:10:13,120 --> 00:10:14,450
本节课中 我们略去了一步

296
00:10:14,560 --> 00:10:15,990
那就是我们在幻灯片中

297
00:10:16,110 --> 00:10:17,670
给出的优化问题

298
00:10:17,770 --> 00:10:18,770
为什么会是这样的

299
00:10:19,040 --> 00:10:19,930
它是如何

300
00:10:20,740 --> 00:10:22,490
得出大间距分类器的

301
00:10:22,600 --> 00:10:23,520
我在本节中没有讲解

302
00:10:23,930 --> 00:10:25,830
在下一节课中

303
00:10:25,870 --> 00:10:26,940
我将略述

304
00:10:27,120 --> 00:10:28,370
这些问题背后的数学原理

305
00:10:28,750 --> 00:10:29,750
来解释

306
00:10:29,850 --> 00:10:31,660
这个优化问题

307
00:10:31,930 --> 00:10:33,410
是如何

308
00:10:33,840 --> 00:10:34,990
得到一个大间距分类器的【教育无边界字幕组】翻译: shamolvzhou 校对/审核: 所罗门捷列夫