ここまでで、様々な 学習アルゴリズムを見てきた。
教師有り学習の中では、 それぞれの学習アルゴリズム同士のパフォーマンスは とても似通っていて、 学習アルゴリズムAを使うか 学習アルゴリズムBを使うかの違いは 重要で無い事が多い。 それよりも重要になる事が多いのは これらのアルゴリズムを 適用する対象のデータの量とか、 または学習アルゴリズムを適用するスキル、 例えば学習アルゴリズムに 与えるフィーチャーの選択とか 正規化パラメータをどう選ぶかとか そういった物の方が 重要な事が 多い。 だが、それでももう一つ、 とても強力で、 業界でもアカデミアでも良く使われている アルゴリズムがある。 それはサポートベクターマシーンと呼ばれていて、 ロジスティック回帰やニューラルネットワークと比べて、 サポートベクターマシーン、 またの名をSVMは、 複雑な非線形の関数を学習する方法として、 場合によってはより明解で、 よりパワフルな事がある。 だから次のビデオで、 それについて 話したい。 このコースの後半で、 様々な教師有り学習アルゴリズムの 簡単なサーベイを行い、 とても簡潔にそれらを紹介するつもりだ。 だがサポートベクターマシーンは その人気があまりにも大きいので、 教師有り学習アルゴリズムの 最後として、 このコースの中のそれなりの時間を費やしたいと思う。 ここまでの学習アルゴリズムの開発と同様、 最適化の目的関数から 始めたいと思う。 ではこのアルゴリズムを 始めよう。 サポートベクターマシーンを記述する為に、 まずはロジスティック回帰から 始めて、 それをちょこっと変更して 本質的にはサポートベクターマシーンが得られる やり方をお見せしたい。 ではロジスティック回帰において、 今や見慣れた仮説の形が これで、そしてsigmoidアクティベーション関数が 右に示してある。 そして数学をいくつか 説明する為に、 zをシータ転置のxを表すのに使う。 では、ロジスティック回帰において 我らが何をするかを見てみよう。 手本があって、 y=1とする、 これの意味は、 トレーニングセットなりテストセットなり クロスバリデーションセットにおいて、y=1という事で、 それはようするに、h(x)が1に近い事を期待する、という事を意味する。 つまり、手本を 正しく分類する事を望んでいて、 h(x)が1に近いことは、 シータ転置のxが 0よりも ずっと大きくなければならない事を意味する。 これは大なり大なりの 記号で、 0よりも、とってもとっても 大きい事を意味する。 そしてそれはとりもなさず、z、つまり シータ転置のxが 0よりもずっと大きい時 この図で 遥か右に位置するという事で、 ロジスティック回帰の出力は1に近くなるという事を意味する。 逆に、 y=0の手本の時は 期待する事は 仮説が0に近い値を 出力する事で それはシータ転置のxが つまりzが、 0よりもずっと小さいという事だ。 何故ならそれに対応する 仮説の出力の値は0に近いから。 ここでロジスティック回帰の コスト関数を見てみると、 見られる結果は 各手本、x、yが このような項として 全体のコストに貢献している。 つまりコスト関数の全体としては、普通は トレーニング手本全体に渡る 和があり、さらに1/mの項もある。 だがここのこの式は、 これこそが、 一つのトレーニング手本の寄与の 項だ、 ロジスティック回帰の目的関数全体への。 今、この仮説の定義の式を取り、 ここに 代入する。 得られた物は、 各トレーニング手本の寄与はこの項だ。 1/mは無視してるが、 この項がロジスティック回帰の 全体のコスト関数への 寄与だ。 今、2つの場合を考えてみよう: y=1の時と y=0の時。 最初のケースとして、 y=1の時を考えよう。 この場合は、 この目的関数の 最初の項だけが重要だ、何故なら この1-yの項は0になるから、 y=1の時は。 つまりy=1の時は 手本x, yの、 yが1の時には、 我らが得るのは この項、 - logの 1+eの-z乗 分の一。 ここで一つ前のスライドと同様、 zをシータ転置xを表すのに 使っている。 もちろん、コストでは 実際はこの -y があるはずだが、 今言ったように、y=1の場合だ。 だからそれは1だ。 それを単に ここに書いたように 整理しただけ。 そしてこの関数をzの関数として、 プロットすると、 こんな曲線が この左下に描いた この線が見られる。 こうして、 zがとても大きい時は つまりシータ転置xが 大きい場合は、 とても小さい値、 コスト関数に ちょっとしか寄与しない zに対応する。 これは、何故 ロジスティック回帰において、 陽性の手本で y =1を見たら シータ転置xに とても大きな値を入れたがる、ある種の説明になっている。 何故ならそれは対応するコスト関数の中の この項が、とても小さくなる事を意味するから。 ここで、サポートベクターマシンを構築する為に、これがやるべき事だ。 このコスト関数取って、 この-log の 1足すe の -z乗 分の一 を、ちょびっと変更する。 この点、 ここにある1を取り、 今後使うコスト関数を書いてみよう、 新しいコスト関数は ここからフラットになり、 そして成長の仕方は 直線で描く、 ロジスティック回帰に 似ているが、しかしこれは 直線。 つまり、マゼンタで今描いた 曲線。
紫というかマゼンダで描いた曲線。 つまりこれは、 ロジスティック回帰で使っていたコスト関数に 極めて近い近似となっている。 2つの線分から 構成されている所が違うが。 右側にはこのフラットな部分があり、 そしてこの左側には 直線の部分がある。 そして直線部分の傾きについてはあんま気にしないでくれ。 それはそんなには重要じゃない。 以上がy=1のときに使う事になる 新しいコスト関数だ。 そして想像できると思うが ロジスティック回帰と極めて似た事をやっていく事になる。 だがやがて明らかになるが、これは サポートベクターマシンの計算的な優位である、 より簡単な最適化問題を あとで与えてくれる事となる。 それは株の取引などにより容易に応用出来る。 ここまではy=1の場合だけを 話してきた。 もう一方のケース、y=0、 この場合は、 コスト関数を見てみると、 この二番目の項だけが 適用される、何故なら 最初の項は、y=0の時は 消え去るから。 つまりここは0となる。 だから上の式で 二番目の項だけが残る。 だから手本のコスト、 つまりコスト関数へと寄与は ここの、この項で 与えられる。 そしてそれをzの関数として プロットすると、、、 だからここで横軸に zを取って、最終的には このカーブとなる。 そしてサポートベクターマシンの為、 ふたたびこの青い線を 似たような物で置き換える。 そして新しいコストで それを置き換えると、 ここは平坦となる。ここは0で、 その後は直線で増加していく。 こんな感じ。 では、これら2つの関数に 名前をつけよう。 この左の関数を cost下付き添字1のzと 呼ぶ。 そしてこの右側の関数を、 cost下付き添字0のzと呼ぶ。 ここで下付き添字は単に y=1に対応しているコストか、 またはy=0に対応しているコストかを示しているに過ぎない。 これらの定義で武装したので、 サポートベクターマシンを構築する準備は整った！ これはロジスティック回帰の コスト関数、Jのシータだ。 この方程式が ちょっと見慣れない、と感じたとしたら、 それは前回はマイナスの符号を 外に置いていた。 だがここでは、マイナスの符号を この式の中に 移動した。 だからちょっと違って見えるかもしれない。 サポートベクターマシンの為に 我らがやる事は 本質的にはここを、 cost1のzで 置き換える。 これはcost1のシータ転置x。 そしてこれを cost0のzで置き換える。 これはcost0のシータ転置xで、 ここで cost1関数は 前のスライドで見た奴で、 こんな感じで、 cost0関数もまた、 前のスライドで見た奴で、 こんな感じの奴。 サポートベクターマシンにおいて、 我らがやるのは、 1/mの和を トレーニング手本に渡って取ることの、 y(i)掛けるcost1の シータ転置x(i) 足すことの 1-y(i) 掛けるcost0のシータ転置x(i)。 そしてさらに、 いつもの正規化パラメータ、 こんな感じ。 ここでサポートベクターマシーンの 慣例により、実際にはちょっと違った 書き方をする。 これをちょっと違う風にパラメトライズする。 まず、1/mの項を 取り除く。 これは単に これは単に ロジスティック回帰とはちょっとだけ異なる コンベンションをサポートベクターマシンでは 人々が偶然使っていた、というだけ。 それはこういう事だ。 つまり単純に 1/mの項を取り除く。 これは最適なシータの値には 違いをうまないはず。 何故なら1/mは単に定数だから。 だからこの最小化問題を 前に1/mを置いて解こうが置かないで解こうが 得られる結果は 同じ最適なシータの 値となる。 それはこういう事だ。 具体例を見よう。 以下のような最適化問題があるとする。 実数のuを、(u-5)の二乗 +1を 最小化するように選ぶ。 この場合、最小になるのは これが最小になるのは u=5の時だ。 今、この目的関数に対し、 これを10掛けると するとこの場合、 最小化問題は 10掛ける (u-5)の二乗 足すことの10 を最小にするuだ。 これを最小にするuは u=5のままだ。 つまり、最小化したい物に、 何か定数を掛けても、 この場合は10を掛けた訳だが、 その事はこの関数を最小にする uの値を、変える事は無い。 つまり同様に、 このmを取り除く為にやったのは、 私がやったのは、 目的関数にある定数、mを 掛けただけだ。 だからそれは、最小になる シータを変化させる事は無い。 2つ目のちょっとしたノーテーションの変更は ロジスティック回帰の代わりに SVMを使う時に、 もっとも一般的なコンベンションだが、それは以下のような物だ。 ロジスティック回帰の時は、 目的関数に2つの項があった。 一つ目の項は トレーニングセットから来る コストだった。 二番目のこの項は、 正規化の項。 そして我らがやらなくてはいけなかったのは、 これらの間のトレードオフを 制御する事だった。つまり、 最小化したいのはA足すことの、 正規化のパラメータ、ラムダに 掛けるなんかの項、Bだ。 ここでAを この最初の項を指すのに使い、 そしてBをこの 二番目の項を指すのに 使う。ラムダは抜きで。 そしてこのAとBの 優先度を考える代わりに、 我らがやったのは、 この正規化のパラメータ、ラムダに 異なる値をセットしていったのだった。 我らは相対的に どれだけトレーニングセットに 良くフィットさせるか、 つまりどれだけAを最小化するかと、 とれだけパラメータの値を小さく保つ事を気にするか、 それがパラメータBだが、 それらの間のトレードオフを取る。 サポートベクターマシンでは、 慣例により、違うパラメータを使う。 一番目の項と二番目の項の間の 重みをコントロールする為に ラムダを使う代わりに、 そこでもまだパラメータを 使う事になるのだが、 それはコンベンションで Cと呼ばれる。 そして、代わりに C掛けるA 足す B を最小化する。 ロジスティック回帰の時は とても大きなラムダの値を用いると、 それの意味する所はBに とても大きな重みを付与する、という事だ。 今回は、もしCにとても小さな 値をセットすると、 それがB に、Aとくらべて とても大きな重みを付与する事になる。 つまり、これはトレードオフをコントロールする 単なる別のやり方、または どれだけ最初の項を最適化するか、 vs どれだけ二番目の項の最適化を重視するか、 をパラメトライズする異なるやり方に過ぎない。 必要に応じて これをパラメータCが、 1/ラムダ と 似たよう役割の物と みなすことができる。 それはこれら2つの 方程式、2つの式が同じになるという訳では無く、 C=1/ラムダ というのはそういう場合という訳では無く、 もしCが1/ラムダ と等しいと これら2つの最適化の目的関数は 同じ結果の値を 与える、という事。 同じ最適値シータを。 それを踏まえると、 ラムダを消して、 定数C をここに書く。 以上の操作で、 サポートベクターマシンの 全体の最適化の目的関数が得られる。 そしてこの関数を 最適化すれば、 SVMにより学習したパラメータを 得られる。 最後に、ロジスティック回帰と異なり、 サポートベクターマシンは確率を出力する訳では無い。 その代わりに、 我らが得るのは このコスト関数を最小化する パラメータ、シータで、 サポートベクターマシンがやるのは、 yが1か0かの予言を 直接行う。 つまり、仮説は、 シータ転置のxが 0より大きければ 1を予言し、 それ以外なら0を予言する。 つまり、学習したパラメータのシータを 得た後は、 これがサポートベクターマシンの仮説の形だ。 以上が、サポートベクターマシンが 何をするかの 数学的な定義だ。 次に続く幾つかのビデオて、 この最適化の目的関数が 何を意味するかを直感的に 把握する事を目指します。 そしてSVMが 学習する仮説の元と どう修正したら、 より複雑な、非線形の関数が 学習出来るかも話します。