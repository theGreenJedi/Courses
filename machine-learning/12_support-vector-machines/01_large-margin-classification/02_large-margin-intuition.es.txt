A veces, la gente habla de las máquinas de soporte vectorial como clasificadores de márgenes amplios. En este video les explicaré qué significa esto y les daré un panorama útil de cómo se ve una hipótesis de una SVM. Aquí tengo mi función de costos para la máquina de soporte vectorial donde a la izquierda tracé mi función costo 1 de “z” que utilizo en ejemplos positivos, y a la derecha tracé la variable 0 de “z”, donde tengo “z” en el ej horizontal. Ahora, pensemos qué hace falta para hacer estas funciones de costo más pequeñas. Si tienes un ejemplo positivo, si “y” es igual a 1, entonces costo 1 de “z” será 0 sólo cuando “z” sea igual o mayor que 1. En otras palabras, si tienes un ejemplo positivo, queremos que teta transpuesta de “x” sea más grande o igual a 1. Y a la inversa, si “y” es igual a 0, como en esta función costo 0 de “z”, entonces sólo en esta región, donde “z” es menor o igual a 1 que tendremos un costo de 0 si “z” es igual a 0. Una propiedad interesante de las máquinas de soporte vectorial es que, si tienes un ejemplo positivo, o sea, si “y” es igual a 1, entonces necesitamos que teta transpuesta de “x” sea mayor que cero. Esto significaría que estaríamos realizando una clasificación correcta porque si teta transpuesta de “x” es mayor que 0, nuestra hipótesis hará una predicción de 0. De manera similar, si tienes un ejemplo negativo, lo que queremos es que teta transpuesta de “x” sea menor que cero. Esto nos asegurará que tenemos el ejemplo correcto. Pero la máquina de soporte vectorial quiere un poco más que esto. No sólo quiere que obtengas el ejemplo correcto, no queremos que sea sólo un poco mayor que cero. Lo que en realidad quiero es que sea mucho mayor a cero; quizá hasta mayor o igual a 1. Y quiero que esto de abajo sea mucho menor a cero. Quizá menor o igual a - 1. Esto construye un factor de seguridad adicional o un factor de margen de seguridad en la máquina de vector de soporte. La regresión logística hace algo similar, pero veamos qué pasa o cuáles son las consecuencias de esto en el contexto de máquinas de soporte vectorial. Lo que me gustaría hacer a continuación es considerar un caso en el que determinamos que esta constante “C” es un valor muy grande. Imaginemos que le damos a “C” un valor muy grande; quizá cien mil o un número enorme. Veamos que hace la máquina de soporte vectorial. Si “C” es muy, muy grande, entonces, cuando minimicemos el objetivo de optimización estaremos más inclinados a elegir un valor para que este primer término sea igual a 0. Intentemos entender el problema de optimización en el contexto de qué se necesitaría para hacer que el primer término en el objetivo sea igual a cero porque, quizá “C” tenga una constante enorme. Esto, con suerte, nos dará una intuición adicional de las hipótesis que aprenderá la máquina de vector de soporte. Ya vimos que cuando tenemos un ejemplo de entrenamiento con un valor asignado de “y” igual a 1, si queremos que el primer término sea 0, necesitamos encontrar el valor de teta para que teta transpuesta de “x” sea mayor o igual a 1. De manera similar, cuando tenemos un ejemplo cuyo valor asignado es 0, para asegurarnos de que el costo 0 de “z” sea 0, necesitamos que teta transpuesta de “x(i)” sea menor menor o igual a - 1. Ahora, si pensamos en nuestro parámetro como ahora, elegir los parámetros nos aseguran que este primer término es igual a cero y nos quedamos con el siguiente problema de optimización. Minimizaremos el primer término 0; es decir, “C” por 0, porque elegiremos los parámetros para que sea igual a 0, más un medio. Después, el segundo término. Este primer término es “C” igual a cero así que lo eliminaremos porque sabemos de antemano que será 0. Esto estará sujeto a la condición de que teta transpuesta de “x(i)” es mayor o igual a 1, si “y(i)” es igual a 1 y teta transpuesta de “x(i)” es menor o igual a menos 1 cuando tenemos un ejemplo negativo. Resulta que cuando resolvemos este problema de optimización, y minimizamos esto como una variable del parámetro teta, obtenemos una barrera de decisión interesante. Si observas un conjunto de datos con ejemplos positivos y negativos como estos, los datos son separables linealmente. Con esto me refiero a que existe una línea recta, o muchas líneas rectas distintas, pueden separar perfectamente los ejemplos negativos y los positivos. Por ejemplo, aquí hay una barrera de decisión que separa los ejemplos positivos y negativos, pero por alguna razón no se ve muy natura, ¿cierto? O podemos trazar otra barrera de decisión que apenas separe los ejemplos positivos y negativos. Pero ninguna de estas líneas parecen opciones muy buenas. La máquina de vector de soporte elegirá esta barrera de decisión que estoy dibujando en negro. Esta parece ser una mejor barrera de decisión que las que dibujé en magenta o en verde. La línea negra parece ser un separador más sólido. Hace un mejor trabajo separando los ejemplos positivos y los negativos. Matemáticamente, lo que hace esta barrera de decisiones negra es tener una distancia mayor. La distancia es lo que llamamos margen. Si la comparamos con estas dos líneas azules adicionales podemos notar que la barrera de decisiones negra tiene una distancia mínima mayor entre los ejemplos de entrenamiento, mientras que las líneas magenta y verde se acercan mucho a los ejemplos de entrenamiento y hacen un trabajo menos efectivo separando los ejemplos positivos y negativos que la línea negra. Por lo tanto, esta distancia se llama margen de la máquina de soporte vectorial y le da a la SVM cierta solidez, porque intenta separar los datos con el mayor margen posible. La máquina de vector de soporte también se llama clasificador de margen amplio. Esta es una consecuencia del problema de optimización que escribimos en la diapositiva anterior. Sé que se preguntan cómo funciona el problema de optimización que escribí en la diapositiva anterior y cómo nos lleva a este clasificador de margen amplio. Sé que no lo he explicado aún. En el siguiente video presentaré un poco del concepto de porqué el problema de optimización nos da un clasificador de margen amplio.  Esta es una variable útil cuando intentas entender los tipos de hipótesis que elegirá la SVM, es decir, cuando intentas separar los ejemplos positivos y negativos con el mayor margen posible. Quiero decir una última cosa acerca de los clasificadores de márgenes amplios en este contexto. Escribimos esta configuración de clasificación de márgenes amplios donde “C”, el concepto de regularización, era muy amplio. Creo que lo determinamos como cien mil o algo similar. Con un conjunto de datos como este, elegiremos la barrera de decisión que separe los ejemplos positivos y negativos por un margen amplio. Ahora, el SVM es, de hecho, un poco más sofisticada de lo que sugiere este margen. En particular, si lo que estamos haciendo es utilizar un clasificador de margen amplio, el algoritmo será sensible a valores atípicos. Entonces, añadamos el ejemplo positivo adicional que se muestra en la pantalla. Si añadimos este ejemplo, parece que para separar los datos con un margen amplio tendremos una barrera de decisión como esta, ¿cierto? como esta línea magenta. No parece ser muy natural cambiar mi barrera de decisiones de la línea negra a la línea magenta con base en ese único ejemplo. De manera que si el parámetro de regularización “C” fuera muy alto, entonces nuestra SVM será adecuada y cambiará la barrera de decisión de la línea negra a la magenta. Pero si “C” fuera razonablemente baja; es decir, si utilizaras “C” con un valor no muy alto entonces conservarías esta barrera de decisión negra. Por supuesto, si los datos no fueran separables linealmente y tuvieras ejemplos positivos ahí, o tuvieras ejemplos negativos acá, la SVM también hará lo correcto. Esta imagen de un clasificador de márgenes amplios es la imagen que nos da el mejor entendimiento para el caso en el que el parámetro de regularización “C” es muy alto. Como recordatorio, “C” juega un papel similar a 1 sobre «lambda», donde «lambda» es el parámetro de regularización que utilizamos anteriormente. Será sólo si 1 sobre «lambda» es mayor o si «lambda» es muy pequeño que obtendremos barreras de decisión como esta magenta. Pero, en la práctica, cuando aplicamos máquinas de vector de soporte y “C” no es muy alta, podemos hacer un mejor trabajo ignorando valores atípicos como estos  y aplicando cosas razonables, aún si los datos no son separables linealmente. Más delante, cuando hablemos de oscilación y varianza en el contexto de máquinas de soporte vectorial, las compensaciones que involucran el parámetro de regularización quedarán más claras. Espero que esto te de un mejor entendimiento de las funciones de la máquina de vector de soporte como clasificadores de márgenes amplios que intentan separar los datos con el mayor margen posible. Técnicamente, esta imagen sólo es verdadera cuando el parámetro “C” es muy alto y es una manera útil de visualizar una máquina de vector de soporte. En este video nos falto mencionar cómo nos lleva el problema de optimización que escribimos en estas diapositivas a un clasificador de márgenes amplios. No lo expliqué en este video, pero en el siguiente presentaré rápidamente las matemáticas que están detrás de esto para explicar un razonamiento independiente de cómo nos lleva el problema de optimización que escribimos al clasificador de margen amplio.