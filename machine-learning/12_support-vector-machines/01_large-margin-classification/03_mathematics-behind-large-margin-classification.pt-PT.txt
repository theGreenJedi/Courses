Nesse vídeo, eu gostaria de explicar a base matemática por trás da classificação de margem larga. Este vídeo é opcional, então fique à vontade para pulá-lo, mas ele pode te dar uma idéia melhor de como o problema de otimização da máquina de vetores de suporte, como leva aos classificadores de margem larga. Para começar, vamos relembrar algumas propriedades do produto interno de vetores. Digamos que temos dois vetores u e v, que têm esta forma, ambos vetores de duas dimensões, então qual seria a forma de u transposto v ? E u transposto v também é chamado "produto interno dos vetores u e v". u é um vetor bidimensional, então eu posso desenhá-lo nessa figura. Vamos assumir que esse é o vetor u. E o que eu quero dizer com isso é que se, no eixo horizontal esse valor equivale ao que quer que u1 seja, e no eixo vertical esta altura equivale ao valor representado por u2, o segundo componente do vetor u. Então, seria muito bom saber o valor da norma do vetor u. Entoa, essas barras duplas à esquerda e à direita de u representam a norma ou comprimento do vetor u. Isso significa simplesmente a distãncia euclideana representada pelo vetor u. E isso, pelo teorema de Pitágoras é igual a u1 ao quadrado mais u2 quadrado, sob a raiz quadrada, não é ? E isso é o comprimento do vetor U, é um número real. É simplesmente isso, qual o comprimento disso, qual o comprimento desse vetor aqui ? Qual é o comprimento dessa seta que eu acabei de desenhar - é a norma de U. Agora vamos voltar e olhar para o vetor v, porque queremos computar o produto interno. Então, v é algum outro vetor com algum, algum valor v1, v2, Então, o vetor v se parece com isso. Agora, vamos volta e e e ver como computar o produto interno entre u e v. Aqui está como isso é feito. Eu pego o vetor v e o projeto sobre o vetor u. Então, eu faço uma projeção ortogonal, ou uma projeção de 90 graus, sobre o vetor u, assim. E agora eu vou medir o comprimento desta linha vermelha que acabei de desenhar. Então, eu vou chamar esse comprimento da linha vermelha P. Então, P é o comprimento, ou a magnitude da projeção do vetor v sobre o vetor u. Deixe-me escrever isso aqui. Então P é o comprimento da projeção do vetor v sobre o vetor u. E é possível mostrar que o produto interno de u transposta vezes v igual a P vezes a norma (ou o comprimento) do vetor u. Então, essa é uma maneira de calcular o produto interno. E se você efetivamente fizer o cálculo geométrico e determinar o valor de P e a norma de u, isso deveria dar no mesmo, chegar à mesma resposta que a outra forma de calcular o produto interno. Certo. Isso significa que, expandindo u transposta vezes v, u transposta [ u1, u2], essa matriz de um por dois, vezes v. Essa operação dá como resultado u1 vezes v1 mais u2 vezes v2. E então o teorema da álgebra linear diz que essas duas fórmulas dão o mesmo resultado. Aliás, u transposta vezes também é igual a v transposta vezes u. Então, se fizermos o mesmo processo ao contrário, ao invés de projetar v sobre u, projetarmos u sobre v. Então, ao realizar o mesmo cálculo, mas com u e v invertidos, efetivamente, vamos chegar ao mesmo resultado, qualquer que seja ele. Somente para deixar claro o que está acontecendo nessa equação, a norma de u é um número real, e P também é um número real. Então, u transposta vezes v é a multiplicação simples de dois números reais, o comprimento p vezes a norma de u. Um último detalhe é que se observarmos P, veremos que P tem um sinal, correto ? E esse sinal pode ser positivo ou negativo. O que eu quero dizer com isso é que, se u é um vetor com esse sentido e v um vetor com esse sentido, Então, se o ângulo entre u e v é maior que noventa graus, se projetarmos v sobre u, teremos uma projeção mais ou menos assim, e esse comprimento é P. E nesse caso, ainda temos que u transposta vezes v é igual a P vezes a norma de u. Exceto que, nesse caso, P é negativo. Então, em casos de produtos internos, se o ângulo entre u e v é menor que noventa graus, então P é o valor positivo do comprimento dessa linha vermelha, enquanto que se o ângulo é maior que 90 graus, então P é o valor negativo do comprimento desse segmento aqui. Então, o produto interno entre dois vetores pode ser negativo, se o ângulo entre eles for maior que 90 graus. É assim que o produto interno de vetores funciona. Vamos usar estas propriedades do produto interno de vetores para tentar entender o objetivo de otimização da máquina de vetores de suporte. Aqui temos o objetivo de otimização para a máquina de vetores de suporte que usamos anteriormente. Somente durante este slide, eu vou fazer uma simplificação, somente para tornar o objetivo mais fácil de analisar. O que vou fazer é ignorar os termos de interceptação. Então, vamos ignorar θ0, definí-lo como 0. Para facilitar o desenho, também vou definir n, o número de parâmetros, como 2. Então, vamos ter somente 2 parâmetros, x1 e x2. Agora, vamos analisar a função objetivo, o objetivo de otimização da SVM. Quando temos somente dois parâmetros, quando n é igual a 2, ela pode ser escrita como metade de θ1² mais θ2², porque temos somente dois parâmetros, θ1 e θ2. Eu vou então reescrever essa fórmula, escrevê-la como metade de θ1² mais θ2² e então raiz quadrada ao quadrado. A razão pela qual eu posso fazer isso é que, para qualquer número w, como você sabe, a raiz quadrada de W elevada ao quadrado, isso é simplesmente igual a w. Então, essa raiz quadrada ao quadrado dá no mesmo. Mas você pode reparar que o termo entre parênteses é igual à norma ou o comprimento do vetor Θ, e o que eu quero mostrar é que se representarmos o vetor Θ assim, como [θ1, θ2], então este termo sublinhado em vermelho é exatamente o comprimento (ou a norma) do vetor Θ, de acordo com a definição de norma do vetor que temos no slide anterior. E de fato isso é igual ao comprimento do vetor Θ, mesmo incluindo θ0 com θ1 e θ2, isso é, se θ0 for igual a zero, como defini aqui. Ou simplesmente o comprimento de [θ1 e θ2]; por agora eu vou ignorar θ0. Então eu vou simplesmente tratar Θ como isso, vou escrever Θ, a norma de Θ como θ1 e θ2 somente mas a matemática funciona de qualquer maneira, incluindo θ0 ou não. Logo, não vai importar para o restante de nossa derivação. Finalmente, isso significa que meu objetivo de otimização é igual a metade da norma de Θ ao quadrado. Então o que a máquina de vetores de suporte está fazendo no objetivo de otimização é minimizando o quadrado da norma do quadrado do comprimento do vetor de parâmetros Θ. Agora vamos analisar estes termos Θ transposto vezes x e entender melhor o que eles representam. Dado o vetor de parâmetros Θ e dado um exemplo x, qual o valor dessa expressão ? E, no slide anterior, nós determinamos o resultado de u transposta vezes v, para os vetores u e v. Então, vamos usar estas definições, com Θ e x(i) representando os vetores u e v. E ver como fica esse gráfico. Então, vamos desenhar, digamos que eu observe um único exemplo de treinamento. Digamos que eu tenha um exemplo positivo, representado por essa cruz aqui, e digamos que esse é o meu exemplo x(i). O que isso significa é que eu marco no eixo horizontal algum valor x(i)1 e no eixo vertical x(i)2. É assim que se marca cada exemplo de treinamento. E ainda que não estejamos pensando neste ponto como um vetor, é isso que ele realmente é, um vetor da origem, de (0, 0) até a posição deste exemplo de treinamento. Agora digamos que temos um vetor de parâmetros Θ e que vou desenhá-lo como um vetor também. Ou seja, vou marcar θ1 aqui e θ2 ali, Então, qual é o produto interno de Θ transposto vezes x(i) ? Usando nosso método de antes, calculamos esse valor ao tomar este exemplo e projetá-lo sobre o vetor de parâmetros Θ. Então vamos olhar para o comprimento deste segmento que estou colorindo de vermelho. Vou chamá-lo de de p expoente i (p(i)) para representar que é uma projeção do i-ésimo exemplo de treinamento sobre o vetor de parâmetros Θ. Então temos que Θ transposto vezes x(i) é igual a, segundo o que determinamos no slide anterior, isso é igual a P vezes o comprimento da norma do vetor Θ. Que também é igual a θ1 . x1 mais θ2 . x2. Então, todas essas são maneiras igualmente válidas de calcular o produto interno entre Θ e x(i). OK. Então, onde chegamos com isso ? Temos então que Θ transposto vezes x(i) é restrito a ser maior ou igual a um ou menor ou igual a menos 1. Isso significa que podemos substituir estas restrições por P(i) vezes norma de Θ. ser maior ou igual a um. Porque Θ transposta vezes x(i) é igual a P(i) vezes a norma de Θ. Então, colocando isso em nosso objetivo de otimização, obtemos essa expressão, Onde, ao invés de Θ transposta vezes x(i), temos esse P(i) vezes a norma de Θ. E lembrando, determinamos também que o objetivo de otimização pode ser escrito como a metade do quadrado da norma de Θ. Então vamos considerar o exemplo de treinamento que temos abaixo e, por agora, continuar com a simplificação de que θ0 é igual a 0. Vamos ver que fronteira de decisão a máquina de vetores de suporte vai escolher. Aqui está uma alternativa, digamos que a máquina de vetores de suporte escolhesse esta fronteira de decisão. Esta não é uma escolha muito boa, porque ela tem margens muito estreitas. A fronteira de decisão passa muito perto dos exemplos de treinamento. Vamos entender porque a máquina de vetores de suporte não vai fazer isso. Para esta escolha de parâmetros, é possível demostrar que o vetor de parâmetros Θ está efetivamente a 90 graus da fronteira de decisão. Então, esta fronteira de decisão verde corresponde a um vetor de parâmetros Θ apontando nesta direção. Falando nisso, a simplificação de que θ0 é igual a 0, ela só implica que a fronteira de decisão tem que passar pela origem (0, 0) aqui. Então, vamos ver quais as implicações para o objetivo de otimização. Digamos que este exemplo aqui, digamos que este é o primeiro exemplo, x(1). Se calcularmos a projeção deste exemplo sobre os parâmetros θ, essa é a projeção, então este pequeno segmento vermelho, ele é igual a p(1). E ele é bem pequeno, correto ? De forma similar, para este exemplo aqui, se o chamarmos de x(2), esse é o segundo exemplo. Então, ao calcularmos a projeção deste exemplo sobre Θ. Bem, Vou desenhar este em magenta. Este pequeno segumento magenta, este é p(2). Essa é a projeção do segundo exemplo sobre a direção do vetor de parâmetros Θ, que fica deste jeito. Então, este pequeno segmento projetado ficou bem curto mesmo. P(2) é um valor negativo, correto, pois p(2) está na direção oposta a p(1), já que este vetor tem um ângulo de mais de 90 graus com o vetor de parâmetros Θ, então p(2) vai ser menor que 0. O que estamos vendo é que estes termos p(i) são números bem pequenos. Então, se considerarmos o objetivo de otimização vemos que, para exemplos positivos precisamos que p(i) vezes a norma de Θ seja maior ou igual a um. Mas, se p(i), se p(1) é bem pequeno, isso significa que precisamos que a norma de Θ seja bem grande, não ? Se p(1) tende a ser pequeno e queremos que p(1) vezes a norma de Θ seja maior ou igual a um, bem, a única maneira disso ser verdadeiro, do produto destes dois números ser grande se p(1) é pequeno, como dissemos, precisamos que a norma de Θ seja grande. Da mesma forma, para o exemplo negativo, precisamos que p(2) vezes a norma de Θ seja menor ou igual a menos um. E já vimos neste exemplo que p(2) vai ser um número negativo bem pequeno, então a única maneira disso acontecer é a norma de Θ ser grande, mas o que estamos fazendo no objetivo de otimização é tentar encontrar um conjunto de parâmetros tal que a norma de Θ seja pequena, então, bem, então essa não parece ser uma direção muito boa para o vetor de parâmetros Θ. Em contraste, vamos observar uma fronteira de decisão diferente. Aqui, vamos dizer que a SVM escolha esta fronteira de decisão. A situação aqui é bem diferente. Se essa é a fronteira de decisão, então esta é a direção correspondente de Θ. Então, com a fronteira de decisão sendo esta, esta linha vertical, ela corresponde a, é possível demonstrar, usando álgebra linear, que a maneira de obter esta fronteira de decisão verde é colocar o vetor Θ a 90 graus dela. E agora, se fizermos as projeções dos dados sobre o vetor horizontal, digamos que, como antes, este exemplo é x(1). Então, quando o projeto sobre o eixo x sobre o vetor Θ, o resultado é este p(1). Este comprimento aqui é p(1) Com outro exemplo, esse exemplo aqui, eu calculo a mesma projeção e o resultado é que este comprimento aqui é p(2), que vai ser negativo. E é fácil de ver que agora p(1) e p(2), estes comprimentos das projeções vão ser bem maiores, e então já que ainda precisamos garantir estas restrições de que p(1) vezes a norma de Θ seja maior que um, como p(1) agora é bem maior a norma de Θ pode ser menor. Então, isso significa que ao escolher a fronteira de decisão à direita ao invés da à esquerda, a SVM pode fazer  a norma do parâmetro Θ ser bem menor. E se podemos diminuir a norma de Θ, vamos por conseqüência fazer o quadrado da norma de Θ menor, que é a razão da SVM escolher esta hipótese à direita ao invés da outra. E é assim que a SVM dá origem ao efeito de classificação de margem larga. Basicamente, se observarmos esta linha verde, se olharmos para esta hipótese verde, queremos que as projeções dos exemplos positivos e negativos sobre Θ sejam grandes, e a única maneira disso ser verdadeiro é se, em volta da linha verde há uma margem bem larga, um espaço largo separando os exemplos positivos e negativos; isso é a magnitude deste espaço, a magnitude dessa margem, exatamente os valores de p(1), p(2), p(3) e assim por diante. Então, ao alargar esta margem, ao aumentar estes termos p(1), p(2), p(3), etc., a SVM pode conseguir definir um valor menor para a norma de Θ, que é o que ela está buscando com o objetivo. E é por isso que a máquina de vetores de suporte acaba com classificadores de margem larga, porque ela está tentando maximizar a norma destes parâmetros p(i), que é a distância dos exemplos de treinamento para a fronteira de decisão. Para finalizar, fizemos toda esta derivação usando a simplificação de que o parâmetro θ0 é igual a 0. O efeito disso, como falei de passagem anteriormente, é que se θ0 é igual a 0, isso significa que estamos considerando somente fronteiras de decisão que passam pela origem, fronteiras de decisão que passam através da origem, como estas. Se permitirmos que θ0 seja diferente de 0, podemos considerar avaliar casos em que a fronteira de decisão não passa pela origem, como esse que acabei de desenhar. Não vou demonstrar toda a derivaçãio disso. O que acontece é que esta mesma prova da margem larga funciona praticamente da mesma maneira. E existe uma generalização deste argumento que acabamos de derivar não vou detalhar aqui, mas ela mostra que mesmo quando θ0 é diferente de 0, a SVM está tentando com este objetivo de otimização (que corresponde ao caso em que C é muito grande) é possível demonstrar que quando θ0 não é igual a 0 a mãquina de vetores de suporte ainda está buscando, está de fato tentando buscar o separador de margem larga entre os exemplos positivos e negativos. Isso explica como a máquina de vetores de suporte é um classificador de margem larga. No próximo vídeo vamos começar a falar sobre como tomar algumas destas idéias de SVM e começar a aplicá-las para construir classificadores não lineares complexos.