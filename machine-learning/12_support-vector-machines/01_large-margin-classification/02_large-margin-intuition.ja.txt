ときどき皆さんはSVMを 大きなマージンの 分類器だといいますが、今回、 みなさんには この意味と、 皆さんに役立つであろう SVMの仮定がどんなものかという 全体像についてお話します。 これがサポートベクターマシンでのコスト関数です。 ここで左に cost1(z)の関数をプロットした、 これは陽性の手本に対して使う物だ。
そして右には cost0(z)をプロットした、 ここでzは横軸に取った。 ここで、これらのコスト関数を小さくすると どうなるかを考えてみよう。 もし陽性の手本があったとして、 つまりy=1の時、 cost1のzは zが1以上の時にだけ 0となる。 言い換えると、 陽性の手本に対しては、 シータ転置xが1以上で あって欲しい、という事。 そして反対に、 もしy=0の時は このcost0(z)関数を見ると、 この領域の時だけ つまりzが1以下の時だけ（訳注: -1の間違いか） cost0(z)は ゼロとなる。 ゼロとなる。 これはサポートベクターマシンの 面白い性質だ。 もし陽性の手本、 つまりy=1の時は 本当に必要なのは シータ転置xが0以上であれば十分なはずだ。 それは正しく分類した、って事なんだから。 だってシータ転置xが0より大きければ 仮説は0を予言するんだから（訳注: 1の間違いか） 同様に、陰性の手本があったら あなたが望むのはただシータ転置xが ゼロより小さければ本当は良くて、それだけで手本で正解出来ている。 だが、サポートベクターマシンは、それよりももうちょっと多くを要求する。 それはぎりぎり手本が正しければ良いだけにとどまらず、 つまり単にゼロよりちょっとでも 大きければ良いというのではなく、 それが要求するのは、ゼロよりも かなり大きいという事。 1よりもちょっと大きい、 と言っている。 そしてこれは0よりもずっと小さくしたい。 たとえば-1以下とかに したい。 つまりこれは、追加のセーフティーファクターを またはセーフティーマージンを サポートベクターマシンに組み込むと言える。 ロジスティック回帰も もちろん似たような事をしていたが、 何が起こるか見てみよう。 またはサポートベクターマシンの文脈では これはとういう結果になるのかを 見てみよう。 具体的には、次に私がやりたいのは、 この定数Cに とても大きな値を セットしてみたい、 というもの。 ではCにとてもおおきな値、 例えば何十万もの値、なんらかの巨大な値をセットしのを想像してみよう。 サポートベクターマシンが何をするか、見てみよう。 もしCがとっても、とっても大きいと、 その場合はこの最適化の目的関数を 最小化する時に、 値を選ぶ時に この項がゼロになるように、 とても高く動機づけされる。 では目的関数のこの最初の項を 0にしよう、というコンテキストで 最適化問題は どうなるかを 理解していこう。 その理由は、 我らはCをとても大きな定数に 設定すると言った。 これが、サポートベクターマシンの 仮説がどんな感じか、さらなる直感を 与えてくれる事を期待している。 既に見たように、 トレーニング手本のラベルy=1の時はいつでも 最初の項を 0にしたいなら やるべき事は シータ転置xが 1以上になるような シータを 探すという事。 同様に、ラベル0の手本の時は いつでも cost0が、、、 cost0(z)が そのコストが0に確実になるには、 シータ転置xを-1以下に しなくては ならない。 つまり、我らの最適化問題を 実際にパラメータを選んで この最初の項を ゼロとしたら、 その後に 残るのは 以下の最適化問題だ。 我らが最小化するのは、 最初の項が0なので、 C掛ける0 だ。何故ならそれが0になるように パラメータを選ぶのだから。 それに足すことの1/2の、 えーと、 二番目の項。 この最初の項は C掛ける0 だから バッテンで消してしまおう。 だってゼロなんだから。 そしてこれは、シータ転置x(i)が 1以上という 制約条件に従う。 y(i)=1の時は シータ転置x(i)が -1以下となる。 陰性の 手本の時には いつでも。 そして結局、 この最適化問題を解くと、 パラメータシータの関数としてこれを最小化すると、 とても興味深い決定境界が得られる。 具体的に、 このようなデータセットを見た時、 そこには陽性と陰性のサンプルがある訳だが、 このデータは線形で分離可能。 それの意味するところは、ある直線ーー たくさんの異なる直線が有り得るが、 それらが陽性と陰性のサンプルを 完璧に分離する、という事。 例えば、これは陽性と陰性の サンプルを分ける決定境界の一つだが、 とても以前な それという 感じはしない。 もっと酷いのを描いてみると、 これも確かに、陽性と陰性のサンプルを 分離している決定境界だが、 だがギリギリだ。 これらはどちらも、そんなに良い選択っぽくは無い。 サポートベクターマシンはそうではなく、 この決定境界を選ぶ、この黒で描いた奴。 そしてそれは、マゼンタや緑で描いた決定境界の どちらよりも、 ずっとマシっぽい。 黒い線の方が、よりロバストな 分離器に見える。 こっちの方が陽性と陰性のサンプルを分けるという仕事をうまくこなしてる。 数学的には、それの意味する所は この黒い決定境界の方が大きな距離を持っているという事。 この距離を、マージンと呼ぶ。 この2つの追加の線を 描いてみると分かるように、 黒の決定境界は、トレーニング手本の中のサンプルへの最小距離が より大きい。 他方マゼンタや緑の線は トレーニング手本に恐ろしいほど近い。 だからそちらの方が、陽性と陰性を分離するには 黒の線に比べるといまいちな仕事しかしていない感じがする。 この距離が サポートベクターマシンの マージンと 呼ばれる物。 そしてこれがSVMに、ある程度のロバストさを与えている。 何故ならそれは、 データを出来るだけ大きなマージンになるように 分離しようとするから。 だからサポートベクターマシンは たまに 大きなマージン分類器 とも 呼ばれている。 そしてこれは実は、前のスライドで書いた 最適化問題の帰結だ。 おっと分かってるって。 前のスライドに 書いた最適化問題が どうなってこの 大きなマージン分類器 になってるかって思ってるんでしょ？ それをまだ説明してないってのは分かってるよ。 それは次のビデオで さっきの最適化問題が なんで 大きなマージン分類器に なるのかの、ちょっとした直感の 説明をするよ。 だがこれは、 SVMはどんな仮説を選ぶのか？を理解したければ、 心にとめておく価値のある 特徴だ。 つまり、陽性と陰性のサンプルを、できるだけ大きなマージンになるように 分離しようとする、という事。 大きなマージン分類器について、 最後に一つだけ言わせてくれ。 この直感的な考え方だと、 我らが書き下した 大きなマージン分類器 は、 C、つまり正規化の定数が 凄く大きな場合の 話だった。 たぶん何十万とかその辺をセットした気がする。 こんなデータセットが与えられたら、 陽性と陰性のサンプルを 大きなマージンになるようの分離する 決定境界選ぶかもしれない。 SVMは実際は この大きなマージンという見方から考えられる物よりは もうちょっと洗練されている。 とりわけ、大きなマージンという特徴だけの 分類器を使っている場合は、 ハズレ値に その学習アルゴリズムは、より敏感となる。 つまり、画面に示したような 陽性のサンプルを 追加してみましょう。 もしサンプルを一つ追加したら、 データを大きなマージンで 分離しようとすると、 こんな決定境界を 学習する事になる。 このマゼンタの線。 でも一つの外れ値、 一つのサンプルに基づいて、 決定境界を 黒の物から マゼンダの物へと 変更するのが本当に良い事なのかは 結構怪しい。 だからもしCが、、、 正規化パラメータのCがとても大きければ、 その場合はこれがSVMが 実際に行う事となる。 それは決定境界を 黒の物からマゼンタの物へ 変更する。 だがCが普通の範囲に小さければ、 もしCに、大き過ぎない値を 使っていれば、 その時は黒の決定境界の ままとなる。 そして、もちろん、もしデータが直線で分離出来ない場合、例えばある陽性のサンプルが ここにある場合とか、 または陰性のサンプルがここにある場合とか、 そういう場合もSVMは 正しい事をする。 そしてこの 大きなマージン分類器の絵は 正規化パラメータCが とても大きい場合にだけ正しい 直感を与えてくれる 絵だ。 そして繰り返すと、 この、 C は 1/ラムダ に対応していて、 このラムダは 以前にあった 正規化パラメータだ。 つまり、この絵は 1/ラムダ がとても大きい時、 つまりラムダがとても小さい時にだけ このマゼンタの決定境界を 得る結果となる。 でも実際にサポートベクターマシンを適用する時は Cはそんなに 凄く凄く大きい値をセットしたりはしないので、 このようなちょっとの外れ値を無視するには もっと良い仕事をしてくれます。 また、データが直線で分けられない時も 公正で納得出来るような結果の仕事をしてくれます。 だがサポートベクターマシンの文脈で バイアスと分散の話をする時に、 それはちょっと後でやる予定ですが、 そのあかつきには、正規化パラメータにまつわる トレードオフはもっとクリアになってるといいな。 以上がサポートベクターマシンの関数が 大きなマージン分類器として 与えられたデータを大きなマージンで どのように分離しようとするかについて、 ある程度の直感を与えてくれるといいな。 技術的にはこの見方は パラメータのCがとても大きな時にしか 成り立たないけど、その考え方は サポートベクターマシンを考える上でとても有用な考え方だ。 このビデオには、 一つ欠けたステップがある。 それは我らがこれらのスライドで書き下した 最適化の問題が どのように実際に 大きなマージンの分類器となるのか、という所。 このビデオではそこはやってない。 次のビデオでは、 背後にある数学を もう少しスケッチしてみる事で 我らの書き下した最適化の問題が どうやって大きなマージン分類器となっているかを 説明したいと 思います。