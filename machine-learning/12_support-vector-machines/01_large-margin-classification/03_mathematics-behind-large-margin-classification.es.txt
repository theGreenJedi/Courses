En este video, me gustaría contarles un poco acerca de las matemáticas que están detrás de una clasificación de margen amplio. Este video es opcional así que siéntete libre de saltártelo; sin embargo te podrá dar un mejor entendimiento acerca de cómo nos lleva el problema de optimización de la máquina de vector de soporte a un clasificador de márgenes amplios. Para iniciar, permítanme recordarles algunas propiedades de cómo se verían los productos internos de un vector. Digamos que tenemos dos vectores “U” y “V” que se ven así y ambos son vectores bidimensionales. Observemos cómo se ve “U” transpuesto de “V”. “U” transpuesto de “V” también es llamado productos internos entre los vectores “U” y “V”. Utilizaré un vector bidimensional para trazarlo en esta figura. Digamos que este es el vector “U”. A lo que me refiero con esto es que en el eje horizontal, el trazo toma el valor que tenga u1 y en el eje vertical la altura del trazo será el valor que tenga u2, que es el segundo componente del vector “U”. Una cantidad que nos sería útil es la norma del vector “U”. Estas de la derecha y la izquierda son barras dobles que denotan la norma o longitud del vector “U”; es decir, la distancia euclidiana del vector “U”. Este teorema de Pitágoras es igual a la raíz cuadrada de “U”1 cuadrada más “U”2 cuadrada. ¿Sí? Esta es la longitud del vector “U” expresada como número real. ¿Cuál es la longitud de esta línea o la longitud de este vector de aquí? ¿Cuál es la longitud de esta flecha que dibujé para ilustrar la norma de “U”? Ahora, regresemos un poco y veamos el vector “V” porque queremos calcular el producto interno. “V” es otro vector con los valores “V”1 y “V”2 y “V” se verá así. Ahora, regresemos para ver cómo calcular el producto interno entre “U” y “V”. A continuación les diré cómo hacerlo. Tomaremos el vector “V” y lo proyectaremos al vector “U”. Tomaré la proyección ortogonal o de 90 grados y la proyectaré en “U” así. Bajaré una línea aquí. Después, mediremos la longitud de esta línea roja que dibujé aquí. Designaré con una “P” la longitud de esta línea roja. “P”, entonces, es la longitud o la magnitud de la proyección del vector “V” en el vector “U”. Lo voy a escribir aquí. Entonces, “P” es la longitud de la proyección del vector “V” en el vector “U”. Es posible que el producto interno “U” transpuesta de “V” sea igual a “P” multiplicado por la norma o la longitud del vector “U”. Esta es una manera de calcular el producto interno. Si desarrollas la geometría y resuelves cuál es “P” y cuál es la norma de “U”, podrás obtener el mismo resultado que con la otra manera de calcular el producto interno que ¿Correcto? en tomar “U” transpuesta de “V” y trasponer “V” como “U1” y “U2”; es decir, como una matriz de uno por dos, por 1 por V. Esto debería darte como resultado “U”1, “V”1 más “U”2, “V”2. El hecho de que estas dos fórmulas arrojen los mismos resultados es un teorema algebraico. Por cierto “U” transpuesta de “V” es igual a “V” transpuesta de “U” de manera que el proceso es el mismo, pero al revés. En vez de proyectar “V” en “U”, puedes proyectar “U” en “V” y seguir con el mismo proceso pero con las columnas de “U” y “V” invertidas. Deberías poder obtener el mismo número, cual sea, para ambas. Aclararé lo que está pasando en esta ecuación: la norma de “U” es un número real al igual que “P”. “U” transpuesta de “V” es la multiplicación regular de dos números reales: la longitud de “P” por la norma de “V”. Un último detalle: si observas la norma de “P”, “P” está firmado y puede ser positivo o negativo. Explicaré esto. Si “U” es un vector que se ve así y “V” es un vector que se ve así y si el ángulo entre “U” y “V” es mayor a 90 grados, entonces, lo que obtendré si proyecto “U” en “V” es una proyección con este aspecto. Esta es la longitud de “P”. Y, en este caso, de nuevo tendré que “U” transpuesta de “V” es igual a “P” por la norma de “U”, excepto por que en este ejemplo “P” será negativa. Así que los productos internos, si el ángulo entre “U” y “V” es menor a 90 grados, entonces “P” es la longitud positiva de esta línea roja, mientras que si este ángulo es mayor a 90 grados, “P” será la longitud negativa de estos segmentos de línea. Por lo tanto, el producto interno entre dos vectores puede ser negativo cuando el ángulo entre ellos es mayor a 90 grados. Así es como funcionan los productos internos de los vectores. Utilizaremos las propiedades del producto interno de los vectores para intentar entender el objetivo de optimización de la máquina de vector de soporte. Aquí tenemos el objetivo de optimización para la máquina de vector de soporte que utilizamos antes. En esta diapositiva, haré una simplificación sólo para facilitar el análisis del objetivo. Lo que haré es ignorar los términos de intersección. Ignoraremos «theta» 0 y la igualaremos a 0. Para facilitar el trazo, pondré “N”, el número de variables, igual a 2. Por lo tanto, tenemos sólo dos variables, “X”1 y “X”2. Ahora, veamos la función objetiva o el objetivo de optimización de la SVM. Aquí tenemos sólo dos variables donde “N” es igual a 2. Esto se puede expresar como un medio de «theta» 1 cuadrada más «theta» 2 cuadrada porque sólo tenemos dos parámetros: «theta» 1 y «theta» 2. Lo que haré a continuación es volver a escribir esto. Lo escribiré como un medio por el cuadrado de la raíz cuadrada de «theta» 1 más «theta» 2. La razón por la cual puedo hacer esto es porque para cualquier número, el cuadrado de la raíz cuadrada de “W” es igual a “W”. La raíz cuadrada al cuadrado debe darte el mismo resultado. Puedes darte cuenta de que este término de adentro del paréntesis es igual a la norma o la longitud del vector «theta». A lo que me refiero con esto es a que si escribo el vector «theta» como «theta» 1, «theta» 2, entones el término que subrayé en rojo es exactamente la longitud o la norma del vector «theta», relativa a la definición de la norma del vector que teníamos en la diapositiva anterior. De hecho, esto es igual a la longitud del vector «theta» sin importar si lo escribes como «theta» 0, «theta» 1, «theta», 2, etc; si «theta» 0 igual a 0, como asumimos anteriormente o como la longitud de «theta» 1, «theta» 2. Para esta diapositiva ignoraremos «theta» 0. Entonces, trataremos «theta» o escribiremos la «theta» normal solamente con «theta» 1 y «theta» 2. Los cálculos resultan correctos de cualquier manera ya sea si incluimos «theta» 0 o si no. Esto no importará para el resto de nuestra derivada. Finalmente, esto indica que mi objetivo de optimización es igual a un medio de la norma de «theta» cuadrada. Lo que está haciendo la máquina de vector de soporte en el objetivo de optimización es minimizar la norma cuadrada o la longitud cuadrada del parámetro vector «theta». Ahora, me gustaría mirar estos términos, «theta» transpuesta de “x” y entender mejor lo que hacen. Con un parámetro vector «theta» dado y un ejemplo “X” dado, ¿cuál es el resultado? En la diapositiva anterior nos dimos cuenta de cómo se ve “U” transpuesta de “V” con diferentes vectores “U” y “V”. Ahora tomaremos esas definiciones en donde «theta» y “X(i)” jugarán el papel de “U” y “V” y veremos cómo se ve nuestro trazo. Digamos que sólo tomo en cuenta un ejemplo de entrenamiento o que sólo tengo un ejemplo positivo expresado con esta cruz roja y denominado “X(i)”. Lo que significa esto es que tracé en el eje horizontal un valor “X(i)1” y en el eje vertical “X(i)2”. Así es como trazo mis ejemplos de entrenamiento. Aunque no hemos estado pensando en esto como un vector, realmente es un vector desde el origen, (0,0) hasta la ubicación de este ejemplo de entrenamiento. Ahora digamos que tenemos un parámetro vector «theta» y que lo trazaremos también como vector. Si trazo «theta» 1 y «theta» 2 ¿cuál es el producto interno de «theta» transpuesta de “X(i)”? Con el método anterior, podemos calcular esto tomando el ejemplo y proyectándolo en mi parámetro vector «theta». Luego veré la longitud de este segmento que estoy marcando con rojo y al que llamaré “P” superíndice “i” para denotar que es la proyección del ejemplo de aprendizaje en el parámetro vector «theta». Lo que tenemos que es «theta» transpuesta de “X(i)” es igual a lo que teníamos en la diapositiva anterior; es decir, será igual a por la longitud o la norma del vector «theta». Esto es igual a «theta» 1 “x” 1 más «theta» 2 “x” 2. Cada uno de estos métodos es igualmente válido para calcular el producto interno entre «theta» y “X(i)”. Ok. ¿En dónde nos deja esto? Lo que significa esto es que estas limitaciones; es «theta» transpuesta de “X(i)” sea más grande o igual a 1 o menor que 1, es que puede remplazar el uso de las limitaciones para que “P(i)” por “X” sea mayor o igual a 1. Porque «theta» transpuesta de “X(i)” es igual a “P(i)” por la norma de «theta». Lo que obtenemos cuando escribimos esto en nuestro objetivo de optimización es, en vez de «theta» transpuesta de “X(i)”, tengo “P(i)” por la norma de «theta». Como recordatorio, mencionamos antes que este objetivo de optimización puede escribirse como un media veces la norma de «theta» al cuadrado. Ahora, consideremos el ejemplo de entrenamiento que está abajo y continuemos utilizando la simplificación de «theta» 0 igual a 0. Veamos qué barrera de decisiones elige la máquina de vector de soporte. Aquí hay una opción: digamos que la máquina de vector de soporte eligiera esta barrera de decisión. No es una buena decisión porque tiene márgenes muy pequeños; por lo tanto, la barrera de decisión pasa muy cerca de los ejemplos de entrenamiento. Veamos por qué la máquina de vector de soporte no hace esto. Para esta elección de parámetros es posible mostrar que el parámetro vector «theta», en realidad, cruza a 90 grados con la barrera de decisión. Entonces, la barrera de decisión verde corresponde al parámetro vector «theta» que apunta en esa dirección. Por cierto, la simplificación de que «theta» 0 es igual a 0 sólo significa que la barrera de decisión debe pasar por el origen (0,0). Ahora, veamos qué implica esto en el objetivo de optimización. Tomemos este ejemplo y digamos que es mi primer ejemplo “X”1. Aquí está la proyección de este ejemplo en el parámetro «theta», Esta es la proyección. Este segmento de línea roja. es igual a “P”1. Esto tendrá un resultado pequeño, ¿cierto? De manera similar, tomaré este ejemplo de aquí es “X”2 o mi segundo ejemplo. Observaré la proyección de este ejemplo en «theta». Entonces, permítanme dibujarlo en magenta. Este pequeño segmento de línea magenta será “P”2. Es decir, la proyección del segundo ejemplo en la dirección del parámetro vector «theta» que se extiende así. Este pequeño segmento de línea proyectada se vuelve muy pequeño. “P”2, de hecho, será un número negativo porque va en la dirección opuesta. Este vector tiene un ángulo mayor a 90 grados y con el parámetro vector «theta» será menor a 0. A lo que llegamos es a que estos términos “P(i)” serán números muy pequeños. Si miramos al objetivo de optimización veremos que para los ejemplos positivos necesitamos que “P(i)” por la norma de «theta» sea mayor que cualquiera de las dos, pero si “P(i)” o “P”1 es muy pequeño, significa que necesitamos que la norma de «theta» sea grande ¿cierto? Si “P”1 de «theta» es pequeña, pero queremos que “P”1 por la norma de «theta» sea mayor a 1, la única manera de lograrlo, la única manera de obtener una ganancia alta para estos dos números si “P”1 es pequeña, es ajustar la norma de «theta» con un valor alto. De manera similar, para nuestros ejemplos negativos necesitamos que “P” 2 por la norma de «theta» sea menor o igual a menos 1. Ya vimos en este ejemplo que “P”2 será un número negativo pequeño. La única manera de obtener este número es si la norma de «theta» es alta. Lo que estamos haciendo en el objetivo de optimización es intentar encontrar una configuración de los parámetros en la que la norma de «theta» sea pequeña. Esto no parece ser una buena dirección para el parámetro vector «theta». En contraste, veamos una barrera de decisión diferente. Digamos que la SVM elige esta barrera de decisión. El panorama será muy diferente. Si esta es la barrera de decisión, esta es la dirección correspondiente de «theta». La barrera de decisión es esta línea vertical y es posible mostrar, utilizando álgebra lineal, que la manera de obtener esta barrera de decisión verde es tener el vector «theta» a 90 grados con respecto a ella. Ahora, veamos la proyección de estos datos en el vector “x”. Digamos que, igual que antes, este ejemplo será “X”1. Cuando proyecto esto en x o en «theta», lo que encontraré es “P”1. Esta longitud es “P”1. El otro ejemplo es “X”2. Si hago la misma proyección en “X”2 me encuentro con que esta longitud, “P”2, es menor a 0. Puedes darte cuenta ahora de que las longitudes proyecciones “P”1 y “P”2 serán mucho mayores. Si aún necesitamos aplicar estas limitaciones, que “P”1 de la norma de «theta» sea igual o mayor que 1, la norma de «theta» podrá ser menor, porque “P”1 es mucho mayor ahora. Lo que significa esto es que al elegir la barrera de decisión que se muestra a la derecha en vez de la de la izquierda, la SVM puede hacer que la norma del parámetro «theta» sea mucho menor. Así que la norma de «theta» será menor y, por lo tanto, la norma cuadrada de «theta» también será menor. Por esto, la SVM elegiría esta hipótesis de la derecha. Así es como la SVM origina este efecto de clasificación de márgenes amplios. Tomando como referencia esta línea verde o esta hipótesis verde, queremos que la proyección de los ejemplos positivos y negativos en «theta» sea grande. La única manera de lograrlo es si, alrededor de esta línea verde hay un margen amplio o un espacio grande que separa los ejemplos negativos de los positivos. La magnitud de este margen es exactamente los valores de “P”1, “P”2, “P”3, etc. y al hacer que este margen sea mayor, la SVM puede terminar con un valor más pequeño de «theta» y es justo lo que queremos hacer en el objetivo. La razón por la cual la máquina termina con clasificadores de márgenes amplios es porque intenta maximizar la norma de estos valores P(i), que son la distancia de los ejemplo de entrenamiento a la barrera de decisión. Finalmente, hicimos toda esta derivada utilizando la simplificación de «theta» 0 igual a 0. El efecto de esto, como mencioné antes, es que si «theta» 0 es igual a 0, estaremos generando barreras de decisión que pasan por el origen. Las barreras de decisión pasarán, así, por el origen. Si permites que «theta» 0 sea mayor que 0, lo que obtendremos serán barreras de decisión que no pasan por el origen, como esta que acabo de dibujar. No haré la derivada completa para esto; sin embargo, resulta que la misma prueba de márgenes amplios funciona del mismo modo. Hay una generalización de este argumento que planteamos hace un momento que nos dice que aún cuando «theta» 0 no es 0, lo que intenta hacer la SVM cuando tienes este objetivo de optimización, que corresponde al caso de cuando C es muy grande.... Es posible mostrar que cuando «theta» 0 no es igual a 0, esta máquina de soporte vectorial aún intenta encontrar un separador de márgenes amplios entre los ejemplos positivos y negativos. Esto explica la función de esta máquina de vector de soporte como clasificador de márgenes amplios. En el siguiente video empezaremos a hablar de cómo tomar algunas de estas ideas de SVM y aplicarlas para construir clasificadores no lineales.