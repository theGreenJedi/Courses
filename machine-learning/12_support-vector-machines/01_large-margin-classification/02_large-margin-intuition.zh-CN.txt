人们有时将支持向量机 看做是大间距分类器 在这一部分 我将介绍其中的含义 这有助于我们 直观理解 SVM模型的 假设是什么样的 这是我的支持向量机模型的代价函数 在左边这里 我画出了关于 z 的代价函数 cost1(z) 此函数用于正样本 而在右边这里我画出了 关于 z 的代价函数 cost0(z) 横轴表示 z 现在让我们考虑一下 最小化这些代价函数的必要条件是什么 如果你有一个正样本 y等于1 则只有在 z 大于等于1时 代价函数 cost1(z) 才等于0 换句话说 如果你有一个正样本 我们会希望 θ 转置乘以 x 大于等于1 反之 如果 y 是等于0的 我们观察一下 函数cost0(z) 它只有在 z小于等于1 的区间里 函数值为0 这是支持向量机的 一个有趣性质 不是么 事实上 如果你有一个正样本 y等于1 则其实我们仅仅要求 θ 转置乘以 x 大于等于0 就能将该样本恰当分出 这是因为如果 θ 转置乘以 x 比0大的话 我们的模型代价函数值为0 类似地 如果你有一个负样本 则仅需要 θ 转置乘以x 小于等于0 就会将负例正确分离 但是 支持向量机的要求更高 不仅仅要能正确分开输入的样本 即不仅仅 要求 θ 转置乘以 x 大于0 我们需要的是 比0值大很多 比如 大于等于1 我也想这个比0小很多 比如我希望它 小于等于-1 这就相当于在支持向量机中嵌入了 一个额外的安全因子 或者说安全的间距因子 当然 逻辑回归 做了类似的事情 但是让我们看一下 在支持向量机中 这个因子会 导致什么结果 具体而言 我接下来 会考虑一个特例 我们将这个常数 C 设置成 一个非常大的值 比如我们假设 C的值为100000 或者其它非常大的数 然后来观察支持向量机会给出什么结果 如果 C 非常大 则最小化代价函数的时候 我们将会很希望 找到一个 使第一项为0的 最优解 因此 让我们 尝试在 代价项的第一项 为0的情形下理解 该优化问题 比如我们可以把 C 设置成了非常大的常数 这将给我们 一些关于支持向量机 模型的直观感受 我们已经看到 输入一个训练样本 标签为 y=1 你想令第一项为0 你需要做的是 找到一个 θ 使得 θ 转置乘以 x 大于等于1 类似地 对于一个训练样本 标签为 y=0 为了使 cost0(z) 函数 这个函数 值为0 我们需要 θ 转置 乘以x 的值 小于等于-1 因此 现在考虑我们的优化问题 选择参数 使得第一项 等于0 就会导致下面的 优化问题 因为我们将 选择参数使第一项为0 因此这个函数的第一项为0 因此是 C 乘以0 加上二分之一 乘以第二项 这里第一项 是C乘以0 因此可以将其删去 因为我知道它是0 这将遵从以下的约束 θ 转置乘以 x(i) 大于或等于0.5 如果 y (i) 是等于1 的 θ 转置乘以x(i) 小于等于-1 如果样本i是 一个负样本 这样 当你 求解这个优化问题的时候 当你最小化这个关于变量 θ 的函数的时候 你会得到一个非常有趣的决策边界 具体而言 如果你考察 这样一个数据集 其中有正样本 也有负样本 可以看到 这个数据集是线性可分的 我的意思是 存在一条直线把正负样本分开 当然有多条不同的直线 可以把 正样本和负样本完全分开 比如 这就是一个决策边界 可以把正样本 和负样本分开 但是多多少少这个 看起来并不是非常自然 是么? 或者我们可以画一条更差的决策界 这是另一条决策边界 可以将正样本和负样本分开 但仅仅是勉强分开 这些决策边界看起来都不是特别好的选择 支持向量机将会选择 这个黑色的决策边界 相较于之前 我用粉色或者绿色 画的决策界 这条黑色的看起来好得多 黑线看起来 是更稳健的决策界 在分离正样本和负样本上它显得的更好 数学上来讲 这是什么意思呢 这条黑线有更大的距离 这个距离叫做间距 (margin) 当画出这两条 额外的蓝线 我们看到黑色的决策界 和训练样本之间有更大的最短距离 然而粉线和蓝线 离训练样本就非常近 在分离样本的时候就会 比黑线表现差 因此这张图片本身就有一定的误导性 这个距离叫做 支持向量机的 间距 而这是支持向量机 具有鲁棒性的原因 因为它努力 用一个最大间距来分离样本 因此支持向量机 有时被称为 大间距分类器 而这其实是 求解上一页幻灯片上优化问题的结果 我知道你也许 想知道 求解上一页幻灯片中的优化问题 为什么会产生这个结果 它是如何产生这个大间距分类器的呢 我知道我还没有解释这一点 在下一节视频中 我将会从直观上 略述 为什么 这个优化问题 会产生大间距分类器 总之这个图示 有助于你 理解 支持向量机模型的做法 即努力将正样本和负样本 用最大的间距分开 在本节课中 关于大间距分类器 我想讲最后一点 我们将这个大间距分类器 中的正则化因子 常数C 设置的非常大 我记得我将其设置为了100000 因此对这样的一个数据集 也许我们将选择 这样的决策界从而最大间距地 分离开正样本和负样本 事实上 支持向量机现在 要比这个大间距分类器所体现的 更成熟 尤其是当你使用 大间距分类器的时候 你的学习算法 会受异常点 (outlier) 的影响 比如我们加入 一个额外的正样本 在这里 如果你加了这个样本 为了将样本 用最大间距分开 也许我最终 会得到一条类似这样的决策界 对么? 就是这条粉色的线 仅仅基于 一个异常值 仅仅基于一个样本 就将 我的决策界 从这条黑线变到这条粉线 这实在是不明智的 而如果正则化参数 C 设置的非常大 这事实上正是 支持向量机将会做的 它将决策界 从黑线 变到了粉线 但是如果 C 设置的小一点 如果你将 C 设置的不要太大 则你最终会得到 这条黑线 当然数据如果不是线性可分的 如果你在这里 有一些正样本 或者 你在这里有一些负样本 则支持向量机也会将它们恰当分开 因此 大间距分类器的描述 真的 仅仅是从直观上给出了 正则化参数 C 非常大的情形 同时 要提醒你 C 的作用 类似于 λ 分之一 λ 是 我们之前使用过 的正则化参数 这只是C非常大的情形 或者等价地 λ 非常小的情形 你最终会得到 类似粉线这样的决策界 但是实际上 应用支持向量机的时候 当 C 不是 非常非常大的时候 它可以忽略掉一些异常点的影响 得到更好的决策界 甚至当你的数据不是线性可分的时候 支持向量机也可以给出好的结果 我们稍后会介绍一点 支持向量机的偏差和方差 希望在那时候 关于如何处理参数的这种平衡会变得 更加清晰 我希望 这节课给出了一些 关于为什么支持向量机 被看做大间距分类器的直观理解 它用最大间距将样本区分开 尽管从技术上讲 这只有当 参数C是非常大的时候是真的 但是它对于理解支持向量机是有益的 本节课中 我们略去了一步 那就是我们在幻灯片中 给出的优化问题 为什么会是这样的 它是如何 得出大间距分类器的 我在本节中没有讲解 在下一节课中 我将略述 这些问题背后的数学原理 来解释 这个优化问题 是如何 得到一个大间距分类器的【教育无边界字幕组】翻译: shamolvzhou 校对/审核: 所罗门捷列夫