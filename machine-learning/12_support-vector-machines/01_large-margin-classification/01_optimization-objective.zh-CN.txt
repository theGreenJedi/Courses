到目前为止 你已经见过一系列不同的学习算法 在监督学习中 许多学习算法的性能 都非常类似 因此 重要的不是 你该选择使用 学习算法A还是学习算法B 而更重要的是 应用这些算法时 所创建的 大量数据 在应用这些算法时 表现情况通常依赖于你的水平 比如 你为学习算法 所设计的 特征量的选择 以及如何选择正则化参数 诸如此类的事 还有一个 更加强大的算法 广泛的应用于 工业界和学术界 它被称为支持向量机(Support Vector Machine) 与逻辑回归和神经网络相比 支持向量机 或者简称SVM 在学习复杂的非线性方程时 提供了一种更为清晰 更加强大的方式 因此 在接下来的视频中 我会探讨 这一算法 在稍后的课程中 我也会对监督学习算法 进行简要的总结 当然  仅仅是作简要描述 但对于支持向量机 鉴于该算法的强大和受欢迎度 在本课中 我会花许多时间来讲解它 它也是我们所介绍的 最后一个监督学习算法 正如我们之前开发的学习算法 我们从 优化目标开始 那么 我们开始学习 这个算法 为了描述支持向量机 事实上 我将会 从逻辑回归开始 展示我们如何 一点一点修改 来得到本质上的支持向量机 那么 在逻辑回归中 我们已经熟悉了 这里的假设函数形式 和右边的S型激励函数 然而 为了解释 一些数学知识 我将用 z 表示 θ 转置乘以 x 现在 让一起考虑下 我们想要逻辑回归做什么 如果有一个 y=1 的样本 我的意思是 不管是在训练集中 或是在测试集中 又或者在交叉验证集中 总之是 y=1 现在 我们希望 h(x) 趋近1 因为 我们想要 正确地将此样本分类 这就意味着 当 h(x) 趋近于1时 θ 转置乘以 x 应当 远大于0 这里的 大于大于号 >> 意思是 远远大于0 这是因为 由于 z 表示 θ 转置乘以 x 当 z 远大于 0时 即到了 该图的右边 你不难发现 此时逻辑回归的输出将趋近于1 相反地 如果我们 有另一个样本 即 y=0 我们希望假设函数 的输出值 将趋近于0 这对应于 θ 转置乘以 x 或者就是 z 会 远小于0 因为对应的 假设函数的输出值趋近0 如果你进一步 观察逻辑回归的代价函数 你会发现 每个样本 (x, y) 都会为总代价函数 增加这里的一项 因此 对于总代价函数 通常会有对所有的训练样本求和 并且这里还有一个1/m项 但是 在逻辑回归中 这里的这一项 就是表示一个训练样本 所对应的表达式 现在 如果我将完整定义的 假设函数 代入这里 那么 我们就会得到 每一个训练样本都影响这一项 现在 先忽略1/m这一项 但是这一项 是影响整个总代价函数 中的这一项的 现在 一起来考虑两种情况 一种是y等于1的情况 一种是y等于0的情况 在第一种情况中 假设 y 等于1 此时 在目标函数中 只需有第一项起作用 因为y等于1时 (1-y) 项将等于0 因此 当在y等于 1的样本中时 即在 (x, y) 中 y等于1 我们得到 -log(1/(1+e^z) ) 这样一项 这里同上一张幻灯片一致 我用 z 表示 θ 转置乘以 x 当然 在代价函数中 y 前面有负号 我们只是这样表示 如果y等于1 代价函数中 这一项也等于1 这样做 是为了简化 此处的表达式 如果画出 关于 z 的函数 你会看到 左下角的 这条曲线 我们同样可以看到 当 z 增大时 也就是相当于 θ 转置乘以x 增大时 z 对应的值 会变的非常小 对整个代价函数而言 影响也非常小 这也就解释了 为什么 逻辑回归在观察到 正样本 y=1 时 试图将 θ^T*x 设置的非常大 因为 在代价函数中的 这一项会变的非常小 现在 开始建立支持向量机 我们从这里开始 我们会从这个代价函数开始 也就是 -log(1/(1+e^z)) 一点一点修改 让我取这里的 z=1 点 我先画出将要用的代价函数 新的代价函数将会 水平的从这里到右边 (图外) 然后我再画一条 同逻辑回归 非常相似的直线 但是 在这里 是一条直线 也就是 我用紫红色画的曲线 就是这条紫红色的曲线 那么 到了这里 已经非常接近 逻辑回归中 使用的代价函数了 只是这里是由两条线段组成 即位于右边的水平部分 和位于左边的 直线部分 先别过多的考虑左边直线部分的斜率 这并不是很重要 但是 这里 我们将使用的新的代价函数 是在 y=1 的前提下的 你也许能想到 这应该能做同逻辑回归中类似的事情 但事实上 在之后的的优化问题中 这会变得更坚定 并且为支持向量机 带来计算上的优势 例如 更容易计算股票交易的问题 等等 目前 我们只是讨论了 y=1 的情况 另外 一种情况是当 y=0 时 此时 如果你仔细观察代价函数 只留下了第二项 因为第一项 被消除了 如果当 y=0 时 那么 这一项也就是0了 所以上述表达式 只留下了第二项 因此 这个样本的代价 或是代价函数的贡献 将会由 这一项表示 并且 如果你将 这一项作为 z 的函数 那么 这里就会得到横轴z 现在 你完成了 支持向量机中的部分内容 同样地 再来一次 我们要替代这一条蓝色的线 用相似的方法 如果我们用 一个新的代价函数来代替 即这条从0点开始的水平直线 然后是一条斜线 像这样 那么 现在让我给 这两个方程命名 左边的函数 我称之为 cost1(z) 同时 在右边函数 我称它为 cost0(z) 这里的下标是指 在代价函数中 对应的 y=1 和 y=0 的情况 拥有了这些定义后 现在 我们就开始构建支持向量机 这是我们在逻辑回归中使用 代价函数 J(θ) 也许这个方程 看起来不是非常熟悉 这是因为 之前 有个负号在方程外面 但是 这里我所做的是 将负号移到了 表达式的里面 这样做使得方程 看起来有些不同 对于支持向量机而言 实质上 我们要将这一 替换为 cost1(z) 也就是cost1(θ^T*x) 同样地 我也将 这一项替换为cost0(z) 也就是代价 cost0(θ^T*x) 这里的代价函数 cost1 就是之前所提到的那条线 看起来是这样的 此外 代价函数 cost0 也是上面所介绍过的那条线 看起来是这样 因此 对于支持向量机 我们得到了这里的最小化问题 即 1/m 乘以 从1加到第 m 个 训练样本 y(i) 再乘以 cost1(θ^T*x(i)) 加上1减去 y(i) 乘以 cost0(θ^T*x(i)) 然后 再加上正则化参数 像这样 现在 按照支持向量机的惯例 事实上 我们的书写 会稍微有些不同 代价函数的参数表示也会稍微有些不同 首先 我们要 除去 1/m 这一项 当然 这仅仅是 仅仅是由于 人们使用支持向量机时 对比于逻辑回归而言 不同的习惯所致 但这里我所说的意思是 你知道 我将要做的是 仅仅除去 1/m 这一项 但是 这也会得出 同样的θ最优值 好的 因为 1/m 仅是个常量 因此 你知道 在这个最小化问题中 无论前面是否有 1/m 这一项 最终我所得到的 最优值θ都是一样的 这里我的意思是 先给你举一个实例 假定有一最小化问题 即要求当 (u-5)^2+1 取得最小值时的 u 值 好的 这时最小值为 当 u=5 时取得最小值 现在 如果我们想要 将这个目标函数 乘上常数10 这里我的最小化问题就变成了 求使得 10×(u-5)^2+10 最小的值u 然而 这里的u值 使得这里最小的u值仍为5 因此 将一些常数 乘以你的最小化项 例如 这里的常数10 这并不会改变 最小化该方程时得到u值 因此 这里我所做的 是删去常量m 也是相同的 现在 我将目标函数 乘上一个常量 m 并不会改变 取得最小值时的 θ 值 第二点概念上的变化 我们只是指在使用 支持向量机时 一些如下的标准惯例 而不是逻辑回归 因此 对于逻辑回归 在目标函数中 我们有两项 第一个是这一项 是来自于 训练样本的代价 第二个是这一项 是我们的正则化项 我们不得不去 用这一项来平衡 这就相当于 我们想要最小化 A 加上 正则化参数 λ 然后乘以 其他项 B 对吧？ 这里的 A 表示 这里的第一项 同时 我用 B 表示 第二项 但不包括 λ 我们不是 优化这里的 A+λ×B 我们所做的 是通过设置 不同正则参数 λ 达到优化目的 这样 我们就能够权衡 对应的项 是使得训练样本拟合的更好 即最小化 A 还是保证正则参数足够小 也即是 对于B项而言 但对于支持向量机 按照惯例 我们将使用一个不同的参数 为了替换这里使用的 λ 来权衡这两项 你知道 就是第一项和第二项 我们 依照惯例使用 一个不同的参数 称为C 同时改为优化目标 C×A+B 因此 在逻辑回归中 如果给定 λ 一个非常大的值 意味着给予B更大的权重 而这里 就对应于将C 设定为非常小的值 那么 相应的将会给 B 比给 A 更大的权重 因此 这只是 一种不同的方式来控制这种权衡 或者一种不同的方法 即用参数来决定 是更关心第一项的优化 还是更关心第二项的优化 当然你也可以 把这里的参数C 考虑成 1/λ 同 1/λ 所扮演的 角色相同 并且这两个方程 或这两个表达式并不相同 因为 C 等于 1/λ 但是也并不全是这样 如果当C等于 1/λ 时 这两个 优化目标应当 得到相同的值 相同的最优值θ 因此 就用它们来代替 那么 我现在删掉这里的 λ 并且用常数 C 来代替这里 因此 这就得到了 在支持向量机中 我们的整个优化目标函数 然后最小化 这个目标函数 得到 SVM 学习到的 参数C 最后 有别于逻辑回归 有别于逻辑回归 输出的概率 在这里 我们的代价函数 当最小化代价函数 获得参数θ时 支持向量机所做的是 它来直接预测 y的值等于1 还是等于0 因此 这个假设函数 会预测1 当 θ^T*x 大于 或者等于0时 或者等于0时 所以学习 参数 θ 就是支持向量机假设函数的形式 那么 这就是 支持向量机 数学上的定义 再接下来的视频中 让我们再回去 从直观的角度看看优化目标 实际上是在做什么 以及 SVM 的假设函数 将会学习什么 同时 也会谈谈 如何 做些许修改 学习更加复杂、非线性的函数 【教育无边界字幕组】翻译：陰岚 校对/审核：所罗门捷列夫