1
00:00:00,750 --> 00:00:02,160
ときどき皆さんはSVMを

2
00:00:02,520 --> 00:00:04,380
大きなマージンの

3
00:00:04,990 --> 00:00:06,950
分類器だといいますが、今回、

4
00:00:07,080 --> 00:00:08,030
みなさんには

5
00:00:08,410 --> 00:00:09,500
この意味と、

6
00:00:09,780 --> 00:00:10,520
皆さんに役立つであろう

7
00:00:11,030 --> 00:00:12,780
SVMの仮定がどんなものかという

8
00:00:13,020 --> 00:00:17,460
全体像についてお話します。

9
00:00:18,070 --> 00:00:19,290
これがサポートベクターマシンでのコスト関数です。

10
00:00:21,310 --> 00:00:22,290
ここで左に

11
00:00:22,790 --> 00:00:24,300
cost1(z)の関数をプロットした、

12
00:00:24,560 --> 00:00:28,100
これは陽性の手本に対して使う物だ。
そして右には

13
00:00:30,080 --> 00:00:31,510
cost0(z)をプロットした、

14
00:00:31,950 --> 00:00:33,850
ここでzは横軸に取った。

15
00:00:34,380 --> 00:00:35,520
ここで、これらのコスト関数を小さくすると

16
00:00:35,650 --> 00:00:38,380
どうなるかを考えてみよう。

17
00:00:39,660 --> 00:00:40,970
もし陽性の手本があったとして、

18
00:00:41,950 --> 00:00:43,170
つまりy=1の時、

19
00:00:43,490 --> 00:00:45,060
cost1のzは

20
00:00:45,200 --> 00:00:46,750
zが1以上の時にだけ

21
00:00:47,700 --> 00:00:50,070
0となる。

22
00:00:50,180 --> 00:00:51,370
言い換えると、

23
00:00:51,510 --> 00:00:52,860
陽性の手本に対しては、

24
00:00:53,110 --> 00:00:54,550
シータ転置xが1以上で

25
00:00:54,870 --> 00:00:55,760
あって欲しい、という事。

26
00:00:56,450 --> 00:00:58,030
そして反対に、

27
00:00:58,150 --> 00:00:59,300
もしy=0の時は

28
00:00:59,510 --> 00:01:00,490
このcost0(z)関数を見ると、

29
00:01:01,560 --> 00:01:03,000
この領域の時だけ

30
00:01:03,200 --> 00:01:04,310
つまりzが1以下の時だけ（訳注: -1の間違いか）

31
00:01:04,460 --> 00:01:05,810
cost0(z)は

32
00:01:06,150 --> 00:01:07,320
ゼロとなる。

33
00:01:07,610 --> 00:01:10,150
ゼロとなる。

34
00:01:10,640 --> 00:01:12,270
これはサポートベクターマシンの

35
00:01:12,560 --> 00:01:13,630
面白い性質だ。

36
00:01:13,800 --> 00:01:15,060
もし陽性の手本、

37
00:01:15,440 --> 00:01:17,650
つまりy=1の時は

38
00:01:18,370 --> 00:01:19,250
本当に必要なのは

39
00:01:19,550 --> 00:01:21,950
シータ転置xが0以上であれば十分なはずだ。

40
00:01:22,970 --> 00:01:25,270
それは正しく分類した、って事なんだから。

41
00:01:25,860 --> 00:01:26,950
だってシータ転置xが0より大きければ

42
00:01:27,510 --> 00:01:28,980
仮説は0を予言するんだから（訳注: 1の間違いか）

43
00:01:29,840 --> 00:01:30,710
同様に、陰性の手本があったら

44
00:01:31,340 --> 00:01:34,090
あなたが望むのはただシータ転置xが

45
00:01:34,850 --> 00:01:37,290
ゼロより小さければ本当は良くて、それだけで手本で正解出来ている。

46
00:01:37,670 --> 00:01:40,230
だが、サポートベクターマシンは、それよりももうちょっと多くを要求する。

47
00:01:40,580 --> 00:01:43,360
それはぎりぎり手本が正しければ良いだけにとどまらず、

48
00:01:44,320 --> 00:01:45,990
つまり単にゼロよりちょっとでも

49
00:01:46,240 --> 00:01:47,580
大きければ良いというのではなく、

50
00:01:47,890 --> 00:01:48,870
それが要求するのは、ゼロよりも

51
00:01:49,060 --> 00:01:50,370
かなり大きいという事。

52
00:01:50,490 --> 00:01:51,430
1よりもちょっと大きい、

53
00:01:51,680 --> 00:01:52,530
と言っている。

54
00:01:52,870 --> 00:01:54,400
そしてこれは0よりもずっと小さくしたい。

55
00:01:54,800 --> 00:01:55,970
たとえば-1以下とかに

56
00:01:56,230 --> 00:01:58,140
したい。

57
00:01:58,830 --> 00:02:00,000
つまりこれは、追加のセーフティーファクターを

58
00:02:00,120 --> 00:02:01,660
またはセーフティーマージンを

59
00:02:02,070 --> 00:02:03,630
サポートベクターマシンに組み込むと言える。

60
00:02:04,030 --> 00:02:05,700
ロジスティック回帰も

61
00:02:06,340 --> 00:02:07,620
もちろん似たような事をしていたが、

62
00:02:07,820 --> 00:02:08,900
何が起こるか見てみよう。

63
00:02:09,110 --> 00:02:10,350
またはサポートベクターマシンの文脈では

64
00:02:10,460 --> 00:02:11,290
これはとういう結果になるのかを

65
00:02:11,360 --> 00:02:13,180
見てみよう。

66
00:02:14,830 --> 00:02:15,740
具体的には、次に私がやりたいのは、

67
00:02:16,010 --> 00:02:17,760
この定数Cに

68
00:02:17,900 --> 00:02:19,130
とても大きな値を

69
00:02:19,460 --> 00:02:21,240
セットしてみたい、

70
00:02:21,400 --> 00:02:23,340
というもの。

71
00:02:23,530 --> 00:02:24,700
ではCにとてもおおきな値、

72
00:02:24,820 --> 00:02:28,080
例えば何十万もの値、なんらかの巨大な値をセットしのを想像してみよう。

73
00:02:29,370 --> 00:02:31,290
サポートベクターマシンが何をするか、見てみよう。

74
00:02:31,580 --> 00:02:33,510
もしCがとっても、とっても大きいと、

75
00:02:33,820 --> 00:02:35,340
その場合はこの最適化の目的関数を

76
00:02:36,350 --> 00:02:38,080
最小化する時に、

77
00:02:38,300 --> 00:02:39,640
値を選ぶ時に

78
00:02:39,950 --> 00:02:41,240
この項がゼロになるように、

79
00:02:41,380 --> 00:02:43,180
とても高く動機づけされる。

80
00:02:44,810 --> 00:02:46,250
では目的関数のこの最初の項を

81
00:02:46,670 --> 00:02:48,320
0にしよう、というコンテキストで

82
00:02:48,430 --> 00:02:49,820
最適化問題は

83
00:02:50,050 --> 00:02:51,520
どうなるかを

84
00:02:51,880 --> 00:02:53,060
理解していこう。

85
00:02:53,470 --> 00:02:54,890
その理由は、

86
00:02:55,000 --> 00:02:56,100
我らはCをとても大きな定数に

87
00:02:56,250 --> 00:02:59,420
設定すると言った。

88
00:02:59,590 --> 00:03:00,780
これが、サポートベクターマシンの

89
00:03:01,300 --> 00:03:02,920
仮説がどんな感じか、さらなる直感を

90
00:03:03,110 --> 00:03:05,520
与えてくれる事を期待している。

91
00:03:06,440 --> 00:03:07,720
既に見たように、

92
00:03:08,140 --> 00:03:09,260
トレーニング手本のラベルy=1の時はいつでも

93
00:03:09,480 --> 00:03:11,350
最初の項を

94
00:03:11,690 --> 00:03:13,850
0にしたいなら

95
00:03:13,950 --> 00:03:15,050
やるべき事は

96
00:03:15,240 --> 00:03:16,280
シータ転置xが

97
00:03:16,450 --> 00:03:17,680
1以上になるような

98
00:03:17,990 --> 00:03:20,380
シータを

99
00:03:20,690 --> 00:03:22,800
探すという事。

100
00:03:23,220 --> 00:03:24,250
同様に、ラベル0の手本の時は

101
00:03:24,960 --> 00:03:26,910
いつでも

102
00:03:27,240 --> 00:03:28,060
cost0が、、、

103
00:03:29,000 --> 00:03:30,520
cost0(z)が

104
00:03:30,610 --> 00:03:31,530
そのコストが0に確実になるには、

105
00:03:31,790 --> 00:03:33,250
シータ転置xを-1以下に

106
00:03:33,810 --> 00:03:36,180
しなくては

107
00:03:37,900 --> 00:03:38,740
ならない。

108
00:03:39,510 --> 00:03:40,770
つまり、我らの最適化問題を

109
00:03:41,050 --> 00:03:43,030
実際にパラメータを選んで

110
00:03:43,360 --> 00:03:45,000
この最初の項を

111
00:03:45,710 --> 00:03:46,750
ゼロとしたら、

112
00:03:47,020 --> 00:03:48,170
その後に

113
00:03:49,130 --> 00:03:50,230
残るのは

114
00:03:50,330 --> 00:03:51,670
以下の最適化問題だ。

115
00:03:52,050 --> 00:03:53,720
我らが最小化するのは、

116
00:03:53,980 --> 00:03:55,360
最初の項が0なので、

117
00:03:55,590 --> 00:03:56,710
C掛ける0 だ。何故ならそれが0になるように

118
00:03:56,870 --> 00:03:58,040
パラメータを選ぶのだから。

119
00:03:58,150 --> 00:03:59,710
それに足すことの1/2の、

120
00:04:00,330 --> 00:04:01,330
えーと、

121
00:04:01,460 --> 00:04:05,440
二番目の項。

122
00:04:05,620 --> 00:04:06,880
この最初の項は C掛ける0 だから

123
00:04:07,160 --> 00:04:08,020
バッテンで消してしまおう。

124
00:04:08,130 --> 00:04:11,210
だってゼロなんだから。

125
00:04:11,380 --> 00:04:12,570
そしてこれは、シータ転置x(i)が

126
00:04:13,400 --> 00:04:15,410
1以上という

127
00:04:16,390 --> 00:04:17,560
制約条件に従う。

128
00:04:18,700 --> 00:04:20,930
y(i)=1の時は

129
00:04:22,180 --> 00:04:24,150
シータ転置x(i)が

130
00:04:24,940 --> 00:04:26,560
-1以下となる。

131
00:04:26,690 --> 00:04:28,060
陰性の

132
00:04:29,030 --> 00:04:31,680
手本の時には

133
00:04:32,110 --> 00:04:34,460
いつでも。

134
00:04:34,540 --> 00:04:35,520
そして結局、

135
00:04:35,660 --> 00:04:37,930
この最適化問題を解くと、

136
00:04:38,070 --> 00:04:39,440
パラメータシータの関数としてこれを最小化すると、

137
00:04:40,710 --> 00:04:42,090
とても興味深い決定境界が得られる。

138
00:04:42,590 --> 00:04:44,870
具体的に、

139
00:04:45,010 --> 00:04:46,470
このようなデータセットを見た時、

140
00:04:46,750 --> 00:04:49,660
そこには陽性と陰性のサンプルがある訳だが、

141
00:04:50,920 --> 00:04:52,430
このデータは線形で分離可能。

142
00:04:52,710 --> 00:04:54,960
それの意味するところは、ある直線ーー

143
00:04:55,530 --> 00:04:56,830
たくさんの異なる直線が有り得るが、

144
00:04:56,920 --> 00:04:57,810
それらが陽性と陰性のサンプルを

145
00:04:58,720 --> 00:05:01,060
完璧に分離する、という事。

146
00:05:01,560 --> 00:05:02,710
例えば、これは陽性と陰性の

147
00:05:04,270 --> 00:05:05,430
サンプルを分ける決定境界の一つだが、

148
00:05:05,570 --> 00:05:06,840
とても以前な

149
00:05:07,030 --> 00:05:07,810
それという

150
00:05:07,900 --> 00:05:09,680
感じはしない。

151
00:05:09,810 --> 00:05:11,050
もっと酷いのを描いてみると、

152
00:05:11,230 --> 00:05:13,540
これも確かに、陽性と陰性のサンプルを

153
00:05:13,710 --> 00:05:14,830
分離している決定境界だが、

154
00:05:14,900 --> 00:05:15,960
だがギリギリだ。

155
00:05:16,120 --> 00:05:18,530
これらはどちらも、そんなに良い選択っぽくは無い。

156
00:05:20,420 --> 00:05:22,880
サポートベクターマシンはそうではなく、

157
00:05:23,140 --> 00:05:26,450
この決定境界を選ぶ、この黒で描いた奴。

158
00:05:29,010 --> 00:05:30,030
そしてそれは、マゼンタや緑で描いた決定境界の

159
00:05:30,760 --> 00:05:32,310
どちらよりも、

160
00:05:32,420 --> 00:05:34,450
ずっとマシっぽい。

161
00:05:34,750 --> 00:05:35,790
黒い線の方が、よりロバストな

162
00:05:36,050 --> 00:05:37,840
分離器に見える。

163
00:05:38,610 --> 00:05:39,710
こっちの方が陽性と陰性のサンプルを分けるという仕事をうまくこなしてる。

164
00:05:39,800 --> 00:05:42,830
数学的には、それの意味する所は

165
00:05:43,530 --> 00:05:45,680
この黒い決定境界の方が大きな距離を持っているという事。

166
00:05:49,160 --> 00:05:50,580
この距離を、マージンと呼ぶ。

167
00:05:50,760 --> 00:05:51,790
この2つの追加の線を

168
00:05:52,380 --> 00:05:54,320
描いてみると分かるように、

169
00:05:54,540 --> 00:05:56,010
黒の決定境界は、トレーニング手本の中のサンプルへの最小距離が

170
00:05:56,240 --> 00:05:59,990
より大きい。

171
00:06:00,120 --> 00:06:01,350
他方マゼンタや緑の線は

172
00:06:01,580 --> 00:06:02,600
トレーニング手本に恐ろしいほど近い。

173
00:06:04,640 --> 00:06:06,100
だからそちらの方が、陽性と陰性を分離するには

174
00:06:06,500 --> 00:06:08,910
黒の線に比べるといまいちな仕事しかしていない感じがする。

175
00:06:09,850 --> 00:06:11,500
この距離が

176
00:06:11,800 --> 00:06:13,600
サポートベクターマシンの

177
00:06:13,960 --> 00:06:16,500
マージンと

178
00:06:16,600 --> 00:06:21,300
呼ばれる物。

179
00:06:21,500 --> 00:06:22,480
そしてこれがSVMに、ある程度のロバストさを与えている。

180
00:06:22,940 --> 00:06:24,010
何故ならそれは、

181
00:06:24,360 --> 00:06:25,530
データを出来るだけ大きなマージンになるように

182
00:06:25,700 --> 00:06:27,440
分離しようとするから。

183
00:06:29,210 --> 00:06:30,250
だからサポートベクターマシンは

184
00:06:30,380 --> 00:06:31,650
たまに 大きなマージン分類器 とも

185
00:06:31,830 --> 00:06:33,930
呼ばれている。

186
00:06:34,170 --> 00:06:36,180
そしてこれは実は、前のスライドで書いた

187
00:06:36,430 --> 00:06:39,370
最適化問題の帰結だ。

188
00:06:40,140 --> 00:06:40,950
おっと分かってるって。

189
00:06:41,100 --> 00:06:42,250
前のスライドに

190
00:06:42,400 --> 00:06:43,900
書いた最適化問題が

191
00:06:44,070 --> 00:06:45,080
どうなってこの

192
00:06:45,280 --> 00:06:47,270
大きなマージン分類器 になってるかって思ってるんでしょ？

193
00:06:48,350 --> 00:06:49,700
それをまだ説明してないってのは分かってるよ。

194
00:06:50,520 --> 00:06:51,570
それは次のビデオで

195
00:06:51,810 --> 00:06:53,340
さっきの最適化問題が

196
00:06:53,500 --> 00:06:55,180
なんで 大きなマージン分類器に

197
00:06:55,430 --> 00:06:57,080
なるのかの、ちょっとした直感の

198
00:06:57,570 --> 00:06:59,630
説明をするよ。

199
00:06:59,790 --> 00:07:00,860
だがこれは、

200
00:07:00,970 --> 00:07:01,780
SVMはどんな仮説を選ぶのか？を理解したければ、

201
00:07:01,920 --> 00:07:03,150
心にとめておく価値のある

202
00:07:03,290 --> 00:07:05,600
特徴だ。

203
00:07:06,140 --> 00:07:07,200
つまり、陽性と陰性のサンプルを、できるだけ大きなマージンになるように

204
00:07:07,270 --> 00:07:10,310
分離しようとする、という事。

205
00:07:12,890 --> 00:07:13,950
大きなマージン分類器について、

206
00:07:14,180 --> 00:07:15,930
最後に一つだけ言わせてくれ。

207
00:07:16,070 --> 00:07:17,900
この直感的な考え方だと、

208
00:07:18,030 --> 00:07:19,340
我らが書き下した 大きなマージン分類器 は、

209
00:07:20,010 --> 00:07:21,040
C、つまり正規化の定数が

210
00:07:21,420 --> 00:07:23,640
凄く大きな場合の

211
00:07:24,160 --> 00:07:25,190
話だった。

212
00:07:25,390 --> 00:07:27,750
たぶん何十万とかその辺をセットした気がする。

213
00:07:28,310 --> 00:07:29,760
こんなデータセットが与えられたら、

214
00:07:30,110 --> 00:07:31,630
陽性と陰性のサンプルを

215
00:07:32,110 --> 00:07:34,000
大きなマージンになるようの分離する

216
00:07:34,140 --> 00:07:36,210
決定境界選ぶかもしれない。

217
00:07:37,370 --> 00:07:39,020
SVMは実際は

218
00:07:39,370 --> 00:07:41,120
この大きなマージンという見方から考えられる物よりは

219
00:07:41,440 --> 00:07:42,920
もうちょっと洗練されている。

220
00:07:43,630 --> 00:07:45,130
とりわけ、大きなマージンという特徴だけの

221
00:07:45,310 --> 00:07:46,490
分類器を使っている場合は、

222
00:07:46,680 --> 00:07:48,850
ハズレ値に

223
00:07:49,020 --> 00:07:50,270
その学習アルゴリズムは、より敏感となる。

224
00:07:50,920 --> 00:07:52,260
つまり、画面に示したような

225
00:07:52,450 --> 00:07:53,990
陽性のサンプルを

226
00:07:54,520 --> 00:07:56,540
追加してみましょう。

227
00:07:57,230 --> 00:07:58,830
もしサンプルを一つ追加したら、

228
00:07:58,950 --> 00:08:00,060
データを大きなマージンで

229
00:08:00,300 --> 00:08:01,410
分離しようとすると、

230
00:08:02,680 --> 00:08:04,300
こんな決定境界を

231
00:08:05,270 --> 00:08:07,260
学習する事になる。

232
00:08:07,540 --> 00:08:09,130
このマゼンタの線。

233
00:08:09,180 --> 00:08:10,210
でも一つの外れ値、

234
00:08:10,440 --> 00:08:11,950
一つのサンプルに基づいて、

235
00:08:12,180 --> 00:08:13,560
決定境界を

236
00:08:13,790 --> 00:08:14,720
黒の物から

237
00:08:14,890 --> 00:08:16,460
マゼンダの物へと

238
00:08:17,060 --> 00:08:17,980
変更するのが本当に良い事なのかは

239
00:08:18,290 --> 00:08:19,960
結構怪しい。

240
00:08:20,980 --> 00:08:23,430
だからもしCが、、、

241
00:08:23,640 --> 00:08:25,740
正規化パラメータのCがとても大きければ、

242
00:08:25,970 --> 00:08:27,110
その場合はこれがSVMが

243
00:08:27,300 --> 00:08:28,130
実際に行う事となる。

244
00:08:28,360 --> 00:08:29,820
それは決定境界を

245
00:08:30,270 --> 00:08:31,530
黒の物からマゼンタの物へ

246
00:08:31,650 --> 00:08:33,650
変更する。

247
00:08:33,810 --> 00:08:35,390
だがCが普通の範囲に小さければ、

248
00:08:35,550 --> 00:08:36,720
もしCに、大き過ぎない値を

249
00:08:37,320 --> 00:08:39,090
使っていれば、

250
00:08:39,260 --> 00:08:40,400
その時は黒の決定境界の

251
00:08:40,610 --> 00:08:44,500
ままとなる。

252
00:08:44,830 --> 00:08:46,880
そして、もちろん、もしデータが直線で分離出来ない場合、例えばある陽性のサンプルが

253
00:08:47,250 --> 00:08:48,790
ここにある場合とか、

254
00:08:49,170 --> 00:08:50,440
または陰性のサンプルがここにある場合とか、

255
00:08:50,980 --> 00:08:52,300
そういう場合もSVMは

256
00:08:52,570 --> 00:08:53,830
正しい事をする。

257
00:08:54,260 --> 00:08:55,710
そしてこの

258
00:08:56,060 --> 00:08:57,770
大きなマージン分類器の絵は

259
00:08:58,090 --> 00:08:59,410
正規化パラメータCが

260
00:08:59,530 --> 00:09:01,720
とても大きい場合にだけ正しい

261
00:09:01,970 --> 00:09:03,440
直感を与えてくれる

262
00:09:03,560 --> 00:09:05,050
絵だ。

263
00:09:05,190 --> 00:09:07,170
そして繰り返すと、

264
00:09:07,420 --> 00:09:08,810
この、

265
00:09:09,650 --> 00:09:11,300
C は 1/ラムダ に対応していて、

266
00:09:11,850 --> 00:09:13,600
このラムダは

267
00:09:13,930 --> 00:09:15,950
以前にあった

268
00:09:16,110 --> 00:09:17,970
正規化パラメータだ。

269
00:09:18,080 --> 00:09:18,880
つまり、この絵は

270
00:09:19,080 --> 00:09:21,060
1/ラムダ がとても大きい時、

271
00:09:21,280 --> 00:09:23,110
つまりラムダがとても小さい時にだけ

272
00:09:23,560 --> 00:09:24,640
このマゼンタの決定境界を

273
00:09:24,850 --> 00:09:27,600
得る結果となる。

274
00:09:28,870 --> 00:09:29,560
でも実際にサポートベクターマシンを適用する時は

275
00:09:30,190 --> 00:09:31,620
Cはそんなに

276
00:09:31,910 --> 00:09:33,180
凄く凄く大きい値をセットしたりはしないので、

277
00:09:34,840 --> 00:09:36,390
このようなちょっとの外れ値を無視するには

278
00:09:36,980 --> 00:09:38,590
もっと良い仕事をしてくれます。

279
00:09:39,150 --> 00:09:40,320
また、データが直線で分けられない時も

280
00:09:40,620 --> 00:09:44,400
公正で納得出来るような結果の仕事をしてくれます。

281
00:09:44,690 --> 00:09:46,810
だがサポートベクターマシンの文脈で

282
00:09:46,980 --> 00:09:47,990
バイアスと分散の話をする時に、

283
00:09:48,170 --> 00:09:50,170
それはちょっと後でやる予定ですが、

284
00:09:50,410 --> 00:09:51,990
そのあかつきには、正規化パラメータにまつわる

285
00:09:52,410 --> 00:09:53,710
トレードオフはもっとクリアになってるといいな。

286
00:09:53,830 --> 00:09:55,280
以上がサポートベクターマシンの関数が

287
00:09:55,580 --> 00:09:57,290
大きなマージン分類器として

288
00:09:57,600 --> 00:09:59,680
与えられたデータを大きなマージンで

289
00:09:59,850 --> 00:10:01,810
どのように分離しようとするかについて、

290
00:10:01,950 --> 00:10:03,040
ある程度の直感を与えてくれるといいな。

291
00:10:03,610 --> 00:10:05,210
技術的にはこの見方は

292
00:10:06,140 --> 00:10:07,160
パラメータのCがとても大きな時にしか

293
00:10:07,460 --> 00:10:08,710
成り立たないけど、その考え方は

294
00:10:10,230 --> 00:10:11,720
サポートベクターマシンを考える上でとても有用な考え方だ。

295
00:10:13,120 --> 00:10:14,450
このビデオには、

296
00:10:14,560 --> 00:10:15,990
一つ欠けたステップがある。

297
00:10:16,110 --> 00:10:17,670
それは我らがこれらのスライドで書き下した

298
00:10:17,770 --> 00:10:18,770
最適化の問題が

299
00:10:19,040 --> 00:10:19,930
どのように実際に

300
00:10:20,740 --> 00:10:22,490
大きなマージンの分類器となるのか、という所。

301
00:10:22,600 --> 00:10:23,520
このビデオではそこはやってない。

302
00:10:23,930 --> 00:10:25,830
次のビデオでは、

303
00:10:25,870 --> 00:10:26,940
背後にある数学を

304
00:10:27,120 --> 00:10:28,370
もう少しスケッチしてみる事で

305
00:10:28,750 --> 00:10:29,750
我らの書き下した最適化の問題が

306
00:10:29,850 --> 00:10:31,660
どうやって大きなマージン分類器となっているかを

307
00:10:31,930 --> 00:10:33,410
説明したいと

308
00:10:33,840 --> 00:10:34,990
思います。