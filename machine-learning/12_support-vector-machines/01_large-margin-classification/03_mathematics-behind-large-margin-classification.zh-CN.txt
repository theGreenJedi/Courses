在这段视频中 介绍一些 大间隔分类背后的数学原理 本节为选学部分 你完全可以跳过它 但是听听这节课可能让你对 支持向量机中的优化问题 以及如何得到 大间距分类器 产生更好的直观理解 首先 让我来给大家复习一下 关于向量内积的知识 假设我有两个向量 u 和 v 我将它们写在这里 两个都是二维向量 我们看一下 u 转置乘以 v 的结果 u 转置乘以 v 也叫做向量 u 和 v 之间的内积 由于是二维向量 我可以 将它们画在这个图上 我们说 这就是向量 u 即 在横轴上 取值为某个u1 而在纵轴上 高度是 某个 u2 作为U的 第二个分量 现在 很容易计算的 一个量就是向量 u 的 范数 这是双竖线 左边一个 右边一个 表示 u 的范数 即 u 的长度 即向量 u 的欧几里得长度 根据 毕达哥拉斯定理 等于 它等于 u1 平方 加上 u2 平方 开根号 这是向量 u 的长度 它是一个实数 现在你知道了 这个的长度是多少 这个向量的长度写在这里了 我刚刚画的这个 向量的长度就知道了 现在让我们回头来看 向量v 因为我们想计算内积 v 是另一个向量 它的两个分量 v1 和 v2 是已知的 向量 v 可以画在这里 现在让我们 来看看如何计算 u 和 v 之间的内积 这就是具体做法 我们将向量 v 投影到 向量 u 上 我们做一个直角投影 或者说一个90度投影 将其投影到 u 上 接下来我度量 这条红线的 长度 我称这条红线的 长度为 p 因此 p 就是长度 或者说是 向量 v 投影到 向量 u 上的量 我将它写下来 p 是 v 投影到 向量 u 上的 长度 因此可以 将 u 转置乘以 v 写作 p 乘以 u 的范数或者说 u的长度 这是计算内积的一种方法 如果你从几何上 画出 p 的值 同时画出 u 的范数 你也会同样地 计算出内积 答案是一样的 对吧 另一个计算公式是 u 转置乘以 v 就是 [u1 u2] 这个一行两列的矩阵 乘以 v 因此 可以得到 u1×v1 加上 u2×v2 根据线性代数的知识 这两个公式 会给出同样的结果 顺便说一句 u 转置乘以 v 等于 v 转置乘以 u 因此如果你将 u 和 v 交换位置 将 u 投影到 v 上 而不是将 v 投影到 u 上 然后做同样地计算 只是把 u 和 v 的位置交换一下 你事实上可以 得到同样的结果 申明一点 在这个等式中 u 的范数是一个实数 p也是一个实数 因此 u 转置乘以 v 就是两个实数 正常相乘 最后一点 需要注意的就是p值 p事实上是有符号的 即它可能是正值 也可能是负值 我的意思是说 如果 u 是一个类似这样的向量 v 是一个类似这样的向量 u 和 v 之间的 夹角大于90度 则如果将 v 投影到 u 上 会得到 这样的一个投影 这是 p 的长度 在这个情形下 我们仍然有 u 转置乘以 v 是等于 p 乘以 u 的范数 唯一一点不同的是 p 在这里是负的 在内积计算中 如果 u 和 v 之间的夹角 小于90度 那么那条红线的长度 p 是正值 然而如果 这个夹角 大于90度 则p 将会是负的 就是这个小线段的长度是负的 因此两个向量之间的内积 也是负的 如果它们之间的夹角大于90度 这就是关于向量内积的知识 我们接下来将会 使用这些关于向量内积的 性质 试图来 理解支持向量机 中的目标函数 这就是我们先前给出的 支持向量机模型中的目标函数 为了讲解方便 我做一点简化 仅仅是为了让目标函数 更容易被分析 我接下来忽略掉截距 令 θ0 等于 0 这样更容易画示意图 我将特征数 n 置为2 因此我们仅有 两个特征 x1 和 x2 现在 我们来看一下目标函数 支持向量机的优化目标函数 当我们仅有两个特征 即 n=2 时 这个式子可以写作 二分之一 θ1 平方加上 θ2 平方 我们只有两个参数 θ1 和θ2 接下来我重写一下 我将其重写成 二分之一 θ1 平方 加上 θ2 平方 开平方根后再平方 我这么做的根据是 对于任何数 w w的平方根 再取平方 得到的就是 w 本身 因此平方根 然后平方 并不会改变值的大小 你可能注意到 括号里面的这一项 是向量 θ 的范数 或者说是向量 θ 的长度 我的意思是 如果我们将 向量 θ 写出来 θ1 θ2 那么我刚刚画红线的这一项 就是向量 θ 的长度或范数 这里我们用的是之前 学过的向量范数的定义 事实上这就 等于向量 θ 的长度 当然你可以将其写作  θ0 θ1 θ2 如果 θ0等于0 那就是 θ1 θ2 的长度 在这里我将忽略 θ0 将 θ 仅仅写作这样 这样来写 θ θ 的范数 仅仅和 θ1 θ2 有关 但是 数学上不管你是否包含 θ0 其实并没有差别 因此在我们接下来的推导中去掉θ0不会有影响 这意味着 我们的目标函数是 等于二分之一 θ范数的平方 因此支持向量机 做的全部事情就是 极小化参数向量 θ 范数的平方 或者说 长度的平方 现在我将要 看看这些项 θ 转置乘以x 更深入地理解它们的含义 给定参数向量θ 给定一个样本 x 这等于什么呢? 在前一页幻灯片上 我们画出了 在不同情形下 u转置乘以v的示意图 我们将会使用这些概念 θ 和 x(i) 就 类似于 u 和 v 让我们看一下示意图 我们考察一个 单一的训练样本 我有一个正样本在这里 用一个叉来表示这个样本 x(i) 意思是 在 水平轴上 取值为 x(i)1 在竖直轴上 取值为 x(i)2 这就是我画出的训练样本 尽管我没有将其 真的看做向量 它事实上 就是一个 始于原点 终点位置在这个训练样本点的向量 现在 我们有 一个参数向量 我会将它也 画成向量 我将 θ1 画在这里 将 θ2 画在这里 那么内积 θ 转置乘以 x(i) 将会是什么呢 使用我们之前的方法 我们计算的方式就是 我将训练样本 投影到参数向量 θ 然后我来看一看 这个线段的长度 我将它画成红色 我将它称为 p 上标 (i) 用来表示这是 第 i 个训练样本 在参数向量 θ 上的投影 根据我们之前幻灯片的内容 我们知道的是 θ 转置乘以 x(i) 等于 就等于 p 乘以 向量 θ 的长度 或 范数 这就等于 θ1 乘以 x1 加上 θ2 x2 这两种方式是等价的 都可以用来计算 θ 和 x(i) 之间的内积 好 这告诉了我们什么呢 这里表达的意思是 这个 θ 转置乘以 x(i) 大于等于1 或者小于-1的 约束是可以被 p(i)乘以x大于等于1 这个约束 所代替的 因为 θ 转置乘以  x(i) 等于 p(i) 乘以 θ 的范数 将其写入我们的优化目标 我们将会得到 没有了约束 θ 转置乘以x(i) 而变成了 p(i) 乘以 θ 的范数 需要提醒一点 我们之前曾讲过 这个优化目标函数可以 被写成二分之一乘以 θ 平方的范数 现在让我们考虑 下面这里的 训练样本 现在 继续使用之前的简化 即 θ0 等于0 我们来看一下支持向量机会选择什么样的决策界 这是一种选择 我们假设支持向量机会 选择这个决策边界 这不是一个非常好的选择 因为它的间距很小 这个决策界离训练样本的距离很近 我们来看一下为什么支持向量机不会选择它 对于这样选择的参数 θ 可以看到 参数向量 θ 事实上 是和决策界是90度正交的 因此这个绿色的决策界 对应着一个参数向量 θ 指向这个方向 顺便提一句 θ0 等于0 的简化仅仅意味着 决策界必须 通过原点 (0,0) 现在让我们看一下 这对于优化目标函数 意味着什么 比如这个样本 我们假设它是我的第一个样本 x(1) 如果我考察这个样本 到参数 θ 的投影 这就是投影 这个短的红线段 就等于p(1) 它非常短 对么 类似地 这个样本 如果它恰好是 x(2) 是我的第二个训练样本 则它到 θ 的投影在这里 是因为你犯罪了 因为你做了错事 我将它画成粉色 这个短的粉色线段 它是 p(2) 第二个样本到 我的参数向量 θ 的投影 因此 这个投影非常短 p(2) 事实上是一个负值 p(2) 是在相反的方向 这个向量 和参数向量 θ 的夹角 大于90度 p(2) 的值小于0 我们会发现 这些 p(i) 将会是非常小的数 因此当我们考察 优化目标函数的时候 对于正样本而言 我们需要 p(i) 乘以 θ 的范数大于等于1 但是如果 p(i) 在这里 如果 p(1) 在这里 非常小 那就意味着 我们需要 θ 的范数 非常大 对么 因为如果 p(1) 很小 而我们希望 p(1) 乘以 θ 大于等于1 令其实现的 唯一的办法就是 这两个数较大 如果 p(1) 小 我们就希望 θ 的范数大 类似地 对于负样本而言 我们需要 p(2) 乘以 θ 的范数 小于等于-1 我们已经在这个样本中 看到 p(2) 会是一个非常小的数 因此唯一的办法 就是 θ 的范数变大 但是我们 的目标函数是 希望 找到一个参数 θ 它的范数 是小的 因此 这看起来 不像是一个好的 参数向量 θ 的选择 相反的 来看一个不同的决策边界 比如说 支持向量机选择了 这个决策界 现在状况会有很大不同 如果这是决策界 这就是 相对应的参数 θ 的方向 因此 在这个 决策界之下 垂直线是决策界 使用线性代数的知识 可以说明 这个绿色的决策界 有一个垂直于它的 向量 θ 现在如果你考察 你的数据在横轴 x 上的投影 比如 这个我之前提到的样本 我的样本 x(1) 当我将它投影到横轴x上 或说投影到θ上 就会得到这样的p(1) 它的长度是 p(1) 另一个样本 那个样本是x(2) 我做同样的投影 我会发现 这是 p(2) 的长度 它是负值 你会注意到 现在 p(1) 和 p(2) 这些投影长度 是长多了 如果我们仍然要满足这些约束 p(1) 乘以 θ 的范数 是比1大的 则因为 p(1) 变大了 θ 的范数就可以变小了 因此这意味着 通过选择 右边的决策界 而不是左边的那个 支持向量机可以 使参数 θ 的范数 变小很多 因此如果我们想 令 θ 的范数变小 从而令 θ 范数的平方 变小 就能让 支持向量机 选择右边的决策界 这就是 支持向量机如何能 有效地产生大间距分类的原因 看这条绿线 这个绿色的决策界 我们希望 正样本和负样本投影到 θ 的值大 要做到这一点 的唯一方式就是选择这条绿线做决策界 这是大间距决策界 来区分开 正样本和负样本 这个间距的值 这个间距的值 就是p(1) p(2) p(3) 等等的值 通过让间距变大 通过这些p(1) p(2) p(3) 等等 的值 支持向量机 最终可以找到 一个较小的 θ 范数 这正是支持向量机中最小化目标函数的目的 以上就是为什么 支持向量机最终会找到 大间距分类器的原因 因为它试图极大化这些 p(i) 的范数 它们是 训练样本到决策边界的距离 最后一点 我们的推导 自始至终使用了这个简化假设 就是参数 θ0 等于0 就像我之前提到的 这个的作用是 θ0 等于 0 的意思是 我们让决策界 通过原点 让决策界通过原点 就像这样 如果你令 θ0 不是0的话 含义就是你希望 决策界不通过原点 比如这样 我将不会做全部的推导 实际上 支持向量机产生大间距分类器的结论 会被证明同样成立 证明方式是非常类似的 是我们刚刚做的 证明的推广 之前视频中说过 即便 θ0 不等于0 支持向量机要做的事情都是优化这个目标函数 对应着 C值非常大的情况 但是可以说明的是 即便 θ0 不等于 0 支持向量机 仍然会 找到 正样本和负样本之间的 大间距分隔 总之 我们解释了为什么支持向量机是一个大间距分类器 在下一节我们 将开始讨论 如何利用支持向量机的原理 应用它们 建立一个复杂的 非线性分类器 【教育无边界字幕组】翻译: shamolvzhou 校对/审核: 所罗门捷列夫