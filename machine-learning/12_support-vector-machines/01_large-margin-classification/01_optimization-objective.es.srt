1
00:00:00,570 --> 00:00:01,860
Hasta ahora ya hemos visto

2
00:00:02,090 --> 00:00:04,860
una gama amplia de algoritmos de aprendizaje distintos. En el aprendizaje supervisado,

3
00:00:05,280 --> 00:00:06,810
el desempeño de muchos algoritmos de aprendizaje supervisado

4
00:00:07,300 --> 00:00:08,830
será muy similar.

5
00:00:09,650 --> 00:00:10,740
Importa poco

6
00:00:11,040 --> 00:00:12,140
si utilizamos el

7
00:00:12,440 --> 00:00:13,450
algoritmo de aprendizaje “A” o el algoritmo

8
00:00:13,660 --> 00:00:15,020
de aprendizaje “B”. Lo

9
00:00:15,190 --> 00:00:16,190
que importa más es

10
00:00:16,360 --> 00:00:17,100
la cantidad de datos en la que

11
00:00:17,330 --> 00:00:18,530
entrenamos estos algoritmos.

12
00:00:19,280 --> 00:00:20,480
También cuenta tu habilidad

13
00:00:20,600 --> 00:00:21,990
al aplicar estos algoritmos; es decir,
la manera en la que elijas

14
00:00:23,150 --> 00:00:24,480
las funciones

15
00:00:24,660 --> 00:00:25,790
designadas para el algoritmo

16
00:00:26,010 --> 00:00:27,030
de aprendizaje y

17
00:00:27,200 --> 00:00:28,530
el parámetro de regularización y

18
00:00:29,190 --> 00:00:31,690
cosas similares. Hay otro algoritmo

19
00:00:31,930 --> 00:00:34,110
muy efectivo

20
00:00:34,380 --> 00:00:35,460
y muy

21
00:00:35,580 --> 00:00:37,400
utilizado tanto en la industria como

22
00:00:38,050 --> 00:00:39,590
en la academia. Se llama máquinas

23
00:00:39,850 --> 00:00:41,080
de vectores de soporte. Comparadas con

24
00:00:41,200 --> 00:00:42,600
la regresión logística y las redes neuronales, las máquinas de

25
00:00:46,770 --> 00:00:48,190
vectores de soporte o “SMV” por sus

26
00:00:48,440 --> 00:00:50,120
siglas en inglés, a veces arrojan

27
00:00:50,890 --> 00:00:52,040
funciones de aprendizaje

28
00:00:52,480 --> 00:00:53,250
complejas no lineales más limpias y efectivas.

29
00:00:54,970 --> 00:00:56,300
Me gustaría dedicar los

30
00:00:56,480 --> 00:00:57,850
siguientes videos a

31
00:00:57,890 --> 00:01:00,100
este tema.

32
00:01:00,400 --> 00:01:01,400
Más tarde en este curso haré

33
00:01:01,540 --> 00:01:02,710
un recuento de la

34
00:01:03,100 --> 00:01:04,340
gama de diferentes algoritmos de aprendizaje

35
00:01:05,200 --> 00:01:06,790
para describirlos brevemente. Sin embargo,

36
00:01:07,430 --> 00:01:08,870
por su popularidad, las máquinas

37
00:01:09,370 --> 00:01:10,840
de vectores de soporte será el

38
00:01:10,980 --> 00:01:11,920
último algoritmo de

39
00:01:12,060 --> 00:01:13,800
aprendizaje supervisado al que le

40
00:01:14,440 --> 00:01:16,710
dedique tiempo en este curso.

41
00:01:19,260 --> 00:01:20,440
Respecto a nuestro desarrollo de otros

42
00:01:20,670 --> 00:01:22,280
algoritmos de aprendizaje, empezaremos por hablar

43
00:01:22,650 --> 00:01:23,940
del objetivo de optimización.

44
00:01:24,750 --> 00:01:26,420
Iniciemos, entonces, con

45
00:01:26,620 --> 00:01:27,920
este algoritmo.

46
00:01:29,420 --> 00:01:30,960
Para describir la máquina

47
00:01:31,270 --> 00:01:32,570
de vector de soporte, comenzaré

48
00:01:32,610 --> 00:01:34,020
con la regresión logística

49
00:01:34,990 --> 00:01:35,990
y mostraré cómo podemos

50
00:01:36,820 --> 00:01:37,630
modificarla un poco para

51
00:01:38,240 --> 00:01:39,260
obtener la máquina de soporte vectorial.

52
00:01:40,290 --> 00:01:41,740
En la regresión logística

53
00:01:41,950 --> 00:01:43,680
tenemos nuestra formulación familiar

54
00:01:43,740 --> 00:01:46,000
de hipótesis a la izquierda y

55
00:01:46,450 --> 00:01:48,590
la función de activación sigmoidal a la derecha.

56
00:01:50,390 --> 00:01:51,330
Para explicar la matemática

57
00:01:51,800 --> 00:01:52,650
utilizaré “z” para denotar

58
00:01:52,850 --> 00:01:55,960
teta transpuesta de “x”.

59
00:01:57,620 --> 00:01:58,650
Ahora, pensemos en qué nos

60
00:01:58,900 --> 00:02:01,150
gustaría que hiciera nuestra regresión logística.

61
00:02:01,270 --> 00:02:02,800
Si tenemos un ejemplo en el que

62
00:02:03,070 --> 00:02:04,360
“y” es igual a 1, con ejemplo me

63
00:02:04,540 --> 00:02:05,480
refiero al

64
00:02:06,100 --> 00:02:07,100
conjunto de entrenamiento,

65
00:02:07,440 --> 00:02:11,780
el de prueba o el de validación cruzada, donde “y” es igual a 1,

66
00:02:12,030 --> 00:02:14,300
entonces esperaremos que “H” de “X” resulte cercano a 1.

67
00:02:14,380 --> 00:02:15,760
Entonces, esperamos clasificar

68
00:02:16,140 --> 00:02:17,330
correctamente ese ejemplo.

69
00:02:18,520 --> 00:02:19,390
Tener “H” de “X” cercano a 1

70
00:02:19,510 --> 00:02:20,710
quiere decir que teta

71
00:02:20,850 --> 00:02:22,080
transpuesta de “x”

72
00:02:22,360 --> 00:02:23,380
debe ser mucho

73
00:02:23,770 --> 00:02:24,990
mayor que 0. Aquí está el

74
00:02:25,330 --> 00:02:26,680
signo doble de mayor que, que

75
00:02:26,900 --> 00:02:28,220
significa “mucho mayor que” 0.

76
00:02:28,530 --> 00:02:30,880
Esto es porque cuando

77
00:02:31,120 --> 00:02:32,840
“z”, es decir, teta transpuesta

78
00:02:32,960 --> 00:02:34,750
de “x”, es mucho

79
00:02:34,940 --> 00:02:35,910
mayor que

80
00:02:36,010 --> 00:02:37,240
0,

81
00:02:37,310 --> 00:02:39,060
hasta la derecha de esta figura,

82
00:02:39,360 --> 00:02:42,430
el resultado de la regresión logística se encuentra cerca del 1.

83
00:02:44,510 --> 00:02:45,580
Por el contrario,

84
00:02:45,630 --> 00:02:46,870
si tenemos un ejemplo donde “y” es

85
00:02:47,000 --> 00:02:48,470
igual a 0, estamos

86
00:02:48,750 --> 00:02:49,620
esperando que la hipótesis

87
00:02:50,420 --> 00:02:51,890
arroje un valor

88
00:02:52,010 --> 00:02:53,850
cercano a 0 que corresponde a teta transpuesta de “x”

89
00:02:54,650 --> 00:02:55,990
o “z” también cercana al

90
00:02:56,250 --> 00:02:57,080
0, ya que

91
00:02:57,440 --> 00:02:58,720
corresponde a la hipótesis

92
00:02:59,160 --> 00:03:01,250
con un valor cercano al cero. Si observas

93
00:03:02,180 --> 00:03:03,590
la función de costos de

94
00:03:03,760 --> 00:03:06,300
la regresión logística,

95
00:03:06,440 --> 00:03:07,470
lo que encuentras es

96
00:03:07,710 --> 00:03:09,400
que cada ejemplo “x” coma “y”

97
00:03:10,190 --> 00:03:11,520
contribuye un término

98
00:03:11,700 --> 00:03:14,320
como este a la función de costos.

99
00:03:15,450 --> 00:03:16,900
Bueno. En la función de costos usualmente

100
00:03:17,390 --> 00:03:18,600
tendremos también una suma

101
00:03:18,890 --> 00:03:21,430
de todos los ejemplos de entrenamiento; el término “m” sobre 1.

102
00:03:22,450 --> 00:03:22,740
Esta expresión

103
00:03:23,240 --> 00:03:24,150
de aquí es el

104
00:03:24,470 --> 00:03:25,450
término que aporta

105
00:03:26,220 --> 00:03:28,490
cada ejemplo de entrenamiento

106
00:03:28,780 --> 00:03:31,550
a la función general objetiva para la regresión logística.

107
00:03:33,250 --> 00:03:34,350
Ahora, si tomo la

108
00:03:35,190 --> 00:03:36,120
definición para la formulación de mi hipótesis

109
00:03:37,030 --> 00:03:38,700
y la añado aquí,

110
00:03:39,790 --> 00:03:40,710
lo que obtendré será

111
00:03:40,920 --> 00:03:43,130
que cada ejemplo de entrenamiento contribuye a este término,

112
00:03:44,270 --> 00:03:45,480
sin tomar en cuenta “m” sobre 1,

113
00:03:45,720 --> 00:03:47,130
a mi función

114
00:03:47,470 --> 00:03:49,470
de costo general para la

115
00:03:49,680 --> 00:03:52,260
regresión logística. Ahora,

116
00:03:52,820 --> 00:03:54,310
consideremos

117
00:03:54,700 --> 00:03:55,970
los dos casos, cuando “y” es igual a 1 y

118
00:03:56,040 --> 00:03:57,250
cuando “y” es igual a 0.

119
00:03:57,820 --> 00:03:59,040
En el primer caso, supongamos que

120
00:03:59,170 --> 00:04:00,260
“y” es igual a

121
00:04:00,520 --> 00:04:01,960
1. En este caso,

122
00:04:02,440 --> 00:04:04,850
sólo nos importa el primer

123
00:04:04,980 --> 00:04:06,910
término del objetivo porque

124
00:04:07,130 --> 00:04:08,830
este término, 1 menos “y”

125
00:04:09,210 --> 00:04:10,510
será igual a 0 si “y” es igual a 1.

126
00:04:13,640 --> 00:04:15,340
Cuando “y” es igual a

127
00:04:15,400 --> 00:04:17,130
1; es decir, cuando en nuestro ejemplo “x” coma

128
00:04:17,310 --> 00:04:18,240
“y”, “y” es igual

129
00:04:18,420 --> 00:04:19,840
a 1, lo que obtendremos

130
00:04:20,010 --> 00:04:21,340
será este término: menos log de 1

131
00:04:21,560 --> 00:04:22,370
sobre 1 más “e” elevado a la “z”

132
00:04:22,860 --> 00:04:25,050
negativa donde, al igual que en la diapositiva anterior,

133
00:04:25,330 --> 00:04:26,480
estoy utilizando “z” para denotar

134
00:04:27,490 --> 00:04:29,430
teta transpuesta de “x”.  Y,

135
00:04:29,640 --> 00:04:30,930
por supuesto, en el costo

136
00:04:31,040 --> 00:04:32,130
tenemos este menos “y”

137
00:04:32,380 --> 00:04:33,490
pero ya dijimos que si “y” es igual a

138
00:04:33,540 --> 00:04:34,790
1, esto será igual a

139
00:04:35,020 --> 00:04:36,500
1. Sólo lo simplifiqué

140
00:04:36,580 --> 00:04:38,010
en la expresión que

141
00:04:38,300 --> 00:04:39,820
está aquí abajo.

142
00:04:41,950 --> 00:04:43,030
Si trazamos esta variable

143
00:04:43,580 --> 00:04:45,080
como una variable de “z”, lo que

144
00:04:45,230 --> 00:04:46,320
encontrarás es la

145
00:04:47,160 --> 00:04:48,630
curva que se muestra

146
00:04:49,220 --> 00:04:50,290
abajo a la izquierda de la diapositiva.

147
00:04:51,120 --> 00:04:52,290
Entonces, si vemos que

148
00:04:52,640 --> 00:04:53,590
cuando “z” es

149
00:04:53,860 --> 00:04:54,930
grande; es decir, cuando

150
00:04:55,440 --> 00:04:56,930
teta transpuesta de “x” es grande,

151
00:04:57,800 --> 00:04:58,790
nos dará un

152
00:04:58,890 --> 00:04:59,900
valor de “z”

153
00:05:00,100 --> 00:05:02,050
muy bajo o una

154
00:05:03,000 --> 00:05:04,650
contribución pequeña

155
00:05:04,740 --> 00:05:06,120
a la función de costos.

156
00:05:06,270 --> 00:05:07,790
Esto explica porqué

157
00:05:08,260 --> 00:05:10,020
cuando la regresión logística trata con un ejemplo positivo,

158
00:05:10,640 --> 00:05:12,200
como “y” igual a 1, intenta

159
00:05:12,860 --> 00:05:14,220
fijar teta transpuesta de “x”

160
00:05:14,650 --> 00:05:15,810
muy alto porque

161
00:05:15,980 --> 00:05:17,440
corresponde a un término pequeño

162
00:05:18,300 --> 00:05:21,490
en esta función de costos. Ahora, para construir una

163
00:05:21,760 --> 00:05:23,640
máquina de soporte vectorial, haremos lo siguiente.

164
00:05:23,740 --> 00:05:24,780
Tomaremos esta función de costos;

165
00:05:25,740 --> 00:05:29,420
este menos log de 1 sobre 1 más “e” elevado a “z” negativa y lo modificaremos un poco.

166
00:05:31,270 --> 00:05:32,450
Tomaré este punto

167
00:05:33,590 --> 00:05:35,120
1 de la recta

168
00:05:36,150 --> 00:05:37,200
y trazaré la función de costos que utilizaré.

169
00:05:37,280 --> 00:05:38,510
La función de costos será

170
00:05:38,870 --> 00:05:40,320
plana de aquí en delante.

171
00:05:42,000 --> 00:05:42,980
Dibujaré algo

172
00:05:43,170 --> 00:05:45,720
que sigue una línea recta,

173
00:05:46,280 --> 00:05:49,230
igual que en la regresión

174
00:05:49,530 --> 00:05:50,710
logística; sin embargo, esta será

175
00:05:50,950 --> 00:05:52,740
una línea recta con estas proporciones.

176
00:05:52,870 --> 00:05:55,040
La línea curva que dibujé

177
00:05:55,190 --> 00:05:57,580
en color magenta o rosa es una

178
00:05:58,090 --> 00:05:59,580
aproximación

179
00:05:59,730 --> 00:06:01,840
cercana a la función de

180
00:06:02,310 --> 00:06:03,480
costos que se utiliza en la

181
00:06:03,900 --> 00:06:05,060
regresión logística, excepto porque

182
00:06:05,130 --> 00:06:06,590
está hecha de dos segmentos de línea. Tengo esta porción

183
00:06:07,490 --> 00:06:09,110
plana de la derecha y

184
00:06:09,430 --> 00:06:11,590
esta porción de

185
00:06:11,860 --> 00:06:14,340
línea recta

186
00:06:14,630 --> 00:06:16,460
de la izquierda. No te preocupes mucho por la pendiente de la porción semivertical.

187
00:06:16,930 --> 00:06:18,930
No importa mucho.

188
00:06:19,180 --> 00:06:21,630
Esta es la

189
00:06:21,730 --> 00:06:23,910
función de costos que utilizaremos, donde “y” es igual a 1.

190
00:06:24,100 --> 00:06:25,240
Puedes imaginar que

191
00:06:25,340 --> 00:06:28,310
debes hacer algo muy similar a la regresión logística,

192
00:06:29,190 --> 00:06:30,470
pero resulta que esto le dará una

193
00:06:30,750 --> 00:06:32,630
ventaja computacional a la máquina de soporte vectorial y

194
00:06:33,690 --> 00:06:34,470
más delante nos

195
00:06:34,890 --> 00:06:37,190
dará un problema de optimización

196
00:06:37,570 --> 00:06:39,670
que sería más sencillo de resolver para el software.

197
00:06:41,050 --> 00:06:41,990
Hasta ahora hablamos de

198
00:06:42,120 --> 00:06:43,300
cuando “y” es igual a 1. El otro caso es cuando

199
00:06:43,370 --> 00:06:44,420
“y” es igual a

200
00:06:44,660 --> 00:06:46,120
0. En este caso,

201
00:06:47,090 --> 00:06:47,870
si vemos el costo,

202
00:06:48,510 --> 00:06:49,880
sólo aplicará el segundo término

203
00:06:50,220 --> 00:06:51,470
ya que el primer

204
00:06:51,610 --> 00:06:52,800
término se elimina

205
00:06:53,330 --> 00:06:54,490
porque si “y” es igual a 0

206
00:06:54,640 --> 00:06:55,670
tendremos un cero también aquí. Entonces nos

207
00:06:55,800 --> 00:06:56,640
quedamos sólo con el

208
00:06:57,040 --> 00:06:58,100
segundo término de la expresión de arriba.

209
00:06:59,150 --> 00:07:00,600
Este término de aquí

210
00:07:00,710 --> 00:07:01,960
nos dará el costo de un

211
00:07:01,980 --> 00:07:03,620
ejemplo, o la contribución de la

212
00:07:03,840 --> 00:07:04,850
función de costos.

213
00:07:05,180 --> 00:07:06,620
Si lo trazamos

214
00:07:06,710 --> 00:07:07,860
como una variable

215
00:07:08,560 --> 00:07:09,750
negativa donde, “z” está en el eje

216
00:07:09,990 --> 00:07:11,290
horizontal, obtendremos

217
00:07:11,400 --> 00:07:13,370
una curva así.

218
00:07:13,470 --> 00:07:14,570
Para la máquina de soporte vectorial,

219
00:07:14,790 --> 00:07:15,540
de nuevo, remplazaremos

220
00:07:16,250 --> 00:07:17,860
esta línea azul con una similar.

221
00:07:18,380 --> 00:07:20,060
Digamos que

222
00:07:20,670 --> 00:07:22,220
la remplazamos con un nuevo costo que

223
00:07:23,480 --> 00:07:24,910
es plano aquí y luego sube como

224
00:07:25,020 --> 00:07:26,230
línea recta.

225
00:07:27,900 --> 00:07:27,900
Ahora les

226
00:07:29,070 --> 00:07:29,710
pondremos nombre a

227
00:07:29,860 --> 00:07:31,950
estas dos variables.

228
00:07:32,830 --> 00:07:33,910
A la variable de la izquierda

229
00:07:34,080 --> 00:07:35,850
la llamaremos

230
00:07:37,140 --> 00:07:38,360
costo subíndice 1 de “z” y a

231
00:07:38,800 --> 00:07:39,650
la variable de la derecha la llamaremos

232
00:07:39,870 --> 00:07:41,700
costo subíndice 0 de

233
00:07:42,980 --> 00:07:44,260
“z”. El subíndice se

234
00:07:44,860 --> 00:07:46,740
refiere al costo correspondiente

235
00:07:47,070 --> 00:07:48,570
a “y” igual a 1 o a “y” igual a 0.

236
00:07:49,930 --> 00:07:51,470
Armados con estas definiciones,

237
00:07:51,580 --> 00:07:54,730
ya estamos listos para construir nuestra máquina de soporte vectorial.

238
00:07:55,000 --> 00:07:56,030
Aquí está la función de costos de

239
00:07:56,300 --> 00:07:57,230
“J” de teta que usamos en la

240
00:07:57,340 --> 00:07:58,440
regresión logística. Si esta

241
00:07:58,770 --> 00:07:59,760
ecuación te parece

242
00:07:59,860 --> 00:08:02,220
extraña es porque antes

243
00:08:02,360 --> 00:08:04,270
teníamos un signo de menos afuera.

244
00:08:04,800 --> 00:08:05,820
Aquí, lo que hice fue mover los

245
00:08:05,930 --> 00:08:07,010
signos de menos

246
00:08:07,610 --> 00:08:08,800
hacia adentro de esta expresión. Eso la

247
00:08:08,950 --> 00:08:09,920
hace lucir un

248
00:08:10,080 --> 00:08:12,970
poco diferente. Para la máquina

249
00:08:13,340 --> 00:08:14,670
de vector de soporte, lo

250
00:08:14,730 --> 00:08:16,550
que haremos, esencialmente, es tomar

251
00:08:16,820 --> 00:08:18,460
esto y remplazarlo

252
00:08:19,080 --> 00:08:21,260
con costo 1 de “z”;

253
00:08:21,740 --> 00:08:23,060
es decir, costo 1 de teta transpuesta de “x”.

254
00:08:23,320 --> 00:08:25,240
Tomaré esto y lo remplazaré

255
00:08:25,300 --> 00:08:27,250
con costo

256
00:08:28,640 --> 00:08:31,420
0 de “z” o, lo que es igual, el costo de 0 de

257
00:08:32,060 --> 00:08:34,090
teta transpuesta de “x”

258
00:08:35,030 --> 00:08:36,680
donde la función de costo 1 corresponde

259
00:08:37,000 --> 00:08:37,740
a esta gráfica, como en la diapositiva

260
00:08:38,170 --> 00:08:39,930
anterior, y

261
00:08:40,890 --> 00:08:42,540
de costo 0 corresponde

262
00:08:42,680 --> 00:08:44,420
a esta otra gráfica, como

263
00:08:44,910 --> 00:08:46,730
en la diapositiva pasada.

264
00:08:46,860 --> 00:08:48,080
Lo que tenemos para la

265
00:08:48,420 --> 00:08:49,360
máquina de soporte vectorial es un

266
00:08:49,910 --> 00:08:52,220
problema de minimización de la suma de

267
00:08:52,340 --> 00:08:55,210
1 sobre “m” sobre

268
00:08:55,400 --> 00:08:58,650
los ejemplos de entrenamiento o de “y(i) por el

269
00:08:59,090 --> 00:09:01,050
costo 1 de teta transpuesta

270
00:09:01,300 --> 00:09:03,910
de “x(i)” más 1 menos

271
00:09:04,650 --> 00:09:06,640
“y(i) por el costo 0 de teta transpuesto de “x(i)”.

272
00:09:07,220 --> 00:09:10,490
Más adelante

273
00:09:10,990 --> 00:09:13,470
mi parámetro de

274
00:09:17,120 --> 00:09:23,280
regularización normal. Ahora,

275
00:09:24,130 --> 00:09:25,280
por costumbre, para la máquina de

276
00:09:25,570 --> 00:09:27,610
vectores de soporte, escribimos

277
00:09:27,790 --> 00:09:29,510
estas expresiones un poco diferente. Determinamos los parámetros

278
00:09:30,570 --> 00:09:31,690
de manera un poco diferente.

279
00:09:31,850 --> 00:09:33,720
Primero, nos olvidaremos

280
00:09:34,130 --> 00:09:35,360
de los términos 1

281
00:09:35,670 --> 00:09:36,860
sobre “m”. Esta es una

282
00:09:37,130 --> 00:09:38,480
costumbre

283
00:09:38,770 --> 00:09:40,380
ligeramente distinta que se utiliza

284
00:09:40,640 --> 00:09:41,930
para las máquinas de soporte vectorial

285
00:09:42,140 --> 00:09:43,400
en comparación con la regresión logística. Aquí está a lo que me refiero.

286
00:09:44,160 --> 00:09:46,180
Lo que haré es deshacerme

287
00:09:46,670 --> 00:09:47,960
de los términos

288
00:09:48,210 --> 00:09:49,450
1 sobre “m”

289
00:09:50,070 --> 00:09:50,860
para obtener el mismo

290
00:09:51,070 --> 00:09:53,030
valor óptimo para teta porque

291
00:09:53,620 --> 00:09:55,020
1 sobre “m” es una constante.

292
00:09:56,420 --> 00:09:57,550
Así que, si resuelvo o no este

293
00:09:57,930 --> 00:09:59,410
problema de minimización con 1

294
00:09:59,580 --> 00:10:00,430
sobre “m”, terminaré

295
00:10:01,100 --> 00:10:02,010
utilizando el mismo

296
00:10:02,490 --> 00:10:03,510
valor óptimo de teta.

297
00:10:04,590 --> 00:10:05,450
Me explicaré mejor dando

298
00:10:05,590 --> 00:10:07,000
un ejemplo concreto.

299
00:10:08,010 --> 00:10:09,170
Supongamos que tengo un problema

300
00:10:09,370 --> 00:10:11,040
de minimización en el que pretendo

301
00:10:11,460 --> 00:10:14,700
minimizar un número real “u” de “u” menos 5 al cuadrado

302
00:10:17,080 --> 00:10:18,540
más 1. El mínimo de esto es,

303
00:10:18,620 --> 00:10:20,040
como sabes,

304
00:10:20,440 --> 00:10:21,900
es “u” igual a 5.

305
00:10:23,090 --> 00:10:23,980
Ahora si tomo esta variable

306
00:10:24,120 --> 00:10:25,800
objetiva y la multiplico por

307
00:10:26,430 --> 00:10:28,240
10, de manera que

308
00:10:28,770 --> 00:10:29,850
mi problema de minimización

309
00:10:30,570 --> 00:10:33,510
es mínimo de “u” de 10, “u” menos

310
00:10:33,960 --> 00:10:35,270
5 al cuadrado más 10.

311
00:10:35,920 --> 00:10:37,650
El valor de “u”

312
00:10:37,670 --> 00:10:40,350
que minimiza esto sigue siendo “u” igual a 5.

313
00:10:40,940 --> 00:10:42,540
Entonces, cuando multiplicas algo que

314
00:10:42,640 --> 00:10:44,160
estás minimizando sobre una

315
00:10:44,360 --> 00:10:45,540
constante, 10 en este caso,

316
00:10:46,010 --> 00:10:47,710
el valor de “u” que

317
00:10:48,290 --> 00:10:51,450
minimiza esta variable no cambia.

318
00:10:52,650 --> 00:10:53,680
Lo que hice,

319
00:10:53,830 --> 00:10:55,120
eliminando la “m”

320
00:10:55,430 --> 00:10:56,940
es

321
00:10:56,990 --> 00:10:58,770
multiplicar mi

322
00:10:59,240 --> 00:11:00,650
función objetiva por una constante “m”;

323
00:11:00,940 --> 00:11:01,920
por lo tanto el valor de

324
00:11:02,360 --> 00:11:04,310
teta que arroja el mínimo no cambia.

325
00:11:05,480 --> 00:11:07,190
El segundo cambio notacional

326
00:11:07,470 --> 00:11:08,560
que, una vez más, es la costumbre

327
00:11:08,740 --> 00:11:10,630
más generalizada cuando utilizamos

328
00:11:11,170 --> 00:11:13,250
SVM en vez de regresión logística, es el siguiente.

329
00:11:14,210 --> 00:11:15,880
Para la regresión logística teníamos

330
00:11:16,520 --> 00:11:18,270
dos términos en nuestra función objetiva.

331
00:11:19,340 --> 00:11:20,500
El primer término

332
00:11:20,920 --> 00:11:22,020
es el costo que obtenemos

333
00:11:22,450 --> 00:11:23,910
del conjunto de entrenamiento y

334
00:11:23,990 --> 00:11:25,730
el segundo término es el término

335
00:11:26,140 --> 00:11:28,330
de regularización.

336
00:11:28,380 --> 00:11:29,460
En esta variable controlábamos

337
00:11:29,870 --> 00:11:30,900
la compensación entre

338
00:11:31,270 --> 00:11:32,600
ambos términos

339
00:11:32,810 --> 00:11:34,760
minimizando “A más

340
00:11:35,760 --> 00:11:38,240
mi parámetro de regularización utilizando «lambda»

341
00:11:39,370 --> 00:11:42,280
multiplicado por otro término

342
00:11:42,430 --> 00:11:43,430
que denominamos B. ¿Sí? Estoy utilizando

343
00:11:43,510 --> 00:11:44,970
A para referirme al primer

344
00:11:45,080 --> 00:11:46,160
término y

345
00:11:46,390 --> 00:11:48,280
B para el

346
00:11:48,490 --> 00:11:49,560
segundo, quizá sin «lambda».

347
00:11:49,650 --> 00:11:52,440
En vez de parametrizar

348
00:11:53,140 --> 00:11:56,090
esto como “A” más «lambda» “B”

349
00:11:56,270 --> 00:11:57,950
Perdón, lo que

350
00:11:58,200 --> 00:11:59,670
hicimos es, al ajustar

351
00:12:00,010 --> 00:12:02,210
diferentes valores para el parámetro de regularización «lambda»

352
00:12:03,060 --> 00:12:04,180
compensamos

353
00:12:04,670 --> 00:12:05,720
cuánto queremos ajustar el conjunto

354
00:12:05,900 --> 00:12:06,780
aprendizaje, es decir,

355
00:12:07,560 --> 00:12:09,390
la minimización de “A”, frente a

356
00:12:09,510 --> 00:12:12,930
qué tan bajos queremos mantener los valores de los parámetros.

357
00:12:13,470 --> 00:12:14,530
Me refiero a los parámetros de “B”.

358
00:12:14,640 --> 00:12:16,170
Para la máquina de soporte vectorial,

359
00:12:16,380 --> 00:12:17,620
por costumbre

360
00:12:18,250 --> 00:12:19,150
utilizaremos un parámetro

361
00:12:19,570 --> 00:12:21,960
diferente. En vez de utilizar «lambda» para

362
00:12:22,180 --> 00:12:23,220
controlar la espera

363
00:12:23,640 --> 00:12:24,730
relativa entre el primer y el segundo término,

364
00:12:24,810 --> 00:12:26,260
utilizaremos un

365
00:12:26,300 --> 00:12:27,370
parámetro diferente

366
00:12:27,710 --> 00:12:29,070
al que llaman

367
00:12:29,290 --> 00:12:31,530
“C” y minimizaremos

368
00:12:31,730 --> 00:12:33,550
“C” por “A” más “B”.

369
00:12:34,430 --> 00:12:39,160
Para la regresión logística

370
00:12:39,380 --> 00:12:41,210
si enviamos un valor muy

371
00:12:41,340 --> 00:12:42,730
grande de «lambda»

372
00:12:42,990 --> 00:12:43,980
obtendremos un

373
00:12:44,260 --> 00:12:45,970
valor grande de “B”. Es decir, si

374
00:12:46,590 --> 00:12:47,640
fijamos a “C” un valor muy

375
00:12:47,960 --> 00:12:49,750
pequeño, el valor correspondiente

376
00:12:50,070 --> 00:12:51,510
de “B”

377
00:12:51,800 --> 00:12:53,530
será mucho más alto que “C” o que “A”.

378
00:12:54,610 --> 00:12:55,730
Esta es otra

379
00:12:55,890 --> 00:12:57,330
manera de controlar la compensación

380
00:12:57,630 --> 00:12:58,970
o una manera distinta

381
00:12:59,060 --> 00:13:01,530
de parametrizar cuánto nos interesa optimizar el primer término frente a cuánto nos interesa optimizar el segundo término.

382
00:13:05,290 --> 00:13:06,250
Si quieres, puedes pensar

383
00:13:06,380 --> 00:13:07,620
que el parámetro “C”

384
00:13:08,180 --> 00:13:09,580
juega un rol muy

385
00:13:09,800 --> 00:13:11,570
similar a 1 sobre

386
00:13:11,890 --> 00:13:13,900
«lambda». No es que

387
00:13:14,080 --> 00:13:16,100
estas dos ecuaciones

388
00:13:16,720 --> 00:13:17,900
o expresiones

389
00:13:18,000 --> 00:13:19,500
sean iguales si “C”

390
00:13:19,650 --> 00:13:21,350
igual a 1 sobre «lambda». Este no es el caso.  Pero si “C” es igual a 1 sobre «lambda»,  entonces,

391
00:13:22,260 --> 00:13:24,510
estos

392
00:13:24,710 --> 00:13:26,670
dos objetivos de optimización

393
00:13:26,940 --> 00:13:28,260
te darán el mismo valor

394
00:13:28,500 --> 00:13:29,460
valor óptimo de teta.
Entonces, para

395
00:13:30,350 --> 00:13:31,180
actualizarnos

396
00:13:31,400 --> 00:13:33,030
voy a  borrar esta «lambda» de aquí y

397
00:13:33,730 --> 00:13:34,940
escribir la constante “C”.

398
00:13:35,030 --> 00:13:37,930
Esto nos da una función

399
00:13:38,170 --> 00:13:40,830
objetiva y general

400
00:13:41,280 --> 00:13:42,650
de optimización para la

401
00:13:42,900 --> 00:13:43,970
máquina de soporte vectorial donde

402
00:13:44,080 --> 00:13:46,200
si minimizas la variable

403
00:13:46,340 --> 00:13:47,410
obtienes los parámetros aprendidos

404
00:13:48,230 --> 00:13:52,800
por la SVM.
Finalmente, en el ámbito de la

405
00:13:52,940 --> 00:13:54,690
regresión logística, la SVM

406
00:13:54,840 --> 00:13:56,110
no arroja

407
00:13:56,220 --> 00:13:57,850
la probabilidad. En lugar de eso, lo que tenemos

408
00:13:57,970 --> 00:13:58,910
es la función de

409
00:13:59,190 --> 00:14:00,600
costos, que minimizamos para

410
00:14:00,730 --> 00:14:02,770
obtener el parámetro teta. Lo que

411
00:14:02,910 --> 00:14:03,900
hace la máquina de soporte vectorial

412
00:14:05,130 --> 00:14:05,970
es predecir directamente

413
00:14:07,050 --> 00:14:08,650
si “y” es igual a 1 o

414
00:14:08,690 --> 00:14:10,390
a 0. Entonces, tendré una hipótesis que

415
00:14:11,310 --> 00:14:12,920
predice un valor de 1 si teta

416
00:14:14,150 --> 00:14:15,630
transpuesta de “x” es

417
00:14:15,890 --> 00:14:17,680
mayor o igual a

418
00:14:18,230 --> 00:14:20,060
0, y sino predice 0.

419
00:14:20,320 --> 00:14:21,560
Una vez que aprendimos

420
00:14:21,610 --> 00:14:23,010
los parámetros teta, esta es la

421
00:14:23,360 --> 00:14:25,980
forma de la hipótesis para la máquina de vectores de soporte.

422
00:14:26,850 --> 00:14:27,870
Esta fue una definición matemática de

423
00:14:27,980 --> 00:14:29,670
lo que hace una

424
00:14:29,840 --> 00:14:31,520
máquina de soporte vectorial.

425
00:14:31,750 --> 00:14:32,870
En los siguientes videos

426
00:14:33,100 --> 00:14:33,900
intentaremos ganar

427
00:14:34,260 --> 00:14:36,030
un mejor entendimiento de a

428
00:14:36,480 --> 00:14:37,660
dónde nos lleva el objetivo de optimización

429
00:14:37,820 --> 00:14:38,840
y de cuáles son las fuentes de la hipótesis

430
00:14:39,720 --> 00:14:41,300
que aprenderá nuestra SVM.

431
00:14:41,700 --> 00:14:43,060
También hablaremos de cómo modificar

432
00:14:43,600 --> 00:14:44,640
esto un poco para obtener variables

433
00:14:44,920 --> 00:14:46,280
complejas no lineales.