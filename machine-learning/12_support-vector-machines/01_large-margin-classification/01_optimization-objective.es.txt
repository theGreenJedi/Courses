Hasta ahora ya hemos visto una gama amplia de algoritmos de aprendizaje distintos. En el aprendizaje supervisado, el desempeño de muchos algoritmos de aprendizaje supervisado será muy similar. Importa poco si utilizamos el algoritmo de aprendizaje “A” o el algoritmo de aprendizaje “B”. Lo que importa más es la cantidad de datos en la que entrenamos estos algoritmos. También cuenta tu habilidad al aplicar estos algoritmos; es decir,
la manera en la que elijas las funciones designadas para el algoritmo de aprendizaje y el parámetro de regularización y cosas similares. Hay otro algoritmo muy efectivo y muy utilizado tanto en la industria como en la academia. Se llama máquinas de vectores de soporte. Comparadas con la regresión logística y las redes neuronales, las máquinas de vectores de soporte o “SMV” por sus siglas en inglés, a veces arrojan funciones de aprendizaje complejas no lineales más limpias y efectivas. Me gustaría dedicar los siguientes videos a este tema. Más tarde en este curso haré un recuento de la gama de diferentes algoritmos de aprendizaje para describirlos brevemente. Sin embargo, por su popularidad, las máquinas de vectores de soporte será el último algoritmo de aprendizaje supervisado al que le dedique tiempo en este curso. Respecto a nuestro desarrollo de otros algoritmos de aprendizaje, empezaremos por hablar del objetivo de optimización. Iniciemos, entonces, con este algoritmo. Para describir la máquina de vector de soporte, comenzaré con la regresión logística y mostraré cómo podemos modificarla un poco para obtener la máquina de soporte vectorial. En la regresión logística tenemos nuestra formulación familiar de hipótesis a la izquierda y la función de activación sigmoidal a la derecha. Para explicar la matemática utilizaré “z” para denotar teta transpuesta de “x”. Ahora, pensemos en qué nos gustaría que hiciera nuestra regresión logística. Si tenemos un ejemplo en el que “y” es igual a 1, con ejemplo me refiero al conjunto de entrenamiento, el de prueba o el de validación cruzada, donde “y” es igual a 1, entonces esperaremos que “H” de “X” resulte cercano a 1. Entonces, esperamos clasificar correctamente ese ejemplo. Tener “H” de “X” cercano a 1 quiere decir que teta transpuesta de “x” debe ser mucho mayor que 0. Aquí está el signo doble de mayor que, que significa “mucho mayor que” 0. Esto es porque cuando “z”, es decir, teta transpuesta de “x”, es mucho mayor que 0, hasta la derecha de esta figura, el resultado de la regresión logística se encuentra cerca del 1. Por el contrario, si tenemos un ejemplo donde “y” es igual a 0, estamos esperando que la hipótesis arroje un valor cercano a 0 que corresponde a teta transpuesta de “x” o “z” también cercana al 0, ya que corresponde a la hipótesis con un valor cercano al cero. Si observas la función de costos de la regresión logística, lo que encuentras es que cada ejemplo “x” coma “y” contribuye un término como este a la función de costos. Bueno. En la función de costos usualmente tendremos también una suma de todos los ejemplos de entrenamiento; el término “m” sobre 1. Esta expresión de aquí es el término que aporta cada ejemplo de entrenamiento a la función general objetiva para la regresión logística. Ahora, si tomo la definición para la formulación de mi hipótesis y la añado aquí, lo que obtendré será que cada ejemplo de entrenamiento contribuye a este término, sin tomar en cuenta “m” sobre 1, a mi función de costo general para la regresión logística. Ahora, consideremos los dos casos, cuando “y” es igual a 1 y cuando “y” es igual a 0. En el primer caso, supongamos que “y” es igual a 1. En este caso, sólo nos importa el primer término del objetivo porque este término, 1 menos “y” será igual a 0 si “y” es igual a 1. Cuando “y” es igual a 1; es decir, cuando en nuestro ejemplo “x” coma “y”, “y” es igual a 1, lo que obtendremos será este término: menos log de 1 sobre 1 más “e” elevado a la “z” negativa donde, al igual que en la diapositiva anterior, estoy utilizando “z” para denotar teta transpuesta de “x”.  Y, por supuesto, en el costo tenemos este menos “y” pero ya dijimos que si “y” es igual a 1, esto será igual a 1. Sólo lo simplifiqué en la expresión que está aquí abajo. Si trazamos esta variable como una variable de “z”, lo que encontrarás es la curva que se muestra abajo a la izquierda de la diapositiva. Entonces, si vemos que cuando “z” es grande; es decir, cuando teta transpuesta de “x” es grande, nos dará un valor de “z” muy bajo o una contribución pequeña a la función de costos. Esto explica porqué cuando la regresión logística trata con un ejemplo positivo, como “y” igual a 1, intenta fijar teta transpuesta de “x” muy alto porque corresponde a un término pequeño en esta función de costos. Ahora, para construir una máquina de soporte vectorial, haremos lo siguiente. Tomaremos esta función de costos; este menos log de 1 sobre 1 más “e” elevado a “z” negativa y lo modificaremos un poco. Tomaré este punto 1 de la recta y trazaré la función de costos que utilizaré. La función de costos será plana de aquí en delante. Dibujaré algo que sigue una línea recta, igual que en la regresión logística; sin embargo, esta será una línea recta con estas proporciones. La línea curva que dibujé en color magenta o rosa es una aproximación cercana a la función de costos que se utiliza en la regresión logística, excepto porque está hecha de dos segmentos de línea. Tengo esta porción plana de la derecha y esta porción de línea recta de la izquierda. No te preocupes mucho por la pendiente de la porción semivertical. No importa mucho. Esta es la función de costos que utilizaremos, donde “y” es igual a 1. Puedes imaginar que debes hacer algo muy similar a la regresión logística, pero resulta que esto le dará una ventaja computacional a la máquina de soporte vectorial y más delante nos dará un problema de optimización que sería más sencillo de resolver para el software. Hasta ahora hablamos de cuando “y” es igual a 1. El otro caso es cuando “y” es igual a 0. En este caso, si vemos el costo, sólo aplicará el segundo término ya que el primer término se elimina porque si “y” es igual a 0 tendremos un cero también aquí. Entonces nos quedamos sólo con el segundo término de la expresión de arriba. Este término de aquí nos dará el costo de un ejemplo, o la contribución de la función de costos. Si lo trazamos como una variable negativa donde, “z” está en el eje horizontal, obtendremos una curva así. Para la máquina de soporte vectorial, de nuevo, remplazaremos esta línea azul con una similar. Digamos que la remplazamos con un nuevo costo que es plano aquí y luego sube como línea recta. Ahora les pondremos nombre a estas dos variables. A la variable de la izquierda la llamaremos costo subíndice 1 de “z” y a la variable de la derecha la llamaremos costo subíndice 0 de “z”. El subíndice se refiere al costo correspondiente a “y” igual a 1 o a “y” igual a 0. Armados con estas definiciones, ya estamos listos para construir nuestra máquina de soporte vectorial. Aquí está la función de costos de “J” de teta que usamos en la regresión logística. Si esta ecuación te parece extraña es porque antes teníamos un signo de menos afuera. Aquí, lo que hice fue mover los signos de menos hacia adentro de esta expresión. Eso la hace lucir un poco diferente. Para la máquina de vector de soporte, lo que haremos, esencialmente, es tomar esto y remplazarlo con costo 1 de “z”; es decir, costo 1 de teta transpuesta de “x”. Tomaré esto y lo remplazaré con costo 0 de “z” o, lo que es igual, el costo de 0 de teta transpuesta de “x” donde la función de costo 1 corresponde a esta gráfica, como en la diapositiva anterior, y de costo 0 corresponde a esta otra gráfica, como en la diapositiva pasada. Lo que tenemos para la máquina de soporte vectorial es un problema de minimización de la suma de 1 sobre “m” sobre los ejemplos de entrenamiento o de “y(i) por el costo 1 de teta transpuesta de “x(i)” más 1 menos “y(i) por el costo 0 de teta transpuesto de “x(i)”. Más adelante mi parámetro de regularización normal. Ahora, por costumbre, para la máquina de vectores de soporte, escribimos estas expresiones un poco diferente. Determinamos los parámetros de manera un poco diferente. Primero, nos olvidaremos de los términos 1 sobre “m”. Esta es una costumbre ligeramente distinta que se utiliza para las máquinas de soporte vectorial en comparación con la regresión logística. Aquí está a lo que me refiero. Lo que haré es deshacerme de los términos 1 sobre “m” para obtener el mismo valor óptimo para teta porque 1 sobre “m” es una constante. Así que, si resuelvo o no este problema de minimización con 1 sobre “m”, terminaré utilizando el mismo valor óptimo de teta. Me explicaré mejor dando un ejemplo concreto. Supongamos que tengo un problema de minimización en el que pretendo minimizar un número real “u” de “u” menos 5 al cuadrado más 1. El mínimo de esto es, como sabes, es “u” igual a 5. Ahora si tomo esta variable objetiva y la multiplico por 10, de manera que mi problema de minimización es mínimo de “u” de 10, “u” menos 5 al cuadrado más 10. El valor de “u” que minimiza esto sigue siendo “u” igual a 5. Entonces, cuando multiplicas algo que estás minimizando sobre una constante, 10 en este caso, el valor de “u” que minimiza esta variable no cambia. Lo que hice, eliminando la “m” es multiplicar mi función objetiva por una constante “m”; por lo tanto el valor de teta que arroja el mínimo no cambia. El segundo cambio notacional que, una vez más, es la costumbre más generalizada cuando utilizamos SVM en vez de regresión logística, es el siguiente. Para la regresión logística teníamos dos términos en nuestra función objetiva. El primer término es el costo que obtenemos del conjunto de entrenamiento y el segundo término es el término de regularización. En esta variable controlábamos la compensación entre ambos términos minimizando “A más mi parámetro de regularización utilizando «lambda» multiplicado por otro término que denominamos B. ¿Sí? Estoy utilizando A para referirme al primer término y B para el segundo, quizá sin «lambda». En vez de parametrizar esto como “A” más «lambda» “B” Perdón, lo que hicimos es, al ajustar diferentes valores para el parámetro de regularización «lambda» compensamos cuánto queremos ajustar el conjunto aprendizaje, es decir, la minimización de “A”, frente a qué tan bajos queremos mantener los valores de los parámetros. Me refiero a los parámetros de “B”. Para la máquina de soporte vectorial, por costumbre utilizaremos un parámetro diferente. En vez de utilizar «lambda» para controlar la espera relativa entre el primer y el segundo término, utilizaremos un parámetro diferente al que llaman “C” y minimizaremos “C” por “A” más “B”. Para la regresión logística si enviamos un valor muy grande de «lambda» obtendremos un valor grande de “B”. Es decir, si fijamos a “C” un valor muy pequeño, el valor correspondiente de “B” será mucho más alto que “C” o que “A”. Esta es otra manera de controlar la compensación o una manera distinta de parametrizar cuánto nos interesa optimizar el primer término frente a cuánto nos interesa optimizar el segundo término. Si quieres, puedes pensar que el parámetro “C” juega un rol muy similar a 1 sobre «lambda». No es que estas dos ecuaciones o expresiones sean iguales si “C” igual a 1 sobre «lambda». Este no es el caso.  Pero si “C” es igual a 1 sobre «lambda»,  entonces, estos dos objetivos de optimización te darán el mismo valor valor óptimo de teta.
Entonces, para actualizarnos voy a  borrar esta «lambda» de aquí y escribir la constante “C”. Esto nos da una función objetiva y general de optimización para la máquina de soporte vectorial donde si minimizas la variable obtienes los parámetros aprendidos por la SVM.
Finalmente, en el ámbito de la regresión logística, la SVM no arroja la probabilidad. En lugar de eso, lo que tenemos es la función de costos, que minimizamos para obtener el parámetro teta. Lo que hace la máquina de soporte vectorial es predecir directamente si “y” es igual a 1 o a 0. Entonces, tendré una hipótesis que predice un valor de 1 si teta transpuesta de “x” es mayor o igual a 0, y sino predice 0. Una vez que aprendimos los parámetros teta, esta es la forma de la hipótesis para la máquina de vectores de soporte. Esta fue una definición matemática de lo que hace una máquina de soporte vectorial. En los siguientes videos intentaremos ganar un mejor entendimiento de a dónde nos lleva el objetivo de optimización y de cuáles son las fuentes de la hipótesis que aprenderá nuestra SVM. También hablaremos de cómo modificar esto un poco para obtener variables complejas no lineales.