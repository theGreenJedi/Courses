1
00:00:00,140 --> 00:00:01,310
Sejauh ini kita telah berbicara tentang

2
00:00:01,640 --> 00:00:03,290
SVM secara abstrak.

3
00:00:03,980 --> 00:00:05,030
Dalam video ini saya ingin

4
00:00:05,200 --> 00:00:06,460
berbicara tentang apa yang sesungguhnya perlu

5
00:00:06,740 --> 00:00:09,410
anda lakukan untuk menjalankan atau menggunakan SVM.

6
00:00:11,320 --> 00:00:12,300
Algoritma Support Vector Machine

7
00:00:12,850 --> 00:00:14,870
merupakan suatu permasalahan optimasi spesifik.

8
00:00:15,530 --> 00:00:16,940
Namun sebagaimana telah saya sebutkan secara singkat pada

9
00:00:17,120 --> 00:00:18,150
video sebelumnya, saya betul-betul

10
00:00:18,380 --> 00:00:20,570
tidak merekomendasikan anda untuk menulis

11
00:00:20,630 --> 00:00:22,810
perangkat lunak (software) anda sendiri untuk mencari nilai parameter theta.

12
00:00:23,950 --> 00:00:26,110
Jadi sebagaimana situasi saat ini, sangat

13
00:00:26,420 --> 00:00:27,730
sedikit dari kita, atau mungkin bahkan

14
00:00:28,090 --> 00:00:29,400
hampir tidak ada dari kita yang berpikir untuk

15
00:00:29,530 --> 00:00:31,680
menulis kode sendiri untuk melakukan inversi matriks

16
00:00:31,950 --> 00:00:33,940
atau menghitung akar dari suatu angka, dan seterusnya.

17
00:00:34,190 --> 00:00:36,570
Kita hanya, anda tahu, memanggil suatu fungsi pustaka (library function) untuk melakukan hal itu.

18
00:00:36,700 --> 00:00:38,090
Dengan cara yang sama,

19
00:00:38,850 --> 00:00:40,310
perangkat lunak untuk menyelesaikan permasalahan

20
00:00:40,620 --> 00:00:42,200
optimasi SVM adalah sangat

21
00:00:42,440 --> 00:00:43,880
kompleks, dan telah ada

22
00:00:43,990 --> 00:00:44,960
periset-periset yang telah

23
00:00:45,110 --> 00:00:47,560
melakukan riset optimasi numerik selama bertahun-tahun.

24
00:00:47,850 --> 00:00:48,960
Jadi anda perlu menggunakan

25
00:00:49,150 --> 00:00:50,550
pustaka perangkat lunak (software library) dan

26
00:00:50,930 --> 00:00:52,270
paket perangkat lunak (software package) yang bagus untuk melakukan hal ini.

27
00:00:52,470 --> 00:00:53,480
Dan karenanya saya sangat merekomendasikan penggunaan

28
00:00:53,860 --> 00:00:55,260
salah satu dari pustaka perangkat lunak yang telah sangat teroptimasi ini

29
00:00:55,710 --> 00:00:57,780
daripada mencoba mengimplementasikannya sendiri.

30
00:00:58,730 --> 00:01:00,680
Dan ada banyak pustaka perangkat lunak yang bagus di luar sana.

31
00:01:00,970 --> 00:01:02,060
Dua hal yang kebetulan saya

32
00:01:02,210 --> 00:01:03,220
paling sering gunakan adalah

33
00:01:03,400 --> 00:01:05,000
liblinear dan libsvm, namun sebetulnya ada

34
00:01:05,410 --> 00:01:06,860
banyak pustaka perangkat lunak yang bagus untuk

35
00:01:07,030 --> 00:01:08,430
melakukan hal ini yang, anda tahu, dapat anda

36
00:01:08,600 --> 00:01:10,190
hubungkan dengan banyak dari

37
00:01:10,450 --> 00:01:11,860
bahasa pemrograman populer yang mungkin

38
00:01:11,950 --> 00:01:14,410
anda pergunakan untuk menulis kode algoritma pembelajaran.

39
00:01:15,280 --> 00:01:16,460
Meskipun anda sebaiknya tidak menulis

40
00:01:16,730 --> 00:01:18,330
perangkat lunak optimasi SVM anda sendiri,

41
00:01:19,120 --> 00:01:20,680
bagaimanapun, ada beberapa hal yang tetap perlu anda lakukan.

42
00:01:21,420 --> 00:01:23,130
Pertama adalah untuk

43
00:01:23,130 --> 00:01:24,230
memilih nilai dari

44
00:01:24,320 --> 00:01:25,640
parameter C. Kita telah sedikit membicarakan

45
00:01:25,940 --> 00:01:26,930
tentang sifat bias/variansi dari

46
00:01:27,040 --> 00:01:28,850
hal ini pada video sebelumnya.

47
00:01:30,290 --> 00:01:31,480
Kedua, anda juga perlu untuk

48
00:01:31,630 --> 00:01:33,040
memilih kernel atau

49
00:01:33,410 --> 00:01:34,880
fungsi kesamaan (similarity function) yang ingin anda pergunakan.

50
00:01:35,730 --> 00:01:37,080
Jadi salah satu pilihan yang mungkin

51
00:01:37,280 --> 00:01:38,980
adalah jika kita memutuskan untuk tidak menggunakan kernel apapun.

52
00:01:40,560 --> 00:01:41,510
Dan tidak menggunakan kernel

53
00:01:41,910 --> 00:01:43,600
diistilahkan juga sebagai kernel linear.

54
00:01:44,130 --> 00:01:45,320
Jadi jika seseorang mengatakan, saya menggunakan

55
00:01:45,530 --> 00:01:46,760
suatu SVM dengan kernel linear,

56
00:01:47,180 --> 00:01:48,330
arti dari hal itu adalah, anda tahu,

57
00:01:48,490 --> 00:01:50,690
ia mempergunakan SVM tanpa

58
00:01:51,020 --> 00:01:52,250
menggunakan kernel, dan itu

59
00:01:52,360 --> 00:01:53,410
adalah versi dari SVM

60
00:01:54,120 --> 00:01:55,870
yang hanya mempergunakan theta transpose X

61
00:01:56,140 --> 00:01:57,620
yang memprediksikan satu nilai theta0

62
00:01:57,850 --> 00:01:59,420
dan theta1 X1

63
00:01:59,740 --> 00:02:01,000
dan seterusnya dan

64
00:02:01,690 --> 00:02:04,160
thetaN XN lebih besar atau sama dengan 0.

65
00:02:05,520 --> 00:02:06,830
Istilah kernel linear ini dapat anda

66
00:02:06,950 --> 00:02:08,250
pandang sebagai, anda tahu,

67
00:02:08,480 --> 00:02:09,290
versi dari SVM

68
00:02:10,340 --> 00:02:12,320
yang menghasilkan suatu pengklasifikasi linear standar.

69
00:02:13,940 --> 00:02:14,700
Jadi itu mungkin adalah salah satu

70
00:02:15,040 --> 00:02:16,160
pilihan yang masuk akal untuk beberapa jenis permasalahan,

71
00:02:17,130 --> 00:02:18,080
dan anda tahu, ada banyak pustaka perangkat lunak

72
00:02:18,470 --> 00:02:20,900
seperti liblinear, yang merupakan

73
00:02:21,210 --> 00:02:22,320
salah satu contoh dari banyak

74
00:02:22,840 --> 00:02:23,880
pustaka perangkat lunak

75
00:02:24,560 --> 00:02:25,620
yang dapat melakukan pelatihan sebuah SVM

76
00:02:25,980 --> 00:02:27,410
tanpa mempergunakan kernel, yang

77
00:02:27,760 --> 00:02:29,470
disebut juga sebagai kernel linear.

78
00:02:29,850 --> 00:02:31,340
Jadi, apa alasan mengapa anda menginginkannya (kernel linear - penj.)?

79
00:02:31,410 --> 00:02:32,820
Jika anda memiliki sejumlah besar

80
00:02:33,150 --> 00:02:34,280
fitur, (dengan kata lain) jika N besar,

81
00:02:34,430 --> 00:02:37,800
dan M, yaitu

82
00:02:37,990 --> 00:02:39,590
jumlah dari data latih sedikit,

83
00:02:39,670 --> 00:02:41,050
maka anda tahu

84
00:02:41,230 --> 00:02:42,300
anda memiliki sejumlah besar

85
00:02:42,360 --> 00:02:43,630
fitur yang jika X, ini adalah

86
00:02:43,710 --> 00:02:45,850
suatu X adalah sebuah Rn, Rn+1.

87
00:02:46,010 --> 00:02:46,940
Jadi jika anda sudah

88
00:02:47,080 --> 00:02:48,700
memiliki jumlah fitur yang besar, dengan

89
00:02:48,800 --> 00:02:50,540
set data latih berukuran kecil, anda tahu, mungkin anda

90
00:02:50,610 --> 00:02:51,430
hanya ingin untuk mendapatkan suatu

91
00:02:51,710 --> 00:02:52,890
pembatas keputusan (decision boundary) yang linear

92
00:02:53,060 --> 00:02:54,420
dan tidak untuk mencari suatu fungsi nonlinear yang sangat rumit,

93
00:02:54,860 --> 00:02:56,980
karena anda mungkin tidak memiliki cukup banyak data.

94
00:02:57,560 --> 00:02:59,330
Dan anda akan mengalami resiko overfitting, jika

95
00:02:59,470 --> 00:03:00,530
anda mencoba untuk mencari suatu fungsi yang sangat rumit

96
00:03:01,540 --> 00:03:03,220
pada ruang fitur berdimensi sangat tinggi

97
00:03:03,980 --> 00:03:04,990
namun set data latih anda

98
00:03:05,040 --> 00:03:07,120
berukuran kecil. Jadi ini adalah

99
00:03:07,340 --> 00:03:08,600
salah satu setting yang masuk akal

100
00:03:08,740 --> 00:03:09,950
sehingga anda mungkin memutuskan untuk

101
00:03:10,700 --> 00:03:11,960
tidak menggunakan suatu kernel, atau

102
00:03:12,250 --> 00:03:15,580
ekuivalen dengan menggunakan apa yang diistilahkan sebagai kernel linear.

103
00:03:15,740 --> 00:03:16,740
Pilihan kedua untuk kernel yang

104
00:03:16,820 --> 00:03:18,010
mungkin anda pergunakan, adalah kernel Gaussian

105
00:03:18,370 --> 00:03:19,920
dan ini adalah yang telah kita lakukan sebelumnya.

106
00:03:21,270 --> 00:03:22,350
Dan jika anda melakukan hal ini, maka

107
00:03:22,440 --> 00:03:23,130
pilihan lain yang perlu anda buat

108
00:03:23,420 --> 00:03:25,980
adalah untuk memilih parameter sigma kuadrat ini

109
00:03:26,850 --> 00:03:29,800
ketika kita juga membicarakan sedikit mengenai timbal-balik bias dan variansi

110
00:03:30,820 --> 00:03:32,360
yang, jika sigma kuadrat nilainya

111
00:03:32,600 --> 00:03:33,890
besar, maka anda cenderung untuk

112
00:03:34,160 --> 00:03:35,580
memperoleh pengklasifikasi dengan bias yang lebih besar dan

113
00:03:35,770 --> 00:03:37,650
variansi yang lebih kecil, namun jika

114
00:03:37,800 --> 00:03:39,700
sigma kuadrat nilainya kecil, maka anda

115
00:03:40,060 --> 00:03:42,360
akan memperoleh pengklasifikasi dengan variansi yang lebih besar dan bias yang lebih kecil.

116
00:03:43,940 --> 00:03:45,350
Jadi apa alasan anda untuk memilih kernel Gaussian?

117
00:03:46,210 --> 00:03:48,050
Ya, jika omisi X dari fitur anda,

118
00:03:48,310 --> 00:03:49,540
maksud saya RN,

119
00:03:49,820 --> 00:03:51,370
dan jika N nilainya kecil,

120
00:03:51,570 --> 00:03:53,890
dan idealnya, anda tahu,

121
00:03:55,660 --> 00:03:57,110
jika N besar, jadi itu adalah

122
00:03:58,470 --> 00:04:00,170
jika kita memiliki, katakanlah

123
00:04:00,550 --> 00:04:02,340
suatu set data latih berdimensi dua,

124
00:04:03,130 --> 00:04:04,880
seperti pada contoh yang saya gambarkan sebelumnya.

125
00:04:05,470 --> 00:04:08,320
Jadi jika N sama dengan 2, namun kita memiliki set data latih yang bernilai besar.

126
00:04:08,680 --> 00:04:09,770
Jadi, anda tahu, saya telah memiliki

127
00:04:09,950 --> 00:04:10,890
data latih dalam jumlah yang cukup besar,

128
00:04:11,650 --> 00:04:12,410
maka mungkin anda sebaiknya menggunakan

129
00:04:12,540 --> 00:04:14,400
suatu kernel untuk dapat memperoleh

130
00:04:14,910 --> 00:04:16,260
suatu pembatas keputusan (decision boundary) yang kompleks dan nonlinear,

131
00:04:16,650 --> 00:04:18,750
dan kernel Gaussian adalah salah satu cara yang baik untuk melakukan hal ini.

132
00:04:19,480 --> 00:04:20,610
Saya akan mengatakan lebih banyak di penghujung

133
00:04:20,720 --> 00:04:22,570
video ini, sedikit lebih banyak mengenai

134
00:04:22,660 --> 00:04:23,760
mengapa anda mungkin memilih sebuah kernel linear,

135
00:04:23,970 --> 00:04:26,310
kernel Gaussian, dan seterusnya.

136
00:04:27,860 --> 00:04:29,740
Namun, secara konkret, jika anda

137
00:04:30,040 --> 00:04:31,210
memutuskan untuk menggunakan kernel Gaussian,

138
00:04:31,720 --> 00:04:33,910
maka inilah yang perlu anda lakukan.

139
00:04:35,380 --> 00:04:36,550
Bergantung pada paket perangkat lunak (software package) SVM apa

140
00:04:37,280 --> 00:04:38,990
yang anda pergunakan, paket itu

141
00:04:39,100 --> 00:04:40,960
mungkin akan meminta anda untuk mengimplementasikan suatu

142
00:04:41,070 --> 00:04:42,200
fungsi kernel, atau untuk mengimplementasikan

143
00:04:43,060 --> 00:04:43,880
fungsi kesamaan.

144
00:04:45,020 --> 00:04:46,750
Jadi jika anda mempergunakan suatu

145
00:04:47,010 --> 00:04:49,820
implementasi SVM pada Octave atau MATLAB,

146
00:04:50,000 --> 00:04:50,720
(implementasi tersebut) mungkin akan meminta anda

147
00:04:50,810 --> 00:04:52,560
untuk menyediakan suatu fungsi untuk

148
00:04:52,690 --> 00:04:54,680
menghitung suatu fitur tertentu dari kernel.

149
00:04:55,110 --> 00:04:56,480
Jadi ini sesungguhnya adalah menghitung f subskrip i

150
00:04:56,770 --> 00:04:57,890
untuk suatu

151
00:04:58,220 --> 00:04:59,560
nilai i tertentu, dengan

152
00:05:00,570 --> 00:05:02,310
f disini hanyalah sebuah

153
00:05:02,330 --> 00:05:03,570
bilangan real, jadi mungkin

154
00:05:03,840 --> 00:05:05,060
saya sebaiknya memindahkan ini, lebih baik ditulis sebagai

155
00:05:05,250 --> 00:05:07,230
f(i), namun apa yang perlu anda

156
00:05:07,510 --> 00:05:08,130
lakukan adalah menulis suatu fungsi kernel

157
00:05:08,480 --> 00:05:09,530
yang mengambil input ini, anda tahu,

158
00:05:10,610 --> 00:05:11,910
suatu contoh data latih atau suatu contoh

159
00:05:12,020 --> 00:05:13,140
data uji apapun yang diambilnya

160
00:05:13,280 --> 00:05:14,640
pada suatu vector X dan mengambil

161
00:05:14,990 --> 00:05:16,220
sebagai masukan salah satu dari

162
00:05:16,370 --> 00:05:18,270
penanda (landmark) dan namun

163
00:05:18,880 --> 00:05:20,750
saya hanya menuliskan X1 dan X2 disini,

164
00:05:20,950 --> 00:05:21,810
karena sesungguhnya penanda (landmark)

165
00:05:21,900 --> 00:05:23,750
adalah juga merupakan contoh data latih.

166
00:05:24,470 --> 00:05:26,160
Namun apa yang perlu anda lakukan

167
00:05:26,400 --> 00:05:27,490
adalah menulis perangkat lunak yang

168
00:05:27,670 --> 00:05:28,960
mengambil input ini, anda tahu, X1, X2,

169
00:05:29,150 --> 00:05:30,320
dan menghitung fungsi kesamaan

170
00:05:30,580 --> 00:05:31,950
semacam ini di antara mereka (masukan-masukan tersebut - penj.)

171
00:05:32,530 --> 00:05:33,470
dan menghasilkan suatu bilangan real.

172
00:05:36,180 --> 00:05:37,430
Jadi, apa yang beberapa paket perangkat lunak SVM

173
00:05:37,580 --> 00:05:39,040
lakukan adalah mengharapkan anda

174
00:05:39,510 --> 00:05:40,860
untuk menyediakan fungsi kernel ini

175
00:05:41,410 --> 00:05:44,580
yang mengambil masukan ini, X1, X2, dan menghasilkan suatu bilangan real.

176
00:05:45,580 --> 00:05:46,460
Dan kemudian paket tersebut akan mengambil alih dari situ

177
00:05:46,850 --> 00:05:49,070
dan akan menghasilkan secara otomatis seluruh fitur, dan

178
00:05:49,410 --> 00:05:51,480
dengan demikian secara otomatis mengambil X dan

179
00:05:51,600 --> 00:05:53,370
memetakannya terhadap f1,

180
00:05:53,420 --> 00:05:54,420
f2, sampai dengan f(m) menggunakan

181
00:05:54,750 --> 00:05:56,200
fungsi yang telah anda tulis tersebut, dan

182
00:05:56,310 --> 00:05:57,190
menghasilkan seluruh fitur dan

183
00:05:57,650 --> 00:05:59,080
melatih support vector machine dari situ.

184
00:05:59,870 --> 00:06:00,800
Namun kadangkala anda perlu untuk

185
00:06:00,880 --> 00:06:04,710
menyediakan fungsi ini sendiri.

186
00:06:05,680 --> 00:06:06,770
Selain itu, jika anda mempergunakan kernel Gaussian, beberapa implementasi SVM akan menyertakan juga kernel Gaussian

187
00:06:06,980 --> 00:06:09,950
dan juga beberapa

188
00:06:10,040 --> 00:06:10,990
kernel lain, karena

189
00:06:11,230 --> 00:06:13,580
kernel Gaussian mungkin merupakan kernel yang paling umum digunakan.

190
00:06:14,880 --> 00:06:16,290
Kernel Gaussian dan linear adalah

191
00:06:16,380 --> 00:06:18,210
benar-benar dua kernel yang jauh paling terkenal.

192
00:06:19,130 --> 00:06:20,230
Satu saja catatan mengenai implementasi.

193
00:06:20,750 --> 00:06:21,820
Jika anda memiliki fitur dengan skala yang sangat berbeda,

194
00:06:22,080 --> 00:06:23,620
adalah penting untuk melakukan

195
00:06:24,700 --> 00:06:26,270
penskalaan fitur sebelum

196
00:06:26,600 --> 00:06:27,780
menggunakan kernel Gaussian.

197
00:06:28,580 --> 00:06:29,180
Dan inilah alasannya.

198
00:06:30,150 --> 00:06:31,600
Jika anda membayangkan komputasi

199
00:06:32,290 --> 00:06:33,570
norma di antara X dan I,

200
00:06:33,790 --> 00:06:34,890
ya, jadi term ini disini,

201
00:06:35,390 --> 00:06:37,150
dan term numerator di sana.

202
00:06:38,300 --> 00:06:39,780
Apa yang dilakukan disini, (oleh) norma

203
00:06:40,070 --> 00:06:40,930
antara X dan I, itu sebetulnya

204
00:06:41,130 --> 00:06:42,140
mengatakan, anda tahu, mari menghitung vektor V,

205
00:06:42,450 --> 00:06:43,290
yang sama dengan X dikurangi I.

206
00:06:43,410 --> 00:06:44,980
Dan kemudian

207
00:06:45,250 --> 00:06:47,940
mari kita menghitung norma dari

208
00:06:48,130 --> 00:06:49,080
vektor V, yaitu selisih

209
00:06:49,170 --> 00:06:50,510
antara X. Jadi norma dari V

210
00:06:50,580 --> 00:06:51,510
sesungguhnya sama dengan

211
00:06:53,360 --> 00:06:54,140
V1 kuadrat ditambah V2 kuadrat

212
00:06:54,250 --> 00:06:55,610
ditambah titik titik titik,

213
00:06:55,830 --> 00:06:58,290
ditambah Vn kuadrat.

214
00:06:58,900 --> 00:07:00,320
Karena disini X adalah di dalam

215
00:07:01,060 --> 00:07:02,200
Rn, atau Rn+1,

216
00:07:02,290 --> 00:07:05,180
namun saya akan mengabaikan, anda tahu, X0.

217
00:07:06,540 --> 00:07:08,420
Jadi, mari kita bayangkan X adalah sebuah Rn,

218
00:07:08,510 --> 00:07:10,800
 kuadrat pada sisi kiri

219
00:07:10,950 --> 00:07:12,320
adalah yang menjadikan hal ini benar.

220
00:07:12,570 --> 00:07:14,090
Jadi ini sama dengan itu,

221
00:07:14,400 --> 00:07:16,120
betul bukan?

222
00:07:17,210 --> 00:07:18,710
Jadi jika dituliskan dengan cara lain, ini

223
00:07:18,850 --> 00:07:20,100
akan menjadi X1 dikurangi I1 kuadrat,

224
00:07:20,290 --> 00:07:22,600
ditambah S2 dikurangi

225
00:07:22,910 --> 00:07:24,590
I2 kuadrat, ditambah

226
00:07:24,910 --> 00:07:26,580
titik titik titik ditambah

227
00:07:27,130 --> 00:07:28,540
Xn dikurangi In kuadrat.

228
00:07:29,720 --> 00:07:30,790
Dan sekarang jika fitur anda

229
00:07:31,850 --> 00:07:33,460
memiliki nilai pada kisaran yang sangat berbeda,

230
00:07:33,940 --> 00:07:35,150
jadi ambil sebagai contoh suatu

231
00:07:35,360 --> 00:07:37,180
prediksi perumahan, jika

232
00:07:38,020 --> 00:07:40,490
data anda adalah berbagai data mengenai rumah.

233
00:07:41,420 --> 00:07:43,000
Dan jika X berada pada

234
00:07:43,140 --> 00:07:44,660
kisaran ribuan kaki persegi

235
00:07:44,950 --> 00:07:47,190
(1ft²≈ 0.09m² - penj.) untuk

236
00:07:48,010 --> 00:07:48,840
fitur pertama, X1.

237
00:07:49,700 --> 00:07:51,630
Tetapi jika fitur kedua anda, X2, adalah jumlah kamar tidur,

238
00:07:52,540 --> 00:07:53,610
jadi jika ini adalah pada

239
00:07:53,730 --> 00:07:56,720
kisaran satu sampai lima kamar tidur, maka

240
00:07:57,810 --> 00:07:59,320
X1 dikurangi I1 akan besar nilainya,

241
00:07:59,780 --> 00:08:00,820
(nilai) ini bisa jadi sekitar seribu kuadrat

242
00:08:01,000 --> 00:08:02,880
sedangkan X2 dikurangi I2

243
00:08:03,200 --> 00:08:04,620
akan jauh lebih kecil nilainya, dan jika seperti itu,

244
00:08:04,750 --> 00:08:06,800
maka pada term yang ini,

245
00:08:08,320 --> 00:08:09,660
kedua jarak itu akan hampir sepenuhnya

246
00:08:10,060 --> 00:08:12,060
didominasi oleh

247
00:08:12,570 --> 00:08:13,280
ukuran rumah

248
00:08:14,390 --> 00:08:15,760
dan jumlah kamar tidur pada dasarnya akan terabaikan.

249
00:08:16,950 --> 00:08:18,060
Dengan demikian, untuk menghindari hal ini

250
00:08:18,230 --> 00:08:19,070
sehingga mesin dapat bekerja dengan baik,

251
00:08:19,360 --> 00:08:21,890
lakukanlah penskalaan fitur.

252
00:08:23,420 --> 00:08:24,830
Dan hal itu (penskalaan fitur) akan memastikan bahwa

253
00:08:25,810 --> 00:08:27,020
SVM akan memberikan perhatian yang seimbang

254
00:08:27,950 --> 00:08:28,870
terhadap seluruh fitur yang anda pergunakan

255
00:08:29,190 --> 00:08:30,450
dan tidak hanya, pada contoh ini,

256
00:08:30,600 --> 00:08:31,870
pada ukuran rumah

257
00:08:32,150 --> 00:08:33,440
yang dominan terhadap fitur-fitur lainnya.

258
00:08:34,700 --> 00:08:35,810
Ketika anda mencoba suatu support vector machine,

259
00:08:36,110 --> 00:08:38,760
kemungkinannya adalah

260
00:08:38,970 --> 00:08:40,000
anda akan menggunakan dua kernel

261
00:08:40,460 --> 00:08:41,750
 yang paling umum dipergunakan,

262
00:08:41,850 --> 00:08:43,120
yaitu kernel linear, yang berarti tidak menggunakan kernel,

263
00:08:43,320 --> 00:08:45,600
atau kernel Gaussian yang telah kita bicarakan.

264
00:08:46,520 --> 00:08:47,390
Dan satu lagi catatan peringatan

265
00:08:47,900 --> 00:08:49,070
yaitu bahwa tidak semua fungsi kesamaan

266
00:08:49,580 --> 00:08:50,590
yang mungkin anda pikirkan

267
00:08:50,770 --> 00:08:52,520
adalah kernel yang valid.

268
00:08:53,450 --> 00:08:54,840
Dan kernel Gaussian dan kernel linear

269
00:08:55,090 --> 00:08:56,410
serta kernel-kernel lain yang terkadang

270
00:08:56,710 --> 00:08:57,850
akan anda lihat dipergunakan oleh orang lain,

271
00:08:58,030 --> 00:08:59,840
semua kernel-kernel itu perlu untuk memenuhi suatu kondisi teknis.

272
00:09:00,380 --> 00:09:02,510
Kondisi teknis tersebut disebut Teorema Mercer

273
00:09:02,630 --> 00:09:03,560
dan alasan dari mengapa anda perlu melakukannya

274
00:09:03,710 --> 00:09:05,430
adalah karena algoritma support vector machine

275
00:09:06,380 --> 00:09:08,140
atau implementasi dari SVM

276
00:09:08,480 --> 00:09:09,560
memiliki banyak trik optimasi numerik

277
00:09:10,050 --> 00:09:11,380
yang cerdik.

278
00:09:12,110 --> 00:09:13,270
Untuk mencari solusi dari

279
00:09:13,340 --> 00:09:15,650
parameter theta secara efisien dan

280
00:09:16,590 --> 00:09:18,840
sesuai dengan gambaran dari rencana awal,

281
00:09:19,470 --> 00:09:21,010
keputusan-keputusan itu diambil untuk membatasi

282
00:09:21,540 --> 00:09:22,900
perhatian kita hanya terhadap kernel-kernel

283
00:09:23,510 --> 00:09:25,860
yang memenuhi kondisi teknis yang disebut Teorema Mercer tersebut.

284
00:09:26,280 --> 00:09:27,360
Dan apa yang dilakukan adalah

285
00:09:27,570 --> 00:09:28,540
untuk memastikan bahwa seluruh dari paket

286
00:09:28,820 --> 00:09:30,270
SVM ini, semua dari paket perangkat lunak

287
00:09:30,500 --> 00:09:32,210
SVM ini dapat mempergunakan

288
00:09:32,310 --> 00:09:34,740
sebagian besar dari kelas optimasi dan mendapatkan

289
00:09:35,280 --> 00:09:37,470
nilai parameter theta dengan sangat cepat.

290
00:09:39,320 --> 00:09:40,340
Jadi, apa yang akhirnya dilakukan oleh kebanyakan orang

291
00:09:40,840 --> 00:09:42,470
adalah mempergunakan salah satu dari kernel linear

292
00:09:42,610 --> 00:09:44,210
atau kernel Gaussian, namun terdapat pula

293
00:09:44,430 --> 00:09:45,610
beberapa kernel lain yang juga

294
00:09:45,940 --> 00:09:47,460
memenuhi Teorema Mercer, dan

295
00:09:47,560 --> 00:09:48,690
anda mungkin akan menjumpai orang lain

296
00:09:48,850 --> 00:09:50,050
mempergunakan (kernel-kernel tersebut), meskipun secara pribadi

297
00:09:50,880 --> 00:09:53,780
saya sangat, sangat jarang mempergunakan kernel-kernel lain tersebut.

298
00:09:54,160 --> 00:09:56,990
Sekedar menyebutkan beberapa dari kernel-kernel lain yang mungkin anda jumpai,

299
00:09:57,990 --> 00:10:00,300
salah satunya adalah kernel polinomial.

300
00:10:01,570 --> 00:10:03,350
Dan untuk kernel tersebut, kesamaan antara

301
00:10:03,800 --> 00:10:05,520
X dan I adalah didefinisikan sebagai,

302
00:10:05,730 --> 00:10:06,760
(salah satu) di antara banyak pilihan

303
00:10:06,830 --> 00:10:07,880
adalah anda dapat mengambil nilai dari

304
00:10:08,640 --> 00:10:10,370
X transpose I kuadrat.

305
00:10:10,960 --> 00:10:13,410
Jadi, ini adalah salah satu ukuran mengenai kemiripan dari X dan I.

306
00:10:13,610 --> 00:10:14,930
Jika X dan I sangat dekat satu sama lain,

307
00:10:15,500 --> 00:10:18,260
maka hasil kali (inner product) dari keduanya akan cenderung besar.

308
00:10:20,200 --> 00:10:21,870
Dan jadi, anda tahu, ini adalah suatu kernel

309
00:10:23,080 --> 00:10:23,520
yang agak tidak biasa

310
00:10:24,000 --> 00:10:25,130
dalam artian tidak sering dipergunakan, namun

311
00:10:26,490 --> 00:10:29,190
ada kemungkinan anda akan menjumpai penggunaannya oleh orang lain.

312
00:10:30,050 --> 00:10:31,810
Ini adalah salah satu versi dari sebuah kernel polinomial.

313
00:10:32,330 --> 00:10:35,090
Yang lain adalah X transpose I pangkat tiga.

314
00:10:36,690 --> 00:10:38,780
Berikut ini adalah contoh dari kernel polinomial.

315
00:10:39,040 --> 00:10:41,270
X transpos I ditambah satu pangkat tiga.

316
00:10:42,560 --> 00:10:43,620
X transpos I ditambah mungkin

317
00:10:43,910 --> 00:10:44,930
suatu angka selain 1, misalnya 5

318
00:10:44,970 --> 00:10:46,680
dan, anda tahu, dipangkatkan 4

319
00:10:47,700 --> 00:10:49,840
sehingga kernel polinomial sesungguhnya memiliki 2 parameter.

320
00:10:50,610 --> 00:10:53,020
Yang pertama adalah, angka berapa yang anda tambahkan di sini?

321
00:10:53,520 --> 00:10:53,920
Bisa jadi 0.

322
00:10:54,430 --> 00:10:58,660
Ini sebetulnya adalah ditambah 0 di sini, begitu pula dengan derajat dari polinomial di sana.

323
00:10:58,680 --> 00:11:01,670
Sehingga derajat pangkat dan angka-angka ini.

324
00:11:02,250 --> 00:11:04,140
Dan bentuk yang lebih umum dari

325
00:11:04,280 --> 00:11:05,530
kernel polinomial adalah X transpose I,

326
00:11:05,720 --> 00:11:07,620
ditambah suatu nilai konstanta,

327
00:11:07,940 --> 00:11:11,510
dan kemudian dipangkatkan

328
00:11:11,800 --> 00:11:14,850
dengan suatu nilai

329
00:11:15,060 --> 00:11:16,720
pada X1, sehingga kedua nilai ini

330
00:11:16,940 --> 00:11:19,650
adalah parameter dari kernel polinomial.

331
00:11:20,510 --> 00:11:22,820
Kernel polinomial hampir selalu, atau biasanya,

332
00:11:23,350 --> 00:11:24,440
memiliki performa yang lebih buruk.

333
00:11:24,820 --> 00:11:25,950
Dan tidak seperti kernel Gaussian, ini tidak

334
00:11:26,270 --> 00:11:28,370
banyak dipergunakan, namun ini mungkin akan anda temui.

335
00:11:29,320 --> 00:11:30,480
Biasanya ini hanya dipergunakan untuk

336
00:11:30,750 --> 00:11:31,710
data dengan X dan I adalah

337
00:11:32,000 --> 00:11:33,180
sepenuhnya non-negatif,

338
00:11:33,740 --> 00:11:34,720
sehingga itu memastikan bahwa hasil kali

339
00:11:34,910 --> 00:11:36,710
dalam ini tidak akan pernah bernilai negatif.

340
00:11:37,850 --> 00:11:40,010
Dan hal ini menangkap intuisi bahwa

341
00:11:40,390 --> 00:11:41,340
jika X dan I adalah sangat mirip

342
00:11:41,540 --> 00:11:44,110
satu sama lain, maka mungkin hasil kali dari keduanya akan bernilai besar.

343
00:11:44,420 --> 00:11:45,590
Mereka (kernel polinomial - penj.) juga memiliki beberapa sifat lain

344
00:11:46,260 --> 00:11:48,080
namun orang biasanya tidak banyak mempergunakannya.

345
00:11:49,130 --> 00:11:50,150
Dan kemudian, tergantung pada apa yang anda lakukan,

346
00:11:50,260 --> 00:11:51,210
ada jenis-jenis kernel lain pula

347
00:11:52,330 --> 00:11:54,950
yang lebih esoterik, yang mungkin akan anda jumpai.

348
00:11:55,670 --> 00:11:57,180
Anda tahu, ada kernel string, ini terkadang

349
00:11:57,340 --> 00:11:58,430
dipergunakan bila data masukan anda

350
00:11:58,550 --> 00:12:01,350
berupa string teks atau tipe-tipe string lainnya.

351
00:12:02,270 --> 00:12:02,940
Ada hal-hal seperti

352
00:12:03,260 --> 00:12:06,000
kernel chi-kuadrat, kernel interseksi histogram, dan sebagainya.

353
00:12:06,690 --> 00:12:08,420
Ada jenis-jenis kernel lain yang lebih esoterik

354
00:12:08,660 --> 00:12:09,840
yang dapat anda pergunakan untuk mengukur kesamaan

355
00:12:10,760 --> 00:12:12,030
di antara objek-objek yang berbeda.

356
00:12:12,660 --> 00:12:13,800
Jadi, sebagai contoh, jika anda berusaha untuk

357
00:12:14,380 --> 00:12:15,840
mengerjakan permasalahan klasifikasi teks tertentu,

358
00:12:16,170 --> 00:12:17,060
dengan input X berupa

359
00:12:17,200 --> 00:12:19,300
suatu string, maka

360
00:12:19,490 --> 00:12:20,490
mungkin kita ingin menemukan

361
00:12:20,550 --> 00:12:22,050
kesamaan di antara dua string

362
00:12:22,430 --> 00:12:24,240
dengan menggunakan kernel string, namun saya

363
00:12:24,520 --> 00:12:26,440
secara pribadi jarang sekali, atau hampir tidak sama sekali,

364
00:12:26,990 --> 00:12:29,340
menggunakan kernel-kernel yang lebih esoterik ini.

365
00:12:29,880 --> 00:12:30,970
Saya pikir saya pernah menggunakan kernel chi-kuadrat

366
00:12:31,170 --> 00:12:32,270
mungkin satu kali dalam

367
00:12:32,340 --> 00:12:33,670
hidup saya, dan kernel histogram

368
00:12:34,240 --> 00:12:35,580
mungkin sekali atau dua kali dalam hidup saya.

369
00:12:35,630 --> 00:12:38,500
Saya sesungguhnya belum pernah menggunakan kernel string. Namun

370
00:12:39,350 --> 00:12:41,560
seandainya anda menemui ini pada aplikasi-aplikasi lain, anda tahu,

371
00:12:42,700 --> 00:12:43,640
jika anda melalukan suatu pencarian web secara cepat

372
00:12:43,860 --> 00:12:44,850
mungkin dengan menggunakan Google

373
00:12:45,040 --> 00:12:46,000
atau dengan menggunakan Bing,

374
00:12:46,590 --> 00:12:48,240
anda seharusnya juga akan menemukan definisi dari kernel-kernel ini.

375
00:12:51,480 --> 00:12:55,680
Selanjutnya dua detail terakhir yang ingin saya bicarakan dalam video ini.
Yang pertama adalah tentang klasifikasi multikelas.

376
00:12:56,370 --> 00:12:59,510
Jadi, anda memiliki 4 kelas luaran, atau biasanya

377
00:12:59,800 --> 00:13:01,880
3 kelas luaran dari pembatas keputusan

378
00:13:02,530 --> 00:13:06,860
(decision boundary) yang tepat diantara kelas-kelas yang anda miliki. Hampir semua paket SVM

379
00:13:07,220 --> 00:13:08,750
memiliki fasilitas klasifikasi

380
00:13:09,030 --> 00:13:10,430
multikelas yang sudah tersedia (built-in).

381
00:13:11,100 --> 00:13:12,060
Jadi jika anda menggunakan pola seperti

382
00:13:12,270 --> 00:13:13,320
itu, anda cukup menggunakan

383
00:13:13,540 --> 00:13:15,370
kedua fungsionalitas, dan itu

384
00:13:15,490 --> 00:13:16,940
seharusnya bekerja dengan cukup baik. Selain itu,

385
00:13:17,790 --> 00:13:18,790
satu cara untuk melakukan hal ini

386
00:13:19,000 --> 00:13:19,880
adalah menggunakan metode

387
00:13:20,000 --> 00:13:21,280
satu-versus-semua (one versus all) yang telah kita

388
00:13:21,370 --> 00:13:23,690
bicarakan saat kita berdiskusi tentang regresi logistik.

389
00:13:24,680 --> 00:13:25,410
Jadi apa yang anda lakukan adalah melatih

390
00:13:26,160 --> 00:13:27,550
k buah SVM (kSVM) jika anda memiliki

391
00:13:27,700 --> 00:13:29,190
k kelas, masing-masing untuk membedakan

392
00:13:29,900 --> 00:13:31,060
setiap kelompok dari anggota kelompok lainnya.

393
00:13:31,850 --> 00:13:32,930
Dan ini akan menghasilkan k vektor parameter,

394
00:13:33,520 --> 00:13:34,530
jadi ini akan memberikan pada anda

395
00:13:34,680 --> 00:13:36,210
theta1, yang akan mencoba

396
00:13:36,530 --> 00:13:38,170
untuk membedakan kelas y = 1

397
00:13:38,630 --> 00:13:39,980
dari semua kelas lainnya

398
00:13:40,130 --> 00:13:41,340
kemudian anda mendapatkan

399
00:13:41,420 --> 00:13:42,910
parameter kedua, vektor theta2,

400
00:13:42,970 --> 00:13:43,910
yang akan anda dapatkan

401
00:13:44,020 --> 00:13:45,420
jika anda memandang y = 2

402
00:13:45,720 --> 00:13:47,080
sebagai kelas positif dan seluruh kelas lainnya

403
00:13:47,460 --> 00:13:48,680
sebagai kelas negatif.

404
00:13:49,260 --> 00:13:50,550
Dan demikian seterusnya hingga

405
00:13:50,800 --> 00:13:52,400
parameter vektor theta k,

406
00:13:52,750 --> 00:13:54,520
yang merupakan vektor parameter untuk

407
00:13:54,600 --> 00:13:56,770
membedakan kelas terakhir y = k

408
00:13:57,360 --> 00:13:59,380
dari kelas-kelas lainnya, dan

409
00:13:59,490 --> 00:14:00,590
pada akhirnya, ini tepat sama dengan

410
00:14:01,270 --> 00:14:02,040
metode satu-versus-semua

411
00:14:02,420 --> 00:14:04,230
yang kita pergunakan pada regresi logistik

412
00:14:04,760 --> 00:14:05,910
dimana anda memprediksi kelas i

413
00:14:06,390 --> 00:14:07,690
yang memiliki nilai theta transpos x

414
00:14:08,030 --> 00:14:11,840
terbesar. Demikian pula dengan klasifikasi multikelas.

415
00:14:12,440 --> 00:14:13,750
Untuk kasus-kasus yang lebih umum

416
00:14:14,300 --> 00:14:15,090
yaitu bahwa kemungkinan besar

417
00:14:15,180 --> 00:14:16,460
bahwa paket perangkat lunak apapun

418
00:14:16,780 --> 00:14:18,010
yang anda gunakan, anda tahu, akan ada

419
00:14:18,340 --> 00:14:19,650
kemungkinan yang cukup beralasan bahwa

420
00:14:19,920 --> 00:14:21,740
(paket tersebut) telah memiliki fungsionalitas klasifikasi multikelas bawaan (built-in)

421
00:14:21,920 --> 00:14:24,410
dan dengan demikian anda tidak perlu khawatir tentang hal ini.

422
00:14:25,280 --> 00:14:27,010
Akhirnya, kita melakukan pengembangan support vector machine

423
00:14:27,210 --> 00:14:28,650
dengan bermula dari regresi logistik

424
00:14:29,090 --> 00:14:31,500
dan kemudian memodifikasi sedikit fungsi biayanya.

425
00:14:31,910 --> 00:14:34,900
Hal terakhir yang ingin kita lakukan dalam video ini adalah berbicara sedikit

426
00:14:35,550 --> 00:14:36,570
tentang kapan anda menggunakan salah satu dari

427
00:14:36,660 --> 00:14:38,840
kedua algoritma ini, jadi katakanlah

428
00:14:39,080 --> 00:14:40,000
n adalah jumlah fitur

429
00:14:40,160 --> 00:14:42,000
dan m adalah jumlah dari data latih.

430
00:14:43,190 --> 00:14:45,250
Jadi, kapankah kita seharusnya menggunakan satu algoritma ketimbang yang lainnya?

431
00:14:47,130 --> 00:14:48,430
Nah, jika n relatif besar

432
00:14:48,980 --> 00:14:50,140
dibandingkan dengan ukuran data latih anda,

433
00:14:50,360 --> 00:14:51,390
jadi sebagai contoh,

434
00:14:52,810 --> 00:14:53,990
jika anda berpikir mengenai hal ini

435
00:14:54,250 --> 00:14:55,180
dengan jumlah fitur yang jauh lebih besar

436
00:14:55,330 --> 00:14:56,870
daripada m, dan ini mungkin,

437
00:14:57,120 --> 00:14:58,210
sebagai contoh, jika anda memiliki suatu

438
00:14:58,320 --> 00:15:00,590
permasalahan klasifikasi teks, yang anda tahu,

439
00:15:01,550 --> 00:15:02,430
dimensi dari vektor fitur adalah,

440
00:15:02,700 --> 00:15:04,160
saya tidak tahu, mungkin, 10 ribu.

441
00:15:05,370 --> 00:15:06,350
Dan jika ukuran data latih anda,

442
00:15:06,720 --> 00:15:08,290
mungkin, 10 sampai dengan,

443
00:15:08,510 --> 00:15:10,250
anda tahu, sampai 1000.

444
00:15:10,500 --> 00:15:12,140
Jadi, bayangkanlah suatu permasalahan

445
00:15:12,320 --> 00:15:14,250
klasifikasi spam, dengan setiap e-mail

446
00:15:14,510 --> 00:15:15,840
mewakili 10.000 fitur yang mewakili

447
00:15:16,150 --> 00:15:18,010
10.000 kata, namun

448
00:15:18,190 --> 00:15:19,550
anda (hanya) memiliki, anda tahu, mungkin 10

449
00:15:19,780 --> 00:15:21,150
data latih atau mungkin sampai dengan 1000 data latih.

450
00:15:22,450 --> 00:15:23,750
Jadi jika n besar relatif terhadap m,

451
00:15:23,890 --> 00:15:25,090
maka yang biasanya akan

452
00:15:25,250 --> 00:15:26,480
saya lakukan adalah menggunakan regresi logistik

453
00:15:26,850 --> 00:15:27,990
atau menggunakannya

454
00:15:28,100 --> 00:15:29,030
sebagai m tanpa menggunakan kernel

455
00:15:29,460 --> 00:15:30,790
atau dengan kata lain, dengan kernel linear.

456
00:15:31,620 --> 00:15:32,430
Karena, jika anda memiliki begitu banyak fitur

457
00:15:32,580 --> 00:15:33,830
dan set data latih berukuran kecil, anda tahu,

458
00:15:34,530 --> 00:15:35,870
suatu fungsi linear kemungkinan akan

459
00:15:36,330 --> 00:15:37,380
berfungsi dengan baik, dan jika anda tidak benar-benar

460
00:15:37,640 --> 00:15:38,790
memiliki data yang cukup untuk melakukan

461
00:15:38,910 --> 00:15:40,760
fitting terhadap fungsi nonlinear yang sangat rumit.

462
00:15:41,340 --> 00:15:42,410
Sekarang jika n kecil,

463
00:15:42,520 --> 00:15:44,020
sedangkan m bernilai sedang,

464
00:15:44,350 --> 00:15:45,890
maksud saya disini adalah

465
00:15:45,940 --> 00:15:47,450
n mungkin besarnya 1 - 1000,

466
00:15:48,040 --> 00:15:50,350
nilai 1 akan dianggap sangat kecil.

467
00:15:50,530 --> 00:15:51,470
Namun mungkin sampai dengan 1000 fitur

468
00:15:51,700 --> 00:15:54,270
dan jika jumlah dari

469
00:15:54,590 --> 00:15:56,180
data latih adalah

470
00:15:56,330 --> 00:15:57,700
sekitar mulai dari 10,

471
00:15:58,210 --> 00:16:00,750
anda tahu, 10 sampai dengan mungkin 10.000 contoh,

472
00:16:01,350 --> 00:16:03,160
atau mungkin sampai dengan 50.000 contoh.

473
00:16:03,630 --> 00:16:06,490
Jika m cukup besar, misalnya kira-kira 10.000, namun tidak sampai satu juta,

474
00:16:06,760 --> 00:16:08,100
jadi jika m berukuran sedang

475
00:16:08,300 --> 00:16:09,950
maka sering kali suatu SVM

476
00:16:10,790 --> 00:16:12,980
dengan kernel linear akan bekerja dengan baik.

477
00:16:13,530 --> 00:16:14,580
Kita telah membicarakan tentang hal ini seawal mungkin,

478
00:16:14,710 --> 00:16:15,800
dengan satu contoh konkret,

479
00:16:16,350 --> 00:16:17,100
yaitu jika anda memiliki data latih berdimensi 2.

480
00:16:17,520 --> 00:16:19,720
Jadi jika n = 2

481
00:16:19,900 --> 00:16:21,010
dimana anda telah memiliki,

482
00:16:21,320 --> 00:16:23,710
anda tahu, data latih dalam jumlah yang cukup besar.

483
00:16:24,710 --> 00:16:25,860
(dalam kasus ini) kernel Gaussian akan

484
00:16:26,130 --> 00:16:28,160
memberikan hasil yang cukup baik dalam memisahkan kelas positif dan negatif.

485
00:16:29,770 --> 00:16:30,890
Suatu skenario ketiga yang menarik

486
00:16:30,980 --> 00:16:32,420
adalah jika n bernilai kecil

487
00:16:32,520 --> 00:16:34,270
sedangkan m besar.

488
00:16:34,890 --> 00:16:36,560
Jadi jika n adalah, mungkin

489
00:16:37,390 --> 00:16:39,280
1 sampai 1000, atau lebih besar,

490
00:16:40,200 --> 00:16:42,750
namun m, mungkin 50.000 atau lebih

491
00:16:43,320 --> 00:16:46,400
hingga jutaan.

492
00:16:47,520 --> 00:16:50,270
Jadi, 50 ribu, seratus ribu, satu juta, satu triliun.

493
00:16:51,290 --> 00:16:54,020
Artinya anda memiliki ukuran data latih yang sangat, sangat besar.

494
00:16:55,240 --> 00:16:56,160
Jadi dalam kasus ini,

495
00:16:56,380 --> 00:16:57,630
suatu SVM dengan kernel Gaussian

496
00:16:57,900 --> 00:16:59,850
akan berjalan dengan agak lambat.

497
00:17:00,160 --> 00:17:02,300
Paket-paket SVM yang tersedia saat ini, jika anda menggunakan

498
00:17:02,410 --> 00:17:04,900
kernel Gaussian, cenderung agak mengalami kesulitan.

499
00:17:05,050 --> 00:17:06,250
Jika anda memiliki, anda tahu, mungkin 50 ribu

500
00:17:06,590 --> 00:17:07,530
data masih dapat tertangani dengan baik, namun

501
00:17:07,620 --> 00:17:10,250
jika anda memiliki satu juta contoh data latih, mungkin

502
00:17:10,450 --> 00:17:11,950
atau bahkan 100 ribu dengan nilai m

503
00:17:12,170 --> 00:17:13,730
yang sangat besar. Paket SVM

504
00:17:14,180 --> 00:17:15,590
yang tersedia saat ini sudah sangat bagus,

505
00:17:15,870 --> 00:17:17,100
namun mereka bisa jadi masih mengalami

506
00:17:17,600 --> 00:17:18,400
sedikit kesulitan jika anda memiliki

507
00:17:19,010 --> 00:17:20,940
ukuran data pelatihan yang sangat, sangat besar ketika menggunakan Kernel Gaussian.

508
00:17:22,050 --> 00:17:23,150
Jadi pada kasus semacam itu, apa yang akan

509
00:17:23,350 --> 00:17:24,960
saya lakukan adalah mencoba untuk

510
00:17:25,330 --> 00:17:26,660
menciptakan lebih banyak fitur

511
00:17:26,800 --> 00:17:28,600
secara manual dan kemudian menggunakan

512
00:17:28,930 --> 00:17:30,340
regresi logistik atau sebuah SVM

513
00:17:30,630 --> 00:17:32,060
tanpa kernel.

514
00:17:33,140 --> 00:17:34,030
Dan seandainya anda melihat pada slide ini

515
00:17:34,230 --> 00:17:35,900
dan anda melihat regresi logistik

516
00:17:36,460 --> 00:17:37,750
atau SVM tanpa kernel,

517
00:17:38,510 --> 00:17:39,890
pada kedua tempat ini, saya telah

518
00:17:39,980 --> 00:17:41,750
semacam memasangkan keduanya.

519
00:17:42,060 --> 00:17:43,050
Ada alasan untuk itu, yaitu bahwa

520
00:17:43,900 --> 00:17:45,640
regresi logistik dan SVM tanpa kernel,

521
00:17:46,000 --> 00:17:47,130
keduanya adalah algoritma-algoritma

522
00:17:47,350 --> 00:17:49,450
yang cukup mirip, dan anda tahu, salah satu dari

523
00:17:49,680 --> 00:17:51,170
baik regresi logistik atau SVM tanpa kernel

524
00:17:51,500 --> 00:17:53,230
biasanya akan melakukan hal-hal

525
00:17:53,380 --> 00:17:54,780
yang hampir sama dan menghasilkan

526
00:17:54,900 --> 00:17:56,690
performa yang hampir sama, namun bergantung pada

527
00:17:57,060 --> 00:18:00,340
detail implementasi anda, salah satu mungkin lebih efisien daripada yang lainnya.

528
00:18:00,930 --> 00:18:02,220
Namun, dimana salah satu dari

529
00:18:02,310 --> 00:18:03,530
algoritma-algoritma ini dapat diterapkan,

530
00:18:03,740 --> 00:18:05,190
regresi logistik atau SVM tanpa kernel,

531
00:18:05,420 --> 00:18:05,840
yang satunya kemungkinan

532
00:18:06,650 --> 00:18:07,600
akan juga dapat bekerja dengan baik.

533
00:18:08,540 --> 00:18:09,660
Namun kekuatan dari SVM

534
00:18:09,720 --> 00:18:11,610
adalah apabila anda

535
00:18:11,810 --> 00:18:14,100
mempergunakan berbagai kernel yang berbeda untuk

536
00:18:14,430 --> 00:18:15,860
mempelajari fungsi-fungsi nonlinear kompleks.

537
00:18:16,680 --> 00:18:20,300
Dan rezim ini, anda tahu, ketika anda

538
00:18:20,550 --> 00:18:22,530
memiliki mungkin sampai dengan 10 ribu contoh, mungkin sampai dengan 50 ribu,

539
00:18:22,610 --> 00:18:25,010
dan jumlah fitur anda

540
00:18:26,580 --> 00:18:27,540
cukup besar.

541
00:18:27,840 --> 00:18:29,230
Itu adalah rezim yang cukup umum

542
00:18:29,670 --> 00:18:30,910
dan mungkin merupakan rezim

543
00:18:31,430 --> 00:18:33,830
dimana sebuah SVM dengan kernel Gaussian akan memberikan performa yang cemerlang.

544
00:18:34,320 --> 00:18:35,640
Mereka dapat melakukan hal-hal yang lebih sulit

545
00:18:35,860 --> 00:18:39,850
dilakukan jika hanya mempergunakan regresi logistik saja.

546
00:18:40,100 --> 00:18:40,930
Dan akhirnya, dimanakah posisi dari jaringan syaraf tiruan (neural network)?

547
00:18:41,120 --> 00:18:42,230
Nah, untuk semua dari permasalahan ini,

548
00:18:42,440 --> 00:18:43,890
dan juga untuk semua rezim yang berbeda ini,

549
00:18:43,960 --> 00:18:46,310
jaringan syaraf tiruan yang dirancang

550
00:18:46,630 --> 00:18:49,110
akan juga dapat bekerja dengan sama baiknya.

551
00:18:50,320 --> 00:18:51,700
Salah satu kerugian, atau salah satu alasan

552
00:18:51,830 --> 00:18:52,980
mengapa kita terkadang tidak mempergunakan jaringan syaraf tiruan

553
00:18:53,220 --> 00:18:54,690
adalah, untuk beberapa dari

554
00:18:54,920 --> 00:18:56,080
permasalahan ini, jaringan syaraf tiruan

555
00:18:56,180 --> 00:18:57,640
mungkin membutuhkan waktu lama dalam proses pelatihannya.

556
00:18:58,250 --> 00:18:59,080
Namun jika anda memiliki paket implementasi SVM

557
00:18:59,350 --> 00:19:01,190
yang sangat baik, itu bisa jadi

558
00:19:01,400 --> 00:19:04,120
dapat berjalan dengan lebih cepat daripada jaringan syaraf tiruan anda.

559
00:19:05,130 --> 00:19:06,130
Dan, meskipun kita tidak menunjukkan hal ini

560
00:19:06,350 --> 00:19:07,520
sebelumnya, ternyata

561
00:19:07,630 --> 00:19:09,800
permasalah optimasi yang dimiliki oleh SVM

562
00:19:10,070 --> 00:19:11,120
adalah suatu permasalahan optimasi konveks,

563
00:19:12,320 --> 00:19:13,830
sehingga paket perangkat lunak

564
00:19:14,410 --> 00:19:15,800
optimasi SVM yang baik

565
00:19:16,160 --> 00:19:17,870
akan selalu dapat menemukan

566
00:19:18,240 --> 00:19:21,370
titik minimum global atau sesuatu yang dekat dengan itu.

567
00:19:21,720 --> 00:19:24,100
Sehingga untuk SVM, anda tidak perlu khawatir mengenai optima lokal.

568
00:19:25,280 --> 00:19:26,440
Dalam praktek optima lokal bukanlah

569
00:19:26,580 --> 00:19:27,920
suatu permasalahan besar bagi jaringan syaraf tiruan

570
00:19:28,090 --> 00:19:29,120
namun ini adalah satu hal yang

571
00:19:29,310 --> 00:19:31,520
tidak perlu dikhawatirkan jika anda menggunakan SVM.

572
00:19:33,350 --> 00:19:34,560
Dan tergantung pada permasalahan anda,

573
00:19:34,910 --> 00:19:37,050
jaringan syaraf tiruan bisa jadi lebih lambat

574
00:19:37,580 --> 00:19:41,020
daripada SVM, terlebih pada rezim seperti ini.

575
00:19:41,420 --> 00:19:42,200
Seandainya panduan yang telah diberikan

576
00:19:42,520 --> 00:19:43,500
disini terkesan agak kurang jelas,

577
00:19:43,860 --> 00:19:44,600
dan jika anda sedang melihat pada suatu permasalahan

578
00:19:46,930 --> 00:19:48,050
dan berpikir, "Panduan ini masih agak

579
00:19:48,170 --> 00:19:49,190
kurang jelas, saya masih tidak sepenuhnya yakin,

580
00:19:49,570 --> 00:19:50,730
apakah seharusnya saya menggunakan algoritma ini

581
00:19:50,780 --> 00:19:52,690
atau algoritma itu?", hal ini betul-betul tidak mengapa.

582
00:19:52,950 --> 00:19:54,100
Ketika saya menghadapi suatu permasalahan

583
00:19:54,330 --> 00:19:55,570
pembelajaran mesin, anda tahu, terkadang

584
00:19:55,730 --> 00:19:57,010
tidak jelas apakah algoritma itu adalah

585
00:19:57,150 --> 00:19:58,700
yang terbaik untuk digunakan, namun sebagaimana

586
00:19:59,540 --> 00:20:00,590
anda lihat pada video-video sebelumnya, sungguh,

587
00:20:01,200 --> 00:20:02,470
anda tahu, memang algoritma itu penting,

588
00:20:02,700 --> 00:20:03,920
namun yang seringkali bahkan lebih penting adalah

589
00:20:04,250 --> 00:20:06,400
hal-hal seperti, berapa banyak data yang anda miliki

590
00:20:07,090 --> 00:20:08,280
dan seperapa terampilnya anda, seberapa bagusnya anda

591
00:20:08,450 --> 00:20:09,500
dalam melakukan analisis error

592
00:20:09,750 --> 00:20:11,450
dan melakukan debugging algoritma pembelajaran,

593
00:20:11,660 --> 00:20:13,090
menemukan cara untuk mendesain

594
00:20:13,220 --> 00:20:15,120
fitur baru dan menentukan fitur-fitur lain apa

595
00:20:15,280 --> 00:20:17,540
yang sebaiknya anda berikan kepada algoritma pembelajaran dan seterusnya.

596
00:20:17,960 --> 00:20:19,110
Dan seringkali hal-hal itu akan lebih berpengaruh

597
00:20:19,660 --> 00:20:20,700
daripada apakah anda menggunakan

598
00:20:20,840 --> 00:20:22,370
regresi logistik atau SVM.

599
00:20:23,280 --> 00:20:24,650
Namun, setelah mengatakan itu,

600
00:20:25,010 --> 00:20:26,180
SVM saat ini masih secara luas

601
00:20:26,630 --> 00:20:27,890
dianggap sebagai salah satu dari

602
00:20:27,950 --> 00:20:29,600
algoritma pembelajaran yang paling ampuh,

603
00:20:29,740 --> 00:20:31,570
dan ada kasus dimana ada cara

604
00:20:31,790 --> 00:20:34,340
yang sangat efektif untuk mempelajari fungsi nonlinear kompleks.

605
00:20:35,150 --> 00:20:36,840
Dan, karena itu saya, bersama dengan

606
00:20:37,040 --> 00:20:38,930
regresi logistik, jaringan syaraf tiruan, SVM,

607
00:20:39,090 --> 00:20:40,630
menggunakan algoritma-algoritma tersebut

608
00:20:40,760 --> 00:20:42,170
untuk pembelajaran dengan cepat, saya pikir

609
00:20:42,440 --> 00:20:43,610
anda berada pada posisi yang sangat baik

610
00:20:44,120 --> 00:20:45,120
untuk dapat membangun sistem pembelajaran mesin

611
00:20:45,310 --> 00:20:46,710
state-of-the-art untuk suatu jangkauan

612
00:20:46,960 --> 00:20:49,110
aplikasi yang luas, dan ini adalah

613
00:20:49,330 --> 00:20:52,460
satu lagi perangkat yang sangat ampuh untuk dimiliki dalam 'persenjataan' anda;

614
00:20:53,160 --> 00:20:54,270
sesuatu yang dipergunakan

615
00:20:54,460 --> 00:20:55,850
di seluruh tempat di Silicon Valley,

616
00:20:56,390 --> 00:20:58,030
atau oleh industri dan pada

617
00:20:58,310 --> 00:20:59,860
lingkungan akademik, untuk membangun banyak

618
00:21:00,120 --> 00:21:01,680
sistem pembelajaran mesin berperforma tinggi.