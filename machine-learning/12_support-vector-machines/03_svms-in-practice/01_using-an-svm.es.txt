Hasta el momento hemos hablado de las SVM a un nivel abstracto. En este video me gustaría necesitas hacer para ejecutar o utilizar una SVM. El algoritmo de la máquina de soporte vectorial presenta un problema de optimización peculiar. Como comenté en el video anterior, no recomiendo que escribas tu propio software para resolver por ti mismo los parámetros teta. Hoy en día, muy pocos de nosotros, quizá ninguno, pensaría en escribir código nosotros mismos para invertir una matriz o sustraer la raíz cuadrada de un número, etc., simplemente utilizamos una función de biblioteca para hacerlo. De la misma manera, el software para resolver el problema de optimización de las SVM es muy complejo. Algunos investigadores han realizado investigaciones de optimización numérica esencial por muchos años para desarrollar excelentes bibliotecas de software y paquetes de software para llevar esto a cabo. Yo recomiendo ampliamente utilizar una de las bibliotecas de software altamente optimizado en vez de intentar implementar algo tú mismo. Existen bibliotecas de software excelentes. Las dos que utilizo más seguido son la SVM lineares pero hay muchas bibliotecas de software excelentes para hacer esto que puedes adaptar a muchos de los lenguajes de programación más importantes que quizá estés utilizando para codificar un algoritmo de aprendizaje. Aunque no deberías escribir tu propio software de optimización de SVM, hay algunas cosas sí de debes hacer. Primero, debes elegir el parámetro “C”. Hablamos de las propiedades de oscilación y varianza en videos anteriores. Segundo, necesitas elegir el kernel o la función de similaridad que deseas utilizar. Una elección pudiera ser si decidimos no utilizar ningún kernel. La idea de que no haya un kernel también se llama kernel lineal. Si alguien dice “utilizo una SVM con un kernel lineal” se refiere a que utilizan una SVM sin un kernel y a que era un versión de SVM que sólo utiliza teta transpuesta de “x”, ¿sí? Esto predice que teta 0 más teta 1, “x1” más el resto más teta “n”, “Xn” es mayor que o igual a 0. El término de kernel linear lo puedes utilizar como la versión de la SVM que resulta en un clasificador estándar lineal. Esta sería una elección razonable para algunos problemas y, como mencioné, hay muchas bibliotecas de software, como SVM linear, que fue mi ejemplo, con las que se puede entrenar una SVM sin un kernel. A esta SVM se le llama kernel lineal. Ahora ¿por qué queremos hacer esto? Si tienes un gran número de variables, si “n” es grande y “m”, el número de ejemplos de entrenamiento, es pequeño, tendrás un número enorme de variables si “x” es “Rn, Rn” + 1. Así que, tenemos un número enorme de variables con un conjunto de entrenamiento pequeño. Quizá sólo quieres ajustar una barrera de decisión lineal y no ajustar una función no lineal complicada porque quizá no tengas suficientes datos. Y si estás intentando ajustar una variable muy complicada en un espacio con una dimensión de variables alta, corres el riesgo de sobreajustar. Por el contrario, si la muestra de tu conjunto de entrenamiento es pequeña,  esta sería una condición razonable en la que puedes decidir no utilizar un kernel o algo equivalente. A esto se le llama kernel lineal. Una segunda elección para el kernel que puedes generar es el kernel Gaussiano, que utilizamos previamente. Si utilizas este kernel, la otra elección que debes tomar es fijar el parámetro «sigma» cuadrada. Cuando hablamos de las compensaciones entre oscilación y varianza hablamos de que cuando «sigma» cuadrada es alta, tiende a generar un clasificador de oscilación alto y varianza baja; por el contrario, si «sigma» cuadrada es pequeña, tendremos un clasificador de varianza alta y oscilación baja. Entonces ¿cuándo podemos elegir el kernel Gaussiano? En tus variables originales “x”, quiero decir “Rn”, “n” si es “n” es bajo y si “m” es alto, tendremos un conjunto de entrenamiento bidimensional como en el ejemplo que ilustré anteriormente. Así que “n” es igual a 2. Aquí tenemos un conjunto de entrenamiento muy grande, así que dibujé una gran cantidad de ejemplos de entrenamiento. Quizá después quieras utilizar un kernel para ajustar una barrera de decisión no lineal más compleja. Una buena opción sería utilizar el kernel Gaussiano. Al final de este video hablaré un poco más acerca de cuando elegimos un kernel lineal, como el Gaussiano, u otros. De manera concreta, si decides utilizar un kernel Gaussiano, aquí está lo que debes hacer: Dependiendo del paquete de software de máquina de soporte vectorial que utilizarás, te preguntará si quieres implementar una función de kernel o de similaridad. Si utilizarás una implementación octave o MATLAB de una SVM, quizá te pida que proveas una función para calcular una variable específica en el kernel. Esto será calcular “f” subíndice “i” para un valor particular de “i”, donde esta “f” es sólo un número real simple. Quizá debería denotar esto como “f(i)”. Lo que debes hacer es escribir una función de kernel que toma esta entrada; es decir, un ejemplo de entrenamiento o un ejemplo de prueba, lo que sea, en un vector “x” y tomar la entrada de alguno de los puntos de referencia. Aquí sólo escribí “x1” y “x2”, pero los puntos de referencia también son ejemplos de entrenamiento. Lo que necesitas hacer, ahora, es escribir el software que tome esa entrada, “x1” y “x2”, y calcule este tipo de función de similaridad entre ellos y arroje un número real. Lo que hacen algunos paquetes de máquina de soporte vectorial es esperar que proveas la función de kernel que toma entrada “x1” y “x2”, y arrojará un número real. También, de estos datos, generará automáticamente todas las variables; es decir, tomara “x” para mapearlo en “f1” y “f2” hasta “f(m)” utilizando la función que escribas. También generará todas las variables y entrenará a la máquina de soporte vectorial a partir de ello. Algunas veces tú necesitas proveer la función. Otras implementaciones de SVM también incluirán el kernel Gaussiano, y otros kernels adicionales, ya que probablemente el kernel Gaussiano es el kernel más común. Los kernels Gaussianos y no lineales son, por mucho, los dos kernels más populares. Añadiré una nota de implementación: Si tienes variables de escalas muy distintas, es importante modificar la escala antes de utilizar el kernel Gaussiano. Explicaré por qué: Imagina el cálculo de la norma entre “x” y “L”. Este término de aquí es como el numerador de acá arriba. Lo que hacemos con esto es calcular el vector “V” que es igual a “x” menos “L” y luego calcularemos la norma del vector “V”, que es la diferencia de “x-L”. La norma de “V” es igual a “V1” cuadrada más “V2” cuadrada más el resto, más “Vn” cuadrada, porque “X” es “Rn” o “Rn” más 1, pero ignoraré “x0” y fingiremos que “x” es “Rn”. El cuadrado en el lado izquierdo hace esta ecuación correcta. Entonces, esto es igual a esto de acá ¿sí? Esto, escrito de otra manera será “x1” menos “L1” cuadrado más “x2” menos “L2” cuadrada más el resto de los términos más “xn” menos “Ln” cuadrada. Ahora veremos qué pasa cuando las variables tienen un amplio rango de valores. Tomaremos el ejemplo de la predicción de la vivienda donde nuestros datos son acerca de casas. Si “x” está en el rango de miles de pies cuadrados para la primera variable “x1”, pero para la segunda variable “x2” se refiere al número de habitaciones y su valor variará de entre 1 y 5 habitaciones, entonces “x1” menos “L1” será enorme; resultaría como en mil pies cuadrados mientras que “x2” menos “L2” será mucho más pequeño. Si ese es el caso, entonces, estas distancias estarán determinadas esencialmente por el tamaño de las casas y el número de cuartos se ignoraría. Para evitar esto y para lograr que funcione nuestra máquina, debemos hacer una modificación de escalas. Esto nos asegura que la SVM le pondrá una atención comparable a todas las variables y no sólo al tamaño de las casas, como en este ejemplo, ignorando todas las otras variables. Cuando probamos una máquina de soporte vectorial, las probabilidades de que utilices los dos kernels más comunes, es decir, el kernel lineal, o sin kernel, y el kernel Gaussiano del que ya hablamos, son altísimas. Agregaré una advertencia: No todas las funciones de similaridad que te encuentres serán kernels válidos. El kernel Gaussiano, el kernel lineal y otros kernels que se utilizan, necesitan satisfacer una condición técnica llamada teorema de Mercer. La razón por la cual se necesita esto es porque un algoritmo o implementación de una máquina de soporte vectorial tiene muchos trucos de optimización numérica para despejar los parámetros teta de manera eficiente. En el diseño original de las SVM se tomó la decisión de restringir nuestra atención solamente a los kernels para satisfacer esta condición técnica llamada teorema de Mercer. Lo que hace esto es asegurarnos de que todos estos paquetes de SVM y de software de SVM puedan utilizar una gran clase de optimizaciones y puedan obtener el parámetro teta rápidamente. Lo que muchos terminan haciendo es utilizar ya sea el kernel lineal o el Gaussiano, pero hay otros kernels que también satisfacen el teorema de Mercer y que puedes encontrar en el trabajo de otros. Yo, personalmente, utilizo otros kernels muy rara vez, si acaso. Mencionaré algunos de los kernels con los que te puedes topar: Uno es el kernel de polinomio. Para este kernel, la similaridad entre “x” y “L” se define, entre muchas opciones, como “x” transpuesta de “L” cuadrada. Esta es una medida de lo similar que resultan “x” y “L”. Si “x” y “L” están muy cerca una de la otra, el producto interno tenderá a ser grande. Bueno, como ya sabes, este es un kernel un tanto inusual. No se utiliza seguido, pero puedes encontrar que algunas personas lo usan. Esta es otra versión de un kernel de polinomio. Otra se expresa “x” transpuesta de “L” al cubo. Todos estos son ejemplos de kernels de polinomios. “x” transpuesta de “L” más 1 al cubo. “x” transpuesta de “L” más un número diferente de 1, como 5, a la cuarta potencia. El kernel de polinomio tiene dos parámetros: Uno es el número que se debe añadir aquí. Puede ser 0. Este de aquí es, en realidad, más 0. El otro es el grado del polinomio. El grado o la potencia y estos números. La formulación más general del kernel de polinomio es “x” transpuesta de “L” más una constante elevada a un grado en X1. Estos dos son parámetros para el kernel de polinomio. Los kernels de polinomio casi siempre tienen un menor desempeño que el kernel Gaussiano y no es un kernel que se utilice mucho pero quizá en algún momento te lo topes. Se utiliza generalmente sólo para datos donde “x” y “L” son estrictamente no negativos. Esto nos asegura que los productos internos tampoco sean negativos. Esto captura la intuición de que “x” y “L” son muy similares uno al otro, por lo que el producto interno entre ellos será grande. Tienen otras propiedades, pero no se utilizan mucho. Después, dependiendo de lo que eses haciendo, hay otros kernels más esotéricos que te puedes encontrar. Está el kernel de secuencia que se utiliza a veces si tus datos de entrada son cadenas de variables u otras cadenas. Hay otros como el kernel chi-square, el kernel de intersección de histograma, etc. Hay otros kernels más raros que podemos utilizar para medir la similaridad entre objetos distintos. Por ejemplo, si intentas hacer algún tipo de problema de clasificación de texto, donde la entrada “x” es una cadena, entonces, quizá quieras encontrar la similaridad entre dos cadenas utilizando el kernel de secuencia, pero personalmente sé que el resultado será raro, si es que lo obtenemos, utilizando kernels extraños.
Creo que he utilizado el kernel chi-square sólo una vez en mi vida y el kernel de histogramas una o dos veces. Nunca he utilizado el kernel de secuencias. Pero, en caso de que te los encuentres en otras aplicaciones, puedes hacer una búsqueda rápida en Google o en Bing y encontrar las definiciones de estos kernels también.
Añadiré sólo dos detalles de los que quiero hablar en este video. Uno es la clasificación de clases múltiples. Digamos que tienes cuatro clases o, de manera más general, “k” clases ¿cómo tomas la decisión más adecuada entre tus clases múltiples? La mayoría, o muchos de los paquetes de SVM ya tienen una funcionalidad de clasificación de clases múltiples integrada. Entonces, si utilizas un paquete como este, sólo debes usar la funcionalidad y con eso debe ser suficiente. De otra manera, una manera de hacer esto es utilizar el método uno contra todos del que hablamos cuando desarrollamos la regresión logística. Entonces, intercambias kSVM si tienes clases "k" para distinguirlas del resto. Esto nos arrojará “k” vectores parámetro; es decir teta 1, que intenta distinguir la clase “y” igual a 1 de todas las otras clases. Después tenemos un segundo parámetro vector teta 2, que es lo que obtenemos cuando tenemos “y” igual a 2 como clase positiva, y todas las demás como negativas, etc., hasta el parámetro vector teta “k”, que es el parámetro vector para distinguir la clase final de todo lo de más. Por último, este es exactamente igual el método uno contra todo que vimos en la regresión logística con el que predijimos la clase (i) con la teta transpuesta de “x” más alta. Esta es la clasificación de clase múltiple. Pero el caso más común es que cualquier paquete de software que utilices tendrá ya integrado una funcionalidad de clasificación de clases múltiples. Si no la tiene, no te preocupes por este resultado. Finalmente, desarrollamos máquinas de soporte vectorial iniciando con la regresión logística y luego modificando un poco la función de costos. Lo último que quiero hacer en este video es hablar un poco acerca de cuando utilizas uno de estos dos algoritmos. Digamos que “n” es el número de variables y “m” es el número de ejemplos de entrenamiento. ¿Cuándo debemos utilizar un algoritmo en vez del otro? Aquí “n” es mayor en relación con el tamaño del conjunto de aprendizaje. Podemos pensar en “n” como un número de variables mayor que “m”. Esto puede presentarse cuando tenemos un problema de clasificación, donde la dimensión del vector de variables es, quizá, 10 mil. El tamaño del conjunto de aprendizaje es de 10 hasta hasta 1000. Imagina un problema de spam donde tenemos 10,000 variables de correos spam que corresponden a 10,000 palabras, pero sólo tienes 10 ejemplos de entrenamiento o hasta 1,000 ejemplos. Entonces, “n” es grande con relación a “m”. Lo que yo haría usualmente es utilizar la regresión logística o utilizarla como “m” sin un kernel o utilizar un kernel lineal, porque si tienes tantas variables con un conjunto de entrenamiento pequeño, una función lineal funcionará porque probablemente no tengas datos suficientes para ajustar una función no lineal complicada. Ahora, “n” es pequeña y “m” es intermedia; a lo que me refiero con esto es que “n” está entre 1 y 1000 (1 sería muy pequeño, pero podemos fijarlo en 1000 variables), y el número de ejemplos de entrenamiento está entre 10 y 10,000 ejemplos, o quizá hasta 50,000 ejemplos; es decir, “m” es muy grande (alrededor de 10,000), pero no tanto como un millón. Si “m” es de un tamaño intermedio como esta, entonces una SVM con un kernel lineal funcionarán bien. Ya hablamos de esto anteriormente con un ejemplo concreto en el que teníamos un conjunto de entrenamiento bidimensional. Entonces, si “n” es igual a 2 y has dibujado un gran número de ejemplos de entrenamiento, el kernel Gaussiano hará un buen trabajo separando las clases negativas y las positivas. Una tercera opción de interés es si “n” es pequeño pero “m” es grande. Si “n” es, de nuevo, entre 1 a 1000, o mayor pero “m” es de entre 50,000 o mayor, hasta millones (50,000 a 100,000 o un millón o un billón). Puedes tener un conjunto de entrenamiento muy muy grande ¿cierto? Si este es el caso, una SVM con kernel Gaussiano será lenta. Los paquetes de SVM actuales, cuando usamos el kernel Gaussiano, tienen algunas dificultades. Si tienes unos 50,000 ejemplos está bien, pero si tienes un millón de ejemplos de entrenamiento, o incluso 100,000 con un valor masivo de “m”. Los paquetes de SVM actuales son muy buenos, pero todavía pueden encontrar dificultades cuando tienes un conjunto de entrenamiento masivo y utilizas el kernel Gaussiano. En este caso, lo que haría es tratar de crear manualmente más variables y luego utilizar la regresión logística o una SVM sin el kernel. Si ves esta diapositiva encontrarás subrayado “regresión logística o SVM sin un kernel” en dos ocasiones. Los puse juntos por una razón en específico. La regresión logística y la SVM sin el kernel son algoritmos muy similares porque, ya sea la regresión logística o la SVM sin un kernel, harán cosas similares y arrojarán un desempeño similar. Dependiendo de tus detalles de implementación, una pudiera resultar más eficiente que la otra. Cuando uno de estos algoritmos funciona, ya sea la regresión logística o la SVM sin kernel, lo más seguro es que el otro también funcione bien. Buena parte de la eficacia de la SVM surge cuando utilizas kernels diferentes para aprender funciones complejas no lineales. El régimen de cuando tienes hasta 10,000 ejemplos o hasta 50,000 ejemplos, y el número de variables es razonablemente grande, es un régimen muy común en el que una máquina de soporte vectorial con kernel fallaría y haría cosas mucho más difíciles que la regresión logística. Finalmente, ¿dónde encajan estas redes neuronales? Para todos estos problemas o para todos estos regímenes diferentes, una red neuronal bien diseñada funcionará bien también. La única desventaja o la única razón por la que a veces evitamos las redes neuronales es que, para algunos problemas, la red neuronal puede ser de entrenamiento lento. Pero con un paquete de implementación de SVM muy bueno que pueda ejecutarse más rápidamente que una red neuronal podrás resolver esto. Aunque no mostramos esto anteriormente, resulta que el problema de optimización que presenta la SVM es un problema de optimización convexo y, por lo tanto, los mejores paquetes de software de optimización de SVM siempre encontrarán el mínimo global o un valor cercano. Al utilizar las SVM no necesitas preocuparte por las óptimas locales. En la práctica, las óptimas locales o son un gran problema para las redes neuronales porque todas resuelven. Esta es una preocupación menos cuando utilizas una SVM. Dependiendo de tu problema, la red neuronal podrá ser más lenta, especialmente en este tipo de regímenes que con la SVM. En caso de que los lineamientos que expliqué aquí parezcan vagos, y si ves tus problemas y los lineamientos no parecen claros o no te queda claro qué algoritmo debes utilizar, está bien. Cuando me enfrento con un problema de aprendizaje automático, a veces no me queda claro cuál es el mejor algoritmo pero, como viste en los videos anteriores, el algoritmo es de importancia, aunque a veces importan más otras cosas como la cantidad de datos que tenemos y qué tan hábil seas realizando análisis de errores, limpiando algoritmos de aprendizaje y entendiendo cómo diseñar nuevas variables y qué otras variables debes darle a tu algoritmo de aprendizaje, etc. Algunas veces estas cosas importarán más que tu decisión de utilizar la regresión logística o la SVM. Una vez dicho esto, la SVM sigue siendo percibida como uno de los algoritmos de aprendizaje más eficaces y existe un régimen que provee una manera efectiva de aprender funciones complejas no lineales. Así que en realidad, creo que utilizando la regresión logística, las redes neuronales y las SVMs estás bien colocado para construir sistemas de aprendizaje automático de última generación para una gama amplia de aplicaciones. Esta es excelente arma en tu arsenal. Un arma que se utiliza por todos lados en Silicon Valley, en la industria y en la academia para construir sistemas de aprendizaje automático de alto rendimiento.