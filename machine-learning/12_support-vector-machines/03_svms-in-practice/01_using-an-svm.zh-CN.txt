目前为止 我们在比较抽象的层面上 讨论了支持向量机 SVM 在这段视频中 为了运行 或者说使用 SVM 你实际上需要做什么 支持向量机算法 是一个特定的优化问题 但是就如我在之前的 视频中简单提到的 我真的不建议你自己写 软件来求解参数θ 就像如今只有很少的人 或者说根本没有人 会考虑自己写代码 来实现对矩阵求逆 或求一个数的平方根等 我们只要调用库函数来实现这些 同样地 用以解决 SVM 优化问题 的软件很复杂 而且已经有 专门研究数值优化很多年 的学者在做这个 因此你需要好的 软件库和好的软件包 来做这个 然后强烈建议使用 一个高度优化的软件库 而不是尝试自己去实现它 有许多好的软件库 我最常用的两个是 liblinear 和 libsvm 但是真的有很多软件库 可以用来 实现这个 你可以在很多主流编程语言 可能是你用来写学习算法的语言 找到这个软件库 尽管你不应该去写 你自己的 SVM 优化软件 但是你也需要做几件事儿 首先是 要选择参数C 我们在之前的视频中 讨论误差/方差的性质时 提到过这个 第二 你也需要 选择核函数或 你想要使用的相似度函数 其中一个选择是 我们选择不用任何核函数 不用核函数这个作法 也叫线性核函数 因此 如果有人说 他的 SVM 用了线性核函数 这就意味着 他在使用 SVM 时 没有用核函数 这种用法的 SVM 只使用了 θ 转置乘以x 当 θ0 + θ1x1 + ... + θnxn 大于等于0时 当 θ0 + θ1x1 + ... + θnxn 大于等于0时 当 θ0 + θ1x1 + ... + θnxn 大于等于0时 预测 y=1 对线性核函数这个术语 你可以把它理解为 这个版本的 SVM 它只是给你一个标准的线性分类器 因此对某些问题来说 它是一个合理的选择 而且你知道 有许多软件库 比如 liblinear 就是众多软件库 中的一个例子 它们可以用来训练的 SVM 是没有核函数的 也叫线性核函数 那么你为什么想要做这样一件事儿呢？ 如果你有大量的特征变量 如果 n 很大 而训练集的样本数 m 很小 那么 你知道 你有大量的特征变量 x 是一个 n+1 维向量 x 是一个 n+1 维向量 那么如果你已经有 大量的特征值 和很小的训练数据集 也许你应该拟合 一个线性的判定边界 不要拟合非常复杂的非线性函数 因为没有足够的数据 如果你想在一个高维特征空间 试着拟合非常复杂的函数 而你的训练集又很小的话 你可能会过度拟合 因此 这应该是 你可能决定不适用核函数 或者等价地说使用线性核函数 或者等价地说使用线性核函数 的一个合理情况 对于核函数的第二个选择是 这个高斯核函数 这个是我们之前见过的 如果你选择这个 那么你要做的另外一个选择是 选择一个参数σ的平方 我们之前讨论如何权衡偏差方差的时候谈到过 如果 σ 的平方很大 那么你就有可能 得到一个较大的误差 较低方差的分类器 但是如果 σ 的平方很小 那么你就会有较大的方差 较低误差的分类器 那么什么时候选择高斯核函数呢？ 如果你原来的特征变量 x 是 n 维的 如果 n 很小 并且 理想情况下 如果 m 很大 那么如果我们有 一个二维的训练集 就像我前面讲到的例子一样 那么n等于2 但是我们有相当大的训练集 我已经画出了 大量的训练样本 那么可能你需要用 一个核函数去拟合一个 更复杂的非线性判定边界 那么高斯核函数会是不错的选择 我会在这个视频的后面部分 讲到更多一些关于 什么时候你可以选择 线性核函数 高斯核函数等 但是具体地说 如果你决定使用高斯核函数 那么下面是你需要做的 根据你所用的 支持向量机软件包 它可能需要你 实现一个核函数 或者实现相似度函数 因此 如果你用 Octave 或者 Matlab 来实现 支持向量机的话 它会要求你提供一个函数 来计算核函数的特定特征 因此这是对一个特定的 i 因此这是对一个特定的 i 计算 fi 这里的 f 只是 一个单一的实数 也许我应该把它写成 fi 但是你需要做的是 写一个核函数 把这个作为输入 一个训练样本 或者一个测试样本 不论是哪个 作为输入的是向量 x 然后把标识点 也作为一个输入 在这里我只写了 x1 和 x2 因为标识点也是训练样本 但是你需要做的是 写一个这样的软件 它把 x1 x2 作为输入 然后计算它们之间的 这种相似度函数 之后返回一个实数 因此一些支持向量机的 包所做的是期望 你能提供一个核函数 能够输入 x1 x2 并返回一个实数 从这里开始 它将自动地生成所有特征变量 它自动地 用你写的这个函数 将 x 映射到对应的 f1 f2 一直到 fm 生成所有的特征值 并从这儿开始训练支持向量机 但是有些时候你却一定要 自己提供这个函数 如果你使用高斯核函数 一些SVM的实现也会包括高斯核函数 和一些其他的核函数 因为高斯核函数可能是最常见的核函数 目前看来 高斯核函数和线性核函数确实是 最普遍的核函数 一个实现过程中的注意事项 如果你有大小很不一样 的特征变量 在使用高斯核函数之前 对它们进行归一化是很重要的 这里有一个 如果假设你在计算 x 和 l 之间的范数 就是这样一个式子 是这里的分子项 这个式子所算的是 x 和 l 之间的范数 就等于说 计算一个向量 v 这个向量 v=x-l 然后计算向量 v 的范数 这也就是 x 和 l 之间的差 这也就是 x 和 l 之间的差 v 的范数等于 v1 的平方 加 v2 的平方加 点点点 加 vn 的平方 因为这里的 x 是 n 维向量 或者说是 n+1 维的 但是我要忽略 x0 因此我们假设 x 是 n 维向量 在左边加上平方就是正确的了 因此这个式子就等于 那个式子 对吧？ 那么另一种不同的写法就是 (x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2 (x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2 (x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2 (x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2 (x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2 现在如果你的特征变量 取值范围很不一样 就拿房价预测来举例 如果你的数据 是一些关于房子的数据 如果特征向量 x 的 第一个变量 x1 的取值 在上千平方英尺 的范围内 但是如果你的第二个特征变量 x2 是卧室的数量 且如果它在 一到五个卧室范围内 那么 x1-l1 将会很大 这有可能上千数值的平方 然而 x2-l2 将会变得很小 在这样的情况下的话 那么在这个式子中 这些间距将几乎 都是由 房子的大小来决定的 从而忽略了卧室的数量 为了避免这种情况 让向量机得以很好地工作 确实需要对特征变量进行归一化 这将会保证SVM 能够同等地关注到 所有不同的特征变量 而不是像例子中那样 只关注到房子的大小 而忽略了其他的特征变量 当你尝试支持向量机时 目前为止你可做的选择是 这两个可能是你 最常用的核函数 线性核函数 也就是不用核函数 或者我们讨论的高斯核函数 这里有一个警告 不是所有你可能提出来 的相似度函数 都是有效的核函数 高斯核函数 线性核函数 以及其他人有时会用到的 另外的核函数 它们全部需要满足一个技术条件 它叫作默塞尔定理 (Mercer's Theorem) 需要满足这个条件的原因是 因为支持向量机算法 或者 SVM 的实现 有许多巧妙的 数值优化技巧 为了有效地求解 参数 θ 在最初的设想里 有一个这样的决定 将我们的注意力仅仅限制在 可以满足默塞尔定理的核函数上 这个定理所做的是 确保所有的SVM包 所有的SVM软件包 能够使用 大量的优化方法 并且快速地得到参数 θ 大多数人最后做的是 要么用线性核函数 要么用高斯核函数 但是还有一些其他核函数 满足默塞尔定理 你可能会遇到其他人使用这些核函数 然而我个人 最后是很少很少使用其他核函数 只是简单提及一下你可能会遇到的其他核函数 一个是多项式核函数 它将 x 和 l 之间的相似度 它将 x 和 l 之间的相似度 定义为 这里有很多种选择 你可以用 x 的转置乘以 I 的平方 那么这就是一个 x 和 l 相似度的估量 如果 x 和 l 相互之间很接近 那么这个内积就会很大 这是一个有些 不寻常的核函数 它并不那么常用 但是你可能会见到有人使用它 这是多项式核函数的一个变体 另一个是 x 转置乘以 I 的立方 这些都是多项式核函数的例子 x 转置乘以 l 加 1 的立方 x 转置乘以 l 加上 一个不是 1 的数 比如 5 的4次方 多项式核函数实际上有两个参数 一个是你在这里要加的数 可能是0 这里就是加的0 同样地 另一个参数是多项式的次数 参数就是多项式的次数和这些数字 多项式核函数的更一般形式是 多项式核函数的更一般形式是 x 转置乘以 l 加上一个常数 x 转置乘以 l 加上一个常数 的某个指数次方 因此这两个 都是多项式核函数的参数 所以多项式核函数几乎总是 或者通常执行的效果 比高斯核函数差一些 所以用得没有那么多 但是你有可能会碰到 通常它只用在 当 x 和 l 都是严格的非负数时 这样以保证这些 内积值永远不会是负数 这扑捉到了这样一个直观感觉 如果 x 和 l 之间非常相似 也许它们之间的内积会很大 它们也有其他的一些性质 但是人们通常用得不多 那么 根据你所做的 你也有可能会碰到 其它一些更加难懂的核函数 比如字符串核函数 如果你的输入数据是文本字符串 或者其他类型的字符串 有时会用到这个核函数 还有一些函数 如卡方核函数 直方图交叉核函数 等等 还有一些难懂的核函数 你可以用它们来估量 不同对象之间的相似性 例如 如果你在尝试 做一些文本分类的问题 在这个问题中 输入变量 x 是一个字符串 我们想要通过 字符串核函数来找到 两个字符串间的相似度但是我 但是我个人很少用 这些更加难懂的核函数 我想我平生可能 用过一次卡方核函数 可能用过一次或者两次 直方图交叉核函数 我实际上没用过字符串核函数 只是以防万一你在其他应用中碰到它们 如果你在网上查一下的话 用 Google 搜索 或者用 Bing 搜索 你会发现这些也是核函数的定义 我想要在这个视频里讨论最后两个细节 一个是在多类分类中 你有4个类别 或者更一般地说是 K 个类别 怎样让 SVM 输出各个类别间合适的判定边界？ 大部分 SVM 许多 SVM 包已经内置了 多类分类的函数了 因此如果你用的是那种软件包 你可以直接用内置函数 你可以直接用内置函数 应该可以工作得很好 不然的话 另一个方式是 一对多 (one-vs.-all) 方法 这个我们在 讲解逻辑回归的时候讨论过 所以你要做的是 要训练 K 个 SVM 如果你有 K 个类别的话 每一个 SVM 把一个类同其他类区分开 这会给你 K 个参数向量 它们是 θ(1) 它把 y=1  这类 θ(1) 它把 y=1  这类 和所有其他类别区分开 和所有其他类别区分开 然后得到第二个参数向量 θ(2) 然后得到第二个参数向量 θ(2) 它是在 y=2 为正类 它是在 y=2 为正类 其他类为负类时得到的 以此类推 一直到参数向量θ(K) 是用于 区分最后一个类别 类别 K 和其他类别的参数向量 最后 这就与 我们在逻辑回归中用到的 一对多方法一样 在逻辑回归中我们只是 取使得 θ(i) 转置乘以 x 最大的类 i 以上是多类分类方法 对于更为常见的情况 很有可能的是 不论你使用什么软件包 都很有可能 已经内置了 多类分类的函数功能 因此你不必担心这个 最后 我们从逻辑回归开始 修改了一下代价函数 从而得到了支持向量机 最后我想要在这个视频中讨论一点的是 对这两个算法 你什么时候应该用哪个呢？ 假设 n 是特征变量的个数 m 是训练样本数 那么我们什么时候用哪一个呢？ 如果 n 相对于你的训练集大小来说较大时 如果 n 相对于你的训练集大小来说较大时 比如 如果特征变量的数量 如果特征变量的数量 远大于 m 这可以是 比如说 如果你有一个文本分类的问题 特征向量的维数 我不知道 有可能是1万 且如果你的训练集大小 可能是 10 可能最多 1000 想象一下垃圾邮件 的分类问题 在这个问题中 你有1万个特征变量 对应于1万个单词 但是你可能有 10 个训练样本 可能最多 1000 个样本 如果 n 相对 m 来说 比较大的话 我通常会使用逻辑回归 或者使用 没有核函数的 SVM 或者叫线性核函数 因为 如果你有许多特征变量 而有相对较小的训练集 一个线性函数就可能工作得不错 而且你也没有 足够的数据 来拟合非常复杂的非线性函数 现在如果 n 较小 而 m 是中等大小 我的意思是 n 可以取 1 - 1000之间的任何数 1是很小的 也许也会到1000个特征 如果训练样本的数量 如果训练样本的数量 可能是从 10 也许是到10,000个样本之间的任何一个值 也许多达5万个样本 所以 m 挺大的 可能是1万 但不是一百万 因此如果 m 大小适中的话 那么通常线性核函数的SVM会工作得很好 那么通常高斯核函数的SVM会工作得很好 这个我们在这之前也讨论过 举一个具体的例子 如果你有一个 二维的训练集 所以 n=2 画上很多训练样本 高斯核函数可以 很好地把正类和负类区分开来 第三种值得关注的情况是 第三种值得关注的情况是 如果 n 很小 但是 m 很大 如果 n 还是 1到1000之间的数 可能会更大一点 但是如果 m 是 5万 或者更大 大到上百万 5万 10万 一百万 二百万 你有很大很大的训练集 如果是这样的情况 那么高斯核函数的支持向量机 运行起来就会很慢 如今的 SVM 包 如果使用高斯核函数的话 会很慢 如果你有5万 那还可以 但是如果你有 一百万个训练样本 或者是十万个 m 的值很大 如今的 SVM 包很好 但是如果你对一个 很大很大的训练集 使用高斯核函数的话 它们还是会有些慢 在这种情况下 我经常会做的是 尝试手动地创建 更多的特征变量 然后使用逻辑回归 或者不带核函数的 SVM 你看这张幻灯片 你看到了逻辑回归 或者不带核函数的 SVM 在这个两个地方都出现了 我把它们放在一起 是有原因的 逻辑回归和不带核函数的 SVM 它们都是非常 相似的算法 不管是逻辑回归 还是不带核函数的 SVM 它们会做相似的事情 并且表现也相似 但是根据你实现的具体情况 其中一个可能会比另一个更加有效 但是如果其中一个算法适用的话 但是如果其中一个算法适用的话 逻辑回归或不带核函数的 SVM 那么另一个算法 也很有可能工作得很好 但是 SVM 的威力 随着你用不同的核函数学习 复杂的非线性函数 而发挥出来 在这个区间 你有多达1万 或者多达5万的样本 而特征变量的数量 这是相当大的 那是一个非常常见的区间 也许在这个区间下 高斯核函数的支持向量机会表现得相当突出 你可以做对逻辑回归来说 会困难得多的事情 最后 神经网络应该在什么时候使用呢？ 对于所有的这些问题 对于所有这些区间 对于所有这些区间 一个设计得很好的神经网络也很可能会非常有效 它的一个缺点是 或者说有时可能不会使用 神经网络的原因是 对于许多这样的问题 神经网络训练起来可能会很慢 但是如果你有一个非常好的 SVM实现包 它会运行得比较快 比神经网络快很多 尽管我们在此之前没有证明过 实际上 SVM 的优化问题 实际上 SVM 的优化问题 是一种凸优化问题 因此好的 SVM 优化软件包 总是会找到 全局最小值 或者接近它的值 对于SVM 你不需要担心局部最优 在实际应用中 局部最优 对神经网络来说不是非常大 但是也不小 所以你在使用 SVM 的时候可以少担心一个问题 根据你的问题 神经网络可能会比 SVM 慢 尤其是在这个区间内 如果你觉得这里给出的参考 看上去有些模糊 如果你在考虑一些问题 觉得这些参考有一些模糊 我仍然不能完全确定 我是该用这个算法 还是该用那个算法 这个其实没关系 当我遇到机器学习问题时 有时确实不清楚 是不是最好用那个算法 是不是最好用那个算法 但是你在之前的视频中看到的 算法确实很重要 但是通常更重要的是 你有多少数据 你有多熟练 是否擅长做误差分析 和调试学习算法 想出如何设计新的特征变量 想出如何设计新的特征变量 以及找出应该输入给学习算法的其它特征变量等方面 通常这些方面会比 你使用逻辑回归 还是 SVM 这方面更加重要 但是 已经说过了 SVM 仍然被广泛认为是 最强大的学习算法之一 最强大的学习算法之一 而且 SVM 在一个区间内 是一个非常有效地学习复杂非线性函数的方法 因此 我实际上 逻辑回归 神经网络 SVM 加在一起 有了这三个学习算法 有了这三个学习算法 我想你已经具备了 在广泛的应用里 构建最前沿的 机器学习系统的能力 它是你的武器库中的另一个非常强大的工具 它被广泛地应用在很多地方 比如在硅谷 在工业界 在学术等领域来建立许多 高性能的机器学习系统 【教育无边界字幕组】翻译：星星之火 校对：竹二个 审核：所罗门捷列夫