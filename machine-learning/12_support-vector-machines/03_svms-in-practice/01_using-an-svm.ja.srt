1
00:00:00,140 --> 00:00:01,310
ここまでの所、我らはSVMについて

2
00:00:01,640 --> 00:00:03,290
かなり抽象的なレベルで議論してきた。

3
00:00:03,980 --> 00:00:05,030
このビデオでは、

4
00:00:05,200 --> 00:00:06,460
SVMを実際に走らせるにあたり、あるいは使うにあたり

5
00:00:06,740 --> 00:00:09,410
実際にやらなくてはいけない事について議論したい。

6
00:00:11,320 --> 00:00:12,300
サポートベクタマシンアルゴリズムは

7
00:00:12,850 --> 00:00:14,870
特定の最適化問題を導く。

8
00:00:15,530 --> 00:00:16,940
だが以前のビデオで

9
00:00:17,120 --> 00:00:18,150
簡単に述べた通り、

10
00:00:18,380 --> 00:00:20,570
パラメータシータを解くソフトウェアを

11
00:00:20,630 --> 00:00:22,810
自力で書く事は、全く推奨しない。

12
00:00:23,950 --> 00:00:26,110
こんにちでは、

13
00:00:26,420 --> 00:00:27,730
我々の中には、

14
00:00:28,090 --> 00:00:29,400
自分で行列の逆行列を求めるコードや

15
00:00:29,530 --> 00:00:31,680
数のルートを計算するコードを書こう、とする人は

16
00:00:31,950 --> 00:00:33,940
かなり少数派、いやほとんど居ないと言っても良かろう。

17
00:00:34,190 --> 00:00:36,570
我らは、これを行うライブラリ関数を呼ぶ。

18
00:00:36,700 --> 00:00:38,090
同様に、

19
00:00:38,850 --> 00:00:40,310
SVMの最適化問題を

20
00:00:40,620 --> 00:00:42,200
解くソフトウェアは、

21
00:00:42,440 --> 00:00:43,880
とても複雑だ。

22
00:00:43,990 --> 00:00:44,960
そして、本質的には数値計算の最適化だけを何年もやってるような

23
00:00:45,110 --> 00:00:47,560
研究者達、というのも存在している。

24
00:00:47,850 --> 00:00:48,960
だから、あなたはこれを行ってくれる

25
00:00:49,150 --> 00:00:50,550
素晴らしいソフトウェアライブラリや

26
00:00:50,930 --> 00:00:52,270
ソフトウェアパッケージを見つける事が出来る。

27
00:00:52,470 --> 00:00:53,480
であるから、自分で実装しよう、なんて考えないで

28
00:00:53,860 --> 00:00:55,260
それらの高度に最適化されたソフトウェアライブラリのどれかを使う事を

29
00:00:55,710 --> 00:00:57,780
強く推奨する。

30
00:00:58,730 --> 00:01:00,680
良いソフトウェアライブラリは、たくさんあるから。

31
00:01:00,970 --> 00:01:02,060
私がもっとも良く使う物を

32
00:01:02,210 --> 00:01:03,220
二つ上げておくと、

33
00:01:03,400 --> 00:01:05,000
liblinearとlibsvmだが、

34
00:01:05,410 --> 00:01:06,860
これを行うソフトウェアライブラリは

35
00:01:07,030 --> 00:01:08,430
これ以外にもたくさんあって、

36
00:01:08,600 --> 00:01:10,190
あなたが学習アルゴリズムを実装している言語に

37
00:01:10,450 --> 00:01:11,860
リンク出来る物も

38
00:01:11,950 --> 00:01:14,410
たくさんあるはずだ。

39
00:01:15,280 --> 00:01:16,460
あなたはSVMの最適化ソフトウェアを

40
00:01:16,730 --> 00:01:18,330
独自実装すべきでは無いのだけれども、

41
00:01:19,120 --> 00:01:20,680
あたながやるべき事も、ちょっとは存在する。

42
00:01:21,420 --> 00:01:23,130
まず最初に、パラメータCを

43
00:01:23,130 --> 00:01:24,230
選ばなくてはいけない。

44
00:01:24,320 --> 00:01:25,640
この事については

45
00:01:25,940 --> 00:01:26,930
前回のビデオの中で、

46
00:01:27,040 --> 00:01:28,850
バイアス/バリアンス の性質の話の時に言及した。

47
00:01:30,290 --> 00:01:31,480
二番目に、あなたが使うカーネル、あるいは

48
00:01:31,630 --> 00:01:33,040
類似度関数を

49
00:01:33,410 --> 00:01:34,880
選ぶ必要がある。

50
00:01:35,730 --> 00:01:37,080
選択の一例としては、

51
00:01:37,280 --> 00:01:38,980
カーネルを使わない、という決断もあり得る。

52
00:01:40,560 --> 00:01:41,510
そしてカーネルを使わない、というアイデアは

53
00:01:41,910 --> 00:01:43,600
線形（リニア）カーネルとも呼ばれる。

54
00:01:44,130 --> 00:01:45,320
だから、誰かが

55
00:01:45,530 --> 00:01:46,760
「俺はSVMをリニアカーネルで使うぜ」と言ったら、

56
00:01:47,180 --> 00:01:48,330
それの意味する事は、

57
00:01:48,490 --> 00:01:50,690
SVMを、カーネル無しで

58
00:01:51,020 --> 00:01:52,250
使う、という事。

59
00:01:52,360 --> 00:01:53,410
そしてそれは、SVMを

60
00:01:54,120 --> 00:01:55,870
単に シータ転置 x で使うというSVMのバージョンという事になる。

61
00:01:56,140 --> 00:01:57,620
それは以下の時に1を予測する、

62
00:01:57,850 --> 00:01:59,420
シータ0+シータ1 x1

63
00:01:59,740 --> 00:02:01,000
足す、、、、っと足すことの シータn xn が、

64
00:02:01,690 --> 00:02:04,160
0以上の時。

65
00:02:05,520 --> 00:02:06,830
この「線形」カーネルという言葉は、

66
00:02:06,950 --> 00:02:08,250
これは標準的な「線形」の分類器を

67
00:02:08,480 --> 00:02:09,290
与えるSVMのバージョンだ、

68
00:02:10,340 --> 00:02:12,320
と考える事が出来る。

69
00:02:13,940 --> 00:02:14,700
さて、これはある種の問題には

70
00:02:15,040 --> 00:02:16,160
リーズナブルな選択となる。

71
00:02:17,130 --> 00:02:18,080
そして世の中には

72
00:02:18,470 --> 00:02:20,900
たくさんのソフトウェアのライブラリ、

73
00:02:21,210 --> 00:02:22,320
たくさんあるそんなソフトウェアライブラリの中の

74
00:02:22,840 --> 00:02:23,880
一つの例としては、liblinearのような物とかが挙げられるが、

75
00:02:24,560 --> 00:02:25,620
とにかく、SVMをカーネル無しで、

76
00:02:25,980 --> 00:02:27,410
またの名を線形カーネルでトレーニング出来る

77
00:02:27,760 --> 00:02:29,470
ソフトウェアライブラリはたくさんある。

78
00:02:29,850 --> 00:02:31,340
では、何故これを使いたい、と思うのだろうか？

79
00:02:31,410 --> 00:02:32,820
もしあなたが、大量のフィーチャーを持っていて、

80
00:02:33,150 --> 00:02:34,280
つまりnが大きくて、

81
00:02:34,430 --> 00:02:37,800
そしてm、

82
00:02:37,990 --> 00:02:39,590
トレーニング手本の数が、小さければ、

83
00:02:39,670 --> 00:02:41,050
その時はつまり、

84
00:02:41,230 --> 00:02:42,300
あなたは大量のフィーチャーを

85
00:02:42,360 --> 00:02:43,630
持っているという事で、

86
00:02:43,710 --> 00:02:45,850
xはRnとかRn+1という事だ。

87
00:02:46,010 --> 00:02:46,940
だから既にたくさんの

88
00:02:47,080 --> 00:02:48,700
フィーチャーがあって、

89
00:02:48,800 --> 00:02:50,540
トレーニングセットのサイズは小さい、

90
00:02:50,610 --> 00:02:51,430
この場合は単に線形の

91
00:02:51,710 --> 00:02:52,890
決定境界へのフィッティングを望むかもしれない。

92
00:02:53,060 --> 00:02:54,420
そしてあまり複雑な非線型の関数にフィッティングしたいとは思わないだろう。

93
00:02:54,860 --> 00:02:56,980
何故なら、十分なデータが無いと、

94
00:02:57,560 --> 00:02:59,330
オーバーフィッティングのリスクがあるからだ。

95
00:02:59,470 --> 00:03:00,530
とても次数の高いフィーチャー空間上の

96
00:03:01,540 --> 00:03:03,220
とても複雑な関数にフィッティングしようとしていて、

97
00:03:03,980 --> 00:03:04,990
しかもトレーニングセットのサイズが小さい時には。

98
00:03:05,040 --> 00:03:07,120
だからこそ、この状況は

99
00:03:07,340 --> 00:03:08,600
カーネルを使わない、

100
00:03:08,740 --> 00:03:09,950
または同じ事だが線形カーネルと呼ばれる物を使う、という

101
00:03:10,700 --> 00:03:11,960
選択をするのが合理的となる

102
00:03:12,250 --> 00:03:15,580
状況だ。

103
00:03:15,740 --> 00:03:16,740
あなたが行いそうな二番目の選択肢は

104
00:03:16,820 --> 00:03:18,010
このガウスカーネルだ。

105
00:03:18,370 --> 00:03:19,920
これは以前に得た物だ。

106
00:03:21,270 --> 00:03:22,350
そしてもしあなたがこれを選ぶなら、

107
00:03:22,440 --> 00:03:23,130
あなたが行わなくてはいけないさらなる選択としては、

108
00:03:23,420 --> 00:03:25,980
パラメータであるシグマ二乗を選ぶ事だ。

109
00:03:26,850 --> 00:03:29,800
これはバイアス-バリアンスのトレードオフの話でちょっと触れたが、

110
00:03:30,820 --> 00:03:32,360
シグマ二乗が大きければ、

111
00:03:32,600 --> 00:03:33,890
高バイアス、低バリアンスの傾向の

112
00:03:34,160 --> 00:03:35,580
分類器となり、

113
00:03:35,770 --> 00:03:37,650
逆にシグマ二乗が

114
00:03:37,800 --> 00:03:39,700
小さければ、

115
00:03:40,060 --> 00:03:42,360
高バリアンス、低バイアスの傾向の分類器となる。

116
00:03:43,940 --> 00:03:45,350
では、どんな時にガウスカーネルを使う事になるだろう？

117
00:03:46,210 --> 00:03:48,050
それはだね。あなたの元々の

118
00:03:48,310 --> 00:03:49,540
フィーチャーxが、

119
00:03:49,820 --> 00:03:51,370
Rnとして、

120
00:03:51,570 --> 00:03:53,890
nが小さい時、そして理想的には、

121
00:03:55,660 --> 00:03:57,110
mが大きい時。

122
00:03:58,470 --> 00:04:00,170
つまり、例えば二次元の

123
00:04:00,550 --> 00:04:02,340
トレーニングセットだとして、

124
00:04:03,130 --> 00:04:04,880
これは以前に書いた物と同様という事だが、

125
00:04:05,470 --> 00:04:08,320
つまりn=2だとして、だがトレーニングセットの総数は多いという場合。

126
00:04:08,680 --> 00:04:09,770
つまり、かなりたくさんの

127
00:04:09,950 --> 00:04:10,890
トレーニング手本を描いた。

128
00:04:11,650 --> 00:04:12,410
するとその場合、

129
00:04:12,540 --> 00:04:14,400
もっと複雑な非線形の決定境界にフィットするような

130
00:04:14,910 --> 00:04:16,260
カーネルを使いたくなる場合がある。

131
00:04:16,650 --> 00:04:18,750
そしてガウスカーネルはこれを行う良い方法だ。

132
00:04:19,480 --> 00:04:20,610
このビデオの最後の方で

133
00:04:20,720 --> 00:04:22,570
線形カーネルを使う場合はどういう時か

134
00:04:22,660 --> 00:04:23,760
ガウスカーネルを選ぶのはどういう場合か、

135
00:04:23,970 --> 00:04:26,310
などについて、話す機会がある。

136
00:04:27,860 --> 00:04:29,740
だが、もし具体的に

137
00:04:30,040 --> 00:04:31,210
ガウスカーネルを使う、と決めたとすると、

138
00:04:31,720 --> 00:04:33,910
あなたがやるべき事はこれだ。

139
00:04:35,380 --> 00:04:36,550
あなたの使うサポートベクタマシンの

140
00:04:37,280 --> 00:04:38,990
ソフトウェアパッケージによっては、

141
00:04:39,100 --> 00:04:40,960
カーネル関数を、あるいは類似度関数を

142
00:04:41,070 --> 00:04:42,200
あなたが実装する必要が

143
00:04:43,060 --> 00:04:43,880
あるものもある。

144
00:04:45,020 --> 00:04:46,750
もしあなたがoctaveやMATLABの

145
00:04:47,010 --> 00:04:49,820
SVMの実装を用いるなら、

146
00:04:50,000 --> 00:04:50,720
カーネルの具体的なフィーチャーを

147
00:04:50,810 --> 00:04:52,560
計算する関数を

148
00:04:52,690 --> 00:04:54,680
あなたが提供する必要があるかもしれない。

149
00:04:55,110 --> 00:04:56,480
これは本当は、

150
00:04:56,770 --> 00:04:57,890
fの下付き添字iを

151
00:04:58,220 --> 00:04:59,560
ある特定の値に対して計算する、

152
00:05:00,570 --> 00:05:02,310
ここでこのfは

153
00:05:02,330 --> 00:05:03,570
単一の実数値にすぎない、

154
00:05:03,840 --> 00:05:05,060
だからここでは、f(i)と書くべきかもしれない。

155
00:05:05,250 --> 00:05:07,230
だがあなたがしなくてはいけない事は、

156
00:05:07,510 --> 00:05:08,130
これ、トレーニング手本やテスト手本や、

157
00:05:08,480 --> 00:05:09,530
とにかくなんであれ

158
00:05:10,610 --> 00:05:11,910
あるベクトルxを

159
00:05:12,020 --> 00:05:13,140
入力として受け取り、

160
00:05:13,280 --> 00:05:14,640
そしてランドマークの一つを

161
00:05:14,990 --> 00:05:16,220
入力として

162
00:05:16,370 --> 00:05:18,270
受け取るが、

163
00:05:18,880 --> 00:05:20,750
ここでは私は単にこれらを

164
00:05:20,950 --> 00:05:21,810
x1, x2と呼ぶ、

165
00:05:21,900 --> 00:05:23,750
何故ならランドマークも実際はトレーニング手本だから。

166
00:05:24,470 --> 00:05:26,160
あなたがやらなくてはいけない事は、

167
00:05:26,400 --> 00:05:27,490
この入力、x1とx2をうけとり

168
00:05:27,670 --> 00:05:28,960
それらの間の、この種の類似度関数を

169
00:05:29,150 --> 00:05:30,320
計算して

170
00:05:30,580 --> 00:05:31,950
一つの実数を返す

171
00:05:32,530 --> 00:05:33,470
ソフトウェアを書く事だ。

172
00:05:36,180 --> 00:05:37,430
そこで幾つかのサポートベクタマシンのパッケージは

173
00:05:37,580 --> 00:05:39,040
あなたがこの種のカーネル関数、

174
00:05:39,510 --> 00:05:40,860
x1とx2を入力に取って実数を返すような関数を

175
00:05:41,410 --> 00:05:44,580
提供する事を期待していて、

176
00:05:45,580 --> 00:05:46,460
そしてそこから、

177
00:05:46,850 --> 00:05:49,070
自動的に全てのフィーチャーを生成して、

178
00:05:49,410 --> 00:05:51,480
つまり自動的に、xに対し

179
00:05:51,600 --> 00:05:53,370
あなたの書いたこの関数を用いて

180
00:05:53,420 --> 00:05:54,420
f1, f2, ...とf(m)まで、

181
00:05:54,750 --> 00:05:56,200
マッピングする。

182
00:05:56,310 --> 00:05:57,190
そして全てのフィーチャーを生成して、

183
00:05:57,650 --> 00:05:59,080
そこからサポートベクタマシンを訓練する。

184
00:05:59,870 --> 00:06:00,800
だが、あなたは時々、

185
00:06:00,880 --> 00:06:04,710
この関数を自分で提供してやる必要がある。

186
00:06:05,680 --> 00:06:06,770
ガウスカーネルを使う時でも、幾つかのSVM実装はガウスカーネルも含んでいて、

187
00:06:06,980 --> 00:06:09,950
そして

188
00:06:10,040 --> 00:06:10,990
それ以外にも幾つかのカーネルを含んでいる物だ。

189
00:06:11,230 --> 00:06:13,580
ガウスカーネルは恐らくもっとも一般的なカーネルなので、

190
00:06:14,880 --> 00:06:16,290
ガウスカーネルと線形カーネルは

191
00:06:16,380 --> 00:06:18,210
本当にぶっちぎりで大人気の二大カーネルなので、たぶんあるだろう。

192
00:06:19,130 --> 00:06:20,230
一つちょっとした実装のノートを。

193
00:06:20,750 --> 00:06:21,820
もしあなたのフィーチャーが、

194
00:06:22,080 --> 00:06:23,620
とても異なるスケールだったら、

195
00:06:24,700 --> 00:06:26,270
ガウスカーネルを使う前に

196
00:06:26,600 --> 00:06:27,780
フィーチャースケーリングをするのが大切だ。

197
00:06:28,580 --> 00:06:29,180
そして以下がその理由だ。

198
00:06:30,150 --> 00:06:31,600
あなたがxとlの間の

199
00:06:32,290 --> 00:06:33,570
ノルムを計算している所を、想像してみよう。

200
00:06:33,790 --> 00:06:34,890
このここの項は、

201
00:06:35,390 --> 00:06:37,150
こっちの分子の項だ。

202
00:06:38,300 --> 00:06:39,780
これが行う事は、

203
00:06:40,070 --> 00:06:40,930
xとlの間のノルムで、

204
00:06:41,130 --> 00:06:42,140
それは実際には、、、、ベクトルvを計算してみよう。

205
00:06:42,450 --> 00:06:43,290
vはイコール x-lだとする。

206
00:06:43,410 --> 00:06:44,980
そして次に、

207
00:06:45,250 --> 00:06:47,940
このベクトルvのノルムを計算する。

208
00:06:48,130 --> 00:06:49,080
それはxとlとの間の

209
00:06:49,170 --> 00:06:50,510
差だ。

210
00:06:50,580 --> 00:06:51,510
するとvのノルムは実際には

211
00:06:53,360 --> 00:06:54,140
イコール v1二乗、

212
00:06:54,250 --> 00:06:55,610
足すことの v2二乗、

213
00:06:55,830 --> 00:06:58,290
足すことの、点点点、足すことのvn二乗。

214
00:06:58,900 --> 00:07:00,320
ここでxはRn、

215
00:07:01,060 --> 00:07:02,200
いや、R n+1だが、

216
00:07:02,290 --> 00:07:05,180
x0は無視する事にする。

217
00:07:06,540 --> 00:07:08,420
そこでxはRnだとしよう、

218
00:07:08,510 --> 00:07:10,800
左側を二乗すると

219
00:07:10,950 --> 00:07:12,320
これは正しくなる。

220
00:07:12,570 --> 00:07:14,090
そうすれば、これはイコール

221
00:07:14,400 --> 00:07:16,120
こっちとなる。

222
00:07:17,210 --> 00:07:18,710
別の書き方で書くと、

223
00:07:18,850 --> 00:07:20,100
これはx1 - l(1)の二乗に、

224
00:07:20,290 --> 00:07:22,600
足すことの x2-l(2)の二乗に、

225
00:07:22,910 --> 00:07:24,590
足すことの

226
00:07:24,910 --> 00:07:26,580
点点点、、、と足すことの

227
00:07:27,130 --> 00:07:28,540
xn - l(n)の二乗。

228
00:07:29,720 --> 00:07:30,790
そして今、あなたのフィーチャーが

229
00:07:31,850 --> 00:07:33,460
とても異なったレンジの値を取るとしよう。

230
00:07:33,940 --> 00:07:35,150
住宅の価格を予測する例で

231
00:07:35,360 --> 00:07:37,180
考えてみると、

232
00:07:38,020 --> 00:07:40,490
データは住宅に関するデータで、

233
00:07:41,420 --> 00:07:43,000
ｘは、

234
00:07:43,140 --> 00:07:44,660
最初のフィーチャーx1に関しては

235
00:07:44,950 --> 00:07:47,190
何千平方フィートの範囲を

236
00:07:48,010 --> 00:07:48,840
取り、

237
00:07:49,700 --> 00:07:51,630
しかし二番目のフィーチャーx2は寝室の数とすると、

238
00:07:52,540 --> 00:07:53,610
寝室の数は範囲としては

239
00:07:53,730 --> 00:07:56,720
1部屋から5部屋程度だろう、

240
00:07:57,810 --> 00:07:59,320
するとx1-l(1)は巨大になりえて、

241
00:07:59,780 --> 00:08:00,820
1000平方フィートとかに成り得るが、

242
00:08:01,000 --> 00:08:02,880
一方でx2-l(2)は

243
00:08:03,200 --> 00:08:04,620
もっと小さくなり、

244
00:08:04,750 --> 00:08:06,800
その場合には、この項、

245
00:08:08,320 --> 00:08:09,660
これらの距離は、

246
00:08:10,060 --> 00:08:12,060
本質的にはほとんど住居のサイズで

247
00:08:12,570 --> 00:08:13,280
支配されてしまい、

248
00:08:14,390 --> 00:08:15,760
寝室の数はほとんど無視されてしまう。

249
00:08:16,950 --> 00:08:18,060
だから学習がうまくいくように

250
00:08:18,230 --> 00:08:19,070
これを避ける為には、

251
00:08:19,360 --> 00:08:21,890
フィーチャースケーリングをせよ。

252
00:08:23,420 --> 00:08:24,830
そうする事で、SVMが

253
00:08:25,810 --> 00:08:27,020
様々なフィーチャーについて

254
00:08:27,950 --> 00:08:28,870
同じような程度の関心を払うようにし、

255
00:08:29,190 --> 00:08:30,450
この例の住居のサイズのように

256
00:08:30,600 --> 00:08:31,870
他のフィーチャーを

257
00:08:32,150 --> 00:08:33,440
塗りつぶしてしまうような事が無い事を、保証出来る。

258
00:08:34,700 --> 00:08:35,810
あなたがサポートベクタマシンを試みる時に、

259
00:08:36,110 --> 00:08:38,760
おそらくもっとも一般的であろう

260
00:08:38,970 --> 00:08:40,000
二つのカーネルは、

261
00:08:40,460 --> 00:08:41,750
線形(linear)カーネル、つまり

262
00:08:41,850 --> 00:08:43,120
カーネル無しか、

263
00:08:43,320 --> 00:08:45,600
または既に話したガウスカーネルだろう。

264
00:08:46,520 --> 00:08:47,390
そしてちょっと注意を。

265
00:08:47,900 --> 00:08:49,070
類似度関数ならば、

266
00:08:49,580 --> 00:08:50,590
どんな物でも

267
00:08:50,770 --> 00:08:52,520
カーネルとして使える、という訳では無い。

268
00:08:53,450 --> 00:08:54,840
そしてガウスカーネル、線形カーネル、

269
00:08:55,090 --> 00:08:56,410
そしてたまに他の人が使ってるのを見かける

270
00:08:56,710 --> 00:08:57,850
その他のカーネルでも、

271
00:08:58,030 --> 00:08:59,840
それらはある技術的条件を満たす必要がある。

272
00:09:00,380 --> 00:09:02,510
それはMercerの定理と言われる物だ。

273
00:09:02,630 --> 00:09:03,560
これが必要となる理由は、

274
00:09:03,710 --> 00:09:05,430
サポートベクタマシンのアルゴリズム、

275
00:09:06,380 --> 00:09:08,140
全てのSVMの実装は、

276
00:09:08,480 --> 00:09:09,560
大量の賢い

277
00:09:10,050 --> 00:09:11,380
数値計算的な最適化のトリックを使ってる、

278
00:09:12,110 --> 00:09:13,270
パラメータのシータについて

279
00:09:13,340 --> 00:09:15,650
効率的に解く為に。

280
00:09:16,590 --> 00:09:18,840
そしてオリジナルのSVMの設計において、

281
00:09:19,470 --> 00:09:21,010
主な関心を、Mercerの定理と呼ばれる

282
00:09:21,540 --> 00:09:22,900
技術的条件を満たすカーネルだけに

283
00:09:23,510 --> 00:09:25,860
集中する、という決定がなされた。

284
00:09:26,280 --> 00:09:27,360
そうする事で、

285
00:09:27,570 --> 00:09:28,540
それらSVMパッケージの全てで、

286
00:09:28,820 --> 00:09:30,270
それら全てのSVMソフトウェアパッケージで

287
00:09:30,500 --> 00:09:32,210
大きなクラスの最適化を

288
00:09:32,310 --> 00:09:34,740
用いる事が出来るようになり、

289
00:09:35,280 --> 00:09:37,470
パラメータのシータをとても早く得られるようになる。

290
00:09:39,320 --> 00:09:40,340
さて、結局の所、ほとんどの人が使うのは

291
00:09:40,840 --> 00:09:42,470
線形カーネルか

292
00:09:42,610 --> 00:09:44,210
ガウスカーネルだ。

293
00:09:44,430 --> 00:09:45,610
だがそれ以外にも幾つか

294
00:09:45,940 --> 00:09:47,460
Mercerの定理を満たすカーネルが存在する。

295
00:09:47,560 --> 00:09:48,690
そして他の人々がそれらを使ってる所を

296
00:09:48,850 --> 00:09:50,050
あなたがみかける事もあるかもしれない。

297
00:09:50,880 --> 00:09:53,780
私個人としては、それ以外のカーネルを使う事ってほんとにほんとーに稀で、全く無いって事は無いにせよほとんど無いけど。

298
00:09:54,160 --> 00:09:56,990
あなたが遭遇するかもしれない幾つかのカーネルをちょっとだけ言及しておくと、

299
00:09:57,990 --> 00:10:00,300
一つ目は多項式カーネル。

300
00:10:01,570 --> 00:10:03,350
その場合、xとlの間の

301
00:10:03,800 --> 00:10:05,520
類似度は、

302
00:10:05,730 --> 00:10:06,760
どう定義されるかというと、

303
00:10:06,830 --> 00:10:07,880
たくさんの選択肢があるが、

304
00:10:08,640 --> 00:10:10,370
xの転置 l の二乗という定義がありうる。

305
00:10:10,960 --> 00:10:13,410
これは、xとlがどれだけ類似してるかを測る、一つの指標である。

306
00:10:13,610 --> 00:10:14,930
xとlがお互いにとても近いと、

307
00:10:15,500 --> 00:10:18,260
内積は大きくなる傾向にある。

308
00:10:20,200 --> 00:10:21,870
このカーネルは

309
00:10:23,080 --> 00:10:23,520
ちょっと珍しいカーネルで、

310
00:10:24,000 --> 00:10:25,130
そんなには使われていない。

311
00:10:26,490 --> 00:10:29,190
だがたまには、使ってる人に出くわす事もあるかもしれない。

312
00:10:30,050 --> 00:10:31,810
これは多項式カーネルの一つのバージョンだ。

313
00:10:32,330 --> 00:10:35,090
もう一つ別の物としては、x転置 l の三乗。

314
00:10:36,690 --> 00:10:38,780
これらは全て、多項式カーネルの例だ。

315
00:10:39,040 --> 00:10:41,270
x転置 l + 1 の三乗、

316
00:10:42,560 --> 00:10:43,620
x転置 l 足すことの

317
00:10:43,910 --> 00:10:44,930
1以外の数でも良い。5とか。

318
00:10:44,970 --> 00:10:46,680
そして4乗。

319
00:10:47,700 --> 00:10:49,840
つまり、多項式カーネルは実際には二つのパラメータがある。

320
00:10:50,610 --> 00:10:53,020
一つはどの数をここに足すか？

321
00:10:53,520 --> 00:10:53,920
これは0でも良い。

322
00:10:54,430 --> 00:10:58,660
ここには実はプラス0がある。
同様に、ここの、多項式の次数(degree)がある。

323
00:10:58,680 --> 00:11:01,670
つまり累乗の指数。そしてこれらの数字。

324
00:11:02,250 --> 00:11:04,140
多項式カーネルの

325
00:11:04,280 --> 00:11:05,530
より一般的な形としては、

326
00:11:05,720 --> 00:11:07,620
x転置 l 足すことの

327
00:11:07,940 --> 00:11:11,510
ある定数(const)

328
00:11:11,800 --> 00:11:14,850
そして指数のある次数(degree)。

329
00:11:15,060 --> 00:11:16,720
つまりこれら二つの両方が

330
00:11:16,940 --> 00:11:19,650
多項式カーネルのパラメータだ。

331
00:11:20,510 --> 00:11:22,820
多項式カーネルは、ほとんどいつでも

332
00:11:23,350 --> 00:11:24,440
ガウスカーネルよりも

333
00:11:24,820 --> 00:11:25,950
より劣ったパフォーマンスを示す。

334
00:11:26,270 --> 00:11:28,370
だからそんなに使われない。でもたまに出くわす事がある物ではある。

335
00:11:29,320 --> 00:11:30,480
通常、これはxとlが

336
00:11:30,750 --> 00:11:31,710
全て0より大きい、

337
00:11:32,000 --> 00:11:33,180
非負のデータに対してのみ使われる。

338
00:11:33,740 --> 00:11:34,720
そうする事で、これらの内積が

339
00:11:34,910 --> 00:11:36,710
決して負にならない事を担保している。

340
00:11:37,850 --> 00:11:40,010
そしてこれは、xとlがとてもお互いに近いと

341
00:11:40,390 --> 00:11:41,340
それら同士の内積が大きくなるだろう、という

342
00:11:41,540 --> 00:11:44,110
直感を捕捉している。

343
00:11:44,420 --> 00:11:45,590
他の性質もあるんだが、

344
00:11:46,260 --> 00:11:48,080
何にせよ人々はあまり使ってない。

345
00:11:49,130 --> 00:11:50,150
そしてさらに、あなたがやっている事に応じて、

346
00:11:50,260 --> 00:11:51,210
ある種より難解なカーネルも

347
00:11:52,330 --> 00:11:54,950
存在して、あなたがそれらに遭遇する事もあるかもしれない。

348
00:11:55,670 --> 00:11:57,180
文字列カーネルというのがある、

349
00:11:57,340 --> 00:11:58,430
これはあなたの入力データがテキスト文字列なり、

350
00:11:58,550 --> 00:12:01,350
それ以外でも文字列の時には、時々使われる事がある。

351
00:12:02,270 --> 00:12:02,940
カイ二乗カーネルという物もあり、

352
00:12:03,260 --> 00:12:06,000
ヒストグラム交差カーネルという物などもある。

353
00:12:06,690 --> 00:12:08,420
これらは異なるオブジェクト同士の

354
00:12:08,660 --> 00:12:09,840
類似度を測る事が出来る

355
00:12:10,760 --> 00:12:12,030
ある種より難解なカーネルだ。

356
00:12:12,660 --> 00:12:13,800
例えば、あなたがある種の

357
00:12:14,380 --> 00:12:15,840
テキスト分類の問題に挑んでいる時には、

358
00:12:16,170 --> 00:12:17,060
入力xが文字列となるので、

359
00:12:17,200 --> 00:12:19,300
我らは二つの

360
00:12:19,490 --> 00:12:20,490
文字列の間の

361
00:12:20,550 --> 00:12:22,050
類似度を、文字列カーネルを用いる事で

362
00:12:22,430 --> 00:12:24,240
知りたいかもしれない。

363
00:12:24,520 --> 00:12:26,440
だが私個人としては、これらのより難解なカーネルは

364
00:12:26,990 --> 00:12:29,340
全く見ないとは言わないが、とても稀にしか見ない。

365
00:12:29,880 --> 00:12:30,970
カイ2乗カーネルを、たぶん私は生涯で、

366
00:12:31,170 --> 00:12:32,270
一回しか使ったことが無いと思うし、

367
00:12:32,340 --> 00:12:33,670
ヒストグラムカーネルは

368
00:12:34,240 --> 00:12:35,580
人生で一回か二回だったと思う。

369
00:12:35,630 --> 00:12:38,500
私は実は、文字列カーネルを自分で使った事は無い。

370
00:12:39,350 --> 00:12:41,560
しかしもしあなたが別のアプリケーションでこれらに遭遇したら、

371
00:12:42,700 --> 00:12:43,640
ちょろっとwebを検索して、

372
00:12:43,860 --> 00:12:44,850
Googleで軽く検索するなり、

373
00:12:45,040 --> 00:12:46,000
Bingで軽く検索すれば、

374
00:12:46,590 --> 00:12:48,240
これらのカーネルの定義を見つける事が出来るはずだ。

375
00:12:51,480 --> 00:12:55,680
このビデオで話しておきたい細かい話が最後に二つ。
一つはマルチクラスの分類問題について。

376
00:12:56,370 --> 00:12:59,510
4つのクラスがあったとしよう、あるいはもっと一般的に

377
00:12:59,800 --> 00:13:01,880
k個のクラスの出力があったとしよう。
複数のクラス間の適切な決定境界を

378
00:13:02,530 --> 00:13:06,860
SVMにどのように計算させたら良いだろうか？

379
00:13:07,220 --> 00:13:08,750
ほとんどのSVM、とは言い過ぎだが多くのSVMパッケージは、既にマルチクラスの分類の機能が

380
00:13:09,030 --> 00:13:10,430
ビルドインされている。

381
00:13:11,100 --> 00:13:12,060
だからもしあなたがそのようなパッケージを使っているなら、

382
00:13:12,270 --> 00:13:13,320
あなたは単に、ビルドインの機能を

383
00:13:13,540 --> 00:13:15,370
使うだけで、

384
00:13:15,490 --> 00:13:16,940
うまくいくはずだ。

385
00:13:17,790 --> 00:13:18,790
そうで無ければ、これを行う方法の一つには、

386
00:13:19,000 --> 00:13:19,880
1 vs allの手法を

387
00:13:20,000 --> 00:13:21,280
使う事だ、これについては

388
00:13:21,370 --> 00:13:23,690
ロジスティック回帰を作ってる時に議論した。

389
00:13:24,680 --> 00:13:25,410
あなたがやる事は、K個のSVMを

390
00:13:26,160 --> 00:13:27,550
トレーニングする、もしあなたがKクラス

391
00:13:27,700 --> 00:13:29,190
あったとしてだが、一度に一つのクラスを

392
00:13:29,900 --> 00:13:31,060
それ以外のクラスと区別するように。

393
00:13:31,850 --> 00:13:32,930
そしてこれは、K個のパラメータベクトルを与える。

394
00:13:33,520 --> 00:13:34,530
つまりこれは、あなたに

395
00:13:34,680 --> 00:13:36,210
パラメータベクトルのシータ1を与え、

396
00:13:36,530 --> 00:13:38,170
これはクラスy=1を

397
00:13:38,630 --> 00:13:39,980
それ以外のクラスから区別しようと

398
00:13:40,130 --> 00:13:41,340
試みる物で、

399
00:13:41,420 --> 00:13:42,910
そして次に二番目のパラメータベクトル、

400
00:13:42,970 --> 00:13:43,910
シータ2を与え、これは

401
00:13:44,020 --> 00:13:45,420
y=2を陽性のクラスとした時に

402
00:13:45,720 --> 00:13:47,080
そしてそれ以外全てを陰性のクラスとした時に

403
00:13:47,460 --> 00:13:48,680
得られるパラメータで、

404
00:13:49,260 --> 00:13:50,550
そうやってパラメータベクトル

405
00:13:50,800 --> 00:13:52,400
シータKまで、

406
00:13:52,750 --> 00:13:54,520
これは最後のクラスKを

407
00:13:54,600 --> 00:13:56,770
それ以外と区別する

408
00:13:57,360 --> 00:13:59,380
パラメータベクトルだ。

409
00:13:59,490 --> 00:14:00,590
最後に、これはまさに

410
00:14:01,270 --> 00:14:02,040
ロジスティック回帰でやった

411
00:14:02,420 --> 00:14:04,230
1 vs ALL法だ。

412
00:14:04,760 --> 00:14:05,910
そこではあなたは

413
00:14:06,390 --> 00:14:07,690
シータ転置 xが最大になったクラスiを

414
00:14:08,030 --> 00:14:11,840
予想とするのだった。以上がマルチクラスの分類の方法だ。

415
00:14:12,440 --> 00:14:13,750
だがもっと一般的なケースとしては、

416
00:14:14,300 --> 00:14:15,090
だいたいは、かなり良い確率で、

417
00:14:15,180 --> 00:14:16,460
あなたが何のソフトウェアパッケージを使ってるにせよ、

418
00:14:16,780 --> 00:14:18,010
だいたいは、

419
00:14:18,340 --> 00:14:19,650
かなりの確率で、そこには

420
00:14:19,920 --> 00:14:21,740
既にマルチクラスの機能がビルドインされている。

421
00:14:21,920 --> 00:14:24,410
だからこの事について思い悩む必要は無い。

422
00:14:25,280 --> 00:14:27,010
最後に、我らはサポートベクタマシンを

423
00:14:27,210 --> 00:14:28,650
ロジスティック回帰から始めて、

424
00:14:29,090 --> 00:14:31,500
コスト関数をちょっと変更していく事で開発してきた。

425
00:14:31,910 --> 00:14:34,900
このビデオで最後にやりたい事は、これらの二つのアルゴリズムの

426
00:14:35,550 --> 00:14:36,570
どちらをいつ使うのか、について

427
00:14:36,660 --> 00:14:38,840
ちょっと話しておきたい。

428
00:14:39,080 --> 00:14:40,000
nをフィーチャーの数として、

429
00:14:40,160 --> 00:14:42,000
mをトレーニング手本の数とする。

430
00:14:43,190 --> 00:14:45,250
さて、いつどちらのアルゴリズムを使い、いつもう一つのアルゴリズムを使うべきだろう？

431
00:14:47,130 --> 00:14:48,430
もしnがトレーニングセットサイズとの相対的に

432
00:14:48,980 --> 00:14:50,140
大きければ、

433
00:14:50,360 --> 00:14:51,390
例を挙げると、

434
00:14:52,810 --> 00:14:53,990
フィーチャーの数が

435
00:14:54,250 --> 00:14:55,180
これがmよりずっと

436
00:14:55,330 --> 00:14:56,870
大きくて、

437
00:14:57,120 --> 00:14:58,210
そしてこれが、例えば、

438
00:14:58,320 --> 00:15:00,590
テキスト分類の問題だとすると、

439
00:15:01,550 --> 00:15:02,430
フィーチャーのベクトルの次元は、

440
00:15:02,700 --> 00:15:04,160
知らんけど例えば1万とかになりがちで、

441
00:15:05,370 --> 00:15:06,350
そしてトレーニングセットのサイズが

442
00:15:06,720 --> 00:15:08,290
10から、

443
00:15:08,510 --> 00:15:10,250
せいぜい1000くらいまでの間。

444
00:15:10,500 --> 00:15:12,140
スパム分類の問題を

445
00:15:12,320 --> 00:15:14,250
想像してみよう。

446
00:15:14,510 --> 00:15:15,840
e-mailスパムでは、1万の単語に対応した

447
00:15:16,150 --> 00:15:18,010
1万個のフィーチャーがあるとする、

448
00:15:18,190 --> 00:15:19,550
でも例えば10通からせいぜい1000通とかの

449
00:15:19,780 --> 00:15:21,150
トレーニング手本しか無いとする。

450
00:15:22,450 --> 00:15:23,750
つまり、nはmとの相対で考えると大きい。

451
00:15:23,890 --> 00:15:25,090
その場合に私が普段やるのは、

452
00:15:25,250 --> 00:15:26,480
ロジスティック回帰を使うか、

453
00:15:26,850 --> 00:15:27,990
カーネル無しの、あるいは線形カーネルの

454
00:15:28,100 --> 00:15:29,030
SVMを

455
00:15:29,460 --> 00:15:30,790
使う。

456
00:15:31,620 --> 00:15:32,430
何故なら、そんなにたくさんのフィーチャーで、

457
00:15:32,580 --> 00:15:33,830
トレーニングセットが小さいと、

458
00:15:34,530 --> 00:15:35,870
線形関数はたぶんいい感じだと思う、

459
00:15:36,330 --> 00:15:37,380
そしてとても複雑な非線形の関数を

460
00:15:37,640 --> 00:15:38,790
フィッティングするには

461
00:15:38,910 --> 00:15:40,760
十分なデータを持ってない。

462
00:15:41,340 --> 00:15:42,410
今、もしnが小さくて、

463
00:15:42,520 --> 00:15:44,020
mが中くらいの大きさだと、

464
00:15:44,350 --> 00:15:45,890
ここでnとしては

465
00:15:45,940 --> 00:15:47,450
1から1000くらいを

466
00:15:48,040 --> 00:15:50,350
イメージしてて、1はとても小さい場合だが、

467
00:15:50,530 --> 00:15:51,470
せいぜい1000フィーチャーくらいまで、

468
00:15:51,700 --> 00:15:54,270
そして

469
00:15:54,590 --> 00:15:56,180
トレーニング手本の総数が

470
00:15:56,330 --> 00:15:57,700
10から1万手本くらいの間の

471
00:15:58,210 --> 00:16:00,750
どこか位。

472
00:16:01,350 --> 00:16:03,160
5万くらいまでの間でもいいかもしれない。

473
00:16:03,630 --> 00:16:06,490
mがとても大きければ、1万くらい。だが100万は行かない。

474
00:16:06,760 --> 00:16:08,100
さて、mがもし

475
00:16:08,300 --> 00:16:09,950
中くらいのサイズの時は、

476
00:16:10,790 --> 00:16:12,980
しばしばSVMに線形カーネルが、うまく機能するだろう。

477
00:16:13,530 --> 00:16:14,580
この場合については以前にも話した、

478
00:16:14,710 --> 00:16:15,800
一つの具体例で。

479
00:16:16,350 --> 00:16:17,100
それは、もしあなたが二次元の

480
00:16:17,520 --> 00:16:19,720
トレーニングセットの時に、

481
00:16:19,900 --> 00:16:21,010
つまり、もしn=2の時で、

482
00:16:21,320 --> 00:16:23,710
たくさんのトレーニング手本が描いてある時などは、

483
00:16:24,710 --> 00:16:25,860
ガウスカーネルは、陽性と陰性のクラスを分離するのに

484
00:16:26,130 --> 00:16:28,160
きわめて良い働きをするだろう。

485
00:16:29,770 --> 00:16:30,890
興味のある三番目の状況としては、

486
00:16:30,980 --> 00:16:32,420
nが小さくて

487
00:16:32,520 --> 00:16:34,270
mが大きい時。

488
00:16:34,890 --> 00:16:36,560
例えばnは1から1000までとか、

489
00:16:37,390 --> 00:16:39,280
もうちょっと大きくてもいいかもしれないが、

490
00:16:40,200 --> 00:16:42,750
しかしmは5万くらいから

491
00:16:43,320 --> 00:16:46,400
100とかまでとか、

492
00:16:47,520 --> 00:16:50,270
5万から10万、100万、200万、と、

493
00:16:51,290 --> 00:16:54,020
とても大きなトレーニングセットのサイズの時、

494
00:16:55,240 --> 00:16:56,160
この場合は、

495
00:16:56,380 --> 00:16:57,630
ガウスカーネルのSVMは

496
00:16:57,900 --> 00:16:59,850
走らせるといくらか遅い。

497
00:17:00,160 --> 00:17:02,300
こんにちのSVMパッケージでは、

498
00:17:02,410 --> 00:17:04,900
ガウスカーネルを使うと、

499
00:17:05,050 --> 00:17:06,250
5万くらいなら、

500
00:17:06,590 --> 00:17:07,530
たぶん問題無い。

501
00:17:07,620 --> 00:17:10,250
だが、100万個のトレーニング手本だと、

502
00:17:10,450 --> 00:17:11,950
または10万個のトレーニング手本で、かつ

503
00:17:12,170 --> 00:17:13,730
nが大きい値の時には、

504
00:17:14,180 --> 00:17:15,590
こんにちのSVMパッケージはとても良い物だが、

505
00:17:15,870 --> 00:17:17,100
それでも大量の、本当に大量の

506
00:17:17,600 --> 00:17:18,400
トレーニングセットの時には

507
00:17:19,010 --> 00:17:20,940
ガウスカーネルを使うとちょっとだけ苦戦するかもしれない。

508
00:17:22,050 --> 00:17:23,150
その場合には、

509
00:17:23,350 --> 00:17:24,960
私が普段やるのは、

510
00:17:25,330 --> 00:17:26,660
手動でフィーチャーを増やして

511
00:17:26,800 --> 00:17:28,600
ロジスティック回帰を使うか、

512
00:17:28,930 --> 00:17:30,340
カーネル無しのSVMを

513
00:17:30,630 --> 00:17:32,060
試す。

514
00:17:33,140 --> 00:17:34,030
そしてもしあなたがこのスライドを見て

515
00:17:34,230 --> 00:17:35,900
ロジスティック回帰とカーネル無しのSVMが

516
00:17:36,460 --> 00:17:37,750
これらの場所がいつも

517
00:17:38,510 --> 00:17:39,890
ペアで一緒に出てきてると

518
00:17:39,980 --> 00:17:41,750
思ったなら、

519
00:17:42,060 --> 00:17:43,050
それには理由があるのだ。

520
00:17:43,900 --> 00:17:45,640
ロジスティック回帰とカーネル無しのSVM、

521
00:17:46,000 --> 00:17:47,130
これらは実際にはかなり

522
00:17:47,350 --> 00:17:49,450
似たアルゴリズムだ。

523
00:17:49,680 --> 00:17:51,170
ロジスティック回帰もカーネル無しのSVMも

524
00:17:51,500 --> 00:17:53,230
通常は極めて似た

525
00:17:53,380 --> 00:17:54,780
振る舞いをする。そして極めて似た

526
00:17:54,900 --> 00:17:56,690
パフォーマンスを与える。

527
00:17:57,060 --> 00:18:00,340
だが実装の詳細によっては、片方がもう片方よりも効率的だったりはするかもしれない。

528
00:18:00,930 --> 00:18:02,220
だが、これらのアルゴリズムの

529
00:18:02,310 --> 00:18:03,530
一つを適用する時、

530
00:18:03,740 --> 00:18:05,190
ロジスティック回帰とカーネル無しのSVMの

531
00:18:05,420 --> 00:18:05,840
どちらも、だいたい同様に

532
00:18:06,650 --> 00:18:07,600
機能する。

533
00:18:08,540 --> 00:18:09,660
だがSVMの威力は、

534
00:18:09,720 --> 00:18:11,610
複雑な非線型の関数を

535
00:18:11,810 --> 00:18:14,100
学習する為に

536
00:18:14,430 --> 00:18:15,860
別のカーネルを使う時に発揮される。

537
00:18:16,680 --> 00:18:20,300
そしてこの形態では、

538
00:18:20,550 --> 00:18:22,530
1万手本くらいまでとか、5万手本くらいまでとかで、

539
00:18:22,610 --> 00:18:25,010
そしてフィーチャーの数は、

540
00:18:26,580 --> 00:18:27,540
ちょうど良い程度に多い、

541
00:18:27,840 --> 00:18:29,230
これはとても良くある形態だ。

542
00:18:29,670 --> 00:18:30,910
そしてこの状況こそ、

543
00:18:31,430 --> 00:18:33,830
カーネルと共にサポートベクタマシンを用いる事が、光り輝く形態だ。

544
00:18:34,320 --> 00:18:35,640
ロジスティック回帰を使ってではもっと大変になるような事を

545
00:18:35,860 --> 00:18:39,850
やる事が出来る。

546
00:18:40,100 --> 00:18:40,930
そして最後に、ニューラルネットワークが適合するのはどういう形態か？

547
00:18:41,120 --> 00:18:42,230
うーん、これら全ての問題に対して、

548
00:18:42,440 --> 00:18:43,890
これらの別々の形態

549
00:18:43,960 --> 00:18:46,310
全てに対して、

550
00:18:46,630 --> 00:18:49,110
良く設計されたニューラルネットワークは、うまく機能するだろう。

551
00:18:50,320 --> 00:18:51,700
一つの欠点としては、あるいは

552
00:18:51,830 --> 00:18:52,980
ニューラルネットワークを使わない事がある

553
00:18:53,220 --> 00:18:54,690
理由の一つには、

554
00:18:54,920 --> 00:18:56,080
これらの問題の中には、

555
00:18:56,180 --> 00:18:57,640
ニューラルネットワークは訓練するのに遅いという場合がある。

556
00:18:58,250 --> 00:18:59,080
だがもしとても良い

557
00:18:59,350 --> 00:19:01,190
SVM実装パッケージがあるなら、

558
00:19:01,400 --> 00:19:04,120
そちらの方が速く走りうる、ニューラルネットワークよりずっと速く走る。

559
00:19:05,130 --> 00:19:06,130
そして、これは証明しなかったが、

560
00:19:06,350 --> 00:19:07,520
SVMの持つ

561
00:19:07,630 --> 00:19:09,800
最適化の問題は、

562
00:19:10,070 --> 00:19:11,120
凸最適化問題である事が

563
00:19:12,320 --> 00:19:13,830
知られている。

564
00:19:14,410 --> 00:19:15,800
だから良いSVM最適化ソフトウェアパッケージは

565
00:19:16,160 --> 00:19:17,870
必ずグローバル最小か、それに近い値を

566
00:19:18,240 --> 00:19:21,370
探してくれる。

567
00:19:21,720 --> 00:19:24,100
だからSVMでは、ローカル最適の問題を心配する必要は無い。

568
00:19:25,280 --> 00:19:26,440
現実問題としては、ニューラルネットワークでも

569
00:19:26,580 --> 00:19:27,920
ローカル最適はそんなに大きな問題では無い。

570
00:19:28,090 --> 00:19:29,120
だけど、、、あー、

571
00:19:29,310 --> 00:19:31,520
とにかくSVMを使えば、心配事が一つ減ると言えば減る。

572
00:19:33,350 --> 00:19:34,560
そして問題によっては、

573
00:19:34,910 --> 00:19:37,050
ニューラルネットワークの方が遅いかもしれない、

574
00:19:37,580 --> 00:19:41,020
特にこの形態の問題では、SVMよりも遅いかもしれない。

575
00:19:41,420 --> 00:19:42,200
ここに示したガイドラインが

576
00:19:42,520 --> 00:19:43,500
ちょっと曖昧だなぁ、と思っても、

577
00:19:43,860 --> 00:19:44,600
もしあなたが何かの問題に際して、

578
00:19:46,930 --> 00:19:48,050
ガイドラインがいまいち曖昧だなぁ、と思っても、

579
00:19:48,170 --> 00:19:49,190
私もいまだに完全には確信が持てる訳では無い、

580
00:19:49,570 --> 00:19:50,730
私はこっちのアルゴリズムを使うべきか？

581
00:19:50,780 --> 00:19:52,690
あるいはあっちのアルゴリズムを使うべきか。それは実際には問題無い。

582
00:19:52,950 --> 00:19:54,100
私が機械学習の問題に直面する時には、

583
00:19:54,330 --> 00:19:55,570
時々、

584
00:19:55,730 --> 00:19:57,010
どのアルゴリズムを使うのがベストなのか、

585
00:19:57,150 --> 00:19:58,700
はっきりしない事がある。

586
00:19:59,540 --> 00:20:00,590
だが以前のビデオで見た通り、

587
00:20:01,200 --> 00:20:02,470
アルゴリズムは確かに重要だけど、

588
00:20:02,700 --> 00:20:03,920
しかししばしばもっと重要なのは、

589
00:20:04,250 --> 00:20:06,400
どれだけの量のデータを持ってるか、

590
00:20:07,090 --> 00:20:08,280
そしてどれだけあなたの技術力が高いか、

591
00:20:08,450 --> 00:20:09,500
エラー分析や学習アルゴリズムをデバッグするのを

592
00:20:09,750 --> 00:20:11,450
どれだけうまく行えるか、

593
00:20:11,660 --> 00:20:13,090
新しいフィーチャーをどうデザインする方法を

594
00:20:13,220 --> 00:20:15,120
どれだけうまく見いだせるか、

595
00:20:15,280 --> 00:20:17,540
学習アルゴリズムに渡す別のフィーチャーをどれだけ上手く見いだせるか、などだったりする。

596
00:20:17,960 --> 00:20:19,110
そしてしばしば、これらの事項は

597
00:20:19,660 --> 00:20:20,700
あなたがロジスティック回帰を使うかSVMを使うか、よりも

598
00:20:20,840 --> 00:20:22,370
もっと重要な事だろう。

599
00:20:23,280 --> 00:20:24,650
だが、言ってきたように、

600
00:20:25,010 --> 00:20:26,180
SVMはいまだに、

601
00:20:26,630 --> 00:20:27,890
もっとも強力な学習アルゴリズムの一つだと

602
00:20:27,950 --> 00:20:29,600
広く受け止められている。

603
00:20:29,740 --> 00:20:31,570
そしてSVMが複雑な非線型の関数を

604
00:20:31,790 --> 00:20:34,340
とても良く学習出来るような事態がある。

605
00:20:35,150 --> 00:20:36,840
そして私も実際、、、

606
00:20:37,040 --> 00:20:38,930
ロジスティック回帰、ニューラルネットワーク、SVM、

607
00:20:39,090 --> 00:20:40,630
これら三つの学習アルゴリズムを

608
00:20:40,760 --> 00:20:42,170
使えるなら、私が思うにあなたは

609
00:20:42,440 --> 00:20:43,610
とても広範な応用に対し

610
00:20:44,120 --> 00:20:45,120
最先端の機械学習システムを

611
00:20:45,310 --> 00:20:46,710
構築するのに

612
00:20:46,960 --> 00:20:49,110
とても良い位置に居る。

613
00:20:49,330 --> 00:20:52,460
そしてこれはもう一つ、あなたの武器庫に備えておくのに良い、とてもパワフルなツールという訳だ。

614
00:20:53,160 --> 00:20:54,270
この一つは、シリコンバレー中で

615
00:20:54,460 --> 00:20:55,850
業界中で、

616
00:20:56,390 --> 00:20:58,030
アカデミックな世界で、

617
00:20:58,310 --> 00:20:59,860
多くの高パフォーマンスの機械学習システムを作るのに

618
00:21:00,120 --> 00:21:01,680
使われている物だ。