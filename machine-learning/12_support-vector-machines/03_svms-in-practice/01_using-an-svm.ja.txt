ここまでの所、我らはSVMについて かなり抽象的なレベルで議論してきた。 このビデオでは、 SVMを実際に走らせるにあたり、あるいは使うにあたり 実際にやらなくてはいけない事について議論したい。 サポートベクタマシンアルゴリズムは 特定の最適化問題を導く。 だが以前のビデオで 簡単に述べた通り、 パラメータシータを解くソフトウェアを 自力で書く事は、全く推奨しない。 こんにちでは、 我々の中には、 自分で行列の逆行列を求めるコードや 数のルートを計算するコードを書こう、とする人は かなり少数派、いやほとんど居ないと言っても良かろう。 我らは、これを行うライブラリ関数を呼ぶ。 同様に、 SVMの最適化問題を 解くソフトウェアは、 とても複雑だ。 そして、本質的には数値計算の最適化だけを何年もやってるような 研究者達、というのも存在している。 だから、あなたはこれを行ってくれる 素晴らしいソフトウェアライブラリや ソフトウェアパッケージを見つける事が出来る。 であるから、自分で実装しよう、なんて考えないで それらの高度に最適化されたソフトウェアライブラリのどれかを使う事を 強く推奨する。 良いソフトウェアライブラリは、たくさんあるから。 私がもっとも良く使う物を 二つ上げておくと、 liblinearとlibsvmだが、 これを行うソフトウェアライブラリは これ以外にもたくさんあって、 あなたが学習アルゴリズムを実装している言語に リンク出来る物も たくさんあるはずだ。 あなたはSVMの最適化ソフトウェアを 独自実装すべきでは無いのだけれども、 あたながやるべき事も、ちょっとは存在する。 まず最初に、パラメータCを 選ばなくてはいけない。 この事については 前回のビデオの中で、 バイアス/バリアンス の性質の話の時に言及した。 二番目に、あなたが使うカーネル、あるいは 類似度関数を 選ぶ必要がある。 選択の一例としては、 カーネルを使わない、という決断もあり得る。 そしてカーネルを使わない、というアイデアは 線形（リニア）カーネルとも呼ばれる。 だから、誰かが 「俺はSVMをリニアカーネルで使うぜ」と言ったら、 それの意味する事は、 SVMを、カーネル無しで 使う、という事。 そしてそれは、SVMを 単に シータ転置 x で使うというSVMのバージョンという事になる。 それは以下の時に1を予測する、 シータ0+シータ1 x1 足す、、、、っと足すことの シータn xn が、 0以上の時。 この「線形」カーネルという言葉は、 これは標準的な「線形」の分類器を 与えるSVMのバージョンだ、 と考える事が出来る。 さて、これはある種の問題には リーズナブルな選択となる。 そして世の中には たくさんのソフトウェアのライブラリ、 たくさんあるそんなソフトウェアライブラリの中の 一つの例としては、liblinearのような物とかが挙げられるが、 とにかく、SVMをカーネル無しで、 またの名を線形カーネルでトレーニング出来る ソフトウェアライブラリはたくさんある。 では、何故これを使いたい、と思うのだろうか？ もしあなたが、大量のフィーチャーを持っていて、 つまりnが大きくて、 そしてm、 トレーニング手本の数が、小さければ、 その時はつまり、 あなたは大量のフィーチャーを 持っているという事で、 xはRnとかRn+1という事だ。 だから既にたくさんの フィーチャーがあって、 トレーニングセットのサイズは小さい、 この場合は単に線形の 決定境界へのフィッティングを望むかもしれない。 そしてあまり複雑な非線型の関数にフィッティングしたいとは思わないだろう。 何故なら、十分なデータが無いと、 オーバーフィッティングのリスクがあるからだ。 とても次数の高いフィーチャー空間上の とても複雑な関数にフィッティングしようとしていて、 しかもトレーニングセットのサイズが小さい時には。 だからこそ、この状況は カーネルを使わない、 または同じ事だが線形カーネルと呼ばれる物を使う、という 選択をするのが合理的となる 状況だ。 あなたが行いそうな二番目の選択肢は このガウスカーネルだ。 これは以前に得た物だ。 そしてもしあなたがこれを選ぶなら、 あなたが行わなくてはいけないさらなる選択としては、 パラメータであるシグマ二乗を選ぶ事だ。 これはバイアス-バリアンスのトレードオフの話でちょっと触れたが、 シグマ二乗が大きければ、 高バイアス、低バリアンスの傾向の 分類器となり、 逆にシグマ二乗が 小さければ、 高バリアンス、低バイアスの傾向の分類器となる。 では、どんな時にガウスカーネルを使う事になるだろう？ それはだね。あなたの元々の フィーチャーxが、 Rnとして、 nが小さい時、そして理想的には、 mが大きい時。 つまり、例えば二次元の トレーニングセットだとして、 これは以前に書いた物と同様という事だが、 つまりn=2だとして、だがトレーニングセットの総数は多いという場合。 つまり、かなりたくさんの トレーニング手本を描いた。 するとその場合、 もっと複雑な非線形の決定境界にフィットするような カーネルを使いたくなる場合がある。 そしてガウスカーネルはこれを行う良い方法だ。 このビデオの最後の方で 線形カーネルを使う場合はどういう時か ガウスカーネルを選ぶのはどういう場合か、 などについて、話す機会がある。 だが、もし具体的に ガウスカーネルを使う、と決めたとすると、 あなたがやるべき事はこれだ。 あなたの使うサポートベクタマシンの ソフトウェアパッケージによっては、 カーネル関数を、あるいは類似度関数を あなたが実装する必要が あるものもある。 もしあなたがoctaveやMATLABの SVMの実装を用いるなら、 カーネルの具体的なフィーチャーを 計算する関数を あなたが提供する必要があるかもしれない。 これは本当は、 fの下付き添字iを ある特定の値に対して計算する、 ここでこのfは 単一の実数値にすぎない、 だからここでは、f(i)と書くべきかもしれない。 だがあなたがしなくてはいけない事は、 これ、トレーニング手本やテスト手本や、 とにかくなんであれ あるベクトルxを 入力として受け取り、 そしてランドマークの一つを 入力として 受け取るが、 ここでは私は単にこれらを x1, x2と呼ぶ、 何故ならランドマークも実際はトレーニング手本だから。 あなたがやらなくてはいけない事は、 この入力、x1とx2をうけとり それらの間の、この種の類似度関数を 計算して 一つの実数を返す ソフトウェアを書く事だ。 そこで幾つかのサポートベクタマシンのパッケージは あなたがこの種のカーネル関数、 x1とx2を入力に取って実数を返すような関数を 提供する事を期待していて、 そしてそこから、 自動的に全てのフィーチャーを生成して、 つまり自動的に、xに対し あなたの書いたこの関数を用いて f1, f2, ...とf(m)まで、 マッピングする。 そして全てのフィーチャーを生成して、 そこからサポートベクタマシンを訓練する。 だが、あなたは時々、 この関数を自分で提供してやる必要がある。 ガウスカーネルを使う時でも、幾つかのSVM実装はガウスカーネルも含んでいて、 そして それ以外にも幾つかのカーネルを含んでいる物だ。 ガウスカーネルは恐らくもっとも一般的なカーネルなので、 ガウスカーネルと線形カーネルは 本当にぶっちぎりで大人気の二大カーネルなので、たぶんあるだろう。 一つちょっとした実装のノートを。 もしあなたのフィーチャーが、 とても異なるスケールだったら、 ガウスカーネルを使う前に フィーチャースケーリングをするのが大切だ。 そして以下がその理由だ。 あなたがxとlの間の ノルムを計算している所を、想像してみよう。 このここの項は、 こっちの分子の項だ。 これが行う事は、 xとlの間のノルムで、 それは実際には、、、、ベクトルvを計算してみよう。 vはイコール x-lだとする。 そして次に、 このベクトルvのノルムを計算する。 それはxとlとの間の 差だ。 するとvのノルムは実際には イコール v1二乗、 足すことの v2二乗、 足すことの、点点点、足すことのvn二乗。 ここでxはRn、 いや、R n+1だが、 x0は無視する事にする。 そこでxはRnだとしよう、 左側を二乗すると これは正しくなる。 そうすれば、これはイコール こっちとなる。 別の書き方で書くと、 これはx1 - l(1)の二乗に、 足すことの x2-l(2)の二乗に、 足すことの 点点点、、、と足すことの xn - l(n)の二乗。 そして今、あなたのフィーチャーが とても異なったレンジの値を取るとしよう。 住宅の価格を予測する例で 考えてみると、 データは住宅に関するデータで、 ｘは、 最初のフィーチャーx1に関しては 何千平方フィートの範囲を 取り、 しかし二番目のフィーチャーx2は寝室の数とすると、 寝室の数は範囲としては 1部屋から5部屋程度だろう、 するとx1-l(1)は巨大になりえて、 1000平方フィートとかに成り得るが、 一方でx2-l(2)は もっと小さくなり、 その場合には、この項、 これらの距離は、 本質的にはほとんど住居のサイズで 支配されてしまい、 寝室の数はほとんど無視されてしまう。 だから学習がうまくいくように これを避ける為には、 フィーチャースケーリングをせよ。 そうする事で、SVMが 様々なフィーチャーについて 同じような程度の関心を払うようにし、 この例の住居のサイズのように 他のフィーチャーを 塗りつぶしてしまうような事が無い事を、保証出来る。 あなたがサポートベクタマシンを試みる時に、 おそらくもっとも一般的であろう 二つのカーネルは、 線形(linear)カーネル、つまり カーネル無しか、 または既に話したガウスカーネルだろう。 そしてちょっと注意を。 類似度関数ならば、 どんな物でも カーネルとして使える、という訳では無い。 そしてガウスカーネル、線形カーネル、 そしてたまに他の人が使ってるのを見かける その他のカーネルでも、 それらはある技術的条件を満たす必要がある。 それはMercerの定理と言われる物だ。 これが必要となる理由は、 サポートベクタマシンのアルゴリズム、 全てのSVMの実装は、 大量の賢い 数値計算的な最適化のトリックを使ってる、 パラメータのシータについて 効率的に解く為に。 そしてオリジナルのSVMの設計において、 主な関心を、Mercerの定理と呼ばれる 技術的条件を満たすカーネルだけに 集中する、という決定がなされた。 そうする事で、 それらSVMパッケージの全てで、 それら全てのSVMソフトウェアパッケージで 大きなクラスの最適化を 用いる事が出来るようになり、 パラメータのシータをとても早く得られるようになる。 さて、結局の所、ほとんどの人が使うのは 線形カーネルか ガウスカーネルだ。 だがそれ以外にも幾つか Mercerの定理を満たすカーネルが存在する。 そして他の人々がそれらを使ってる所を あなたがみかける事もあるかもしれない。 私個人としては、それ以外のカーネルを使う事ってほんとにほんとーに稀で、全く無いって事は無いにせよほとんど無いけど。 あなたが遭遇するかもしれない幾つかのカーネルをちょっとだけ言及しておくと、 一つ目は多項式カーネル。 その場合、xとlの間の 類似度は、 どう定義されるかというと、 たくさんの選択肢があるが、 xの転置 l の二乗という定義がありうる。 これは、xとlがどれだけ類似してるかを測る、一つの指標である。 xとlがお互いにとても近いと、 内積は大きくなる傾向にある。 このカーネルは ちょっと珍しいカーネルで、 そんなには使われていない。 だがたまには、使ってる人に出くわす事もあるかもしれない。 これは多項式カーネルの一つのバージョンだ。 もう一つ別の物としては、x転置 l の三乗。 これらは全て、多項式カーネルの例だ。 x転置 l + 1 の三乗、 x転置 l 足すことの 1以外の数でも良い。5とか。 そして4乗。 つまり、多項式カーネルは実際には二つのパラメータがある。 一つはどの数をここに足すか？ これは0でも良い。 ここには実はプラス0がある。
同様に、ここの、多項式の次数(degree)がある。 つまり累乗の指数。そしてこれらの数字。 多項式カーネルの より一般的な形としては、 x転置 l 足すことの ある定数(const) そして指数のある次数(degree)。 つまりこれら二つの両方が 多項式カーネルのパラメータだ。 多項式カーネルは、ほとんどいつでも ガウスカーネルよりも より劣ったパフォーマンスを示す。 だからそんなに使われない。でもたまに出くわす事がある物ではある。 通常、これはxとlが 全て0より大きい、 非負のデータに対してのみ使われる。 そうする事で、これらの内積が 決して負にならない事を担保している。 そしてこれは、xとlがとてもお互いに近いと それら同士の内積が大きくなるだろう、という 直感を捕捉している。 他の性質もあるんだが、 何にせよ人々はあまり使ってない。 そしてさらに、あなたがやっている事に応じて、 ある種より難解なカーネルも 存在して、あなたがそれらに遭遇する事もあるかもしれない。 文字列カーネルというのがある、 これはあなたの入力データがテキスト文字列なり、 それ以外でも文字列の時には、時々使われる事がある。 カイ二乗カーネルという物もあり、 ヒストグラム交差カーネルという物などもある。 これらは異なるオブジェクト同士の 類似度を測る事が出来る ある種より難解なカーネルだ。 例えば、あなたがある種の テキスト分類の問題に挑んでいる時には、 入力xが文字列となるので、 我らは二つの 文字列の間の 類似度を、文字列カーネルを用いる事で 知りたいかもしれない。 だが私個人としては、これらのより難解なカーネルは 全く見ないとは言わないが、とても稀にしか見ない。 カイ2乗カーネルを、たぶん私は生涯で、 一回しか使ったことが無いと思うし、 ヒストグラムカーネルは 人生で一回か二回だったと思う。 私は実は、文字列カーネルを自分で使った事は無い。 しかしもしあなたが別のアプリケーションでこれらに遭遇したら、 ちょろっとwebを検索して、 Googleで軽く検索するなり、 Bingで軽く検索すれば、 これらのカーネルの定義を見つける事が出来るはずだ。 このビデオで話しておきたい細かい話が最後に二つ。
一つはマルチクラスの分類問題について。 4つのクラスがあったとしよう、あるいはもっと一般的に k個のクラスの出力があったとしよう。
複数のクラス間の適切な決定境界を SVMにどのように計算させたら良いだろうか？ ほとんどのSVM、とは言い過ぎだが多くのSVMパッケージは、既にマルチクラスの分類の機能が ビルドインされている。 だからもしあなたがそのようなパッケージを使っているなら、 あなたは単に、ビルドインの機能を 使うだけで、 うまくいくはずだ。 そうで無ければ、これを行う方法の一つには、 1 vs allの手法を 使う事だ、これについては ロジスティック回帰を作ってる時に議論した。 あなたがやる事は、K個のSVMを トレーニングする、もしあなたがKクラス あったとしてだが、一度に一つのクラスを それ以外のクラスと区別するように。 そしてこれは、K個のパラメータベクトルを与える。 つまりこれは、あなたに パラメータベクトルのシータ1を与え、 これはクラスy=1を それ以外のクラスから区別しようと 試みる物で、 そして次に二番目のパラメータベクトル、 シータ2を与え、これは y=2を陽性のクラスとした時に そしてそれ以外全てを陰性のクラスとした時に 得られるパラメータで、 そうやってパラメータベクトル シータKまで、 これは最後のクラスKを それ以外と区別する パラメータベクトルだ。 最後に、これはまさに ロジスティック回帰でやった 1 vs ALL法だ。 そこではあなたは シータ転置 xが最大になったクラスiを 予想とするのだった。以上がマルチクラスの分類の方法だ。 だがもっと一般的なケースとしては、 だいたいは、かなり良い確率で、 あなたが何のソフトウェアパッケージを使ってるにせよ、 だいたいは、 かなりの確率で、そこには 既にマルチクラスの機能がビルドインされている。 だからこの事について思い悩む必要は無い。 最後に、我らはサポートベクタマシンを ロジスティック回帰から始めて、 コスト関数をちょっと変更していく事で開発してきた。 このビデオで最後にやりたい事は、これらの二つのアルゴリズムの どちらをいつ使うのか、について ちょっと話しておきたい。 nをフィーチャーの数として、 mをトレーニング手本の数とする。 さて、いつどちらのアルゴリズムを使い、いつもう一つのアルゴリズムを使うべきだろう？ もしnがトレーニングセットサイズとの相対的に 大きければ、 例を挙げると、 フィーチャーの数が これがmよりずっと 大きくて、 そしてこれが、例えば、 テキスト分類の問題だとすると、 フィーチャーのベクトルの次元は、 知らんけど例えば1万とかになりがちで、 そしてトレーニングセットのサイズが 10から、 せいぜい1000くらいまでの間。 スパム分類の問題を 想像してみよう。 e-mailスパムでは、1万の単語に対応した 1万個のフィーチャーがあるとする、 でも例えば10通からせいぜい1000通とかの トレーニング手本しか無いとする。 つまり、nはmとの相対で考えると大きい。 その場合に私が普段やるのは、 ロジスティック回帰を使うか、 カーネル無しの、あるいは線形カーネルの SVMを 使う。 何故なら、そんなにたくさんのフィーチャーで、 トレーニングセットが小さいと、 線形関数はたぶんいい感じだと思う、 そしてとても複雑な非線形の関数を フィッティングするには 十分なデータを持ってない。 今、もしnが小さくて、 mが中くらいの大きさだと、 ここでnとしては 1から1000くらいを イメージしてて、1はとても小さい場合だが、 せいぜい1000フィーチャーくらいまで、 そして トレーニング手本の総数が 10から1万手本くらいの間の どこか位。 5万くらいまでの間でもいいかもしれない。 mがとても大きければ、1万くらい。だが100万は行かない。 さて、mがもし 中くらいのサイズの時は、 しばしばSVMに線形カーネルが、うまく機能するだろう。 この場合については以前にも話した、 一つの具体例で。 それは、もしあなたが二次元の トレーニングセットの時に、 つまり、もしn=2の時で、 たくさんのトレーニング手本が描いてある時などは、 ガウスカーネルは、陽性と陰性のクラスを分離するのに きわめて良い働きをするだろう。 興味のある三番目の状況としては、 nが小さくて mが大きい時。 例えばnは1から1000までとか、 もうちょっと大きくてもいいかもしれないが、 しかしmは5万くらいから 100とかまでとか、 5万から10万、100万、200万、と、 とても大きなトレーニングセットのサイズの時、 この場合は、 ガウスカーネルのSVMは 走らせるといくらか遅い。 こんにちのSVMパッケージでは、 ガウスカーネルを使うと、 5万くらいなら、 たぶん問題無い。 だが、100万個のトレーニング手本だと、 または10万個のトレーニング手本で、かつ nが大きい値の時には、 こんにちのSVMパッケージはとても良い物だが、 それでも大量の、本当に大量の トレーニングセットの時には ガウスカーネルを使うとちょっとだけ苦戦するかもしれない。 その場合には、 私が普段やるのは、 手動でフィーチャーを増やして ロジスティック回帰を使うか、 カーネル無しのSVMを 試す。 そしてもしあなたがこのスライドを見て ロジスティック回帰とカーネル無しのSVMが これらの場所がいつも ペアで一緒に出てきてると 思ったなら、 それには理由があるのだ。 ロジスティック回帰とカーネル無しのSVM、 これらは実際にはかなり 似たアルゴリズムだ。 ロジスティック回帰もカーネル無しのSVMも 通常は極めて似た 振る舞いをする。そして極めて似た パフォーマンスを与える。 だが実装の詳細によっては、片方がもう片方よりも効率的だったりはするかもしれない。 だが、これらのアルゴリズムの 一つを適用する時、 ロジスティック回帰とカーネル無しのSVMの どちらも、だいたい同様に 機能する。 だがSVMの威力は、 複雑な非線型の関数を 学習する為に 別のカーネルを使う時に発揮される。 そしてこの形態では、 1万手本くらいまでとか、5万手本くらいまでとかで、 そしてフィーチャーの数は、 ちょうど良い程度に多い、 これはとても良くある形態だ。 そしてこの状況こそ、 カーネルと共にサポートベクタマシンを用いる事が、光り輝く形態だ。 ロジスティック回帰を使ってではもっと大変になるような事を やる事が出来る。 そして最後に、ニューラルネットワークが適合するのはどういう形態か？ うーん、これら全ての問題に対して、 これらの別々の形態 全てに対して、 良く設計されたニューラルネットワークは、うまく機能するだろう。 一つの欠点としては、あるいは ニューラルネットワークを使わない事がある 理由の一つには、 これらの問題の中には、 ニューラルネットワークは訓練するのに遅いという場合がある。 だがもしとても良い SVM実装パッケージがあるなら、 そちらの方が速く走りうる、ニューラルネットワークよりずっと速く走る。 そして、これは証明しなかったが、 SVMの持つ 最適化の問題は、 凸最適化問題である事が 知られている。 だから良いSVM最適化ソフトウェアパッケージは 必ずグローバル最小か、それに近い値を 探してくれる。 だからSVMでは、ローカル最適の問題を心配する必要は無い。 現実問題としては、ニューラルネットワークでも ローカル最適はそんなに大きな問題では無い。 だけど、、、あー、 とにかくSVMを使えば、心配事が一つ減ると言えば減る。 そして問題によっては、 ニューラルネットワークの方が遅いかもしれない、 特にこの形態の問題では、SVMよりも遅いかもしれない。 ここに示したガイドラインが ちょっと曖昧だなぁ、と思っても、 もしあなたが何かの問題に際して、 ガイドラインがいまいち曖昧だなぁ、と思っても、 私もいまだに完全には確信が持てる訳では無い、 私はこっちのアルゴリズムを使うべきか？ あるいはあっちのアルゴリズムを使うべきか。それは実際には問題無い。 私が機械学習の問題に直面する時には、 時々、 どのアルゴリズムを使うのがベストなのか、 はっきりしない事がある。 だが以前のビデオで見た通り、 アルゴリズムは確かに重要だけど、 しかししばしばもっと重要なのは、 どれだけの量のデータを持ってるか、 そしてどれだけあなたの技術力が高いか、 エラー分析や学習アルゴリズムをデバッグするのを どれだけうまく行えるか、 新しいフィーチャーをどうデザインする方法を どれだけうまく見いだせるか、 学習アルゴリズムに渡す別のフィーチャーをどれだけ上手く見いだせるか、などだったりする。 そしてしばしば、これらの事項は あなたがロジスティック回帰を使うかSVMを使うか、よりも もっと重要な事だろう。 だが、言ってきたように、 SVMはいまだに、 もっとも強力な学習アルゴリズムの一つだと 広く受け止められている。 そしてSVMが複雑な非線型の関数を とても良く学習出来るような事態がある。 そして私も実際、、、 ロジスティック回帰、ニューラルネットワーク、SVM、 これら三つの学習アルゴリズムを 使えるなら、私が思うにあなたは とても広範な応用に対し 最先端の機械学習システムを 構築するのに とても良い位置に居る。 そしてこれはもう一つ、あなたの武器庫に備えておくのに良い、とてもパワフルなツールという訳だ。 この一つは、シリコンバレー中で 業界中で、 アカデミックな世界で、 多くの高パフォーマンスの機械学習システムを作るのに 使われている物だ。