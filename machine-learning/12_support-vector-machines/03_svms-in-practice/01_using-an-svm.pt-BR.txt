Até agora, falamos sobre SVMs num nível um tanto abstrato. Neste vídeo, eu gostaria de falar sobre o que é realmente preciso para se usar uma SVM. O algoritmo de máquina de vetores de suporte apresenta um certo problema de otimização. Mas, como mencionei brevemente num vídeo anterior, não aconselho escrever o seu próprio software para resolver os parametros Θ. Da mesma forma que, hoje em dia, poucos ou quase nenhum de nós pensaria em escrever seu próprio código para inverter uma matriz, ou fazer a raiz quadrada de um número, (usamos uma função de uma bilbioteca para isso) da mesma forma, o software para resolver o problema de otimização de SVM é muito complexo , e existem pesquisadores que estudam por muitos anos somente sobre otimização numérica, para criar boas bibliotecas de software, bons pacotes de software que façam isto. Eu recomendo fortemente que você use uma das bibliotecas de software altamente otimizadas, em vez de tentar implementar algo sozinho. E existe um monte de bibliotecas boas por aí. As duas que eu mais uso são "liblinear" e "libsvm", mas existem muitas bibliotecas de software boas para fazer isso, que você pode usar em muitas das linguagens de programação mais comuns que você pode usar para escrever um algoritmo de aprendizagem. Mesmo que você não deva escrever seu próprio software de otimização para SVM, existem algumas coisas que você deve fazer. A primeira é fazer uma escolha para o parâmetro "C". Conversamos um pouco sobre as propriedades de viés e variância anteriormente. Segundo, você também precisa escolher o kernel, a função de similaridade que você quer utilizar. Uma escolha pode ser não utilizar nenhum kernel. A ideia de não usar kernel é também chamada "kernel linear". Assim, se alguém disser que usa uma SVM com um kernel linear, o que isso significa é que usam uma SVM sem utilizar um kernel. É uma versão de SVM que simplesmente estima "y = 1" quando "θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0". θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0". θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0". O termo "linear" pode ser entendido percebendo que esse é o kernel que fornece um classificador linear comum. Essa seria uma escolha razoável para alguns problemas, e muitas bibliotecas de software, como "liblinear", que é um exemplo de vários, de uma biblioteca de software que consegue treinar uma SVM sem usar um kernel, usando um kernel linear. Mas por que você faria isso? Se você tiver um número grande de parâmetros, se "n" é grande, e o número de exemplos de treinamento, "m", é pequeno, você você tem um número enorme de recursos, pois "x" pertence a "ℝⁿ" ou "ℝⁿ⁺¹". Se você já tem um número grande de recursos, com um conjunto de treino pequeno, talvez você queira só ajustar uma fronteira de decisão linear, sem tentar encontrar uma função não-linear complicada, porque você pode não ter dados suficientes. E você corre o risco de sobreajustar se tentar ajustar uma função muito complicada em um espaço de dimensão alta e o conjunto de treino é pequeno. Esse é um cenário onde você pode decidir não usar um kernel, ou, equivalentemente, usar um kernel linear. Outra escolha para o kernel que você pode fazer é o kernel gaussiano, que vimos anteriormente. Se você fizer isso, a outra escolha que precisará fazer é a do parâmetro "σ²", que influencia a relação de viés e variância, pois quando "σ²" é grande, tende-se a encontrar um classificador de alto viés e baixa variância, mas se "σ²" é pequeno, o classificador tem variância maior e viés menor. Quando você escolheria um kernel gaussiano? Bom, se a dimensão dos recursos "x", em "ℝⁿ", é pequena, e, idealmente, "m" é grande, é como quando temos, digamos, um conjunto de treino com duas dimensões, como o exemplo que desenhei antes. Se "n = 2", mas o conjunto de treino é grande. Eu desenhei um número mais ou menos grande de exemplos de treinamento, nesse caso você talvez queira usar um kernel que se ajuste a uma fronteira de decisão mais complexa. O kernel gaussiano seria uma boa maneira de fazer isso. Mais para o fim do vídeo, falarei um pouco sobre quando você pode escolher um kernel linear, gaussiano, ou outro. Mas na prática, se você decidir usar um kernel gaussiano, isso é o que você precisa fazer. Dependendo do pacote de software para máquinas de suporte de vetores que você usa, ele pode pedir a você que implemente uma função de kernel, uma função de similaridade. Assim, se você está usando uma implementação de MATLAB ou Octave para SVM, eles podem pedir que você forneça uma função para computar uma característica do kernel. Isso está calculando "fᵢ" para um valor específico de "i", enquanto "f" aqui é só um número real, então talvez isto estaria melhor escrito como "fᵢ", mas o que você precisa fazer é escrever uma função de kernel que toma esta entrada, um exemplo de treinamento, ou um exemplo de teste, o que seja, ela recebe um vetor "x⁽ⁱ⁾" e também recebe uma das referências, mas eu escrevi "x1" e "x2" aqui porque as referências são exemplos de treinamento também. Mas o que você precisa fazer é escrever software que tome essas entradas, "x1" e "x2", e computa uma medida de similaridade entre eles, e retorna um número real. Assim, alguns pacotes de máquinas de vetores de suporte esperam que você forneça essa função de kernel que toma como entrada "x1" e "x2" e retorna um número real. O pacote parte daí e gera todos os recursos automaticamente. Ou seja, automaticamente toma "x" e mapeia para "f₁", "f₂", até "fₘ" usando essa função que você escreveu, gera todos os parâmetros e treina a máquina de vetores de suporte daí pra frente. Mas às vezes você mesmo precisa fornecer essa função. Mas se você estiver usando o kernel gaussiano, algumas implementações de SVM incluem esse kernel e alguns outros também, já que o gaussiano é provavelmente o mais comum. Os kernels gaussiano e linear são de longe os dois mais populares. Só um comentário de implementação. Se você tiver recursos com escalas muito diferentes, é importante normalizar os parâmetros antes de usar o kernel gaussiano. E aqui está o porquê. Vamos pensar no cálculo da norma da diferença entre "x" e "l", este termo aqui, que corresponde ao numerador ali. O valor da norma entre "x" e "l" é o comprimento do vetor "v", que é igual a "x - l". É, portanto, "||v||", ou seja, "||x - l||". A norma de "v" é igual a "v₁² + v₂² + ··· + vₙ²". igual a "v₁² + v₂² + ··· + vₙ²". igual a "v₁² + v₂² + ··· + vₙ²". Isso porque "x" está em "ℝⁿ", ou "ℝⁿ⁺¹", mas eu vou ignorar "x₀". Vamos imaginar que "x" está em "ℝⁿ", e eu errei aqui, faltou um quadrado do lado esquerdo para ficar correto. Então isto é igual àquilo, certo? Escrito de forma diferente, isto é "(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²". "(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²". "(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²". "(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²". "(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²". Agora, se seus recursos tomam escalas de valores muito diferentes, por exemplo no caso de estimar os valores de casas, se seus dados são sobre casas. Se "x" está na ordem de grandeza de milhares de pés quadrados no primeiro recurso, "x₁", mas o seu segundo recurso "x₂", é o número de quartos, este está na faixa de um a cinco quartos, então "(x₁ - l₁)²" será enorme. Isso poderia ser algo como "1000²", enquanto "x₂ - l₂" será muito menor, e, se esse for o caso, o segundo termo, do número de quartos, será quase completamente dominado pelos tamanhos das casas e o número de quartos será praticamente ignorado. Assim, para evitar isso e fazer com que a SVM funcione bem, normalize os recursos. Isso irá garantir que a SVM dê uma atenção comparável a todos os diferentes recursos, e não só se concentrar em um, como nesse exemplo dos tamanhos das casas, ignorando os outros recursos. Quando você aplicar uma máquina de vetores de suporte, é muito provável que você usará um dos dois kernels que vimos, o kernel linear (sem kernel) ou o kernel gaussiano. Só um aviso, nem todas as funções de similaridade em que você pode pensar são kernels válidos. Todos os kernels que você usar, o gaussiano, o linear, ou outros, todos devem satisfazer uma condição técnica. Ela é chamada "teorema de Mercer", e as razões para isso são os truques de otimização numérica que algoritmos que implementam SVM fazem para achar a solução dos parâmetros "θ" eficientemente. No design original de SVMs, tomou-se a decisão de restringir a atenção somente a kernels que satisfazem essa condição técnica chamada teorema de Mercer. O que isso faz é garantir que todos os pacotes software para SVM podem usar a grande quantidade de otimizações e encontrar o parâmetro "θ" rapidamente. Assim, a maioria das pessoas acaba usando o kernel linear ou o gaussiano, mas existem alguns outros kernels que também satisfazem o teorema de Mercer e que você pode encontrar pessoas utilizando, apesar de que eu, pessoalmente, usar outros kernels muito raramente, ou nunca. Só para citar alguns outros kernels que você pode encontrar. Um é o kernel polinomial. Nesse a similaridade entre "x" e "l" pode ser definida de várias formas, mas pode-se tomar "(x' · l)²". Essa é uma medida de similaridade para "x" e "l". Se "x" e "l" são muito próximos, o produto interno tende a ser grande. Esse é um kernel um pouco incomum. Ele não é usado com muita frequência, mas você pode encontrar algumas pessoas utilizando. Essa é uma versão do kernel polinomial. Outra é "(x' · l)³". Estes são todos exemplos de kernels polinomiais: "(x' · l + 1)³", "x' · l" mais um número diferente de 1, por exemplo 5 e tudo elevado à quarta potência. Assim, o kernel polinomial tem dois parâmetros. Um é o número que adicionamos aqui. Pode ser 0. Aqui estamos adicionando 0; e o outro parâmetro é o grau do polinômio. Assim, os parâmetros são o grau e esses números. A forma mais geral do kernel polinomial é "x · l" mais uma constante, e a soma elevada a algum grau no expoente, portanto, esses são os dois parâmetros do kernel polinomial. O kernel polinomial quase sempre tem pior desempenho que o kernel gaussiano e não é muito utilizado, mas é algo que você pode encontrar por aí. Normalmente é usado só em dados onde "x" e "l" são estritamente não negativos, o que garante que os produtos internos nunca são negativos. E isso captura a intuição de que se "x" e "l" são muito similares, o produto interno deles será grande. Eles têm algumas outras propriedades também, mas não são muito usados. Além disso, dependendo do que você está fazendo, existem outros kernels mais esotéricos por aí. Existe o kernel de strings, que é usado às vezes quando as entradas são strings de texto, ou outros tipos. Existem coisas como o kernel chi-quadrado, o kernel de interseção de histograma, e por aí vai. Existem kernels mais esotéricos que você pode usar para medir similaridade entre objetos diferentes. Por exemplo, se você está tentando resolver algum problema de classificação de texto, onde a entrada "x" é uma string, talvez você queira encontrar a similaridade entre duas strings usando o kernel de strings, mas eu, pessoalmente, uso esses kernels mais esotéricos muito raramente. Eu acho que usei o kernel chi-quadrado talvez uma vez na vida, e o kernel de histograma umas duas. Eu nunca usei o kernel de strings. Mas caso você encontre isso em outras aplicações, se você fizer uma busca rápida no Google ou Bing, você deve encontrar definições desses outros kernels. Dois últimos detalhes que quero falar sobre neste vídeo.
Um é sobre classificação multi-classe. Se você tem 4 classes, ou em geral, "K" classes, como você faz com que a SVM gere fronteiras de decisão apropriadas entre as múltiplas classes? Muitos pacotes de SVM têm funcionalidade de classificação multi-classe internamente. Se você estiver usando um pacote como esse, use a funcionalidade dada, e tudo deve funcionar. Senão, uma maneira de fazer isso é usar o método "um contra todos" que discutimos quando estávamos desenvolvendo regressão logística. Você precisa treinar "K" SVMs se você tem "K" classes, uma para distinguir cada classe do restante. Isso te fornecerá "K" vetores de parâmetros, ou seja, "θ⁽¹⁾", que tenta distinguir a classe "y = 1" do restante, e o segundo vetor de parâmetros, "θ⁽²⁾", é o que você encontra quando "y = 2" é a classe positiva e as outras todas são a negativa, e por aí vai até o vetor de parâmetros "θ⁽ᴷ⁾", que é o vetor de parâmetros para distinguir a última classe, "K", do restante. Isso é exatamente o mesmo que o método "um contra todos" da regressão logística, onde você estima a classe "i" com o maior "θ⁽ⁱ⁾' · x". Essa é a classificação multi-classe para SVMs. Para os casos mais comuns, onde há uma boa chance que o pacote de software que você usar já tem internamente funcionalidade de classificação multi-classe, você não precisa se preocupar com isso. Finalmente, nós desenvolvemos máquinas de vetores de suporte começando com regressão logística e modificando a função custo. A última coisa que quero fazer neste vídeo é falar um pouco sobre quando você vai usar um algoritmo ou o outro. Digamos que "n" é o número de recursos e "m" é o número de exemplos de trinamento. Quando nós usamos um algoritmo em vez do outro? Bom, se "n" é maior relativo ao tamanho do seu conjunto de treino, por exemplo, se você tomar um caso onde o número de recursos é muito maior que "m", por exemplo, se você tiver um problema de classificação de texto, onde a dimensão do vetor de recursos é algo como 10000, e o seu conjunto de treino é algo entre 10 e 1.000. Imagine um problema de classificação de e-mails em spam, onde você tem 10.000 recursos, correspondendo a 10.000 palavras, mas você tem algo entre 10 e 1.000 exemplos. Se "n" é grande relativamente a "m", o que eu geralmente faço é usar regressão logística, ou uso a SVM sem kernel, com kernel linear. Porque se você tem tantos recursos com um conjunto de treino pequeno, uma função linear provavelmente vai funcionar, e você não tem dados suficientes para ajustar uma função não-linear complicada. Agora, se "n" é pequeno e "m" é intermediário, talvez "n" seja algo até 1.000 recursos algo até 1.000 recursos e o número de exemplos de treinamento é algo entre 10 e 10.000, ou até 50.000. Se "m" é grande, como 10.000, mas não um milhão. Se "m" é um tamanho intermediário, normalmente uma SVM com kernel gaussiano funciona bem. Nós falamos sobre isso antes também, com um exemplo, o caso onde o conjunto tem duas dimensões. Se "n = 2", onde você tem um número grande de exemplos de treinamento. um kernel gaussiano funcionará bem para separar as classes positivas e negativas. Um terceiro caso que é de interesse é quando "n" é pequeno e "m" é grande. Aqui "n" é, novamente algo menor que 1.000, mas pode ser maior. Mas se "m" é algo maior que 50.000, por exemplo, 50.000, 100.000, um milhão, um trilhão. Você tem um conjunto de treino muito, muito grande. Se esse é o caso, uma SVM com kernel gaussiano vai executar meio lentamente. Os pacotes de SVM de hoje, se você usar um kernel gaussiano, tendem a fazer bastatnte esforço. Se você tem algo como 50.000, tudo bem, mas se é um milhão, ou até uns 100.000, um valor enorme de m, os pacotes de SVM de hoje são bons, mas podem ter dificuldade com um tamanho de conjunto de treino massivo usando um kernel gaussiano. Nesse caso, o que eu normalmente faço é tentar criar manualmente mais recursos, e então usar regressão logística ou uma SVM sem kernel. Se você olhar esse slide e vir regressão logística e SVM sem kernel nesses dois lugares, os dois juntos, existe uma razão para isso. A regressão logística e SVM sem kernal são algoritmos bem parecidos, e tanto regressão logística quanto SVM sem kernel vão fazer coisas bem parecidas e ter desempenho bem similar, mas, dependendo dos detalhes de implementação, uma delas pode ser mais eficiente que a outra. Entretanto, quando um dos algoritmos se aplica, regressão logística ou SVM sem kernel, o outro deve funcionar bem. Mas muito do poder das SVMs vêm quando você usa kernels diferentes para aprender funções não-lineares complexas. Nesse caso, onde você tem até 10 ou 50.000 exemplos, e o número de recursos é razoavelmente grande, é um cenário bastante comum e é o caso onde a máquina de vetores de suporte deve brilhar. Você pode fazer coisas que são muito mais difíceis de se fazer com regressão logística. Finalmente, onde que as redes neurais se encontram? Para todos esses cenários, uma rede neural bem feita deve funcionar bem também. A única desvantagem, a razão que pode nos impedir de usar redes neurais é que para alguns desses problemas a rede neural pode ser lenta para se treinar. Mas se você tiver um pacote de SVM bom, ele pode rodar mais rápido, um bom tanto mais rápido que a rede neural. E, apesar de não termos mostrado isso antes, acontece que o problema de otimização da SVM é um problema de otimização convexa, portanto bons pacotes de software de otimização para SVM vão sempre achar o mínimo global ou algo próximo dele. Portanto, com SVMs você não precisa se preocupar com ótimos locais. Na prática, ótimos locais não são um grande problema para redes neurais, mas isso é uma coisa a menos para se preocupar se estiver usando SVM. Dependendo do seu problema, a rede neural pode ser mais lenta, principalmente nesse tipo de cenário, que uma SVM. Caso as orientações que dei aqui pareçam vagas e você olhar para um problema e as orientações são vagas, não tenho certeza se devo usar este algoritmo ou aquele, tudo bem. Quando eu olho para um problema de aprendizado de máquina, às vezes não é claro qual é o melhor algoritmo para se usar, mas, como você viu em vídeos anteriores, o algoritmo faz diferença, mas frequentemente o que mais faz diferença são coisas como a quantidade de dados que você tem. E a sua habilidade, o quão bom você é em fazer análise de erros e fazer debugging em algoritmos de aprendizagem, descobrir como criar novos recursos, descobrir que recursos fornecer ao seu algoritmo de aprendizagem, e etc. Frequentemente, essas coisas vão fazer mais diferença do que se você está usando regressão logística ou SVM. Mas, tendo dito isso, a SVM é amplamente reconhecida como um dos algoritmos de aprendizagem mais poderosos, e existe esse cenário onde existe uma maneira muito eficiente de aprender funções lineares complexas. Então, com regressão logística, redes neurais, SVMs, usando esse algoritmos de aprendizagem, acredito que você está em uma posição muito boa para construir sistemas de aprendizado de máquina de ponta para uma larga gama de aplicações, e essa é outra ferramenta poderosa para se ter no seu arsenal. Uma que é usada por todo canto no Vale do Silício, na indústria e na academia, para construir muitos sistemas de aprendizado de máquina de alto desempenho.