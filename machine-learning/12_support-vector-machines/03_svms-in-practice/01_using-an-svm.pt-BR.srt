1
00:00:00,140 --> 00:00:01,310
Até agora, falamos sobre

2
00:00:01,640 --> 00:00:03,290
SVMs num nível um tanto abstrato.

3
00:00:03,980 --> 00:00:05,030
Neste vídeo, eu gostaria de

4
00:00:05,200 --> 00:00:06,460
falar sobre o que é realmente preciso

5
00:00:06,740 --> 00:00:09,410
para se usar uma SVM.

6
00:00:11,320 --> 00:00:12,300
O algoritmo de máquina de vetores de suporte

7
00:00:12,850 --> 00:00:14,870
apresenta um certo problema de otimização.

8
00:00:15,530 --> 00:00:16,940
Mas, como mencionei brevemente num

9
00:00:17,120 --> 00:00:18,150
vídeo anterior, não aconselho

10
00:00:18,380 --> 00:00:20,570
escrever o seu próprio

11
00:00:20,630 --> 00:00:22,810
software para resolver os parametros Θ.

12
00:00:23,950 --> 00:00:26,110
Da mesma forma que, hoje em dia,

13
00:00:26,420 --> 00:00:27,730
poucos ou quase nenhum

14
00:00:28,090 --> 00:00:29,400
de nós pensaria em

15
00:00:29,530 --> 00:00:31,680
escrever seu próprio código para inverter uma matriz,

16
00:00:31,950 --> 00:00:33,940
ou fazer a raiz quadrada de um número,

17
00:00:34,190 --> 00:00:36,570
(usamos uma função de uma bilbioteca para isso)

18
00:00:36,700 --> 00:00:38,090
da mesma forma,

19
00:00:38,850 --> 00:00:40,310
o software para resolver o problema

20
00:00:40,620 --> 00:00:42,200
de otimização de SVM

21
00:00:42,440 --> 00:00:43,880
é muito complexo , e existem pesquisadores

22
00:00:43,990 --> 00:00:44,960
que estudam

23
00:00:45,110 --> 00:00:47,560
por muitos anos somente sobre otimização numérica,

24
00:00:47,850 --> 00:00:48,960
para criar

25
00:00:49,150 --> 00:00:50,550
boas bibliotecas de software, bons

26
00:00:50,930 --> 00:00:52,270
pacotes de software que façam isto.

27
00:00:52,470 --> 00:00:53,480
Eu recomendo fortemente que você

28
00:00:53,860 --> 00:00:55,260
use uma das bibliotecas de software

29
00:00:55,710 --> 00:00:57,780
altamente otimizadas, em vez de tentar implementar algo sozinho.

30
00:00:58,730 --> 00:01:00,680
E existe um monte de bibliotecas boas por aí.

31
00:01:00,970 --> 00:01:02,060
As duas que eu mais uso

32
00:01:02,210 --> 00:01:03,220
são "liblinear" e

33
00:01:03,400 --> 00:01:05,000
"libsvm", mas existem

34
00:01:05,410 --> 00:01:06,860
muitas bibliotecas de software boas

35
00:01:07,030 --> 00:01:08,430
para fazer isso, que você pode

36
00:01:08,600 --> 00:01:10,190
usar em muitas das linguagens

37
00:01:10,450 --> 00:01:11,860
de programação mais comuns que você

38
00:01:11,950 --> 00:01:14,410
pode usar para escrever um algoritmo de aprendizagem.

39
00:01:15,280 --> 00:01:16,460
Mesmo que você não deva escrever

40
00:01:16,730 --> 00:01:18,330
seu próprio software de otimização para SVM,

41
00:01:19,120 --> 00:01:20,680
existem algumas coisas que você deve fazer.

42
00:01:21,420 --> 00:01:23,130
A primeira é fazer

43
00:01:23,130 --> 00:01:24,230
uma escolha para o

44
00:01:24,320 --> 00:01:25,640
parâmetro "C".

45
00:01:25,940 --> 00:01:26,930
Conversamos um pouco sobre as

46
00:01:27,040 --> 00:01:28,850
propriedades de viés e variância anteriormente.

47
00:01:30,290 --> 00:01:31,480
Segundo, você também precisa

48
00:01:31,630 --> 00:01:33,040
escolher o kernel, a função

49
00:01:33,410 --> 00:01:34,880
de similaridade que você quer utilizar.

50
00:01:35,730 --> 00:01:37,080
Uma escolha pode ser

51
00:01:37,280 --> 00:01:38,980
não utilizar nenhum kernel.

52
00:01:40,560 --> 00:01:41,510
A ideia de não usar kernel

53
00:01:41,910 --> 00:01:43,600
é também chamada "kernel linear".

54
00:01:44,130 --> 00:01:45,320
Assim, se alguém disser que

55
00:01:45,530 --> 00:01:46,760
usa uma SVM com um kernel linear,

56
00:01:47,180 --> 00:01:48,330
o que isso significa é que

57
00:01:48,490 --> 00:01:50,690
usam uma SVM sem

58
00:01:51,020 --> 00:01:52,250
utilizar um kernel.

59
00:01:52,360 --> 00:01:53,410
É uma versão de SVM

60
00:01:54,120 --> 00:01:55,870
que simplesmente estima

61
00:01:56,140 --> 00:01:57,620
"y = 1" quando

62
00:01:57,850 --> 00:01:59,420
"θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0".

63
00:01:59,740 --> 00:02:01,000
θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0".

64
00:02:01,690 --> 00:02:04,160
θ₀ + θ₁ · x₁  + ··· + θₙ · xₙ ≥ 0".

65
00:02:05,520 --> 00:02:06,830
O termo "linear" pode

66
00:02:06,950 --> 00:02:08,250
ser entendido

67
00:02:08,480 --> 00:02:09,290
percebendo que

68
00:02:10,340 --> 00:02:12,320
esse é o kernel que fornece um classificador linear comum.

69
00:02:13,940 --> 00:02:14,700
Essa seria uma

70
00:02:15,040 --> 00:02:16,160
escolha razoável para alguns problemas,

71
00:02:17,130 --> 00:02:18,080
e muitas bibliotecas

72
00:02:18,470 --> 00:02:20,900
de software, como "liblinear",

73
00:02:21,210 --> 00:02:22,320
que é um exemplo de vários,

74
00:02:22,840 --> 00:02:23,880
de uma biblioteca de software

75
00:02:24,560 --> 00:02:25,620
que consegue treinar uma SVM

76
00:02:25,980 --> 00:02:27,410
sem usar um kernel,

77
00:02:27,760 --> 00:02:29,470
usando um kernel linear.

78
00:02:29,850 --> 00:02:31,340
Mas por que você faria isso?

79
00:02:31,410 --> 00:02:32,820
Se você tiver um número grande

80
00:02:33,150 --> 00:02:34,280
de parâmetros, se "n"

81
00:02:34,430 --> 00:02:37,800
é grande, e o número

82
00:02:37,990 --> 00:02:39,590
de exemplos de treinamento, "m",

83
00:02:39,670 --> 00:02:41,050
é pequeno, você

84
00:02:41,230 --> 00:02:42,300
você tem um número enorme

85
00:02:42,360 --> 00:02:43,630
de recursos, pois "x" pertence

86
00:02:43,710 --> 00:02:45,850
a "ℝⁿ" ou "ℝⁿ⁺¹".

87
00:02:46,010 --> 00:02:46,940
Se você já tem

88
00:02:47,080 --> 00:02:48,700
um número grande de recursos, com

89
00:02:48,800 --> 00:02:50,540
um conjunto de treino pequeno, talvez

90
00:02:50,610 --> 00:02:51,430
você queira só

91
00:02:51,710 --> 00:02:52,890
ajustar uma fronteira de decisão linear,

92
00:02:53,060 --> 00:02:54,420
sem tentar encontrar uma função não-linear

93
00:02:54,860 --> 00:02:56,980
complicada, porque você pode não ter dados suficientes.

94
00:02:57,560 --> 00:02:59,330
E você corre o risco de sobreajustar

95
00:02:59,470 --> 00:03:00,530
se tentar ajustar uma função muito complicada

96
00:03:01,540 --> 00:03:03,220
em um espaço de dimensão alta

97
00:03:03,980 --> 00:03:04,990
e o conjunto de treino

98
00:03:05,040 --> 00:03:07,120
é pequeno.

99
00:03:07,340 --> 00:03:08,600
Esse é um cenário onde

100
00:03:08,740 --> 00:03:09,950
você pode decidir

101
00:03:10,700 --> 00:03:11,960
não usar um kernel, ou,

102
00:03:12,250 --> 00:03:15,580
equivalentemente, usar um kernel linear.

103
00:03:15,740 --> 00:03:16,740
Outra escolha para o kernel

104
00:03:16,820 --> 00:03:18,010
que você pode fazer é o kernel

105
00:03:18,370 --> 00:03:19,920
gaussiano, que vimos anteriormente.

106
00:03:21,270 --> 00:03:22,350
Se você fizer isso, a outra

107
00:03:22,440 --> 00:03:23,130
escolha que precisará fazer é

108
00:03:23,420 --> 00:03:25,980
a do parâmetro "σ²",

109
00:03:26,850 --> 00:03:29,800
que influencia a relação de viés e variância,

110
00:03:30,820 --> 00:03:32,360
pois quando "σ²"

111
00:03:32,600 --> 00:03:33,890
é grande, tende-se

112
00:03:34,160 --> 00:03:35,580
a encontrar um classificador

113
00:03:35,770 --> 00:03:37,650
de alto viés e baixa variância,

114
00:03:37,800 --> 00:03:39,700
mas se "σ²" é pequeno, o

115
00:03:40,060 --> 00:03:42,360
classificador tem variância maior e viés menor.

116
00:03:43,940 --> 00:03:45,350
Quando você escolheria um kernel gaussiano?

117
00:03:46,210 --> 00:03:48,050
Bom, se a dimensão

118
00:03:48,310 --> 00:03:49,540
dos recursos "x",

119
00:03:49,820 --> 00:03:51,370
em "ℝⁿ",

120
00:03:51,570 --> 00:03:53,890
é pequena, e, idealmente,

121
00:03:55,660 --> 00:03:57,110
"m" é grande,

122
00:03:58,470 --> 00:04:00,170
é como quando temos,

123
00:04:00,550 --> 00:04:02,340
digamos, um conjunto de treino com duas dimensões,

124
00:04:03,130 --> 00:04:04,880
como o exemplo que desenhei antes.

125
00:04:05,470 --> 00:04:08,320
Se "n = 2", mas o conjunto de treino é grande.

126
00:04:08,680 --> 00:04:09,770
Eu desenhei um número

127
00:04:09,950 --> 00:04:10,890
mais ou menos grande de exemplos de treinamento,

128
00:04:11,650 --> 00:04:12,410
nesse caso você talvez queira usar

129
00:04:12,540 --> 00:04:14,400
um kernel que se ajuste a uma

130
00:04:14,910 --> 00:04:16,260
fronteira de decisão mais complexa.

131
00:04:16,650 --> 00:04:18,750
O kernel gaussiano seria uma boa maneira de fazer isso.

132
00:04:19,480 --> 00:04:20,610
Mais para o fim do vídeo,

133
00:04:20,720 --> 00:04:22,570
falarei um pouco sobre

134
00:04:22,660 --> 00:04:23,760
quando você pode escolher

135
00:04:23,970 --> 00:04:26,310
um kernel linear, gaussiano, ou outro.

136
00:04:27,860 --> 00:04:29,740
Mas na prática, se você

137
00:04:30,040 --> 00:04:31,210
decidir usar um kernel gaussiano,

138
00:04:31,720 --> 00:04:33,910
isso é o que você precisa fazer.

139
00:04:35,380 --> 00:04:36,550
Dependendo do pacote de software para

140
00:04:37,280 --> 00:04:38,990
máquinas de suporte de vetores que você usa,

141
00:04:39,100 --> 00:04:40,960
ele pode pedir a você que implemente

142
00:04:41,070 --> 00:04:42,200
uma função de kernel, uma

143
00:04:43,060 --> 00:04:43,880
função de similaridade.

144
00:04:45,020 --> 00:04:46,750
Assim, se você está usando uma

145
00:04:47,010 --> 00:04:49,820
implementação de MATLAB ou Octave

146
00:04:50,000 --> 00:04:50,720
para SVM, eles podem pedir que

147
00:04:50,810 --> 00:04:52,560
você forneça uma função para

148
00:04:52,690 --> 00:04:54,680
computar uma característica do kernel.

149
00:04:55,110 --> 00:04:56,480
Isso está calculando

150
00:04:56,770 --> 00:04:57,890
"fᵢ" para um

151
00:04:58,220 --> 00:04:59,560
valor específico de "i",

152
00:05:00,570 --> 00:05:02,310
enquanto "f" aqui é só um

153
00:05:02,330 --> 00:05:03,570
número real, então talvez

154
00:05:03,840 --> 00:05:05,060
isto estaria melhor escrito

155
00:05:05,250 --> 00:05:07,230
como "fᵢ", mas o que você precisa

156
00:05:07,510 --> 00:05:08,130
fazer é escrever

157
00:05:08,480 --> 00:05:09,530
uma função de kernel que toma esta entrada,

158
00:05:10,610 --> 00:05:11,910
um exemplo de treinamento, ou

159
00:05:12,020 --> 00:05:13,140
um exemplo de teste, o que seja, ela

160
00:05:13,280 --> 00:05:14,640
recebe um vetor "x⁽ⁱ⁾" e

161
00:05:14,990 --> 00:05:16,220
também recebe uma

162
00:05:16,370 --> 00:05:18,270
das referências, mas

163
00:05:18,880 --> 00:05:20,750
eu escrevi "x1" e "x2"

164
00:05:20,950 --> 00:05:21,810
aqui porque as

165
00:05:21,900 --> 00:05:23,750
referências são exemplos de treinamento também.

166
00:05:24,470 --> 00:05:26,160
Mas o que você precisa fazer

167
00:05:26,400 --> 00:05:27,490
é escrever software que

168
00:05:27,670 --> 00:05:28,960
tome essas entradas, "x1" e "x2",

169
00:05:29,150 --> 00:05:30,320
e computa uma medida

170
00:05:30,580 --> 00:05:31,950
de similaridade entre eles,

171
00:05:32,530 --> 00:05:33,470
e retorna um número real.

172
00:05:36,180 --> 00:05:37,430
Assim, alguns pacotes de máquinas

173
00:05:37,580 --> 00:05:39,040
de vetores de suporte esperam

174
00:05:39,510 --> 00:05:40,860
que você forneça essa função de kernel

175
00:05:41,410 --> 00:05:44,580
que toma como entrada "x1" e "x2" e retorna um número real.

176
00:05:45,580 --> 00:05:46,460
O pacote parte daí

177
00:05:46,850 --> 00:05:49,070
e gera todos os recursos automaticamente.

178
00:05:49,410 --> 00:05:51,480
Ou seja, automaticamente toma "x"

179
00:05:51,600 --> 00:05:53,370
e mapeia para "f₁",

180
00:05:53,420 --> 00:05:54,420
"f₂", até "fₘ" usando

181
00:05:54,750 --> 00:05:56,200
essa função que você escreveu,

182
00:05:56,310 --> 00:05:57,190
gera todos os parâmetros

183
00:05:57,650 --> 00:05:59,080
e treina a máquina de vetores de suporte daí pra frente.

184
00:05:59,870 --> 00:06:00,800
Mas às vezes você mesmo

185
00:06:00,880 --> 00:06:04,710
precisa fornecer essa função.

186
00:06:05,680 --> 00:06:06,770
Mas se você estiver usando o kernel gaussiano,

187
00:06:06,980 --> 00:06:09,950
algumas implementações de SVM incluem esse kernel

188
00:06:10,040 --> 00:06:10,990
e alguns outros também,

189
00:06:11,230 --> 00:06:13,580
já que o gaussiano é provavelmente o mais comum.

190
00:06:14,880 --> 00:06:16,290
Os kernels gaussiano e linear

191
00:06:16,380 --> 00:06:18,210
são de longe os dois mais populares.

192
00:06:19,130 --> 00:06:20,230
Só um comentário de implementação.

193
00:06:20,750 --> 00:06:21,820
Se você tiver recursos com

194
00:06:22,080 --> 00:06:23,620
escalas muito diferentes, é importante

195
00:06:24,700 --> 00:06:26,270
normalizar os parâmetros antes

196
00:06:26,600 --> 00:06:27,780
de usar o kernel gaussiano.

197
00:06:28,580 --> 00:06:29,180
E aqui está o porquê.

198
00:06:30,150 --> 00:06:31,600
Vamos pensar no cálculo da

199
00:06:32,290 --> 00:06:33,570
norma da diferença entre

200
00:06:33,790 --> 00:06:34,890
"x" e "l", este termo aqui,

201
00:06:35,390 --> 00:06:37,150
que corresponde ao numerador ali.

202
00:06:38,300 --> 00:06:39,780
O valor da norma

203
00:06:40,070 --> 00:06:40,930
entre "x" e "l"

204
00:06:41,130 --> 00:06:42,140
é o comprimento do vetor

205
00:06:42,450 --> 00:06:43,290
"v", que é igual

206
00:06:43,410 --> 00:06:44,980
a "x - l".

207
00:06:45,250 --> 00:06:47,940
É, portanto, "||v||",

208
00:06:48,130 --> 00:06:49,080
ou seja,

209
00:06:49,170 --> 00:06:50,510
"||x - l||".

210
00:06:50,580 --> 00:06:51,510
A norma de "v" é

211
00:06:53,360 --> 00:06:54,140
igual a "v₁² + v₂² + ··· + vₙ²".

212
00:06:54,250 --> 00:06:55,610
igual a "v₁² + v₂² + ··· + vₙ²".

213
00:06:55,830 --> 00:06:58,290
igual a "v₁² + v₂² + ··· + vₙ²".

214
00:06:58,900 --> 00:07:00,320
Isso porque "x" está em

215
00:07:01,060 --> 00:07:02,200
"ℝⁿ", ou "ℝⁿ⁺¹",

216
00:07:02,290 --> 00:07:05,180
mas eu vou ignorar "x₀".

217
00:07:06,540 --> 00:07:08,420
Vamos imaginar que "x" está

218
00:07:08,510 --> 00:07:10,800
em "ℝⁿ", e eu errei aqui, faltou

219
00:07:10,950 --> 00:07:12,320
um quadrado do lado esquerdo para ficar correto.

220
00:07:12,570 --> 00:07:14,090
Então isto é igual

221
00:07:14,400 --> 00:07:16,120
àquilo, certo?

222
00:07:17,210 --> 00:07:18,710
Escrito de forma diferente, isto é

223
00:07:18,850 --> 00:07:20,100
"(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²".

224
00:07:20,290 --> 00:07:22,600
"(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²".

225
00:07:22,910 --> 00:07:24,590
"(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²".

226
00:07:24,910 --> 00:07:26,580
"(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²".

227
00:07:27,130 --> 00:07:28,540
"(x₁ - l₁)² + (x₂ - l₂)² + ··· + (xₙ - lₙ)²".

228
00:07:29,720 --> 00:07:30,790
Agora, se seus recursos tomam

229
00:07:31,850 --> 00:07:33,460
escalas de valores muito diferentes,

230
00:07:33,940 --> 00:07:35,150
por exemplo no caso de

231
00:07:35,360 --> 00:07:37,180
estimar os valores de casas,

232
00:07:38,020 --> 00:07:40,490
se seus dados são sobre casas.

233
00:07:41,420 --> 00:07:43,000
Se "x" está na ordem de

234
00:07:43,140 --> 00:07:44,660
grandeza de milhares de

235
00:07:44,950 --> 00:07:47,190
pés quadrados no primeiro

236
00:07:48,010 --> 00:07:48,840
recurso, "x₁",

237
00:07:49,700 --> 00:07:51,630
mas o seu segundo recurso "x₂", é o número de quartos,

238
00:07:52,540 --> 00:07:53,610
este está na faixa de um

239
00:07:53,730 --> 00:07:56,720
a cinco quartos, então

240
00:07:57,810 --> 00:07:59,320
"(x₁ - l₁)²" será enorme.

241
00:07:59,780 --> 00:08:00,820
Isso poderia ser algo como "1000²",

242
00:08:01,000 --> 00:08:02,880
enquanto "x₂ - l₂"

243
00:08:03,200 --> 00:08:04,620
será muito menor, e, se esse for

244
00:08:04,750 --> 00:08:06,800
o caso, o segundo termo,

245
00:08:08,320 --> 00:08:09,660
do número de quartos, será

246
00:08:10,060 --> 00:08:12,060
quase completamente dominado

247
00:08:12,570 --> 00:08:13,280
pelos tamanhos das casas

248
00:08:14,390 --> 00:08:15,760
e o número de quartos será praticamente ignorado.

249
00:08:16,950 --> 00:08:18,060
Assim, para evitar isso

250
00:08:18,230 --> 00:08:19,070
e fazer com que a SVM

251
00:08:19,360 --> 00:08:21,890
funcione bem, normalize os recursos.

252
00:08:23,420 --> 00:08:24,830
Isso irá garantir que a SVM

253
00:08:25,810 --> 00:08:27,020
dê uma atenção comparável

254
00:08:27,950 --> 00:08:28,870
a todos os diferentes recursos,

255
00:08:29,190 --> 00:08:30,450
e não só se concentrar em um,

256
00:08:30,600 --> 00:08:31,870
como nesse exemplo dos tamanhos

257
00:08:32,150 --> 00:08:33,440
das casas, ignorando os outros recursos.

258
00:08:34,700 --> 00:08:35,810
Quando você aplicar uma máquina

259
00:08:36,110 --> 00:08:38,760
de vetores de suporte, é muito provável

260
00:08:38,970 --> 00:08:40,000
que você usará um dos dois

261
00:08:40,460 --> 00:08:41,750
kernels que vimos,

262
00:08:41,850 --> 00:08:43,120
o kernel linear (sem kernel)

263
00:08:43,320 --> 00:08:45,600
ou o kernel gaussiano.

264
00:08:46,520 --> 00:08:47,390
Só um aviso,

265
00:08:47,900 --> 00:08:49,070
nem todas as funções de similaridade

266
00:08:49,580 --> 00:08:50,590
em que você pode pensar

267
00:08:50,770 --> 00:08:52,520
são kernels válidos.

268
00:08:53,450 --> 00:08:54,840
Todos os kernels que você

269
00:08:55,090 --> 00:08:56,410
usar, o gaussiano, o linear,

270
00:08:56,710 --> 00:08:57,850
ou outros, todos devem

271
00:08:58,030 --> 00:08:59,840
satisfazer uma condição técnica.

272
00:09:00,380 --> 00:09:02,510
Ela é chamada "teorema de Mercer",

273
00:09:02,630 --> 00:09:03,560
e as razões

274
00:09:03,710 --> 00:09:05,430
para isso são os truques

275
00:09:06,380 --> 00:09:08,140
de otimização numérica

276
00:09:08,480 --> 00:09:09,560
que algoritmos que

277
00:09:10,050 --> 00:09:11,380
implementam SVM fazem

278
00:09:12,110 --> 00:09:13,270
para achar a solução dos

279
00:09:13,340 --> 00:09:15,650
parâmetros "θ" eficientemente.

280
00:09:16,590 --> 00:09:18,840
No design original de SVMs,

281
00:09:19,470 --> 00:09:21,010
tomou-se a decisão de restringir

282
00:09:21,540 --> 00:09:22,900
a atenção somente a kernels que

283
00:09:23,510 --> 00:09:25,860
satisfazem essa condição técnica chamada teorema de Mercer.

284
00:09:26,280 --> 00:09:27,360
O que isso faz é

285
00:09:27,570 --> 00:09:28,540
garantir que todos

286
00:09:28,820 --> 00:09:30,270
os pacotes software

287
00:09:30,500 --> 00:09:32,210
para SVM podem usar

288
00:09:32,310 --> 00:09:34,740
a grande quantidade de otimizações

289
00:09:35,280 --> 00:09:37,470
e encontrar o parâmetro "θ" rapidamente.

290
00:09:39,320 --> 00:09:40,340
Assim, a maioria das pessoas

291
00:09:40,840 --> 00:09:42,470
acaba usando o kernel linear

292
00:09:42,610 --> 00:09:44,210
ou o gaussiano, mas existem

293
00:09:44,430 --> 00:09:45,610
alguns outros kernels que também

294
00:09:45,940 --> 00:09:47,460
satisfazem o teorema de Mercer e que

295
00:09:47,560 --> 00:09:48,690
você pode encontrar pessoas utilizando,

296
00:09:48,850 --> 00:09:50,050
apesar de que eu, pessoalmente,

297
00:09:50,880 --> 00:09:53,780
usar outros kernels muito raramente, ou nunca.

298
00:09:54,160 --> 00:09:56,990
Só para citar alguns outros kernels que você pode encontrar.

299
00:09:57,990 --> 00:10:00,300
Um é o kernel polinomial.

300
00:10:01,570 --> 00:10:03,350
Nesse a similaridade

301
00:10:03,800 --> 00:10:05,520
entre "x" e "l" pode ser

302
00:10:05,730 --> 00:10:06,760
definida de várias formas,

303
00:10:06,830 --> 00:10:07,880
mas pode-se tomar

304
00:10:08,640 --> 00:10:10,370
"(x' · l)²".

305
00:10:10,960 --> 00:10:13,410
Essa é uma medida de similaridade para "x" e "l".

306
00:10:13,610 --> 00:10:14,930
Se "x" e "l" são muito próximos,

307
00:10:15,500 --> 00:10:18,260
o produto interno tende a ser grande.

308
00:10:20,200 --> 00:10:21,870
Esse é um kernel um pouco

309
00:10:23,080 --> 00:10:23,520
incomum.

310
00:10:24,000 --> 00:10:25,130
Ele não é usado com muita frequência, mas

311
00:10:26,490 --> 00:10:29,190
você pode encontrar algumas pessoas utilizando.

312
00:10:30,050 --> 00:10:31,810
Essa é uma versão do kernel polinomial.

313
00:10:32,330 --> 00:10:35,090
Outra é "(x' · l)³".

314
00:10:36,690 --> 00:10:38,780
Estes são todos exemplos de kernels polinomiais:

315
00:10:39,040 --> 00:10:41,270
"(x' · l + 1)³",

316
00:10:42,560 --> 00:10:43,620
"x' · l" mais um

317
00:10:43,910 --> 00:10:44,930
número diferente de 1, por exemplo 5

318
00:10:44,970 --> 00:10:46,680
e tudo elevado à quarta potência.

319
00:10:47,700 --> 00:10:49,840
Assim, o kernel polinomial tem dois parâmetros.

320
00:10:50,610 --> 00:10:53,020
Um é o número que adicionamos aqui.

321
00:10:53,520 --> 00:10:53,920
Pode ser 0.

322
00:10:54,430 --> 00:10:58,660
Aqui estamos adicionando 0; e o outro parâmetro é o grau do polinômio.

323
00:10:58,680 --> 00:11:01,670
Assim, os parâmetros são o grau e esses números.

324
00:11:02,250 --> 00:11:04,140
A forma mais geral do

325
00:11:04,280 --> 00:11:05,530
kernel polinomial é

326
00:11:05,720 --> 00:11:07,620
"x · l" mais uma

327
00:11:07,940 --> 00:11:11,510
constante, e a soma

328
00:11:11,800 --> 00:11:14,850
elevada a algum grau

329
00:11:15,060 --> 00:11:16,720
no expoente, portanto,

330
00:11:16,940 --> 00:11:19,650
esses são os dois parâmetros do kernel polinomial.

331
00:11:20,510 --> 00:11:22,820
O kernel polinomial quase sempre

332
00:11:23,350 --> 00:11:24,440
tem pior desempenho que o

333
00:11:24,820 --> 00:11:25,950
kernel gaussiano e não é muito

334
00:11:26,270 --> 00:11:28,370
utilizado, mas é algo que você pode encontrar por aí.

335
00:11:29,320 --> 00:11:30,480
Normalmente é usado só

336
00:11:30,750 --> 00:11:31,710
em dados onde "x" e "l"

337
00:11:32,000 --> 00:11:33,180
são estritamente não negativos,

338
00:11:33,740 --> 00:11:34,720
o que garante que

339
00:11:34,910 --> 00:11:36,710
os produtos internos nunca são negativos.

340
00:11:37,850 --> 00:11:40,010
E isso captura a intuição de que

341
00:11:40,390 --> 00:11:41,340
se "x" e "l" são muito

342
00:11:41,540 --> 00:11:44,110
similares, o produto interno deles será grande.

343
00:11:44,420 --> 00:11:45,590
Eles têm algumas outras propriedades

344
00:11:46,260 --> 00:11:48,080
também, mas não são muito usados.

345
00:11:49,130 --> 00:11:50,150
Além disso, dependendo do que

346
00:11:50,260 --> 00:11:51,210
você está fazendo,

347
00:11:52,330 --> 00:11:54,950
existem outros kernels mais esotéricos por aí.

348
00:11:55,670 --> 00:11:57,180
Existe o kernel de strings, que

349
00:11:57,340 --> 00:11:58,430
é usado às vezes quando as

350
00:11:58,550 --> 00:12:01,350
entradas são strings de texto, ou outros tipos.

351
00:12:02,270 --> 00:12:02,940
Existem coisas

352
00:12:03,260 --> 00:12:06,000
como o kernel chi-quadrado, o kernel de interseção de histograma, e por aí vai.

353
00:12:06,690 --> 00:12:08,420
Existem kernels mais esotéricos que

354
00:12:08,660 --> 00:12:09,840
você pode usar para medir similaridade

355
00:12:10,760 --> 00:12:12,030
entre objetos diferentes.

356
00:12:12,660 --> 00:12:13,800
Por exemplo, se você está tentando

357
00:12:14,380 --> 00:12:15,840
resolver algum problema de classificação

358
00:12:16,170 --> 00:12:17,060
de texto, onde

359
00:12:17,200 --> 00:12:19,300
a entrada "x" é uma string,

360
00:12:19,490 --> 00:12:20,490
talvez você queira encontrar a

361
00:12:20,550 --> 00:12:22,050
similaridade entre duas strings

362
00:12:22,430 --> 00:12:24,240
usando o kernel de strings, mas

363
00:12:24,520 --> 00:12:26,440
eu, pessoalmente, uso esses

364
00:12:26,990 --> 00:12:29,340
kernels mais esotéricos muito raramente.

365
00:12:29,880 --> 00:12:30,970
Eu acho que usei o kernel chi-quadrado

366
00:12:31,170 --> 00:12:32,270
talvez uma vez na

367
00:12:32,340 --> 00:12:33,670
vida, e o kernel de histograma

368
00:12:34,240 --> 00:12:35,580
umas duas.

369
00:12:35,630 --> 00:12:38,500
Eu nunca usei o kernel de strings.

370
00:12:39,350 --> 00:12:41,560
Mas caso você encontre isso em outras aplicações, se você

371
00:12:42,700 --> 00:12:43,640
fizer uma busca

372
00:12:43,860 --> 00:12:44,850
rápida no Google

373
00:12:45,040 --> 00:12:46,000
ou Bing,

374
00:12:46,590 --> 00:12:48,240
você deve encontrar definições desses outros kernels.

375
00:12:51,480 --> 00:12:55,680
Dois últimos detalhes que quero falar sobre neste vídeo.
Um é sobre classificação multi-classe.

376
00:12:56,370 --> 00:12:59,510
Se você tem 4 classes, ou em geral,

377
00:12:59,800 --> 00:13:01,880
"K" classes, como você faz com que a SVM

378
00:13:02,530 --> 00:13:06,860
gere fronteiras de decisão apropriadas entre as múltiplas classes?

379
00:13:07,220 --> 00:13:08,750
Muitos pacotes de SVM têm funcionalidade

380
00:13:09,030 --> 00:13:10,430
de classificação multi-classe internamente.

381
00:13:11,100 --> 00:13:12,060
Se você estiver usando um

382
00:13:12,270 --> 00:13:13,320
pacote como esse, use

383
00:13:13,540 --> 00:13:15,370
a funcionalidade dada,

384
00:13:15,490 --> 00:13:16,940
e tudo deve funcionar.

385
00:13:17,790 --> 00:13:18,790
Senão, uma maneira de fazer

386
00:13:19,000 --> 00:13:19,880
isso é usar o

387
00:13:20,000 --> 00:13:21,280
método "um contra todos" que

388
00:13:21,370 --> 00:13:23,690
discutimos quando estávamos desenvolvendo regressão logística.

389
00:13:24,680 --> 00:13:25,410
Você precisa

390
00:13:26,160 --> 00:13:27,550
treinar "K" SVMs se você

391
00:13:27,700 --> 00:13:29,190
tem "K" classes, uma para distinguir

392
00:13:29,900 --> 00:13:31,060
cada classe do restante.

393
00:13:31,850 --> 00:13:32,930
Isso te fornecerá "K" vetores

394
00:13:33,520 --> 00:13:34,530
de parâmetros, ou seja,

395
00:13:34,680 --> 00:13:36,210
"θ⁽¹⁾", que tenta

396
00:13:36,530 --> 00:13:38,170
distinguir a classe

397
00:13:38,630 --> 00:13:39,980
"y = 1"

398
00:13:40,130 --> 00:13:41,340
do restante, e o

399
00:13:41,420 --> 00:13:42,910
segundo vetor de parâmetros,

400
00:13:42,970 --> 00:13:43,910
"θ⁽²⁾", é o que

401
00:13:44,020 --> 00:13:45,420
você encontra quando

402
00:13:45,720 --> 00:13:47,080
"y = 2" é a classe positiva

403
00:13:47,460 --> 00:13:48,680
e as outras todas são a negativa,

404
00:13:49,260 --> 00:13:50,550
e por aí vai

405
00:13:50,800 --> 00:13:52,400
até o vetor de parâmetros "θ⁽ᴷ⁾",

406
00:13:52,750 --> 00:13:54,520
que é o vetor de parâmetros para

407
00:13:54,600 --> 00:13:56,770
distinguir a última classe,

408
00:13:57,360 --> 00:13:59,380
"K", do restante.

409
00:13:59,490 --> 00:14:00,590
Isso é exatamente

410
00:14:01,270 --> 00:14:02,040
o mesmo que o método

411
00:14:02,420 --> 00:14:04,230
"um contra todos" da regressão logística,

412
00:14:04,760 --> 00:14:05,910
onde você estima a classe

413
00:14:06,390 --> 00:14:07,690
"i" com o maior "θ⁽ⁱ⁾' · x".

414
00:14:08,030 --> 00:14:11,840
Essa é a classificação multi-classe para SVMs.

415
00:14:12,440 --> 00:14:13,750
Para os casos mais comuns,

416
00:14:14,300 --> 00:14:15,090
onde há uma boa

417
00:14:15,180 --> 00:14:16,460
chance que o pacote de software

418
00:14:16,780 --> 00:14:18,010
que você usar

419
00:14:18,340 --> 00:14:19,650
já tem internamente

420
00:14:19,920 --> 00:14:21,740
funcionalidade de classificação multi-classe,

421
00:14:21,920 --> 00:14:24,410
você não precisa se preocupar com isso.

422
00:14:25,280 --> 00:14:27,010
Finalmente, nós desenvolvemos máquinas

423
00:14:27,210 --> 00:14:28,650
de vetores de suporte começando com

424
00:14:29,090 --> 00:14:31,500
regressão logística e modificando a função custo.

425
00:14:31,910 --> 00:14:34,900
A última coisa que quero fazer neste vídeo é falar um pouco sobre

426
00:14:35,550 --> 00:14:36,570
quando você vai usar um

427
00:14:36,660 --> 00:14:38,840
algoritmo ou o outro.

428
00:14:39,080 --> 00:14:40,000
Digamos que "n" é o número

429
00:14:40,160 --> 00:14:42,000
de recursos e "m" é o número de exemplos de trinamento.

430
00:14:43,190 --> 00:14:45,250
Quando nós usamos um algoritmo em vez do outro?

431
00:14:47,130 --> 00:14:48,430
Bom, se "n" é maior

432
00:14:48,980 --> 00:14:50,140
relativo ao tamanho do seu

433
00:14:50,360 --> 00:14:51,390
conjunto de treino, por exemplo,

434
00:14:52,810 --> 00:14:53,990
se você tomar um caso onde o

435
00:14:54,250 --> 00:14:55,180
número de recursos é

436
00:14:55,330 --> 00:14:56,870
muito maior que "m",

437
00:14:57,120 --> 00:14:58,210
por exemplo, se você tiver

438
00:14:58,320 --> 00:15:00,590
um problema de classificação de texto, onde a

439
00:15:01,550 --> 00:15:02,430
dimensão do vetor

440
00:15:02,700 --> 00:15:04,160
de recursos é algo como 10000,

441
00:15:05,370 --> 00:15:06,350
e o seu conjunto

442
00:15:06,720 --> 00:15:08,290
de treino é algo entre

443
00:15:08,510 --> 00:15:10,250
10 e 1.000.

444
00:15:10,500 --> 00:15:12,140
Imagine um problema

445
00:15:12,320 --> 00:15:14,250
de classificação de e-mails em spam,

446
00:15:14,510 --> 00:15:15,840
onde você tem 10.000

447
00:15:16,150 --> 00:15:18,010
recursos, correspondendo a 10.000 palavras,

448
00:15:18,190 --> 00:15:19,550
mas você tem algo entre 10

449
00:15:19,780 --> 00:15:21,150
e 1.000 exemplos.

450
00:15:22,450 --> 00:15:23,750
Se "n" é grande relativamente

451
00:15:23,890 --> 00:15:25,090
a "m", o que eu

452
00:15:25,250 --> 00:15:26,480
geralmente faço é usar

453
00:15:26,850 --> 00:15:27,990
regressão logística, ou uso a SVM

454
00:15:28,100 --> 00:15:29,030
sem kernel,

455
00:15:29,460 --> 00:15:30,790
com kernel linear.

456
00:15:31,620 --> 00:15:32,430
Porque se você tem tantos

457
00:15:32,580 --> 00:15:33,830
recursos com um conjunto de treino pequeno,

458
00:15:34,530 --> 00:15:35,870
uma função linear provavelmente

459
00:15:36,330 --> 00:15:37,380
vai funcionar, e você

460
00:15:37,640 --> 00:15:38,790
não tem dados suficientes

461
00:15:38,910 --> 00:15:40,760
para ajustar uma função não-linear complicada.

462
00:15:41,340 --> 00:15:42,410
Agora, se "n" é

463
00:15:42,520 --> 00:15:44,020
pequeno e "m" é

464
00:15:44,350 --> 00:15:45,890
intermediário,

465
00:15:45,940 --> 00:15:47,450
talvez "n" seja

466
00:15:48,040 --> 00:15:50,350
algo até 1.000 recursos

467
00:15:50,530 --> 00:15:51,470
algo até 1.000 recursos

468
00:15:51,700 --> 00:15:54,270
e o número

469
00:15:54,590 --> 00:15:56,180
de exemplos

470
00:15:56,330 --> 00:15:57,700
de treinamento é algo

471
00:15:58,210 --> 00:16:00,750
entre 10 e 10.000,

472
00:16:01,350 --> 00:16:03,160
ou até 50.000.

473
00:16:03,630 --> 00:16:06,490
Se "m" é grande, como 10.000, mas não um milhão.

474
00:16:06,760 --> 00:16:08,100
Se "m" é

475
00:16:08,300 --> 00:16:09,950
um tamanho intermediário,

476
00:16:10,790 --> 00:16:12,980
normalmente uma SVM com kernel gaussiano funciona bem.

477
00:16:13,530 --> 00:16:14,580
Nós falamos sobre isso antes

478
00:16:14,710 --> 00:16:15,800
também, com um exemplo,

479
00:16:16,350 --> 00:16:17,100
o caso onde

480
00:16:17,520 --> 00:16:19,720
o conjunto tem duas dimensões.

481
00:16:19,900 --> 00:16:21,010
Se "n = 2", onde você

482
00:16:21,320 --> 00:16:23,710
tem um número grande de exemplos de treinamento.

483
00:16:24,710 --> 00:16:25,860
um kernel gaussiano funcionará

484
00:16:26,130 --> 00:16:28,160
bem para separar as classes positivas e negativas.

485
00:16:29,770 --> 00:16:30,890
Um terceiro caso que é

486
00:16:30,980 --> 00:16:32,420
de interesse é quando "n"

487
00:16:32,520 --> 00:16:34,270
é pequeno e "m" é grande.

488
00:16:34,890 --> 00:16:36,560
Aqui "n" é, novamente

489
00:16:37,390 --> 00:16:39,280
algo menor que 1.000, mas pode ser maior.

490
00:16:40,200 --> 00:16:42,750
Mas se "m" é algo

491
00:16:43,320 --> 00:16:46,400
maior que 50.000,

492
00:16:47,520 --> 00:16:50,270
por exemplo, 50.000, 100.000, um milhão, um trilhão.

493
00:16:51,290 --> 00:16:54,020
Você tem um conjunto de treino muito, muito grande.

494
00:16:55,240 --> 00:16:56,160
Se esse é o caso,

495
00:16:56,380 --> 00:16:57,630
uma SVM com

496
00:16:57,900 --> 00:16:59,850
kernel gaussiano vai executar meio lentamente.

497
00:17:00,160 --> 00:17:02,300
Os pacotes de SVM de hoje, se você

498
00:17:02,410 --> 00:17:04,900
usar um kernel gaussiano, tendem a fazer bastatnte esforço.

499
00:17:05,050 --> 00:17:06,250
Se você tem algo

500
00:17:06,590 --> 00:17:07,530
como 50.000,

501
00:17:07,620 --> 00:17:10,250
tudo bem, mas se é um milhão,

502
00:17:10,450 --> 00:17:11,950
ou até uns 100.000,

503
00:17:12,170 --> 00:17:13,730
um valor enorme de m,

504
00:17:14,180 --> 00:17:15,590
os pacotes de SVM de hoje

505
00:17:15,870 --> 00:17:17,100
são bons, mas podem ter dificuldade

506
00:17:17,600 --> 00:17:18,400
com um tamanho

507
00:17:19,010 --> 00:17:20,940
de conjunto de treino massivo usando um kernel gaussiano.

508
00:17:22,050 --> 00:17:23,150
Nesse caso, o que

509
00:17:23,350 --> 00:17:24,960
eu normalmente faço é tentar

510
00:17:25,330 --> 00:17:26,660
criar manualmente mais

511
00:17:26,800 --> 00:17:28,600
recursos, e então usar

512
00:17:28,930 --> 00:17:30,340
regressão logística ou uma SVM

513
00:17:30,630 --> 00:17:32,060
sem kernel.

514
00:17:33,140 --> 00:17:34,030
Se você olhar esse

515
00:17:34,230 --> 00:17:35,900
slide e vir regressão logística

516
00:17:36,460 --> 00:17:37,750
e SVM sem kernel

517
00:17:38,510 --> 00:17:39,890
nesses dois lugares,

518
00:17:39,980 --> 00:17:41,750
os dois juntos, existe uma

519
00:17:42,060 --> 00:17:43,050
razão para isso.

520
00:17:43,900 --> 00:17:45,640
A regressão logística e SVM

521
00:17:46,000 --> 00:17:47,130
sem kernal são algoritmos

522
00:17:47,350 --> 00:17:49,450
bem parecidos, e tanto

523
00:17:49,680 --> 00:17:51,170
regressão logística quanto SVM

524
00:17:51,500 --> 00:17:53,230
sem kernel vão fazer

525
00:17:53,380 --> 00:17:54,780
coisas bem parecidas e ter

526
00:17:54,900 --> 00:17:56,690
desempenho bem similar, mas, dependendo

527
00:17:57,060 --> 00:18:00,340
dos detalhes de implementação, uma delas pode ser mais eficiente que a outra.

528
00:18:00,930 --> 00:18:02,220
Entretanto, quando um dos

529
00:18:02,310 --> 00:18:03,530
algoritmos se aplica, regressão

530
00:18:03,740 --> 00:18:05,190
logística ou SVM sem kernel,

531
00:18:05,420 --> 00:18:05,840
o outro

532
00:18:06,650 --> 00:18:07,600
deve funcionar bem.

533
00:18:08,540 --> 00:18:09,660
Mas muito do poder das

534
00:18:09,720 --> 00:18:11,610
SVMs vêm quando você usa

535
00:18:11,810 --> 00:18:14,100
kernels diferentes para aprender

536
00:18:14,430 --> 00:18:15,860
funções não-lineares complexas.

537
00:18:16,680 --> 00:18:20,300
Nesse caso, onde você tem até

538
00:18:20,550 --> 00:18:22,530
10 ou 50.000 exemplos,

539
00:18:22,610 --> 00:18:25,010
e o número de recursos

540
00:18:26,580 --> 00:18:27,540
é razoavelmente grande,

541
00:18:27,840 --> 00:18:29,230
é um cenário bastante comum

542
00:18:29,670 --> 00:18:30,910
e é o caso onde a máquina

543
00:18:31,430 --> 00:18:33,830
de vetores de suporte deve brilhar.

544
00:18:34,320 --> 00:18:35,640
Você pode fazer coisas que são

545
00:18:35,860 --> 00:18:39,850
muito mais difíceis de se fazer com regressão logística.

546
00:18:40,100 --> 00:18:40,930
Finalmente, onde

547
00:18:41,120 --> 00:18:42,230
que as redes neurais se encontram?

548
00:18:42,440 --> 00:18:43,890
Para todos esses

549
00:18:43,960 --> 00:18:46,310
cenários, uma rede

550
00:18:46,630 --> 00:18:49,110
neural bem feita deve funcionar bem também.

551
00:18:50,320 --> 00:18:51,700
A única desvantagem,

552
00:18:51,830 --> 00:18:52,980
a razão que pode nos impedir

553
00:18:53,220 --> 00:18:54,690
de usar redes neurais é que

554
00:18:54,920 --> 00:18:56,080
para alguns desses problemas

555
00:18:56,180 --> 00:18:57,640
a rede neural pode ser lenta para se treinar.

556
00:18:58,250 --> 00:18:59,080
Mas se você tiver um pacote de SVM

557
00:18:59,350 --> 00:19:01,190
bom, ele pode rodar mais

558
00:19:01,400 --> 00:19:04,120
rápido, um bom tanto mais rápido que a rede neural.

559
00:19:05,130 --> 00:19:06,130
E, apesar de não termos mostrado isso

560
00:19:06,350 --> 00:19:07,520
antes, acontece que

561
00:19:07,630 --> 00:19:09,800
o problema de otimização da

562
00:19:10,070 --> 00:19:11,120
SVM é um problema

563
00:19:12,320 --> 00:19:13,830
de otimização convexa,

564
00:19:14,410 --> 00:19:15,800
portanto bons pacotes de software

565
00:19:16,160 --> 00:19:17,870
de otimização para SVM vão sempre

566
00:19:18,240 --> 00:19:21,370
achar o mínimo global ou algo próximo dele.

567
00:19:21,720 --> 00:19:24,100
Portanto, com SVMs você não precisa se preocupar com ótimos locais.

568
00:19:25,280 --> 00:19:26,440
Na prática, ótimos locais não

569
00:19:26,580 --> 00:19:27,920
são um grande problema para redes

570
00:19:28,090 --> 00:19:29,120
neurais, mas isso é

571
00:19:29,310 --> 00:19:31,520
uma coisa a menos para se preocupar se estiver usando SVM.

572
00:19:33,350 --> 00:19:34,560
Dependendo do seu problema, a rede

573
00:19:34,910 --> 00:19:37,050
neural pode ser mais lenta,

574
00:19:37,580 --> 00:19:41,020
principalmente nesse tipo de cenário, que uma SVM.

575
00:19:41,420 --> 00:19:42,200
Caso as orientações

576
00:19:42,520 --> 00:19:43,500
que dei aqui

577
00:19:43,860 --> 00:19:44,600
pareçam vagas

578
00:19:46,930 --> 00:19:48,050
e você olhar para um problema

579
00:19:48,170 --> 00:19:49,190
e as orientações são vagas,

580
00:19:49,570 --> 00:19:50,730
não tenho certeza se devo

581
00:19:50,780 --> 00:19:52,690
usar este algoritmo ou aquele, tudo bem.

582
00:19:52,950 --> 00:19:54,100
Quando eu olho para um problema de

583
00:19:54,330 --> 00:19:55,570
aprendizado de máquina, às vezes não é

584
00:19:55,730 --> 00:19:57,010
claro qual é o

585
00:19:57,150 --> 00:19:58,700
melhor algoritmo para se usar, mas,

586
00:19:59,540 --> 00:20:00,590
como você viu em vídeos anteriores,

587
00:20:01,200 --> 00:20:02,470
o algoritmo faz diferença,

588
00:20:02,700 --> 00:20:03,920
mas frequentemente o que mais faz

589
00:20:04,250 --> 00:20:06,400
diferença são coisas como a quantidade de dados que você tem.

590
00:20:07,090 --> 00:20:08,280
E a sua habilidade, o quão

591
00:20:08,450 --> 00:20:09,500
bom você é em fazer

592
00:20:09,750 --> 00:20:11,450
análise de erros e fazer debugging

593
00:20:11,660 --> 00:20:13,090
em algoritmos de aprendizagem,

594
00:20:13,220 --> 00:20:15,120
descobrir como criar novos recursos,

595
00:20:15,280 --> 00:20:17,540
descobrir que recursos fornecer ao seu algoritmo de aprendizagem, e etc.

596
00:20:17,960 --> 00:20:19,110
Frequentemente, essas coisas vão fazer

597
00:20:19,660 --> 00:20:20,700
mais diferença do que se você está

598
00:20:20,840 --> 00:20:22,370
usando regressão logística ou SVM.

599
00:20:23,280 --> 00:20:24,650
Mas, tendo dito isso,

600
00:20:25,010 --> 00:20:26,180
a SVM é amplamente

601
00:20:26,630 --> 00:20:27,890
reconhecida como um dos

602
00:20:27,950 --> 00:20:29,600
algoritmos de aprendizagem mais poderosos,

603
00:20:29,740 --> 00:20:31,570
e existe esse cenário onde existe uma

604
00:20:31,790 --> 00:20:34,340
maneira muito eficiente de aprender funções lineares complexas.

605
00:20:35,150 --> 00:20:36,840
Então, com regressão

606
00:20:37,040 --> 00:20:38,930
logística, redes neurais, SVMs,

607
00:20:39,090 --> 00:20:40,630
usando esse algoritmos

608
00:20:40,760 --> 00:20:42,170
de aprendizagem, acredito que

609
00:20:42,440 --> 00:20:43,610
você está em uma posição muito boa

610
00:20:44,120 --> 00:20:45,120
para construir sistemas de

611
00:20:45,310 --> 00:20:46,710
aprendizado de máquina de ponta

612
00:20:46,960 --> 00:20:49,110
para uma larga gama de aplicações, e

613
00:20:49,330 --> 00:20:52,460
essa é outra ferramenta poderosa para se ter no seu arsenal.

614
00:20:53,160 --> 00:20:54,270
Uma que é usada por todo

615
00:20:54,460 --> 00:20:55,850
canto no Vale do Silício,

616
00:20:56,390 --> 00:20:58,030
na indústria e

617
00:20:58,310 --> 00:20:59,860
na academia, para construir muitos

618
00:21:00,120 --> 00:21:01,680
sistemas de aprendizado de máquina de alto desempenho.