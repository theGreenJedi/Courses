1
00:00:00,140 --> 00:00:01,310
目前为止 我们在比较抽象的层面上

2
00:00:01,640 --> 00:00:03,290
讨论了支持向量机 SVM

3
00:00:03,980 --> 00:00:05,030
在这段视频中 

4
00:00:05,200 --> 00:00:06,460
为了运行 或者说使用 SVM

5
00:00:06,740 --> 00:00:09,410
你实际上需要做什么

6
00:00:11,320 --> 00:00:12,300
支持向量机算法

7
00:00:12,850 --> 00:00:14,870
是一个特定的优化问题

8
00:00:15,530 --> 00:00:16,940
但是就如我在之前的

9
00:00:17,120 --> 00:00:18,150
视频中简单提到的

10
00:00:18,380 --> 00:00:20,570
我真的不建议你自己写

11
00:00:20,630 --> 00:00:22,810
软件来求解参数θ

12
00:00:23,950 --> 00:00:26,110
就像如今只有很少的人

13
00:00:26,420 --> 00:00:27,730
或者说根本没有人

14
00:00:28,090 --> 00:00:29,400
会考虑自己写代码

15
00:00:29,530 --> 00:00:31,680
来实现对矩阵求逆

16
00:00:31,950 --> 00:00:33,940
或求一个数的平方根等

17
00:00:34,190 --> 00:00:36,570
我们只要调用库函数来实现这些

18
00:00:36,700 --> 00:00:38,090
同样地

19
00:00:38,850 --> 00:00:40,310
用以解决 SVM 优化问题

20
00:00:40,620 --> 00:00:42,200
的软件很复杂

21
00:00:42,440 --> 00:00:43,880
而且已经有

22
00:00:43,990 --> 00:00:44,960
专门研究数值优化很多年

23
00:00:45,110 --> 00:00:47,560
的学者在做这个

24
00:00:47,850 --> 00:00:48,960
因此你需要好的

25
00:00:49,150 --> 00:00:50,550
软件库和好的软件包

26
00:00:50,930 --> 00:00:52,270
来做这个

27
00:00:52,470 --> 00:00:53,480
然后强烈建议使用

28
00:00:53,860 --> 00:00:55,260
一个高度优化的软件库

29
00:00:55,710 --> 00:00:57,780
而不是尝试自己去实现它

30
00:00:58,730 --> 00:01:00,680
有许多好的软件库

31
00:01:00,970 --> 00:01:02,060
我最常用的两个是

32
00:01:02,210 --> 00:01:03,220
liblinear 和 libsvm

33
00:01:03,400 --> 00:01:05,000
但是真的有很多软件库

34
00:01:05,410 --> 00:01:06,860
可以用来

35
00:01:07,030 --> 00:01:08,430
实现这个

36
00:01:08,600 --> 00:01:10,190
你可以在很多主流编程语言

37
00:01:10,450 --> 00:01:11,860
可能是你用来写学习算法的语言

38
00:01:11,950 --> 00:01:14,410
找到这个软件库

39
00:01:15,280 --> 00:01:16,460
尽管你不应该去写

40
00:01:16,730 --> 00:01:18,330
你自己的 SVM 优化软件

41
00:01:19,120 --> 00:01:20,680
但是你也需要做几件事儿

42
00:01:21,420 --> 00:01:23,130
首先是

43
00:01:23,130 --> 00:01:24,230
要选择参数C

44
00:01:24,320 --> 00:01:25,640
我们在之前的视频中

45
00:01:25,940 --> 00:01:26,930
讨论误差/方差的性质时

46
00:01:27,040 --> 00:01:28,850
提到过这个

47
00:01:30,290 --> 00:01:31,480
第二 你也需要

48
00:01:31,630 --> 00:01:33,040
选择核函数或

49
00:01:33,410 --> 00:01:34,880
你想要使用的相似度函数

50
00:01:35,730 --> 00:01:37,080
其中一个选择是

51
00:01:37,280 --> 00:01:38,980
我们选择不用任何核函数

52
00:01:40,560 --> 00:01:41,510
不用核函数这个作法

53
00:01:41,910 --> 00:01:43,600
也叫线性核函数

54
00:01:44,130 --> 00:01:45,320
因此 如果有人说

55
00:01:45,530 --> 00:01:46,760
他的 SVM 用了线性核函数

56
00:01:47,180 --> 00:01:48,330
这就意味着

57
00:01:48,490 --> 00:01:50,690
他在使用 SVM 时

58
00:01:51,020 --> 00:01:52,250
没有用核函数

59
00:01:52,360 --> 00:01:53,410
这种用法的 SVM

60
00:01:54,120 --> 00:01:55,870
只使用了 θ 转置乘以x

61
00:01:56,140 --> 00:01:57,620
当 θ0 + θ1x1 + ... + θnxn 大于等于0时

62
00:01:57,850 --> 00:01:59,420
当 θ0 + θ1x1 + ... + θnxn 大于等于0时

63
00:01:59,740 --> 00:02:01,000
当 θ0 + θ1x1 + ... + θnxn 大于等于0时

64
00:02:01,690 --> 00:02:04,160
预测 y=1

65
00:02:05,520 --> 00:02:06,830
对线性核函数这个术语

66
00:02:06,950 --> 00:02:08,250
你可以把它理解为

67
00:02:08,480 --> 00:02:09,290
这个版本的 SVM

68
00:02:10,340 --> 00:02:12,320
它只是给你一个标准的线性分类器

69
00:02:13,940 --> 00:02:14,700
因此对某些问题来说

70
00:02:15,040 --> 00:02:16,160
它是一个合理的选择

71
00:02:17,130 --> 00:02:18,080
而且你知道 有许多软件库

72
00:02:18,470 --> 00:02:20,900
比如 liblinear

73
00:02:21,210 --> 00:02:22,320
就是众多软件库

74
00:02:22,840 --> 00:02:23,880
中的一个例子

75
00:02:24,560 --> 00:02:25,620
它们可以用来训练的 SVM

76
00:02:25,980 --> 00:02:27,410
是没有核函数的

77
00:02:27,760 --> 00:02:29,470
也叫线性核函数

78
00:02:29,850 --> 00:02:31,340
那么你为什么想要做这样一件事儿呢？

79
00:02:31,410 --> 00:02:32,820
如果你有大量的特征变量

80
00:02:33,150 --> 00:02:34,280
如果 n 很大

81
00:02:34,430 --> 00:02:37,800
而训练集的样本数

82
00:02:37,990 --> 00:02:39,590
m 很小

83
00:02:39,670 --> 00:02:41,050
那么 你知道

84
00:02:41,230 --> 00:02:42,300
你有大量的特征变量

85
00:02:42,360 --> 00:02:43,630
x 是一个 n+1 维向量

86
00:02:43,710 --> 00:02:45,850
x 是一个 n+1 维向量

87
00:02:46,010 --> 00:02:46,940
那么如果你已经有

88
00:02:47,080 --> 00:02:48,700
大量的特征值

89
00:02:48,800 --> 00:02:50,540
和很小的训练数据集

90
00:02:50,610 --> 00:02:51,430
也许你应该拟合

91
00:02:51,710 --> 00:02:52,890
一个线性的判定边界

92
00:02:53,060 --> 00:02:54,420
不要拟合非常复杂的非线性函数

93
00:02:54,860 --> 00:02:56,980
因为没有足够的数据

94
00:02:57,560 --> 00:02:59,330
如果你想在一个高维特征空间

95
00:02:59,470 --> 00:03:00,530
试着拟合非常复杂的函数

96
00:03:01,540 --> 00:03:03,220
而你的训练集又很小的话

97
00:03:03,980 --> 00:03:04,990
你可能会过度拟合

98
00:03:05,040 --> 00:03:07,120
因此 这应该是

99
00:03:07,340 --> 00:03:08,600
你可能决定不适用核函数

100
00:03:08,740 --> 00:03:09,950
或者等价地说使用线性核函数

101
00:03:10,700 --> 00:03:11,960
或者等价地说使用线性核函数

102
00:03:12,250 --> 00:03:15,580
的一个合理情况

103
00:03:15,740 --> 00:03:16,740
对于核函数的第二个选择是

104
00:03:16,820 --> 00:03:18,010
这个高斯核函数

105
00:03:18,370 --> 00:03:19,920
这个是我们之前见过的

106
00:03:21,270 --> 00:03:22,350
如果你选择这个

107
00:03:22,440 --> 00:03:23,130
那么你要做的另外一个选择是

108
00:03:23,420 --> 00:03:25,980
选择一个参数σ的平方

109
00:03:26,850 --> 00:03:29,800
我们之前讨论如何权衡偏差方差的时候谈到过

110
00:03:30,820 --> 00:03:32,360
如果 σ 的平方很大

111
00:03:32,600 --> 00:03:33,890
那么你就有可能

112
00:03:34,160 --> 00:03:35,580
得到一个较大的误差

113
00:03:35,770 --> 00:03:37,650
较低方差的分类器

114
00:03:37,800 --> 00:03:39,700
但是如果 σ 的平方很小

115
00:03:40,060 --> 00:03:42,360
那么你就会有较大的方差 较低误差的分类器

116
00:03:43,940 --> 00:03:45,350
那么什么时候选择高斯核函数呢？

117
00:03:46,210 --> 00:03:48,050
如果你原来的特征变量

118
00:03:48,310 --> 00:03:49,540
x 是 n 维的

119
00:03:49,820 --> 00:03:51,370
如果 n 很小

120
00:03:51,570 --> 00:03:53,890
并且 理想情况下

121
00:03:55,660 --> 00:03:57,110
如果 m 很大

122
00:03:58,470 --> 00:04:00,170
那么如果我们有

123
00:04:00,550 --> 00:04:02,340
一个二维的训练集

124
00:04:03,130 --> 00:04:04,880
就像我前面讲到的例子一样

125
00:04:05,470 --> 00:04:08,320
那么n等于2 但是我们有相当大的训练集

126
00:04:08,680 --> 00:04:09,770
我已经画出了

127
00:04:09,950 --> 00:04:10,890
大量的训练样本

128
00:04:11,650 --> 00:04:12,410
那么可能你需要用

129
00:04:12,540 --> 00:04:14,400
一个核函数去拟合一个

130
00:04:14,910 --> 00:04:16,260
更复杂的非线性判定边界

131
00:04:16,650 --> 00:04:18,750
那么高斯核函数会是不错的选择

132
00:04:19,480 --> 00:04:20,610
我会在这个视频的后面部分

133
00:04:20,720 --> 00:04:22,570
讲到更多一些关于

134
00:04:22,660 --> 00:04:23,760
什么时候你可以选择

135
00:04:23,970 --> 00:04:26,310
线性核函数 高斯核函数等

136
00:04:27,860 --> 00:04:29,740
但是具体地说

137
00:04:30,040 --> 00:04:31,210
如果你决定使用高斯核函数

138
00:04:31,720 --> 00:04:33,910
那么下面是你需要做的

139
00:04:35,380 --> 00:04:36,550
根据你所用的

140
00:04:37,280 --> 00:04:38,990
支持向量机软件包

141
00:04:39,100 --> 00:04:40,960
它可能需要你

142
00:04:41,070 --> 00:04:42,200
实现一个核函数

143
00:04:43,060 --> 00:04:43,880
或者实现相似度函数

144
00:04:45,020 --> 00:04:46,750
因此 如果你用

145
00:04:47,010 --> 00:04:49,820
Octave 或者 Matlab 来实现

146
00:04:50,000 --> 00:04:50,720
支持向量机的话

147
00:04:50,810 --> 00:04:52,560
它会要求你提供一个函数

148
00:04:52,690 --> 00:04:54,680
来计算核函数的特定特征

149
00:04:55,110 --> 00:04:56,480
因此这是对一个特定的 i

150
00:04:56,770 --> 00:04:57,890
因此这是对一个特定的 i

151
00:04:58,220 --> 00:04:59,560
计算 fi

152
00:05:00,570 --> 00:05:02,310
这里的 f 只是

153
00:05:02,330 --> 00:05:03,570
一个单一的实数

154
00:05:03,840 --> 00:05:05,060
也许我应该把它写成 fi

155
00:05:05,250 --> 00:05:07,230
但是你需要做的是

156
00:05:07,510 --> 00:05:08,130
写一个核函数

157
00:05:08,480 --> 00:05:09,530
把这个作为输入

158
00:05:10,610 --> 00:05:11,910
一个训练样本

159
00:05:12,020 --> 00:05:13,140
或者一个测试样本

160
00:05:13,280 --> 00:05:14,640
不论是哪个 作为输入的是向量 x

161
00:05:14,990 --> 00:05:16,220
然后把标识点

162
00:05:16,370 --> 00:05:18,270
也作为一个输入

163
00:05:18,880 --> 00:05:20,750
在这里我只写了

164
00:05:20,950 --> 00:05:21,810
x1 和 x2

165
00:05:21,900 --> 00:05:23,750
因为标识点也是训练样本

166
00:05:24,470 --> 00:05:26,160
但是你需要做的是

167
00:05:26,400 --> 00:05:27,490
写一个这样的软件

168
00:05:27,670 --> 00:05:28,960
它把 x1 x2 作为输入

169
00:05:29,150 --> 00:05:30,320
然后计算它们之间的

170
00:05:30,580 --> 00:05:31,950
这种相似度函数

171
00:05:32,530 --> 00:05:33,470
之后返回一个实数

172
00:05:36,180 --> 00:05:37,430
因此一些支持向量机的

173
00:05:37,580 --> 00:05:39,040
包所做的是期望

174
00:05:39,510 --> 00:05:40,860
你能提供一个核函数

175
00:05:41,410 --> 00:05:44,580
能够输入 x1 x2 并返回一个实数

176
00:05:45,580 --> 00:05:46,460
从这里开始

177
00:05:46,850 --> 00:05:49,070
它将自动地生成所有特征变量

178
00:05:49,410 --> 00:05:51,480
它自动地

179
00:05:51,600 --> 00:05:53,370
用你写的这个函数

180
00:05:53,420 --> 00:05:54,420
将 x 映射到对应的

181
00:05:54,750 --> 00:05:56,200
f1 f2 一直到 fm

182
00:05:56,310 --> 00:05:57,190
生成所有的特征值

183
00:05:57,650 --> 00:05:59,080
并从这儿开始训练支持向量机

184
00:05:59,870 --> 00:06:00,800
但是有些时候你却一定要

185
00:06:00,880 --> 00:06:04,710
自己提供这个函数

186
00:06:05,680 --> 00:06:06,770
如果你使用高斯核函数

187
00:06:06,980 --> 00:06:09,950
一些SVM的实现也会包括高斯核函数

188
00:06:10,040 --> 00:06:10,990
和一些其他的核函数

189
00:06:11,230 --> 00:06:13,580
因为高斯核函数可能是最常见的核函数

190
00:06:14,880 --> 00:06:16,290
目前看来 高斯核函数和线性核函数确实是

191
00:06:16,380 --> 00:06:18,210
最普遍的核函数

192
00:06:19,130 --> 00:06:20,230
一个实现过程中的注意事项

193
00:06:20,750 --> 00:06:21,820
如果你有大小很不一样

194
00:06:22,080 --> 00:06:23,620
的特征变量

195
00:06:24,700 --> 00:06:26,270
在使用高斯核函数之前

196
00:06:26,600 --> 00:06:27,780
对它们进行归一化是很重要的

197
00:06:28,580 --> 00:06:29,180
这里有一个

198
00:06:30,150 --> 00:06:31,600
如果假设你在计算

199
00:06:32,290 --> 00:06:33,570
x 和 l 之间的范数

200
00:06:33,790 --> 00:06:34,890
就是这样一个式子

201
00:06:35,390 --> 00:06:37,150
是这里的分子项

202
00:06:38,300 --> 00:06:39,780
这个式子所算的是

203
00:06:40,070 --> 00:06:40,930
x 和 l 之间的范数

204
00:06:41,130 --> 00:06:42,140
就等于说

205
00:06:42,450 --> 00:06:43,290
计算一个向量 v

206
00:06:43,410 --> 00:06:44,980
这个向量 v=x-l

207
00:06:45,250 --> 00:06:47,940
然后计算向量 v 的范数

208
00:06:48,130 --> 00:06:49,080
这也就是 x 和 l 之间的差

209
00:06:49,170 --> 00:06:50,510
这也就是 x 和 l 之间的差

210
00:06:50,580 --> 00:06:51,510
v 的范数等于

211
00:06:53,360 --> 00:06:54,140
v1 的平方

212
00:06:54,250 --> 00:06:55,610
加 v2 的平方加

213
00:06:55,830 --> 00:06:58,290
点点点 加 vn 的平方

214
00:06:58,900 --> 00:07:00,320
因为这里的 x 是 n 维向量

215
00:07:01,060 --> 00:07:02,200
或者说是 n+1 维的

216
00:07:02,290 --> 00:07:05,180
但是我要忽略 x0

217
00:07:06,540 --> 00:07:08,420
因此我们假设

218
00:07:08,510 --> 00:07:10,800
x 是 n 维向量

219
00:07:10,950 --> 00:07:12,320
在左边加上平方就是正确的了

220
00:07:12,570 --> 00:07:14,090
因此这个式子就等于

221
00:07:14,400 --> 00:07:16,120
那个式子 对吧？

222
00:07:17,210 --> 00:07:18,710
那么另一种不同的写法就是

223
00:07:18,850 --> 00:07:20,100
(x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2

224
00:07:20,290 --> 00:07:22,600
(x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2

225
00:07:22,910 --> 00:07:24,590
(x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2

226
00:07:24,910 --> 00:07:26,580
(x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2

227
00:07:27,130 --> 00:07:28,540
(x1-l1)^2+(x2-l2)^2+...+(xn-ln)^2

228
00:07:29,720 --> 00:07:30,790
现在如果你的特征变量

229
00:07:31,850 --> 00:07:33,460
取值范围很不一样

230
00:07:33,940 --> 00:07:35,150
就拿房价预测来举例

231
00:07:35,360 --> 00:07:37,180
如果你的数据

232
00:07:38,020 --> 00:07:40,490
是一些关于房子的数据

233
00:07:41,420 --> 00:07:43,000
如果特征向量 x 的

234
00:07:43,140 --> 00:07:44,660
第一个变量 x1 的取值

235
00:07:44,950 --> 00:07:47,190
在上千平方英尺

236
00:07:48,010 --> 00:07:48,840
的范围内

237
00:07:49,700 --> 00:07:51,630
但是如果你的第二个特征变量 x2 是卧室的数量

238
00:07:52,540 --> 00:07:53,610
且如果它在

239
00:07:53,730 --> 00:07:56,720
一到五个卧室范围内

240
00:07:57,810 --> 00:07:59,320
那么 x1-l1 将会很大

241
00:07:59,780 --> 00:08:00,820
这有可能上千数值的平方

242
00:08:01,000 --> 00:08:02,880
然而 x2-l2

243
00:08:03,200 --> 00:08:04,620
将会变得很小

244
00:08:04,750 --> 00:08:06,800
在这样的情况下的话 那么在这个式子中

245
00:08:08,320 --> 00:08:09,660
这些间距将几乎

246
00:08:10,060 --> 00:08:12,060
都是由

247
00:08:12,570 --> 00:08:13,280
房子的大小来决定的

248
00:08:14,390 --> 00:08:15,760
从而忽略了卧室的数量

249
00:08:16,950 --> 00:08:18,060
为了避免这种情况

250
00:08:18,230 --> 00:08:19,070
让向量机得以很好地工作

251
00:08:19,360 --> 00:08:21,890
确实需要对特征变量进行归一化

252
00:08:23,420 --> 00:08:24,830
这将会保证SVM

253
00:08:25,810 --> 00:08:27,020
能够同等地关注到

254
00:08:27,950 --> 00:08:28,870
所有不同的特征变量

255
00:08:29,190 --> 00:08:30,450
而不是像例子中那样

256
00:08:30,600 --> 00:08:31,870
只关注到房子的大小

257
00:08:32,150 --> 00:08:33,440
而忽略了其他的特征变量

258
00:08:34,700 --> 00:08:35,810
当你尝试支持向量机时

259
00:08:36,110 --> 00:08:38,760
目前为止你可做的选择是

260
00:08:38,970 --> 00:08:40,000
这两个可能是你

261
00:08:40,460 --> 00:08:41,750
最常用的核函数

262
00:08:41,850 --> 00:08:43,120
线性核函数 也就是不用核函数

263
00:08:43,320 --> 00:08:45,600
或者我们讨论的高斯核函数

264
00:08:46,520 --> 00:08:47,390
这里有一个警告

265
00:08:47,900 --> 00:08:49,070
不是所有你可能提出来

266
00:08:49,580 --> 00:08:50,590
的相似度函数

267
00:08:50,770 --> 00:08:52,520
都是有效的核函数

268
00:08:53,450 --> 00:08:54,840
高斯核函数 线性核函数

269
00:08:55,090 --> 00:08:56,410
以及其他人有时会用到的

270
00:08:56,710 --> 00:08:57,850
另外的核函数

271
00:08:58,030 --> 00:08:59,840
它们全部需要满足一个技术条件

272
00:09:00,380 --> 00:09:02,510
它叫作默塞尔定理 (Mercer's Theorem)

273
00:09:02,630 --> 00:09:03,560
需要满足这个条件的原因是

274
00:09:03,710 --> 00:09:05,430
因为支持向量机算法

275
00:09:06,380 --> 00:09:08,140
或者 SVM 的实现

276
00:09:08,480 --> 00:09:09,560
有许多巧妙的

277
00:09:10,050 --> 00:09:11,380
数值优化技巧

278
00:09:12,110 --> 00:09:13,270
为了有效地求解

279
00:09:13,340 --> 00:09:15,650
参数 θ

280
00:09:16,590 --> 00:09:18,840
在最初的设想里

281
00:09:19,470 --> 00:09:21,010
有一个这样的决定

282
00:09:21,540 --> 00:09:22,900
将我们的注意力仅仅限制在

283
00:09:23,510 --> 00:09:25,860
可以满足默塞尔定理的核函数上

284
00:09:26,280 --> 00:09:27,360
这个定理所做的是

285
00:09:27,570 --> 00:09:28,540
确保所有的SVM包

286
00:09:28,820 --> 00:09:30,270
所有的SVM软件包

287
00:09:30,500 --> 00:09:32,210
能够使用

288
00:09:32,310 --> 00:09:34,740
大量的优化方法

289
00:09:35,280 --> 00:09:37,470
并且快速地得到参数 θ

290
00:09:39,320 --> 00:09:40,340
大多数人最后做的是

291
00:09:40,840 --> 00:09:42,470
要么用线性核函数

292
00:09:42,610 --> 00:09:44,210
要么用高斯核函数

293
00:09:44,430 --> 00:09:45,610
但是还有一些其他核函数

294
00:09:45,940 --> 00:09:47,460
满足默塞尔定理

295
00:09:47,560 --> 00:09:48,690
你可能会遇到其他人使用这些核函数

296
00:09:48,850 --> 00:09:50,050
然而我个人

297
00:09:50,880 --> 00:09:53,780
最后是很少很少使用其他核函数

298
00:09:54,160 --> 00:09:56,990
只是简单提及一下你可能会遇到的其他核函数

299
00:09:57,990 --> 00:10:00,300
一个是多项式核函数

300
00:10:01,570 --> 00:10:03,350
它将 x 和 l 之间的相似度

301
00:10:03,800 --> 00:10:05,520
它将 x 和 l 之间的相似度

302
00:10:05,730 --> 00:10:06,760
定义为

303
00:10:06,830 --> 00:10:07,880
这里有很多种选择

304
00:10:08,640 --> 00:10:10,370
你可以用 x 的转置乘以 I 的平方

305
00:10:10,960 --> 00:10:13,410
那么这就是一个 x 和 l 相似度的估量

306
00:10:13,610 --> 00:10:14,930
如果 x 和 l 相互之间很接近

307
00:10:15,500 --> 00:10:18,260
那么这个内积就会很大

308
00:10:20,200 --> 00:10:21,870
这是一个有些

309
00:10:23,080 --> 00:10:23,520
不寻常的核函数

310
00:10:24,000 --> 00:10:25,130
它并不那么常用

311
00:10:26,490 --> 00:10:29,190
但是你可能会见到有人使用它

312
00:10:30,050 --> 00:10:31,810
这是多项式核函数的一个变体

313
00:10:32,330 --> 00:10:35,090
另一个是 x 转置乘以 I 的立方

314
00:10:36,690 --> 00:10:38,780
这些都是多项式核函数的例子

315
00:10:39,040 --> 00:10:41,270
x 转置乘以 l 加 1 的立方

316
00:10:42,560 --> 00:10:43,620
x 转置乘以 l 加上

317
00:10:43,910 --> 00:10:44,930
一个不是 1 的数 比如 5

318
00:10:44,970 --> 00:10:46,680
的4次方

319
00:10:47,700 --> 00:10:49,840
多项式核函数实际上有两个参数

320
00:10:50,610 --> 00:10:53,020
一个是你在这里要加的数

321
00:10:53,520 --> 00:10:53,920
可能是0

322
00:10:54,430 --> 00:10:58,660
这里就是加的0 同样地 另一个参数是多项式的次数

323
00:10:58,680 --> 00:11:01,670
参数就是多项式的次数和这些数字

324
00:11:02,250 --> 00:11:04,140
多项式核函数的更一般形式是

325
00:11:04,280 --> 00:11:05,530
多项式核函数的更一般形式是

326
00:11:05,720 --> 00:11:07,620
x 转置乘以 l 加上一个常数

327
00:11:07,940 --> 00:11:11,510
x 转置乘以 l 加上一个常数

328
00:11:11,800 --> 00:11:14,850
的某个指数次方

329
00:11:15,060 --> 00:11:16,720
因此这两个

330
00:11:16,940 --> 00:11:19,650
都是多项式核函数的参数

331
00:11:20,510 --> 00:11:22,820
所以多项式核函数几乎总是

332
00:11:23,350 --> 00:11:24,440
或者通常执行的效果

333
00:11:24,820 --> 00:11:25,950
比高斯核函数差一些

334
00:11:26,270 --> 00:11:28,370
所以用得没有那么多 但是你有可能会碰到

335
00:11:29,320 --> 00:11:30,480
通常它只用在

336
00:11:30,750 --> 00:11:31,710
当 x 和 l

337
00:11:32,000 --> 00:11:33,180
都是严格的非负数时

338
00:11:33,740 --> 00:11:34,720
这样以保证这些

339
00:11:34,910 --> 00:11:36,710
内积值永远不会是负数

340
00:11:37,850 --> 00:11:40,010
这扑捉到了这样一个直观感觉

341
00:11:40,390 --> 00:11:41,340
如果 x 和 l 之间非常相似

342
00:11:41,540 --> 00:11:44,110
也许它们之间的内积会很大

343
00:11:44,420 --> 00:11:45,590
它们也有其他的一些性质

344
00:11:46,260 --> 00:11:48,080
但是人们通常用得不多

345
00:11:49,130 --> 00:11:50,150
那么 根据你所做的

346
00:11:50,260 --> 00:11:51,210
你也有可能会碰到

347
00:11:52,330 --> 00:11:54,950
其它一些更加难懂的核函数

348
00:11:55,670 --> 00:11:57,180
比如字符串核函数

349
00:11:57,340 --> 00:11:58,430
如果你的输入数据是文本字符串

350
00:11:58,550 --> 00:12:01,350
或者其他类型的字符串 有时会用到这个核函数

351
00:12:02,270 --> 00:12:02,940
还有一些函数

352
00:12:03,260 --> 00:12:06,000
如卡方核函数 直方图交叉核函数 等等

353
00:12:06,690 --> 00:12:08,420
还有一些难懂的核函数

354
00:12:08,660 --> 00:12:09,840
你可以用它们来估量

355
00:12:10,760 --> 00:12:12,030
不同对象之间的相似性

356
00:12:12,660 --> 00:12:13,800
例如 如果你在尝试

357
00:12:14,380 --> 00:12:15,840
做一些文本分类的问题

358
00:12:16,170 --> 00:12:17,060
在这个问题中

359
00:12:17,200 --> 00:12:19,300
输入变量 x 是一个字符串

360
00:12:19,490 --> 00:12:20,490
我们想要通过

361
00:12:20,550 --> 00:12:22,050
字符串核函数来找到

362
00:12:22,430 --> 00:12:24,240
两个字符串间的相似度但是我

363
00:12:24,520 --> 00:12:26,440
但是我个人很少用

364
00:12:26,990 --> 00:12:29,340
这些更加难懂的核函数

365
00:12:29,880 --> 00:12:30,970
我想我平生可能

366
00:12:31,170 --> 00:12:32,270
用过一次卡方核函数

367
00:12:32,340 --> 00:12:33,670
可能用过一次或者两次

368
00:12:34,240 --> 00:12:35,580
直方图交叉核函数

369
00:12:35,630 --> 00:12:38,500
我实际上没用过字符串核函数

370
00:12:39,350 --> 00:12:41,560
只是以防万一你在其他应用中碰到它们

371
00:12:42,700 --> 00:12:43,640
如果你在网上查一下的话

372
00:12:43,860 --> 00:12:44,850
用 Google 搜索

373
00:12:45,040 --> 00:12:46,000
或者用 Bing 搜索

374
00:12:46,590 --> 00:12:48,240
你会发现这些也是核函数的定义

375
00:12:51,480 --> 00:12:55,680
我想要在这个视频里讨论最后两个细节

376
00:12:56,370 --> 00:12:59,510
一个是在多类分类中 你有4个类别

377
00:12:59,800 --> 00:13:01,880
或者更一般地说是 K 个类别 

378
00:13:02,530 --> 00:13:06,860
怎样让 SVM 输出各个类别间合适的判定边界？

379
00:13:07,220 --> 00:13:08,750
大部分 SVM 许多 SVM 包已经内置了

380
00:13:09,030 --> 00:13:10,430
多类分类的函数了

381
00:13:11,100 --> 00:13:12,060
因此如果你用的是那种软件包

382
00:13:12,270 --> 00:13:13,320
你可以直接用内置函数

383
00:13:13,540 --> 00:13:15,370
你可以直接用内置函数

384
00:13:15,490 --> 00:13:16,940
应该可以工作得很好

385
00:13:17,790 --> 00:13:18,790
不然的话 另一个方式是

386
00:13:19,000 --> 00:13:19,880
一对多 (one-vs.-all) 方法

387
00:13:20,000 --> 00:13:21,280
这个我们在

388
00:13:21,370 --> 00:13:23,690
讲解逻辑回归的时候讨论过

389
00:13:24,680 --> 00:13:25,410
所以你要做的是

390
00:13:26,160 --> 00:13:27,550
要训练 K 个 SVM

391
00:13:27,700 --> 00:13:29,190
如果你有 K 个类别的话

392
00:13:29,900 --> 00:13:31,060
每一个 SVM 把一个类同其他类区分开

393
00:13:31,850 --> 00:13:32,930
这会给你 K 个参数向量

394
00:13:33,520 --> 00:13:34,530
它们是

395
00:13:34,680 --> 00:13:36,210
θ(1) 它把 y=1  这类

396
00:13:36,530 --> 00:13:38,170
θ(1) 它把 y=1  这类

397
00:13:38,630 --> 00:13:39,980
和所有其他类别区分开

398
00:13:40,130 --> 00:13:41,340
和所有其他类别区分开

399
00:13:41,420 --> 00:13:42,910
然后得到第二个参数向量 θ(2)

400
00:13:42,970 --> 00:13:43,910
然后得到第二个参数向量 θ(2)

401
00:13:44,020 --> 00:13:45,420
它是在 y=2 为正类

402
00:13:45,720 --> 00:13:47,080
它是在 y=2 为正类

403
00:13:47,460 --> 00:13:48,680
其他类为负类时得到的

404
00:13:49,260 --> 00:13:50,550
以此类推

405
00:13:50,800 --> 00:13:52,400
一直到参数向量θ(K)

406
00:13:52,750 --> 00:13:54,520
是用于

407
00:13:54,600 --> 00:13:56,770
区分最后一个类别 类别 K

408
00:13:57,360 --> 00:13:59,380
和其他类别的参数向量

409
00:13:59,490 --> 00:14:00,590
最后 这就与

410
00:14:01,270 --> 00:14:02,040
我们在逻辑回归中用到的

411
00:14:02,420 --> 00:14:04,230
一对多方法一样

412
00:14:04,760 --> 00:14:05,910
在逻辑回归中我们只是

413
00:14:06,390 --> 00:14:07,690
取使得 θ(i) 转置乘以 x 最大的类 i

414
00:14:08,030 --> 00:14:11,840
以上是多类分类方法

415
00:14:12,440 --> 00:14:13,750
对于更为常见的情况

416
00:14:14,300 --> 00:14:15,090
很有可能的是

417
00:14:15,180 --> 00:14:16,460
不论你使用什么软件包

418
00:14:16,780 --> 00:14:18,010
都很有可能

419
00:14:18,340 --> 00:14:19,650
已经内置了

420
00:14:19,920 --> 00:14:21,740
多类分类的函数功能

421
00:14:21,920 --> 00:14:24,410
因此你不必担心这个

422
00:14:25,280 --> 00:14:27,010
最后 我们从逻辑回归开始

423
00:14:27,210 --> 00:14:28,650
修改了一下代价函数

424
00:14:29,090 --> 00:14:31,500
从而得到了支持向量机

425
00:14:31,910 --> 00:14:34,900
最后我想要在这个视频中讨论一点的是

426
00:14:35,550 --> 00:14:36,570
对这两个算法

427
00:14:36,660 --> 00:14:38,840
你什么时候应该用哪个呢？

428
00:14:39,080 --> 00:14:40,000
假设 n 是特征变量的个数

429
00:14:40,160 --> 00:14:42,000
m 是训练样本数

430
00:14:43,190 --> 00:14:45,250
那么我们什么时候用哪一个呢？

431
00:14:47,130 --> 00:14:48,430
如果 n 相对于你的训练集大小来说较大时

432
00:14:48,980 --> 00:14:50,140
如果 n 相对于你的训练集大小来说较大时

433
00:14:50,360 --> 00:14:51,390
比如

434
00:14:52,810 --> 00:14:53,990
如果特征变量的数量

435
00:14:54,250 --> 00:14:55,180
如果特征变量的数量

436
00:14:55,330 --> 00:14:56,870
远大于 m

437
00:14:57,120 --> 00:14:58,210
这可以是 比如说

438
00:14:58,320 --> 00:15:00,590
如果你有一个文本分类的问题

439
00:15:01,550 --> 00:15:02,430
特征向量的维数

440
00:15:02,700 --> 00:15:04,160
我不知道 有可能是1万

441
00:15:05,370 --> 00:15:06,350
且如果你的训练集大小

442
00:15:06,720 --> 00:15:08,290
可能是 10

443
00:15:08,510 --> 00:15:10,250
可能最多 1000

444
00:15:10,500 --> 00:15:12,140
想象一下垃圾邮件

445
00:15:12,320 --> 00:15:14,250
的分类问题 在这个问题中

446
00:15:14,510 --> 00:15:15,840
你有1万个特征变量

447
00:15:16,150 --> 00:15:18,010
对应于1万个单词

448
00:15:18,190 --> 00:15:19,550
但是你可能有 10 个训练样本

449
00:15:19,780 --> 00:15:21,150
可能最多 1000 个样本

450
00:15:22,450 --> 00:15:23,750
如果 n 相对 m 来说

451
00:15:23,890 --> 00:15:25,090
比较大的话

452
00:15:25,250 --> 00:15:26,480
我通常会使用逻辑回归

453
00:15:26,850 --> 00:15:27,990
或者使用

454
00:15:28,100 --> 00:15:29,030
没有核函数的 SVM

455
00:15:29,460 --> 00:15:30,790
或者叫线性核函数

456
00:15:31,620 --> 00:15:32,430
因为 如果你有许多特征变量

457
00:15:32,580 --> 00:15:33,830
而有相对较小的训练集

458
00:15:34,530 --> 00:15:35,870
一个线性函数就可能工作得不错

459
00:15:36,330 --> 00:15:37,380
而且你也没有

460
00:15:37,640 --> 00:15:38,790
足够的数据

461
00:15:38,910 --> 00:15:40,760
来拟合非常复杂的非线性函数

462
00:15:41,340 --> 00:15:42,410
现在如果 n 较小

463
00:15:42,520 --> 00:15:44,020
而 m 是中等大小

464
00:15:44,350 --> 00:15:45,890
我的意思是

465
00:15:45,940 --> 00:15:47,450
n 可以取

466
00:15:48,040 --> 00:15:50,350
1 - 1000之间的任何数 1是很小的

467
00:15:50,530 --> 00:15:51,470
也许也会到1000个特征

468
00:15:51,700 --> 00:15:54,270
如果训练样本的数量

469
00:15:54,590 --> 00:15:56,180
如果训练样本的数量

470
00:15:56,330 --> 00:15:57,700
可能是从

471
00:15:58,210 --> 00:16:00,750
10 也许是到10,000个样本之间的任何一个值

472
00:16:01,350 --> 00:16:03,160
也许多达5万个样本

473
00:16:03,630 --> 00:16:06,490
所以 m 挺大的 可能是1万 但不是一百万

474
00:16:06,760 --> 00:16:08,100
因此如果 m 大小适中的话

475
00:16:08,300 --> 00:16:09,950
那么通常线性核函数的SVM会工作得很好

476
00:16:10,790 --> 00:16:12,980
那么通常高斯核函数的SVM会工作得很好

477
00:16:13,530 --> 00:16:14,580
这个我们在这之前也讨论过

478
00:16:14,710 --> 00:16:15,800
举一个具体的例子

479
00:16:16,350 --> 00:16:17,100
如果你有一个

480
00:16:17,520 --> 00:16:19,720
二维的训练集

481
00:16:19,900 --> 00:16:21,010
所以 n=2

482
00:16:21,320 --> 00:16:23,710
画上很多训练样本

483
00:16:24,710 --> 00:16:25,860
高斯核函数可以

484
00:16:26,130 --> 00:16:28,160
很好地把正类和负类区分开来

485
00:16:29,770 --> 00:16:30,890
第三种值得关注的情况是

486
00:16:30,980 --> 00:16:32,420
第三种值得关注的情况是

487
00:16:32,520 --> 00:16:34,270
如果 n 很小 但是 m 很大

488
00:16:34,890 --> 00:16:36,560
如果 n 还是

489
00:16:37,390 --> 00:16:39,280
1到1000之间的数 可能会更大一点

490
00:16:40,200 --> 00:16:42,750
但是如果 m 是

491
00:16:43,320 --> 00:16:46,400
5万 或者更大 大到上百万

492
00:16:47,520 --> 00:16:50,270
5万 10万 一百万 二百万

493
00:16:51,290 --> 00:16:54,020
你有很大很大的训练集

494
00:16:55,240 --> 00:16:56,160
如果是这样的情况

495
00:16:56,380 --> 00:16:57,630
那么高斯核函数的支持向量机

496
00:16:57,900 --> 00:16:59,850
运行起来就会很慢

497
00:17:00,160 --> 00:17:02,300
如今的 SVM 包

498
00:17:02,410 --> 00:17:04,900
如果使用高斯核函数的话 会很慢

499
00:17:05,050 --> 00:17:06,250
如果你有5万 那还可以

500
00:17:06,590 --> 00:17:07,530
但是如果你有

501
00:17:07,620 --> 00:17:10,250
一百万个训练样本

502
00:17:10,450 --> 00:17:11,950
或者是十万个

503
00:17:12,170 --> 00:17:13,730
m 的值很大

504
00:17:14,180 --> 00:17:15,590
如今的 SVM 包很好

505
00:17:15,870 --> 00:17:17,100
但是如果你对一个

506
00:17:17,600 --> 00:17:18,400
很大很大的训练集

507
00:17:19,010 --> 00:17:20,940
使用高斯核函数的话 它们还是会有些慢

508
00:17:22,050 --> 00:17:23,150
在这种情况下

509
00:17:23,350 --> 00:17:24,960
我经常会做的是

510
00:17:25,330 --> 00:17:26,660
尝试手动地创建

511
00:17:26,800 --> 00:17:28,600
更多的特征变量

512
00:17:28,930 --> 00:17:30,340
然后使用逻辑回归

513
00:17:30,630 --> 00:17:32,060
或者不带核函数的 SVM

514
00:17:33,140 --> 00:17:34,030
你看这张幻灯片

515
00:17:34,230 --> 00:17:35,900
你看到了逻辑回归

516
00:17:36,460 --> 00:17:37,750
或者不带核函数的 SVM

517
00:17:38,510 --> 00:17:39,890
在这个两个地方都出现了

518
00:17:39,980 --> 00:17:41,750
我把它们放在一起

519
00:17:42,060 --> 00:17:43,050
是有原因的

520
00:17:43,900 --> 00:17:45,640
逻辑回归和不带核函数的 SVM

521
00:17:46,000 --> 00:17:47,130
它们都是非常

522
00:17:47,350 --> 00:17:49,450
相似的算法

523
00:17:49,680 --> 00:17:51,170
不管是逻辑回归

524
00:17:51,500 --> 00:17:53,230
还是不带核函数的 SVM

525
00:17:53,380 --> 00:17:54,780
它们会做相似的事情

526
00:17:54,900 --> 00:17:56,690
并且表现也相似

527
00:17:57,060 --> 00:18:00,340
但是根据你实现的具体情况 其中一个可能会比另一个更加有效

528
00:18:00,930 --> 00:18:02,220
但是如果其中一个算法适用的话

529
00:18:02,310 --> 00:18:03,530
但是如果其中一个算法适用的话

530
00:18:03,740 --> 00:18:05,190
逻辑回归或不带核函数的 SVM

531
00:18:05,420 --> 00:18:05,840
那么另一个算法

532
00:18:06,650 --> 00:18:07,600
也很有可能工作得很好

533
00:18:08,540 --> 00:18:09,660
但是 SVM 的威力

534
00:18:09,720 --> 00:18:11,610
随着你用不同的核函数学习

535
00:18:11,810 --> 00:18:14,100
复杂的非线性函数

536
00:18:14,430 --> 00:18:15,860
而发挥出来

537
00:18:16,680 --> 00:18:20,300
在这个区间

538
00:18:20,550 --> 00:18:22,530
你有多达1万 或者多达5万的样本

539
00:18:22,610 --> 00:18:25,010
而特征变量的数量

540
00:18:26,580 --> 00:18:27,540
这是相当大的

541
00:18:27,840 --> 00:18:29,230
那是一个非常常见的区间

542
00:18:29,670 --> 00:18:30,910
也许在这个区间下

543
00:18:31,430 --> 00:18:33,830
高斯核函数的支持向量机会表现得相当突出

544
00:18:34,320 --> 00:18:35,640
你可以做对逻辑回归来说

545
00:18:35,860 --> 00:18:39,850
会困难得多的事情

546
00:18:40,100 --> 00:18:40,930
最后 神经网络应该在什么时候使用呢？

547
00:18:41,120 --> 00:18:42,230
对于所有的这些问题

548
00:18:42,440 --> 00:18:43,890
对于所有这些区间

549
00:18:43,960 --> 00:18:46,310
对于所有这些区间

550
00:18:46,630 --> 00:18:49,110
一个设计得很好的神经网络也很可能会非常有效

551
00:18:50,320 --> 00:18:51,700
它的一个缺点是

552
00:18:51,830 --> 00:18:52,980
或者说有时可能不会使用

553
00:18:53,220 --> 00:18:54,690
神经网络的原因是

554
00:18:54,920 --> 00:18:56,080
对于许多这样的问题

555
00:18:56,180 --> 00:18:57,640
神经网络训练起来可能会很慢

556
00:18:58,250 --> 00:18:59,080
但是如果你有一个非常好的

557
00:18:59,350 --> 00:19:01,190
SVM实现包

558
00:19:01,400 --> 00:19:04,120
它会运行得比较快 比神经网络快很多

559
00:19:05,130 --> 00:19:06,130
尽管我们在此之前没有证明过

560
00:19:06,350 --> 00:19:07,520
实际上 SVM 的优化问题

561
00:19:07,630 --> 00:19:09,800
实际上 SVM 的优化问题

562
00:19:10,070 --> 00:19:11,120
是一种凸优化问题

563
00:19:12,320 --> 00:19:13,830
因此好的

564
00:19:14,410 --> 00:19:15,800
SVM 优化软件包

565
00:19:16,160 --> 00:19:17,870
总是会找到

566
00:19:18,240 --> 00:19:21,370
全局最小值 或者接近它的值

567
00:19:21,720 --> 00:19:24,100
对于SVM 你不需要担心局部最优

568
00:19:25,280 --> 00:19:26,440
在实际应用中 局部最优

569
00:19:26,580 --> 00:19:27,920
对神经网络来说不是非常大

570
00:19:28,090 --> 00:19:29,120
但是也不小

571
00:19:29,310 --> 00:19:31,520
所以你在使用 SVM 的时候可以少担心一个问题

572
00:19:33,350 --> 00:19:34,560
根据你的问题

573
00:19:34,910 --> 00:19:37,050
神经网络可能会比 SVM 慢

574
00:19:37,580 --> 00:19:41,020
尤其是在这个区间内

575
00:19:41,420 --> 00:19:42,200
如果你觉得这里给出的参考

576
00:19:42,520 --> 00:19:43,500
看上去有些模糊

577
00:19:43,860 --> 00:19:44,600
如果你在考虑一些问题

578
00:19:46,930 --> 00:19:48,050
觉得这些参考有一些模糊

579
00:19:48,170 --> 00:19:49,190
我仍然不能完全确定

580
00:19:49,570 --> 00:19:50,730
我是该用这个算法

581
00:19:50,780 --> 00:19:52,690
还是该用那个算法 这个其实没关系

582
00:19:52,950 --> 00:19:54,100
当我遇到机器学习问题时

583
00:19:54,330 --> 00:19:55,570
有时确实不清楚

584
00:19:55,730 --> 00:19:57,010
是不是最好用那个算法

585
00:19:57,150 --> 00:19:58,700
是不是最好用那个算法

586
00:19:59,540 --> 00:20:00,590
但是你在之前的视频中看到的

587
00:20:01,200 --> 00:20:02,470
算法确实很重要

588
00:20:02,700 --> 00:20:03,920
但是通常更重要的是

589
00:20:04,250 --> 00:20:06,400
你有多少数据

590
00:20:07,090 --> 00:20:08,280
你有多熟练

591
00:20:08,450 --> 00:20:09,500
是否擅长做误差分析

592
00:20:09,750 --> 00:20:11,450
和调试学习算法

593
00:20:11,660 --> 00:20:13,090
想出如何设计新的特征变量

594
00:20:13,220 --> 00:20:15,120
想出如何设计新的特征变量

595
00:20:15,280 --> 00:20:17,540
以及找出应该输入给学习算法的其它特征变量等方面

596
00:20:17,960 --> 00:20:19,110
通常这些方面会比

597
00:20:19,660 --> 00:20:20,700
你使用逻辑回归

598
00:20:20,840 --> 00:20:22,370
还是 SVM 这方面更加重要

599
00:20:23,280 --> 00:20:24,650
但是 已经说过了

600
00:20:25,010 --> 00:20:26,180
SVM 仍然被广泛认为是

601
00:20:26,630 --> 00:20:27,890
最强大的学习算法之一

602
00:20:27,950 --> 00:20:29,600
最强大的学习算法之一

603
00:20:29,740 --> 00:20:31,570
而且 SVM 在一个区间内

604
00:20:31,790 --> 00:20:34,340
是一个非常有效地学习复杂非线性函数的方法

605
00:20:35,150 --> 00:20:36,840
因此 我实际上

606
00:20:37,040 --> 00:20:38,930
逻辑回归 神经网络 SVM 加在一起

607
00:20:39,090 --> 00:20:40,630
有了这三个学习算法

608
00:20:40,760 --> 00:20:42,170
有了这三个学习算法

609
00:20:42,440 --> 00:20:43,610
我想你已经具备了

610
00:20:44,120 --> 00:20:45,120
在广泛的应用里

611
00:20:45,310 --> 00:20:46,710
构建最前沿的

612
00:20:46,960 --> 00:20:49,110
机器学习系统的能力

613
00:20:49,330 --> 00:20:52,460
它是你的武器库中的另一个非常强大的工具

614
00:20:53,160 --> 00:20:54,270
它被广泛地应用在很多地方

615
00:20:54,460 --> 00:20:55,850
比如在硅谷

616
00:20:56,390 --> 00:20:58,030
在工业界

617
00:20:58,310 --> 00:20:59,860
在学术等领域来建立许多

618
00:21:00,120 --> 00:21:01,680
高性能的机器学习系统 【教育无边界字幕组】翻译：星星之火 校对：竹二个 审核：所罗门捷列夫