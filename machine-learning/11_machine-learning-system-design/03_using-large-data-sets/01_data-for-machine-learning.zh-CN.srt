1
00:00:00,390 --> 00:00:03,570
在之前的视频中 我们讨论了评价指标

2
00:00:04,730 --> 00:00:05,840
在本节课的视频中

3
00:00:06,080 --> 00:00:07,230
我要稍微转换一下

4
00:00:07,480 --> 00:00:08,900
讨论一下机器学习系统设计中

5
00:00:09,570 --> 00:00:10,990
另一个重要的方面

6
00:00:11,800 --> 00:00:13,290
这往往涉及到

7
00:00:13,470 --> 00:00:14,990
用来训练的数据

8
00:00:15,270 --> 00:00:17,110
有多少

9
00:00:17,310 --> 00:00:18,440
在之前的一些视频中

10
00:00:18,620 --> 00:00:20,320
我曾告诫大家不要盲目地开始

11
00:00:20,690 --> 00:00:21,660
而是花大量的时间

12
00:00:21,980 --> 00:00:23,300
来收集大量的数据

13
00:00:23,420 --> 00:00:24,730
因为数据

14
00:00:25,040 --> 00:00:26,360
有时是唯一能实际起到作用的

15
00:00:27,510 --> 00:00:28,580
但事实证明

16
00:00:28,820 --> 00:00:30,270
在一定条件下

17
00:00:30,550 --> 00:00:31,580
我会在这个视频里讲到

18
00:00:31,770 --> 00:00:33,590
这些条件是什么

19
00:00:33,820 --> 00:00:35,440
得到大量的数据并在

20
00:00:35,730 --> 00:00:36,730
某种类型的学习算法中进行训练

21
00:00:37,010 --> 00:00:38,160
可以是一种

22
00:00:38,240 --> 00:00:39,470
有效的方法来获得

23
00:00:39,770 --> 00:00:41,320
一个具有良好性能的学习算法

24
00:00:42,810 --> 00:00:44,280
而这种情况往往出现在

25
00:00:44,710 --> 00:00:45,930
这些条件对于你的问题

26
00:00:46,310 --> 00:00:47,850
都成立 并且

27
00:00:47,970 --> 00:00:48,770
你能够得到大量数据的

28
00:00:48,980 --> 00:00:50,070
情况下 这可以是

29
00:00:50,210 --> 00:00:51,330
一个很好的方式来获得

30
00:00:51,560 --> 00:00:52,970
非常高性能的学习算法

31
00:00:53,990 --> 00:00:55,620
因此 在这段视频中

32
00:00:56,320 --> 00:00:58,960
让我们一起讨论一下这个问题

33
00:00:59,110 --> 00:00:59,820
我先讲一个故事

34
00:01:01,080 --> 00:01:02,620
很多很多年前 我认识的两位研究人员

35
00:01:03,400 --> 00:01:05,380
Michele Banko 和

36
00:01:05,520 --> 00:01:08,110
Eric Brill 进行了一项有趣的研究

37
00:01:09,820 --> 00:01:11,290
他们感兴趣的是研究

38
00:01:11,550 --> 00:01:12,910
使用不同的学习算法的效果

39
00:01:13,290 --> 00:01:15,210
与将这些效果

40
00:01:15,380 --> 00:01:17,420
使用到不同训练数据集上 两者的比较

41
00:01:18,020 --> 00:01:19,550
他们当时考虑这样一个问题

42
00:01:20,170 --> 00:01:22,120
如何在易混淆的词之间进行分类

43
00:01:22,550 --> 00:01:23,610
比如 在这样的句子中：

44
00:01:24,410 --> 00:01:26,990
早餐我吃了__个鸡蛋 (to,two,too)

45
00:01:27,940 --> 00:01:28,890
在这个例子中

46
00:01:29,450 --> 00:01:32,390
早餐我吃了2个鸡蛋

47
00:01:33,510 --> 00:01:34,530
这是一个

48
00:01:35,320 --> 00:01:37,800
易混淆的单词的例子 而这是另外一组情况

49
00:01:38,240 --> 00:01:39,650
于是他们把诸如这样的机器学习问题

50
00:01:39,950 --> 00:01:41,580
当做一类监督学习问题

51
00:01:41,780 --> 00:01:43,190
并尝试将其分类

52
00:01:43,970 --> 00:01:45,210
什么样的词

53
00:01:45,400 --> 00:01:46,560
在一个英文句子特定的位置

54
00:01:47,540 --> 00:01:48,140
才是合适的

55
00:01:51,010 --> 00:01:52,110
他们用了几种不同的学习算法

56
00:01:52,340 --> 00:01:53,520
这些算法都是

57
00:01:53,730 --> 00:01:55,210
在他们2001年进行研究的时候

58
00:01:55,310 --> 00:01:56,110
都已经

59
00:01:56,410 --> 00:01:57,670
被公认是比较领先的

60
00:01:57,730 --> 00:01:59,220
因此他们使用了一个方差

61
00:01:59,750 --> 00:02:01,140
用于逻辑回归上的一个方差

62
00:02:01,630 --> 00:02:03,180
被称作"感知器" (perceptron)

63
00:02:03,760 --> 00:02:05,160
他们也采取了一些

64
00:02:05,250 --> 00:02:06,700
过去常用

65
00:02:07,090 --> 00:02:08,620
但是现在比较少用的算法

66
00:02:08,830 --> 00:02:10,600
比如 Winnow 算法

67
00:02:10,750 --> 00:02:11,980
很类似于

68
00:02:12,380 --> 00:02:13,410
回归问题

69
00:02:13,660 --> 00:02:15,580
但在一些方面又有所不同

70
00:02:16,140 --> 00:02:18,220
过去用得比较多

71
00:02:18,480 --> 00:02:19,380
但现在用得不太多

72
00:02:20,180 --> 00:02:21,180
还有一种基于内存的学习算法

73
00:02:21,430 --> 00:02:23,240
现在也用得比较少了

74
00:02:23,610 --> 00:02:25,940
但是我稍后会讨论一点

75
00:02:26,230 --> 00:02:27,230
而且他们用了一个朴素算法

76
00:02:27,690 --> 00:02:29,240
这个我们将在

77
00:02:29,410 --> 00:02:32,380
这门课程中讨论到

78
00:02:32,690 --> 00:02:34,400
这些具体算法的细节不那么重要

79
00:02:35,040 --> 00:02:36,080
想象一下

80
00:02:36,430 --> 00:02:40,380
就是选了四种分类算法 这些具体算法并不重要

81
00:02:41,980 --> 00:02:42,990
他们所做的就是

82
00:02:43,140 --> 00:02:44,160
改变了训练数据集的大小

83
00:02:44,500 --> 00:02:45,390
并尝试将这些学习算法

84
00:02:45,450 --> 00:02:47,070
用于不同大小的

85
00:02:47,210 --> 00:02:49,640
训练数据集中 这就是他们得到的结果

86
00:02:50,300 --> 00:02:51,310
这些趋势非常明显

87
00:02:51,470 --> 00:02:53,170
首先大部分算法

88
00:02:53,290 --> 00:02:55,470
都具有相似的性能

89
00:02:56,200 --> 00:02:57,760
其次 随着训练

90
00:02:58,150 --> 00:02:59,760
数据集的增大

91
00:02:59,860 --> 00:03:00,970
在横轴上代表

92
00:03:01,280 --> 00:03:02,510
以百万为单位的

93
00:03:04,070 --> 00:03:05,360
训练集大小

94
00:03:05,420 --> 00:03:07,440
从0.1个百万到1000百万

95
00:03:07,720 --> 00:03:09,060
也就是到了

96
00:03:09,330 --> 00:03:10,980
10亿规模的训练集的样本

97
00:03:11,090 --> 00:03:11,860
这些算法的性能

98
00:03:12,870 --> 00:03:15,360
也都对应地增强了

99
00:03:15,740 --> 00:03:16,610
事实上 如果

100
00:03:16,650 --> 00:03:18,600
你选择任意一个算法 可能是

101
00:03:19,000 --> 00:03:21,320
选择了一个"劣等的"算法

102
00:03:21,490 --> 00:03:22,650
如果你给这个

103
00:03:23,190 --> 00:03:26,150
劣等算法更多的数据 那么

104
00:03:26,390 --> 00:03:27,570
从这些列子中看起来的话 它看上去

105
00:03:27,670 --> 00:03:30,330
很有可能会其他算法更好 甚至会比"优等算法"更好

106
00:03:32,200 --> 00:03:33,270
由于这项原始的研究

107
00:03:33,720 --> 00:03:35,850
非常具有影响力 因此已经有

108
00:03:36,360 --> 00:03:37,500
一系列许多不同的

109
00:03:37,830 --> 00:03:39,020
研究显示了类似的结果

110
00:03:39,550 --> 00:03:40,840
这些结果表明 许多不同的

111
00:03:41,150 --> 00:03:42,270
学习算法有时倾向于

112
00:03:42,630 --> 00:03:44,290
表现出非常相似的表现

113
00:03:44,460 --> 00:03:46,060
这还取决于一些细节 

114
00:03:46,490 --> 00:03:48,320
但是真正能提高性能的

115
00:03:48,520 --> 00:03:51,570
是你能够给一个算法大量的训练数据

116
00:03:53,190 --> 00:03:54,640
像这样的结果

117
00:03:55,010 --> 00:03:56,030
引起了一种

118
00:03:56,130 --> 00:03:57,360
在机器学习中

119
00:03:57,510 --> 00:03:58,920
的普遍共识：

120
00:03:59,180 --> 00:04:00,460
"取得成功的人不是拥有最好算法的人

121
00:04:00,600 --> 00:04:01,720
而是拥有最多数据的人"

122
00:04:02,810 --> 00:04:04,260
那么这种说法

123
00:04:04,460 --> 00:04:06,240
在什么时候是真 什么时候是假呢？

124
00:04:06,560 --> 00:04:07,710
因为如果我们有一个学习算法

125
00:04:07,850 --> 00:04:09,000
并且如果这种说法是真的

126
00:04:09,150 --> 00:04:10,590
那么得到大量

127
00:04:10,820 --> 00:04:11,970
的数据通常是

128
00:04:12,620 --> 00:04:13,830
保证我们

129
00:04:14,180 --> 00:04:15,700
具有一个高性能算法

130
00:04:15,900 --> 00:04:17,360
的最佳方式 而不是

131
00:04:17,520 --> 00:04:20,080
去争辩应该用什么样的算法

132
00:04:21,710 --> 00:04:23,200
假如有这样一些假设

133
00:04:23,330 --> 00:04:25,130
在这些假设下有

134
00:04:25,660 --> 00:04:28,230
大量我们认为有用的训练集

135
00:04:29,780 --> 00:04:31,310
我们假设在我们的

136
00:04:31,410 --> 00:04:33,210
机器学习问题中 特征值

137
00:04:34,080 --> 00:04:36,560
x 包含了足够的信息

138
00:04:36,830 --> 00:04:38,600
这些信息可以帮助我们用来准确地预测 y

139
00:04:40,380 --> 00:04:41,490
例如 如果我们采用了

140
00:04:41,790 --> 00:04:44,860
我们前一张幻灯片里的所有容易混淆的词

141
00:04:45,740 --> 00:04:47,040
假如说它能够描述 x

142
00:04:47,520 --> 00:04:48,360
捕捉到需要填写

143
00:04:49,090 --> 00:04:51,620
的空白处周围的词语

144
00:04:51,840 --> 00:04:53,630
那么特征捕捉到之后

145
00:04:54,220 --> 00:04:56,440
我们就希望有 对于“早饭我吃了__鸡蛋”

146
00:04:57,350 --> 00:04:58,220
那么这就有

147
00:04:58,480 --> 00:04:59,970
大量的信息来告诉我

148
00:05:00,170 --> 00:05:01,050
中间我需要填的

149
00:05:01,420 --> 00:05:03,640
词是“两个” (two)

150
00:05:03,850 --> 00:05:06,640
而不是单词 to 或 too

151
00:05:09,650 --> 00:05:11,270
因此特征捕捉

152
00:05:11,620 --> 00:05:13,390
哪怕是周围词语中的一个词

153
00:05:13,560 --> 00:05:15,360
就能够给我足够的

154
00:05:15,790 --> 00:05:17,640
信息来确定出

155
00:05:17,780 --> 00:05:18,830
标签 y 是什么

156
00:05:19,300 --> 00:05:20,190
换句话说

157
00:05:20,750 --> 00:05:21,760
从这三组易混淆的词中

158
00:05:22,100 --> 00:05:23,520
我应该选什么

159
00:05:23,930 --> 00:05:25,610
词来填空

160
00:05:27,110 --> 00:05:28,320
这就是一个例子

161
00:05:28,460 --> 00:05:29,840
特征值 x 有充足的信息

162
00:05:30,410 --> 00:05:32,270
来确定 y

163
00:05:32,470 --> 00:05:33,240
举一个反例

164
00:05:34,690 --> 00:05:36,010
设想一个

165
00:05:36,470 --> 00:05:38,090
房子价格的问题

166
00:05:38,340 --> 00:05:39,330
房子只有大小信息

167
00:05:39,390 --> 00:05:40,350
没有其他特征

168
00:05:42,060 --> 00:05:42,060
那么

169
00:05:42,820 --> 00:05:43,890
如果我告诉你

170
00:05:44,150 --> 00:05:45,270
这个房子有

171
00:05:45,370 --> 00:05:48,100
500平方英尺 但是我没有告诉你其他的特征信息

172
00:05:48,530 --> 00:05:49,520
我也不告诉你这个

173
00:05:49,590 --> 00:05:51,990
房子位于这个城市房价比较昂贵的区域

174
00:05:52,590 --> 00:05:53,710
如果我也不告诉你

175
00:05:53,840 --> 00:05:55,290
这所房子的

176
00:05:55,500 --> 00:05:57,030
房间数量 或者

177
00:05:57,180 --> 00:05:58,400
它里面陈设了多漂亮的家具

178
00:05:58,790 --> 00:06:00,540
或这个房子是新的还是旧的

179
00:06:01,090 --> 00:06:02,290
我不告诉你其他任何信息

180
00:06:02,540 --> 00:06:03,360
除了这个房子

181
00:06:03,520 --> 00:06:05,440
有500平方英尺以外

182
00:06:05,720 --> 00:06:07,160
然而除此之外还有许多其他因素

183
00:06:07,340 --> 00:06:08,280
会影响房子的价格

184
00:06:08,470 --> 00:06:09,940
不仅仅是房子的大小

185
00:06:10,320 --> 00:06:11,330
如果所有

186
00:06:11,440 --> 00:06:12,910
你所知道的只有房子的尺寸 那么事实上

187
00:06:13,050 --> 00:06:14,610
是很难准确预测它的价格的

188
00:06:16,220 --> 00:06:16,860
这是对于这个假设

189
00:06:17,280 --> 00:06:18,230
的一个反例

190
00:06:18,880 --> 00:06:20,300
假设是特征能够提供足够的信息

191
00:06:20,800 --> 00:06:23,260
来在需要的水平上预测出价格

192
00:06:24,090 --> 00:06:25,180
我经常思考 如果我想测试这样一个

193
00:06:25,540 --> 00:06:26,730
假设的方式是什么

194
00:06:26,940 --> 00:06:29,160
我经常这样问自己：

195
00:06:30,260 --> 00:06:31,660
给定一个输入特征向量 x

196
00:06:32,180 --> 00:06:33,320
给定这些特征值

197
00:06:33,380 --> 00:06:35,440
也给定了相同的可用的信息和学习算法

198
00:06:36,510 --> 00:06:38,690
如果我们去请教这个领域的人类专家

199
00:06:39,680 --> 00:06:41,570
一个人类专家能够

200
00:06:41,720 --> 00:06:43,160
准确或自信地预测出

201
00:06:43,490 --> 00:06:45,390
y 的值吗？

202
00:06:45,630 --> 00:06:46,730
第一个例子 如果我们去

203
00:06:46,980 --> 00:06:49,420
找你认识的一个英语专家

204
00:06:49,810 --> 00:06:51,260
比如你找到了一个

205
00:06:51,390 --> 00:06:53,740
英语说得很好的人 那么

206
00:06:53,940 --> 00:06:55,230
一个英语方面的专家

207
00:06:55,940 --> 00:06:57,260
大部分像

208
00:06:57,450 --> 00:06:59,730
你和我这样的人

209
00:07:00,160 --> 00:07:01,080
我们可能不难

210
00:07:01,170 --> 00:07:02,370
预测出在这种情况下

211
00:07:02,620 --> 00:07:03,960
该使用什么样的语言

212
00:07:04,290 --> 00:07:05,550
一个英语说得好的人 应该可以预测得很好

213
00:07:05,850 --> 00:07:06,710
因此这就给了我信心

214
00:07:07,470 --> 00:07:08,640
x能够让我们

215
00:07:08,810 --> 00:07:10,550
准确地预测y 但是与此相反

216
00:07:11,240 --> 00:07:13,550
如果我们去找一个价格上的专家

217
00:07:14,040 --> 00:07:16,390
比如 可能是一个房地产经纪人

218
00:07:16,950 --> 00:07:18,090
或者职业售楼小姐

219
00:07:18,610 --> 00:07:19,450
如果我只是告诉他们

220
00:07:19,550 --> 00:07:20,440
一个房子的大小

221
00:07:20,530 --> 00:07:21,860
然后问他们房子的价格

222
00:07:22,240 --> 00:07:23,410
那么即使是擅长房价评估

223
00:07:23,600 --> 00:07:25,210
或者售房方面的专家

224
00:07:25,600 --> 00:07:26,520
也不能告诉我

225
00:07:26,550 --> 00:07:28,280
房子的价格是多少

226
00:07:29,000 --> 00:07:31,060
所以在房价的例子中

227
00:07:31,600 --> 00:07:33,300
只知道房子的大小并不能

228
00:07:33,460 --> 00:07:34,960
给我足够的信息来预测

229
00:07:35,920 --> 00:07:36,870
房子的价格

230
00:07:37,690 --> 00:07:39,890
如果这个假设是成立的

231
00:07:41,200 --> 00:07:42,650
那么让我们来看一看

232
00:07:43,040 --> 00:07:44,230
大量的数据是有帮助的情况

233
00:07:45,020 --> 00:07:46,370
假设特征值有

234
00:07:46,650 --> 00:07:47,870
足够的信息

235
00:07:48,050 --> 00:07:49,380
来预测 y 值

236
00:07:49,540 --> 00:07:50,750
假设我们使用一种

237
00:07:50,960 --> 00:07:52,380
需要大量参数的

238
00:07:52,600 --> 00:07:54,430
学习算法

239
00:07:54,580 --> 00:07:56,020
比如有很多特征的

240
00:07:56,280 --> 00:07:58,090
逻辑回归或线性回归

241
00:07:58,550 --> 00:07:59,490
或者我有时做的一件事

242
00:07:59,950 --> 00:08:00,740
我经常做的一件事

243
00:08:00,960 --> 00:08:03,300
就是用带有许多隐藏单元的神经网络

244
00:08:03,860 --> 00:08:05,230
那又是另外一种

245
00:08:05,500 --> 00:08:07,420
带有很多参数的学习算法了

246
00:08:08,470 --> 00:08:10,280
这些都是非常强大的学习算法

247
00:08:10,350 --> 00:08:12,350
它们有很多参数

248
00:08:13,040 --> 00:08:14,810
这些参数可以拟合非常复杂的函数

249
00:08:16,750 --> 00:08:17,550
因此我要调用这些

250
00:08:18,630 --> 00:08:19,720
我将把这些算法想象成

251
00:08:20,510 --> 00:08:21,970
低偏差算法 因为

252
00:08:22,140 --> 00:08:23,540
我们能够拟合非常复杂的函数

253
00:08:25,480 --> 00:08:26,740
而且因为我们有

254
00:08:27,260 --> 00:08:28,920
非常强大的学习算法

255
00:08:29,380 --> 00:08:30,590
这些学习算法能够拟合非常复杂的函数

256
00:08:31,680 --> 00:08:33,470
很有可能 如果我们

257
00:08:34,070 --> 00:08:35,790
用这些数据运行

258
00:08:35,940 --> 00:08:37,250
这些算法 这种算法能

259
00:08:37,430 --> 00:08:38,770
很好地拟合训练集

260
00:08:39,200 --> 00:08:40,680
因此

261
00:08:40,940 --> 00:08:43,230
训练误差就会很低

262
00:08:44,520 --> 00:08:45,520
现在假设我们使用了

263
00:08:46,020 --> 00:08:47,790
非常非常大的训练集

264
00:08:48,190 --> 00:08:49,370
在这种情况下 如果我们

265
00:08:49,430 --> 00:08:51,460
有一个庞大的训练集 那么

266
00:08:51,630 --> 00:08:53,490
尽管我们希望有很多参数

267
00:08:53,760 --> 00:08:56,080
但是如果训练集比

268
00:08:56,360 --> 00:08:57,450
比参数的数量还大

269
00:08:57,840 --> 00:08:59,450
甚至是更多 那么这些

270
00:08:59,640 --> 00:09:01,490
算法就不太可能会过度拟合

271
00:09:02,590 --> 00:09:03,660
因为我们有如此

272
00:09:03,710 --> 00:09:05,680
庞大的训练集

273
00:09:06,070 --> 00:09:07,870
并且不太可能过度拟合

274
00:09:08,070 --> 00:09:09,090
也就是说训练

275
00:09:09,390 --> 00:09:10,860
误差有希望

276
00:09:11,050 --> 00:09:13,270
接近测试误差

277
00:09:13,960 --> 00:09:15,160
最后把这两个

278
00:09:15,350 --> 00:09:16,770
放在一起 训练集

279
00:09:16,990 --> 00:09:18,590
误差很小 而

280
00:09:18,700 --> 00:09:19,870
测试集误差又接近

281
00:09:20,360 --> 00:09:22,290
训练误差

282
00:09:22,460 --> 00:09:24,510
这两个就意味着

283
00:09:24,710 --> 00:09:26,630
测试集的误差

284
00:09:27,780 --> 00:09:28,450
也会很小

285
00:09:30,000 --> 00:09:32,610
另一种考虑

286
00:09:32,720 --> 00:09:33,930
这个问题的角度是

287
00:09:34,700 --> 00:09:35,740
为了有一个高

288
00:09:35,880 --> 00:09:37,630
性能的学习算法 我们希望

289
00:09:37,930 --> 00:09:40,470
它不要有高的偏差和方差

290
00:09:42,060 --> 00:09:43,270
因此偏差问题 我么将

291
00:09:43,350 --> 00:09:44,700
通过确保

292
00:09:44,880 --> 00:09:45,910
有一个具有很多

293
00:09:46,170 --> 00:09:47,670
参数的学习算法来解决 以便

294
00:09:47,840 --> 00:09:48,930
我们能够得到一个较低偏差的算法

295
00:09:50,110 --> 00:09:51,460
并且通过用

296
00:09:51,610 --> 00:09:53,240
非常大的训练集来保证

297
00:09:53,760 --> 00:09:55,590
我们在此没有方差问题

298
00:09:55,840 --> 00:09:57,280
我们的算法将

299
00:09:57,430 --> 00:09:59,100
没有方差 并且

300
00:09:59,340 --> 00:10:00,940
通过将这两个值放在一起

301
00:10:01,870 --> 00:10:02,830
我们最终可以得到一个低

302
00:10:02,900 --> 00:10:03,990
误差和低方差

303
00:10:04,990 --> 00:10:06,920
的学习算法 这

304
00:10:07,140 --> 00:10:08,300
使得我们能够

305
00:10:08,710 --> 00:10:10,150
很好地测试测试数据集

306
00:10:10,430 --> 00:10:12,140
从根本上来说 这是一个关键

307
00:10:13,020 --> 00:10:14,560
的假设：特征值

308
00:10:14,940 --> 00:10:16,750
有足够的信息量 且我们

309
00:10:16,900 --> 00:10:17,960
有一类很好的函数

310
00:10:18,400 --> 00:10:19,580
这是为什么能保证低误差的关键所在

311
00:10:20,760 --> 00:10:21,750
它有大量的

312
00:10:22,110 --> 00:10:25,010
训练数据集 这能保证得到更多的方差值

313
00:10:27,150 --> 00:10:28,310
因此这给我们提出了

314
00:10:28,410 --> 00:10:29,820
一些可能的条件

315
00:10:30,090 --> 00:10:31,610
一些对于

316
00:10:31,870 --> 00:10:33,730
问题的认识 如果

317
00:10:33,860 --> 00:10:34,790
你有大量的数据

318
00:10:34,960 --> 00:10:36,150
而且你训练了一种

319
00:10:36,380 --> 00:10:38,930
带有很多参数的学习算法 那么这将

320
00:10:39,120 --> 00:10:39,870
会是一个很好的方式来提供

321
00:10:40,060 --> 00:10:42,490
一个高性能的学习算法

322
00:10:43,480 --> 00:10:44,140
我觉得关键的测试

323
00:10:44,230 --> 00:10:45,520
我常常问自己

324
00:10:45,820 --> 00:10:47,100
首先 一个人类专家

325
00:10:47,200 --> 00:10:48,360
看到了特征值 x

326
00:10:48,880 --> 00:10:49,890
能很有信心的预测出

327
00:10:50,030 --> 00:10:51,080
y值吗？ 因为这可以

328
00:10:51,210 --> 00:10:53,050
证明 y

329
00:10:53,320 --> 00:10:55,040
可以根据特征值 x

330
00:10:55,140 --> 00:10:57,010
被准确地预测出来 其次

331
00:10:57,510 --> 00:10:58,630
我们实际上能得到一组

332
00:10:58,820 --> 00:11:00,150
庞大的训练集并且在这个

333
00:11:00,350 --> 00:11:01,470
训练集中训练一个有

334
00:11:01,540 --> 00:11:03,290
很多参数的学习算法吗？

335
00:11:03,520 --> 00:11:04,420
如果你不能做到这两者

336
00:11:04,870 --> 00:11:06,300
那么更多时候

337
00:11:06,460 --> 00:11:08,570
你会得到一个性能很好的学习算法 【教育无边界字幕组】翻译：星星之火 校对/审核：所罗门捷列夫