No vídeo anterior, falamos de métricas de avaliação. Neste vídeo, eu gostaria de mudar um pouco o foco e falar sobre outro aspecto importante no design de sistemas de aprendizagem de máquina, que sempre aparece, que é a questão da quantidade de dados usada no treinamento. Nos vídeos passados, eu adverti sobre seguir as cegas e gastar muito tempo reunindo uma grande quantidade de dados, pois só em algumas situações isso realmente vai ser útil. Porém sob certas condições, e eu vou dizer neste vídeo quais são essas condições, tomar uma grande quantidade de dados e treinar um certo tipo de algoritmo de aprendizagem, pode ser uma forma muito eficaz de fazer um algoritmo de aprendizagem obter uma performance muito boa. Ocorre com bastante frequência que se essas condições existirem para seu problema e se você tiver a possibilidade de obter uma grande quantidade de dados, essa pode ser uma boa forma de obter um algoritmo de aprendizagem de alta performance. Neste vídeo vamos falar mais disso. Vou começar com uma história. Muitos, muitos anos atrás, dois pesquisadores que eu conheço, Michelle Banko e Eric Brill conduziram o seguinte fascinante estudo. Eles estavam interessados em estudar o efeito de usar diferentes algoritmos de aprendizagem versus testá-los em diferentes conjuntos de treinamento, eles estavam pensando no problema de classificação entre palavras confundíveis, por exemplo, na sentença: "no café da manhã eu comi", deve ser "T-O, T-W-O ou T-O-O"? No caso, para este exemplo, "no café da manhã eu comi T-W-O, 2 ovos". Este é um exemplo de um conjunto de palavras confundíveis, e este é um conjunto diferente. Eles pegaram problemas de aprendizado de máquina como estes, um tipo de problema de aprendizado supervisionado para tentar classificar qual é a palavra apropriada em uma certa posição numa sentença em inglês. Eles pegaram diferentes algoritmos de de aprendizado que eram considerado estado da arte na época que eles realizaram o estudo, em 2001. Eles pegaram uma variância da regressão logística e a chamada Perceptron. Eles também pegaram alguns algoritmos que eram bastante usados na época, mas que são menos utilizados ultimamente, também um algoritmo muito similar com o que é a regressão mas diferente em certos aspectos, que não é muito usado hoje em dia, pegaram o que chamamos algoritmo de aprendizado baseado em memória, novamente pouco usado hoje em dia. Falaremos disso mais tarde. E usaram um algoritmo Naïve Bayes, que é algo sobre o qual eu irei falar nesse curso. Os algoritmos exatos e estes detalhes não são importantes. Pense nisso como só uma seleção de quatro diferentes algoritmos de classificação, os algoritmos em si não são importantes. O que eles fizeram foi variar o tamanho do conjunto de treinamento e testar esses algoritmos de aprendizado nas extensões dos conjuntos de treino, obtendo esse resultado. Os padrões são muito claros. Primeiro, a maioria desses algoritmos tiveram uma performance notadamente semelhante. E segundo, conforme o tamanho do conjunto de treinamento aumenta (no eixo horizontal está o tamanho do conjunto de treinamento em milhões, indo de cem mil a mil milhões, ou seja, um bilhão de exemplos de treinamento) a performance de todos esse algoritmos aumentam monotonamente, e de fato, se você escolher qualquer algoritmo, mesmo que escolha um "algoritmo inferior", se for fornecido a este "algoritmo inferior" mais dados, então devido a estes dados, ele provavelmente se sairá melhor que um "algoritmo superior". Desde este primeiro estudo, o qual foi muito influente, houveram vários outros estudos diferentes mostrando resultados similares nos quais vários algoritmos de de aprendizagem de máquina tendem, podem ocasionalmente resultar em uma faixa de performance bem similar, mas o que realmente impacta a performance é uma grande quantidade de dados para treino. E resultados como este levaram a um ditado em aprendizado de máquina que geralmente em aprendizado de máquina não é quem tem o melhor algoritmo que ganha, e sim que tem mais dados. Então, quando isso é verdade e quando não é? Pois nós temos um algoritmo de aprendizagem para o qual isso é verdade, então pegar uma grande quantidade de dados geralmente pode ser o melhor modo de assegurar que temos um algoritmo de alta performance em vez de debater sobre quais dos itens utilizar. Vamos tentar estabelecer uma série de hipóteses para as quais nós acreditamos que ter um conjunto de treinamento grande ajuda. Vamos assumir que em nosso problema de aprendizagem de máquina, o parâmetro x tem informação suficiente para predizermos y precisamente. Por exemplo, se pegarmos o problema de palavra confundíveis que tínhamos no slide anterior, digamos que esse parâmetro x pegue as palavras adjacentes ao espaço que queremos preencher. Então esse parâmetro terá a sentença "no café da manhã eu comi, espaço, ovos". E isso é informação suficiente para me dizer que a palavra que eu quero no meio é dois (TWO) e não a palavra TO ou TOO. Então o parâmetro pega, uma destas palavras adjacentes e isso me dá informação suficiente pra decidir sem ambiguidade qual qual é o valor de y, ou em outras palavras qual é a palavra que eu devo usar para preencher o espaço dado esse conjunto de 3 palavras confundíveis. Esse é um exemplo de que o parâmetro x tem informação suficiente para predizer y. . Como um contra exemplo considere o problema de predizer o preço de uma casa a partir somente de seu tamanho sem nenhum outro parâmetro. Então imagine que eu te diga que a casa tem 500 pés quadrados, mas eu não lhe dê nenhum outro parâmetro. Eu não te diga que a casa fica em uma parte cara da cidade. Ou que eu não te diga o número de quartos na casa, ou quão bem mobiliada ela é, ou ainda se a casa é nova ou velha. Se eu não te disser mais nada além de que essa é uma casa de 500 pés quadrados, tem tantos outros fatores que podem influenciar no preço da casa além do seu tamanho, que se você só souber o tamanho, será muito difícil prever o preço precisamente. Então este seria um contra exemplo do pressuposto que os parâmetro têm informação suficiente para predizer o preço em um nível desejado de acurácia. A forma que eu vejo de testar esse pressuposto, uma forma que eu geralmente penso é: "dado o parâmetro x, dados os parâmetros, dadas as mesmas informações disponíveis para um algoritmo de aprendizagem. Se nós formos a um especialista humano nesse assunto. Esse especialista consegue ou esse especialista prediz com precisão o valor de y?" Para esse primeiro exemplo se formos a um falante fluente da língua inglesa, se você for a alguém que fale inglês bem, um especialista em inglês ou a maioria das pessoas como eu ou você, provavelmente seríamos capazes de predizer qual palavra deve ir aqui, um falante fluente de inglês pode predizer isso facilmente, e isso me dá a certeza de que o x nos permite predizer y com precisão. Em contraste se formos a um especialista de preços, como um corretor experiente, alguém que vende casas para viver, se eu lhe disser apenas o tamanho da casa e qual seu preço, mesmo um especialista em avaliação ou venda de casas não será capaz de me dizer, e portanto está certo que para o exemplo de preços da casas apenas o tamanho não me dá informação suficiente para predizer o preço da casa. Vamos manter essa suposição. Vamos ver então, quando ter muitos dados ajuda. Suponha que os parâmetros têm informação suficiente para predizer o valor de y. E vamos supor que nós usamos um algoritmo de aprendizagem com um grande número de parâmetros, por exemplo regressão logística ou regressão linear com um grande número de parâmetros. Uma coisa que eu faço as vezes, na verdade uma coisa que eu geralmente faço é usar rede neural com várias unidades ocultas. Este seria um outro algoritmo de aprendizagem com vários parâmetros. Todos esses são algoritmos de aprendizagem eficazes com vários parâmetros que podem se ajustar a funções muito complexas. Vou chamá-los de, vou pensar nisso como algoritmos de baixo-viés porque podemos ajustar funções muito complexas e porque temos um algoritmo de aprendizagem muito eficaz, eles podem se ajustar a funções muito complexas, é provável que se nós executarmos esses algoritmos em nosso conjuntos de dados, eles serão capazes de se ajustar ao conjunto de treinamento, e esperamos que o erro de treinamento seja pequeno. Agora vamos dizer que nós usamos um conjunto de treinamento realmente grande, neste caso, se nós temos um conjunto de treinamento enorme, esperamos que mesmo tendo muitos parâmetros se o conjunto de treinamento for ainda maior que o número de parâmetros então será improvável que estes algoritmos sobreajustem. Certo, porque nós temos esse conjunto de treinamento enorme, e por improvável de sobreajustar significa que o erro de treinamento, espera-se, estará próximo ao erro de teste. Finalmente juntando estes dois, que o erro do conjunto de treinamento é baixo e que o erro do conjunto de teste é semelhante ao erro de treinamento, estes dois juntos implicam que o erro do conjunto de testes também será baixo. Outra forma de se pensar sobre isto é que para termos um algoritmo de aprendizado de alta perfomance nós queremos que ele não tenha um viés alto e que não tenha uma variância alta. O problema do viés nós vamos resolver nos certificando que temor um algoritmo de aprendizagem com muitos parâmetros e isso nos dá um algoritmo de baixo viés, e usando um conjunto de treinamento bem grande, isso garante que nós não teremos um problema com a variância também. Então espera-se que nosso algoritmo tenha uma variância baixa, e então colocando estes dois juntos, nós ficamos com um algoritmo de aprendizado com baixo viés e baixa variância, e isto nos permite sair bem no conjunto de treinamento. E fundamentalmente esses são os ingredientes chave em assumir que os parâmetros contêm informação suficiente e que nós temos uma rica classe de funções o que garante um viés baixo, e que tendo esse conjunto de treinamento grande o que garante baixa variância. Assim nós teremos uma série de condições que ajuda a entender qual é o tipo de problema, se você tem uma grande quantidade de dados e vai treinar um algoritmo de aprendizado com muito parâmetros, essa pode ser uma boa maneira de fazer um algoritmo de aprendizagem de alta performance, de fato, eu acho que o teste chave que eu geralmente me pergunto são primeiro, uma humano especialista pode ver os parâmetros x e predizer confiantemente o valor de y. Pois essa é uma certificação de que y pode ser predito com precisão a partir dos parâmetros x e segundo, nós podemos de fato ter um conjunto de treinamento grande, e treinar o algoritmo de aprendizagem com muitos parâmetros no conjunto de treinamento, se ambos forem verdade isso geralmente resulta em um algoritmo de aprendizagem com boa performance.