En el video anterior hablamos de las métricas de evaluación. En este video me gustaría desviarme un poco del curso y tocar otro aspecto importante que se presenta seguido en el diseño de sistemas de aprendizaje automático. Se trata de la cuestión de cuántos datos necesito para entrenar. En videos anteriores les advertí acerca de invertir ciegamente nuestro tiempo recolectando muchos datos pensando que es lo único que nos puede ayudar, pero resulta que bajo ciertas circunstancias, en este video hablaré de esas circunstancias, recolectar datos y entrenar con un cierto tipo de algoritmo de aprendizaje puede ser una manera efectiva de mejorar considerablemente el desempeño de un algoritmo de aprendizaje. Esto sucede con frecuencia. Si las circunstancias pueden aplicarse a tu problema y si puedes recolectar muchos datos, esta solución puede ser muy efectiva para obtener un algoritmo de aprendizaje con un excelente desempeño. En este video hablaremos más de ello. Empezaré con una historia: Hace muchos años, dos investigadores que conozco, Michelle Banko y Eric Broule llevaron a cabo un estudio fascinante. Estaban interesados en estudiar los efectos de utilizar algoritmos de aprendizaje distintos en comparación con los efectos de probarlos en conjuntos de entrenamiento de diferentes tamaños. Estaban considerando el problema de la clasificación de palabras confusas. Por ejemplo, en el enunciado “for breakfast I ate” ¿deberá ser “to, two” o “too”? En este ejemplo será “for breakfast I ate (two) (2) eggs”. Para este ejemplo tenemos este conjunto de palabras confusas y este es otro conjunto. Ellos tomaron problemas de aprendizaje automático como estos; es decir, problemas de aprendizaje supervisado para intentar categorizar cuál es la palabra correcta en una posición dada en un enunciado en Inglés. Tomaron diferentes algoritmos de aprendizaje que se consideraban de última generación en ese entonces, cuando realizaron el estudio, en 2001. Tomaron la varianza de una regresión logística y la llamaron Perceptron. También tomaron algunos de los algoritmos que eran muy modernos entonces y ahora son menos usados como el algoritmo Winnow, que también es muy similar a la regresión logística, pero diferente en algunos aspectos. Era muy utilizado y ahora no lo es tanto. Tomaron también un algoritmo de aprendizaje basado en memoria que también se utiliza menos actualmente. Hablaré más de eso adelante. Por último, utilizaron un algoritmo Naïve Bayes, que es de lo que hablaremos en este curso. Los algoritmos exactos de estos detalles no son importantes. Piensa en esto como sólo elegir cuatro algoritmos de clasificaciones diferentes; los algoritmos exactos no son importantes. Lo que hicieron con esto fue variar el tamaño del conjunto de entrenamiento y aplicar estos algoritmos de aprendizaje en la variedad de tamaños de conjuntos de entrenamiento. Este es el resultado que obtuvieron. Las tendencias son muy claras ¿cierto? Primero, la mayoría de estos algoritmos arrojan desempeños muy similares y segundo, a medida que incrementa el conjunto de aprendizaje en el eje horizontal, que es el tamaño del conjunto de entrenamiento en millones, sube de cien mil a mil millones; es decir, un billón de ejemplos de entrenamiento. El desempeño de los algoritmos aumentó monótonamente. De hecho, si eliges cualquier algoritmo, quizá un algoritmo inferior, pero le das más datos, entonces, con base en estos ejemplos, parece que será un algoritmo superior. A partir de este estudio original, que tuvo mucha influencia, han habido una serie de estudios diferentes que muestran resultados similares que indican que muchos algoritmos de aprendizaje diferentes tienden a dar rangos de desempeño muy similares, dependiendo de los detalles. Lo que impulsa el algoritmo es contar con muchos datos de entrenamiento. Resultados como estos han llevado a pensar que en el aprendizaje automático no gana quien tiene el mejor algoritmo, sino quien tiene más datos. Pero, ¿cuando es cierto y cuándo no? Porque tenemos algoritmos de aprendizaje para los cuales es verdad; recolectar muchos datos a veces es la mejor manera de asegurarnos de que tenemos un algoritmo de aprendizaje con un desempeño alto en vez de debatir sobre si vale la pena preocuparse por cuál de estos algoritmos se debe usar. Vamos a tratar de diseñar un conjunto de supuestos bajo los cuales tener un conjunto de entrenamiento masivo sería de ayuda. Supongamos que en nuestro problema de aprendizaje automático, las funciones “x” tienen suficiente información para predecir “y” con precisión. Por ejemplo, si tomamos el problema de palabras confusas que teníamos en la diapositiva anterior, digamos que las funciones “x” capturan las palabras que rodean el espacio en blanco que intentamos llenar; es decir, estas funciones capturan el enunciado “For breakfast I ate _____ eggs”. Esta es información suficiente para decirme que la palabra que falta es “two” y que no es “to” ni “too”. Las funciones capturan las palabras que rodean el espacio y eso me da información suficiente para decidir de manera inequívoca cuál es el valor asignado a “y” o,en otras palabras, cuál es la palabra que debo usar para llenar el espacio de un conjunto de tres palabras posibles. Este es un ejemplo si las funciones “X” tienen suficiente información para especificar “y”. Para ilustrar un ejemplo contrario a este, considera un problema en el que debemos predecir el precio de una casa sólo por el tamaño de la casa, sin tomar en cuenta variables. Las funciones te digo que la casa es de 500 pies cuadrados pero no te doy ninguna otra función. No te digo si está en una zona cara de la ciudad ni el número de habitaciones de la casa o cuál es la calidad de los muebles ni si la casa e vieja o nueva. No te digo nada más que es una casa de 500 pies cuadrados. Pero hay tantos factores que pudieran afectar el precio de una casa aparte del tamaño que, si todo lo que sabes de la casa es el tamaño, será muy difícil predecir su precio con precisión. Este sería un ejemplo contrario al supuesto de que las funciones tienen información suficiente para predecir el precio con el nivel deseado de precisión. Cuando pienso en probar este supuesto, una manera de hacerlo es preguntarme qué pasaría si consultáramos a un humano experto en este campo, y le diéramos las funciones “x” de entrada, es decir, la misma información disponible para un algoritmo de aprendizaje. ¿Podría predecir un experto humano con certeza el valor de “y? Para este ejemplo supongamos que vamos con un experto en el idioma inglés, alguien que habla muy bien el inglés; es decir, la mayoría de las personas como tú y yo. Probablemente seríamos capaces de predecir qué palabra corresponde al espacio? Quien habla inglés puede predecir esto. Esto me da la certeza de que “x” nos permite predecir “y” con precisión. Pero si por el contrario consultamos a un experto en precios, como un experto en bienes raíces o alguien que se dedique a vender casas, les damos el tamaño de la casa y les preguntamos cuál es el precio, ni siquiera ellos, expertos en precios y en ventas de casas, serían capaces de decirme. Entonces, para el ejemplo del precio de casas, saber solamente el precio no me da información suficiente para predecir el precio de la casa. Digamos que este supuesto nos ayuda; veamos en qué casos nos puede ayudar tener muchos datos. Supongamos que las funciones tienen información suficiente para predecir el valor de “y”, y supongamos que vamos a utilizar un algoritmo de aprendizaje con un número mayor de parámetros. Quizá la regresión logística o la regresión lineal con un gran número de funciones. Otra cosa que también hago a veces es utilizar una red neuronal con muchas unidades ocultas. Esa sería otra manera de tener un algoritmo de aprendizaje con muchos parámetros. Todos estos son algoritmos efectivos de aprendizaje con muchos parámetros que pueden ajustar funciones muy complejas. Pensaré en estos como algoritmos de baja oscilación  porque podemos ajustar funciones muy complejas. Debido a que tenemos algoritmos de aprendizaje efectivos que pueden ajustar funciones complejas, las probabilidades apuntan a que, si aplicamos estos algoritmos a conjuntos de datos, seremos capaces de ajustar bien el conjunto de entrenamiento y, con suerte, el error de entrenamiento será bajo. Ahora digamos que utilizamos un conjunto de entrenamiento masivo. En ese caso, si tenemos un conjunto de entrenamiento masivo, entonces, aunque tengamos muchos parámetros, siempre y cuando el conjunto de entrenamiento sea más grande que el número de parámetros, no será probable que estos algoritmos causen sobreajuste. Justo porque tenemos un conjunto de aprendizaje muy grande. Cuando digo que no es probable que cause sobreajuste me refiero a que el error de entrenamiento será, con suerte, cercano al error de prueba. Por último, si vemos al error de prueba y de aprendizaje juntos, si el error del conjunto de entrenamiento es bajo y el error del conjunto de prueba es cercano al error de entrenamiento, lo que significa es que el error del conjunto de prueba también será bajo. Otra manera de pensar en esto es que, para tener un desempeño alto, el algoritmo de aprendizaje que queremos no debe tener un alta oscilación  ni una varianza alta. Vamos a tratar el problema de sesgo asegurándonos de tener un algoritmo de aprendizaje con muchos parámetros. Esto genera un algoritmo de baja oscilación. Y para asegurarnos de evitar un problema de varianza, podemos utilizar un conjunto de entrenamiento muy grande. Así, nuestro algoritmo tendrá una varianza baja. Cuando ponemos estos dos elementos juntos, obtenemos un algoritmo de oscilación y varianza bajos y esto nos permite tener un buen desempeño en el conjunto de prueba. Además, los ingredientes clave que nos garantizan que las funciones tienen información suficiente y tienen una clase rica en funciones es lo que garantiza el baja oscilación, y por tanto tiene un conjunto de entrenamiento masivo que garantiza más varianza. Esto nos da un conjunto de condiciones y un entendimiento de los tipos de problemas que podemos resolver cuando tenemos muchos datos y entrenamos un algoritmo de aprendizaje con muchos parámetros que podrían mejorar el desempeño de nuestro algoritmo de aprendizaje. Y, creo que las pruebas clave que me pregunto generalmente son ¿puede un experto humano tomar las funciones “x” y predecir con certeza el valor de “y”? Esta sería una confirmación de que “y” puede ser predicha con certeza a partir de las funciones “x”. Y la segunda prueba clave es, ¿podemos obtener un conjunto de entrenamiento grande, y entrenar el algoritmo de aprendizaje con muchos parámetros en el conjunto de aprendizaje? Si puedes realizar ambas pruebas, obtendrás un algoritmo de aprendizaje con un buen desempeño.