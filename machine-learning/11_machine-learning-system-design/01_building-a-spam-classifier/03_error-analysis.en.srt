1
00:00:00,220 --> 00:00:04,610
In the last video I talked about how, when
faced with a machine learning problem,

2
00:00:04,610 --> 00:00:08,320
there are often lots of different ideas
for how to improve the algorithm.

3
00:00:08,320 --> 00:00:12,280
In this video, let's talk about
the concept of error analysis.

4
00:00:12,280 --> 00:00:15,293
Which will hopefully give you a way
to more systematically make some of

5
00:00:15,293 --> 00:00:16,120
these decisions.

6
00:00:17,530 --> 00:00:21,030
If you're starting work on
a machine learning problem, or

7
00:00:21,030 --> 00:00:22,705
building a machine learning application.

8
00:00:22,705 --> 00:00:26,600
It's often considered very
good practice to start,

9
00:00:26,600 --> 00:00:31,460
not by building a very complicated system
with lots of complex features and so on.

10
00:00:31,460 --> 00:00:35,560
But to instead start by building
a very simple algorithm that you can

11
00:00:35,560 --> 00:00:37,220
implement quickly.

12
00:00:37,220 --> 00:00:40,930
And when I start with a learning
problem what I usually do is spend

13
00:00:40,930 --> 00:00:41,930
at most one day,

14
00:00:41,930 --> 00:00:46,990
like literally at most 24 hours, To try
to get something really quick and dirty.

15
00:00:46,990 --> 00:00:50,520
Frankly not at all sophisticated system
but get something really quick and

16
00:00:50,520 --> 00:00:55,920
dirty running, and implement it and
then test it on my cross-validation data.

17
00:00:55,920 --> 00:00:59,424
Once you've done that you can
then plot learning curves,

18
00:00:59,424 --> 00:01:02,940
this is what we talked about
in the previous set of videos.

19
00:01:02,940 --> 00:01:05,368
But plot learning curves
of the training and

20
00:01:05,368 --> 00:01:09,962
test errors to try to figure out if you're
learning algorithm maybe suffering from

21
00:01:09,962 --> 00:01:13,160
high bias or high variance,
or something else.

22
00:01:13,160 --> 00:01:16,100
And use that to try to
decide if having more data,

23
00:01:16,100 --> 00:01:18,700
more features, and so
on are likely to help.

24
00:01:18,700 --> 00:01:22,440
And the reason that this is a good
approach is often, when you're just

25
00:01:22,440 --> 00:01:26,490
starting out on a learning problem,
there's really no way to tell in advance.

26
00:01:26,490 --> 00:01:29,990
Whether you need more complex features,
or whether you need more data,

27
00:01:29,990 --> 00:01:31,310
or something else.

28
00:01:31,310 --> 00:01:33,910
And it's just very hard to
tell in advance, that is,

29
00:01:33,910 --> 00:01:38,795
in the absence of evidence,
in the absence of seeing a learning curve.

30
00:01:38,795 --> 00:01:41,380
It's just incredibly
difficult to figure out

31
00:01:41,380 --> 00:01:43,770
where you should be spending your time.

32
00:01:43,770 --> 00:01:47,730
And it's often by implementing even a
very, very quick and dirty implementation.

33
00:01:47,730 --> 00:01:52,400
And by plotting learning curves,
that helps you make these decisions.

34
00:01:52,400 --> 00:01:56,789
So if you like you can to think of this as
a way of avoiding whats sometimes called

35
00:01:56,789 --> 00:01:59,730
premature optimization
in computer programming.

36
00:01:59,730 --> 00:02:05,037
And this idea that says we should let
evidence guide our decisions on where

37
00:02:05,037 --> 00:02:10,980
to spend our time rather than use
gut feeling, which is often wrong.

38
00:02:10,980 --> 00:02:13,070
In addition to plotting learning curves,

39
00:02:13,070 --> 00:02:18,080
one other thing that's often very useful
to do is what's called error analysis.

40
00:02:18,080 --> 00:02:21,940
And what I mean by that is that when
building say a spam classifier.

41
00:02:21,940 --> 00:02:25,684
I will often look at my
cross validation set and

42
00:02:25,684 --> 00:02:31,170
manually look at the emails that
my algorithm is making errors on.

43
00:02:31,170 --> 00:02:33,000
So look at the spam e-mails and

44
00:02:33,000 --> 00:02:37,850
non-spam e-mails that the algorithm
is misclassifying and see if you can

45
00:02:37,850 --> 00:02:43,000
spot any systematic patterns in what
type of examples it is misclassifying.

46
00:02:43,000 --> 00:02:44,833
And often, by doing that,

47
00:02:44,833 --> 00:02:49,230
this is the process that will
inspire you to design new features.

48
00:02:49,230 --> 00:02:51,897
Or they'll tell you what
are the current things or

49
00:02:51,897 --> 00:02:53,940
current shortcomings of the system.

50
00:02:53,940 --> 00:02:58,620
And give you the inspiration you need
to come up with improvements to it.

51
00:02:58,620 --> 00:03:01,350
Concretely, here's a specific example.

52
00:03:01,350 --> 00:03:05,770
Let's say you've built
a spam classifier and

53
00:03:05,770 --> 00:03:09,690
you have 500 examples in
your cross validation set.

54
00:03:09,690 --> 00:03:13,690
And let's say in this example that
the algorithm has a very high error rate.

55
00:03:13,690 --> 00:03:17,810
And this classifies 100 of these
cross validation examples.

56
00:03:18,810 --> 00:03:24,490
So what I do is manually examine these
100 errors and manually categorize them.

57
00:03:24,490 --> 00:03:28,010
Based on things like what type
of email it is, what cues or

58
00:03:28,010 --> 00:03:32,470
what features you think might have helped
the algorithm classify them correctly.

59
00:03:32,470 --> 00:03:38,469
So, specifically, by what type of email it
is, if I look through these 100 errors,

60
00:03:38,469 --> 00:03:43,007
I might find that maybe the most
common types of spam emails in these

61
00:03:43,007 --> 00:03:48,910
classifies are maybe emails on pharma or
pharmacies, trying to sell drugs.

62
00:03:48,910 --> 00:03:53,080
Maybe emails that are trying to
sell replicas such as fake watches,

63
00:03:53,080 --> 00:04:00,340
fake random things, maybe some
emails trying to steal passwords,.

64
00:04:00,340 --> 00:04:04,693
These are also called phishing emails,
that's another big category of emails, and

65
00:04:04,693 --> 00:04:06,030
maybe other categories.

66
00:04:06,030 --> 00:04:09,249
So in terms of classify
what type of email it is,

67
00:04:09,249 --> 00:04:14,030
I would actually go through and
count up my hundred emails.

68
00:04:14,030 --> 00:04:19,460
Maybe I find that 12 of them is label
emails, or pharma emails, and maybe 4 of

69
00:04:19,460 --> 00:04:23,700
them are emails trying to sell replicas,
that sell fake watches or something.

70
00:04:23,700 --> 00:04:29,000
And maybe I find that 53 of them
are these what's called phishing emails,

71
00:04:29,000 --> 00:04:32,660
basically emails trying to persuade
you to give them your password.

72
00:04:32,660 --> 00:04:35,330
And 31 emails are other types of emails.

73
00:04:35,330 --> 00:04:37,860
And it's by counting up the number of

74
00:04:37,860 --> 00:04:41,370
emails in these different categories
that you might discover, for example.

75
00:04:41,370 --> 00:04:43,015
That the algorithm is doing really,

76
00:04:43,015 --> 00:04:47,170
particularly poorly on emails
trying to steal passwords.

77
00:04:47,170 --> 00:04:50,310
And that may suggest that it
might be worth your effort

78
00:04:50,310 --> 00:04:53,120
to look more carefully at
that type of email and

79
00:04:53,120 --> 00:04:56,540
see if you can come up with better
features to categorize them correctly.

80
00:04:57,550 --> 00:05:01,100
And, also what I might do
is look at what cues or

81
00:05:01,100 --> 00:05:05,830
what additional features might have
helped the algorithm classify the emails.

82
00:05:05,830 --> 00:05:09,830
So let's say that some of our
hypotheses about things or

83
00:05:09,830 --> 00:05:13,280
features that might help us
classify emails better are.

84
00:05:13,280 --> 00:05:17,751
Trying to detect deliberate
misspellings versus unusual email

85
00:05:17,751 --> 00:05:21,150
routing versus unusual
spamming punctuation.

86
00:05:21,150 --> 00:05:23,700
Such as if people use a lot
of exclamation marks.

87
00:05:23,700 --> 00:05:28,885
And once again I would manually go
through and let's say I find five cases

88
00:05:28,885 --> 00:05:34,760
of this and 16 of this and 32 of this and
a bunch of other types of emails as well.

89
00:05:34,760 --> 00:05:39,550
And if this is what you get on your cross
validation set, then it really tells you

90
00:05:39,550 --> 00:05:43,906
that maybe deliberate spellings is
a sufficiently rare phenomenon that maybe

91
00:05:43,906 --> 00:05:49,530
it's not worth all the time trying to
write algorithms that detect that.

92
00:05:49,530 --> 00:05:54,320
But if you find that a lot of spammers are
using, you know, unusual punctuation, then

93
00:05:54,320 --> 00:05:58,760
maybe that's a strong sign that it might
actually be worth your while to spend

94
00:05:58,760 --> 00:06:03,350
the time to develop more sophisticated
features based on the punctuation.

95
00:06:03,350 --> 00:06:07,580
So this sort of error analysis,
which is really the process

96
00:06:07,580 --> 00:06:11,560
of manually examining the mistakes
that the algorithm makes,

97
00:06:11,560 --> 00:06:16,000
can often help guide you to the most
fruitful avenues to pursue.

98
00:06:16,000 --> 00:06:19,738
And this also explains why I often
recommend implementing a quick and

99
00:06:19,738 --> 00:06:21,940
dirty implementation of an algorithm.

100
00:06:21,940 --> 00:06:26,210
What we really want to do is figure out
what are the most difficult examples for

101
00:06:26,210 --> 00:06:27,730
an algorithm to classify.

102
00:06:27,730 --> 00:06:32,502
And very often for different algorithms,
for different learning algorithms

103
00:06:32,502 --> 00:06:37,000
they'll often find similar
categories of examples difficult.

104
00:06:37,000 --> 00:06:38,100
And by having a quick and

105
00:06:38,100 --> 00:06:42,940
dirty implementation, that's often a quick
way to let you identify some errors and

106
00:06:42,940 --> 00:06:45,970
quickly identify what
are the hard examples.

107
00:06:45,970 --> 00:06:48,160
So that you can focus
your effort on those.

108
00:06:49,240 --> 00:06:54,280
Lastly, when developing learning
algorithms, one other useful tip is

109
00:06:54,280 --> 00:07:00,150
to make sure that you have a numerical
evaluation of your learning algorithm.

110
00:07:01,915 --> 00:07:05,560
And what I mean by that is you if
you're developing a learning algorithm,

111
00:07:05,560 --> 00:07:08,090
it's often incredibly helpful.

112
00:07:08,090 --> 00:07:11,300
If you have a way of evaluating
your learning algorithm

113
00:07:11,300 --> 00:07:15,670
that just gives you back a single real
number, maybe accuracy, maybe error.

114
00:07:15,670 --> 00:07:20,330
But the single real number that tells you
how well your learning algorithm is doing.

115
00:07:20,330 --> 00:07:23,870
I'll talk more about this specific
concept in later videos, but

116
00:07:23,870 --> 00:07:25,810
here's a specific example.

117
00:07:25,810 --> 00:07:29,127
Let's say we're trying to decide whether
or not we should treat words like

118
00:07:29,127 --> 00:07:32,320
discount, discounts, discounted,
discounting as the same word.

119
00:07:32,320 --> 00:07:36,420
So you know maybe one way to do
that is to just look at the first

120
00:07:37,630 --> 00:07:39,680
few characters in the word like, you know.

121
00:07:39,680 --> 00:07:43,250
If you just look at the first few
characters of a word, then you

122
00:07:44,600 --> 00:07:48,640
figure out that maybe all of these
words roughly have similar meanings.

123
00:07:50,500 --> 00:07:54,585
In natural language processing, the way
that this is done is actually using a type

124
00:07:54,585 --> 00:07:56,610
of software called stemming software.

125
00:07:56,610 --> 00:08:00,462
And if you ever want to do this yourself,
search on a web-search engine for

126
00:08:00,462 --> 00:08:04,314
the porter stemmer, and that would be
one reasonable piece of software for

127
00:08:04,314 --> 00:08:07,984
doing this sort of stemming,
which will let you treat all these words,

128
00:08:07,984 --> 00:08:10,760
discount, discounts, and
so on, as the same word.

129
00:08:13,990 --> 00:08:18,610
But using a stemming software that
basically looks at the first few

130
00:08:18,610 --> 00:08:22,260
alphabets of a word, more of less,
it can help, but it can hurt.

131
00:08:22,260 --> 00:08:25,920
And it can hurt because for
example, the software may

132
00:08:25,920 --> 00:08:30,670
mistake the words universe and
university as being the same thing.

133
00:08:30,670 --> 00:08:34,900
Because, you know, these two words
start off with the same alphabets.

134
00:08:37,320 --> 00:08:42,642
So if you're trying to decide whether or
not to use stemming software for

135
00:08:42,642 --> 00:08:46,360
a spam cross classifier,
it's not always easy to tell.

136
00:08:46,360 --> 00:08:51,190
And in particular, error analysis
may not actually be helpful for

137
00:08:51,190 --> 00:08:55,610
deciding if this sort of
stemming idea is a good idea.

138
00:08:55,610 --> 00:09:00,129
Instead, the best way to figure out
if using stemming software is good to

139
00:09:00,129 --> 00:09:04,424
help your classifier is if you have
a way to very quickly just try it and

140
00:09:04,424 --> 00:09:05,480
see if it works.

141
00:09:08,400 --> 00:09:09,903
And in order to do this,

142
00:09:09,903 --> 00:09:15,990
having a way to numerically evaluate your
algorithm is going to be very helpful.

143
00:09:15,990 --> 00:09:20,980
Concretely, maybe the most natural thing
to do is to look at the cross validation

144
00:09:20,980 --> 00:09:24,690
error of the algorithm's performance
with and without stemming.

145
00:09:24,690 --> 00:09:27,250
So, if you run your algorithm
without stemming and

146
00:09:27,250 --> 00:09:31,250
end up with 5 percent
classification error.

147
00:09:31,250 --> 00:09:32,670
And you rerun it and

148
00:09:32,670 --> 00:09:37,700
you end up with 3 percent classification
error, then this decrease in error

149
00:09:37,700 --> 00:09:43,140
very quickly allows you to decide that it
looks like using stemming is a good idea.

150
00:09:43,140 --> 00:09:46,570
For this particular problem,
there's a very natural, single,

151
00:09:46,570 --> 00:09:50,960
real number evaluation metric,
namely the cross validation error.

152
00:09:50,960 --> 00:09:54,790
We'll see later examples where
coming up with this sort of single,

153
00:09:54,790 --> 00:09:58,840
real number evaluation metric
will need a little bit more work.

154
00:09:58,840 --> 00:10:03,230
But as we'll see in a later video,
doing so would also then let you make

155
00:10:03,230 --> 00:10:06,660
these decisions much more quickly of say,
whether or not to use stemming.

156
00:10:08,180 --> 00:10:13,010
And, just as one more quick example, let's
say that you're also trying to decide

157
00:10:13,010 --> 00:10:15,970
whether or not to distinguish
between upper versus lower case.

158
00:10:15,970 --> 00:10:20,480
So, you know, as the word, mom,
were upper case, and versus lower case m,

159
00:10:20,480 --> 00:10:24,090
should that be treated as the same word or
as different words?

160
00:10:24,090 --> 00:10:26,609
Should this be treated as the same
feature, or as different features?

161
00:10:27,610 --> 00:10:31,110
And so, once again, because we have
a way to evaluate our algorithm.

162
00:10:31,110 --> 00:10:35,300
If you try this down here,
if I stopped distinguishing upper and

163
00:10:35,300 --> 00:10:39,280
lower case,
maybe I end up with 3.2 percent error.

164
00:10:39,280 --> 00:10:44,270
And I find that therefore, this does
worse than if I use only stemming.

165
00:10:44,270 --> 00:10:48,850
So, this let's me very quickly
decide to go ahead and

166
00:10:48,850 --> 00:10:52,190
to distinguish or to not distinguish
between upper and lowercase.

167
00:10:52,190 --> 00:10:56,590
So when you're developing a learning
algorithm, very often you'll be trying out

168
00:10:56,590 --> 00:11:00,990
lots of new ideas and lots of new
versions of your learning algorithm.

169
00:11:00,990 --> 00:11:04,370
If every time you try out a new idea,
if you end up

170
00:11:04,370 --> 00:11:07,810
manually examining a bunch of examples
again to see if it got better or

171
00:11:07,810 --> 00:11:11,530
worse, that's gonna make it
really hard to make decisions on.

172
00:11:11,530 --> 00:11:12,630
Do you use stemming or not?

173
00:11:12,630 --> 00:11:15,220
Do you distinguish upper and
lower case or not?

174
00:11:15,220 --> 00:11:18,470
But by having a single real
number evaluation metric,

175
00:11:18,470 --> 00:11:21,850
you can then just look and see, oh,
did the arrow go up or did it go down?

176
00:11:21,850 --> 00:11:27,430
And you can use that to much more
rapidly try out new ideas and

177
00:11:27,430 --> 00:11:31,370
almost right away tell if
your new idea has improved or

178
00:11:31,370 --> 00:11:34,100
worsened the performance
of the learning algorithm.

179
00:11:34,100 --> 00:11:38,190
And this will let you often
make much faster progress.

180
00:11:38,190 --> 00:11:42,410
So the recommended, strongly recommended
the way to do error analysis is

181
00:11:42,410 --> 00:11:45,530
on the cross validations there
rather than the test set.

182
00:11:45,530 --> 00:11:49,500
But, you know, there are people that will
do this on the test set, even though

183
00:11:49,500 --> 00:11:54,590
that's definitely a less mathematic
appropriate, certainly a less recommended

184
00:11:54,590 --> 00:11:58,640
way to, thing to do than to do error
analysis on your cross validation set.

185
00:11:58,640 --> 00:12:03,390
Set to wrap up this video, when starting
on a new machine learning problem,

186
00:12:03,390 --> 00:12:07,070
what I almost always recommend
is to implement a quick and

187
00:12:07,070 --> 00:12:10,120
dirty implementation of
your learning out of them.

188
00:12:10,120 --> 00:12:15,170
And I've almost never seen anyone spend
too little time on this quick and

189
00:12:15,170 --> 00:12:16,500
dirty implementation.

190
00:12:16,500 --> 00:12:22,420
I've pretty much only ever seen
people spend much too much

191
00:12:22,420 --> 00:12:26,550
time building their first, supposedly,
quick and dirty implementation.

192
00:12:26,550 --> 00:12:29,810
So really,
don't worry about it being too quick, or

193
00:12:29,810 --> 00:12:32,080
don't worry about it being too dirty.

194
00:12:32,080 --> 00:12:35,120
But really,
implement something as quickly as you can.

195
00:12:35,120 --> 00:12:37,560
And once you have
the initial implementation,

196
00:12:37,560 --> 00:12:41,170
this is then a powerful tool for
deciding where to spend your time next.

197
00:12:41,170 --> 00:12:43,674
Because first you can look
at the errors it makes, and

198
00:12:43,674 --> 00:12:46,958
do this sort of error analysis to see
what other mistakes it makes, and

199
00:12:46,958 --> 00:12:49,500
use that to inspire further development.

200
00:12:49,500 --> 00:12:51,390
And second, assuming your quick and

201
00:12:51,390 --> 00:12:55,710
dirty implementation incorporated
a single real number evaluation metric.

202
00:12:55,710 --> 00:12:59,970
This can then be a vehicle for
you to try out different ideas and

203
00:12:59,970 --> 00:13:03,880
quickly see if the different ideas you're
trying out are improving the performance

204
00:13:03,880 --> 00:13:04,990
of your algorithm.

205
00:13:04,990 --> 00:13:08,470
And therefore let you, maybe much more
quickly make decisions about what

206
00:13:08,470 --> 00:13:11,835
things to fold in and what things to
incorporate into your learning algorithm.