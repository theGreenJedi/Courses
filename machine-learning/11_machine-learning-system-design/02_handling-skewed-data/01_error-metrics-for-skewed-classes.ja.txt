前回のビデオでは エラー分析と エラーのメトリクスを持つ必要性を議論した。 エラーのメトリクスとは 学習アルゴリズムがどれだけうまくやっているかを語る 単一の実数値による評価指標の事だった。 評価とエラーのメトリクスという 文脈においては、 一つの重要なケースとして あなたの学習アルゴリズムに対して 適切なエラーの指標、 適切な評価の指標を得るのが トリッキーになってまう場合というのがある。 そのケースとは、いわゆる スキューした(歪んだ)クラス、と言われる物だ。 それがどういう事か、説明しよう。 ガンの分類の問題を考えよう。 医療患者のフィーチャーがあって、 そして彼らがガンかどうかを 判断したいとする。 つまりこれは 以前に見た、腫瘍が 悪性か良性かの分類の例に似ている。 ガンの患者をy=1と、 そしてそれ以外の患者を y=0としよう。 ロジスティック回帰の分類器をトレーニングして テストセットに対して 分類器をテストしてみたら、 1パーセントのエラーを得たとする。 つまり99パーセント正しい診断をした。 うーん、これはなかなか素晴らしい結果、、、かしら？ 99パーセントの場合は正しいんだから。 でもここで、 トレーニングとテストセットの0.5パーセントの患者しか 実際にガンに かかってないとする。 つまりたった 1パーセントの半分の患者しか、 スクリーニングプロセスを越えて実際にガンとならない。 この場合、1パーセントのエラーは もはやそんなに素晴らしくは見えない。 特に、ここにあるコードなら、 これは実の所学習すらしてない コードで、単に入力として フィーチャーxを受け取りながらただそれを無視して y=0と言うだけのコード。 そしてどんな時でも、誰もガンじゃない、と 予測するという物。 このアルゴリズムは実際に 0.5パーセントのエラーを得る。 つまりこんなのですら さっき得た1パーセントのエラーよりマシになってしまう。 そしてこれは非学習アルゴリズムで 見ての通り、単にどんな時でも y=0と予測するだけの物だ。 つまり、このような状況、 陽性の手本と陰性の手本の割合が とても両極端となるような 状況では、 その場合、 陽性の手本の総数が 陰性の手本の総数よりも ずっと、ずっと少ない、何故なら y=1はとてもレアだから。 これが我らが スキューしたクラス、と呼ぶ物だ。 単にある一方のクラスが、 他方のクラスよりも ずっとたくさんあるような場合。 そういう場合は単にいつでも y=0と予測しておけば、 またはいつでもy=1と予測しておけば、 アルゴリズムはとてもうまく振る舞う事が出来る。 さて、分類エラー、または分類の正確さを 評価指標として使う際の 問題点は、以下のようになる。 あなたは手元に、99.2%の正確さが得られる アルゴリズムを持ってたとしよう。 つまりエラーは0.8%だ。 そしてあなたは自分のアルゴリズムに 変更を加えて、 そして今やあなたは 99.5%の正確さを得たとする。 つまり0.5%のエラーだ。 ではこれは、アルゴリズムの改善と言えるだろうか？ 単一の実数評価指標を 持っている事の 利点の一つとしては、 これが我らに、アルゴリズムに良い変更をしたかを 素早く判断する助けとなるというのがある。 99.2%の正確さが99.5%の正確さになったら、 何か有用な事をしたのだろうか？それとも 我らのコードを単に y=0を単により多く 予測するだけのコードに 置き換えたのだろうか？ つまり、とてもスキューしたクラスの場合には、 単に分類の正確さを使うというのは より難しくなる。何故なら とても高い分類の正確さを得る、またはとても低いエラーを 得るという事は出来て、 そしてそれがあなたの分類器を 改善したかは、 いつも明らかという訳では無い、 何故ならどんな時でもy=0を予測する、 というのは、そんなに良い分類器とは 思えない。 だが、単にy=0と、より多く予測するだけで、 エラーを減少させる事が 出来てしまい、これは0.5%まで 減らす事が出来る。 スキューしたクラスに 直面している時には、 もっと別のエラーの指標、 あるいは別の評価指標が、 欲しくなる。 そんな評価指標の一つには、 Preicision(精度)とRecall(再現率)という物がある。 これが何なのかを説明していこう。 テストセットに対して分類器を評価しているとしよう。 例えば テストセットにある 手本の 実際のクラスが 1か0のどちらかとする。 つまりバイナリ分類問題だ。 そして我らの学習アルゴリズムが 行う事は、 そのクラスの何らかの値を予測する事で、 だから我らの学習アルゴリズムは テストセットの各手本に対して 値を予測し、 その予測する値もまた 1か0のどちらかだ。 そこで以下のように2x2のテーブルを 書いてみよう。 これらのエントリは、 実際のクラスと予測されたクラスに 基づいて埋める。 もし我らが、 実際のクラスが1で 予測されたクラスが1の手本を持つ時には、 それはtrue positive(真陽性)と 呼ばれる手本である。 その意味する所は、我らのアルゴリズムは それが陽性だと予測して、しかも実際にも陽性の 手本だった場合。 もし我らのアルゴリズムが ある物を陰性、クラス0と予測して、 そして実際のクラスもまたクラス0だった時には、 それはtrue negative(真陰性）と呼ばれる。 我らは0と予測し、実際に0だ。 その他の二つの箱は、 我らの学習アルゴリズムが クラス1だと予測したが、 実際のクラスが0の時には、 これはfalse positive (偽陽性)と呼ばれる。 その意味する所は、我らのアルゴリズムは、 ある患者がガンを持っていると予測しておきながら、 実際には患者はガンを持っていなかった。 そして最後の箱は、0、1。 これはfalse negative（偽陰性）と呼ぶ。 何故なら我らのアルゴリズムは0を予測し、 しかし実際のクラスは1だから。 こうして、我らは 小さなある種の2x2のテーブルを得た、 それは実際のクラスと予測されたクラスに 基づいた物だ。 そしてここに、 我らのアルゴリズムのパフォーマンスを評価する 別の方法がある。 我らは二つの数を計算する事にする。 最初の物は、Precision（精度）と呼ばれる物。 それが伝える事は、 我らがガンだと予測した 全ての患者のうち、 どれだけの割合の人が実際にガンだったのか？ これを書き下してみよう。 分類器のPrecisionとは、 true positiveの総数を 陽性と予測した総数で 割った物だ。 いいかい？ つまり、我らが実際に赴いて、 「あなたはガンだと、我らは思ってる」と告げた患者のうち、 それらの患者全員の中で、 実際にガンを持ってる割合はどれだけか？ それがPrecision(精度)と言われる物だ。 そしてこれの別の書き方としては、 true positiveと そして分母は、 陽性と予測された総数、 それはつまり テーブルの最初の行の エントリの和だ。 つまりそれは、true positiveを割る事のtrue positiveに… ここでpositiveをPOS、と略す事にしよう、 そしてそこに足す事の false positive、再びpositiveをPOSと 省略して書く。 これがPrecision（精度）と呼ばれる物だ。 見て分かるように、高いPrecisionは良い事だ。 その意味する所は、我らが赴き、 「大変残念ですが、あなたはガンだと思います」と 告げた患者全てに対して、 高いPrecisionが意味する事は、その患者のグループの 大多数が、 我らが正しく予測を行った事になり、 つまり実際にガンを持っている。 我らが計算する二番目の数値は Recall（再現率）と呼ばれる物で、 Recallが言う事は、 テストセットなり クロスバリデーションセットなり、 とにかくデータセットのうち全てのガンの患者 に対して、 彼らのうち どれだけの割合を 正しくガンだと、検出出来たか？という事。 つまり、全ての患者がガンだったとして、 それらのうち、何人の元に 我らは実際におもむき、 正しくも「あなた方には治療が必要だと我らは思っている」と告げる事が出来るだろうか？ つまり、これを書き下すと、 Recall(再現率)は以下のように定義出来る： 陽性の、、、true positiveの 総数、つまり、 実際にガンを持ってる患者の中から、 我らが正しくガンを持っていると 予測出来た総数だが、 それを割る事の、 実際に陽性である患者の 総数で割る。 つまりこれは、 実際にガンを持っている人々の正しい陽性の数だ。 どれだけの割合にフラグ立てして 処置に送れるか？ つまり、これを違う形に 書き直すと、分母は 実際の陽性の数だから、 つまりそれは ここの最初の列のエントリの和だ。 つまり、別の書き方で書くと、 これはつまり、true positiveの総数を true positiveの総数 足す事の false negativeで 割った物、 という事になる。 そしてここでも、高いRecall(再現率)になるのは、良い事だ。 そしてPrecision(精度)とRecall(再現率)を計算する事により、 我らは分類器がどれだけ うまく振舞っているかについて、 より良い感覚を得られる。 そして例えば、 いつもy=0を予測するような 学習アルゴリズムだとすると、 それが一回もガンだと 予測しないとすると、 その場合、この分類器は Recall(再現率) = 0となる。 何故ならそこには、 true positiveは全く無いから。 つまりこれは、 分類器がいつもy=0を予測しているという事を知る 簡単な方法となっている。 それはとても良い分類器、とは言えなかろう。 そしてより一般的に言って、 とてもスキューしたクラスの セッティングにおいても、 アルゴリズムが ある種のチート（ずる）をして、 とても高いPrecision(精度)と とても高いRecall(再現率)を いつもy=0と予測する、というような 何かしら簡単な方法で 達成するのは 不可能だ。 だから、高いPrecisionまたは高いRecallの 分類器というのは、 実際に良い分類器だという事に より確信を持てる。 そしてこれは我らに、 より有用な評価指標で 我らのアルゴリズムがうまい事やっているかを より直接的に理解させてくれるような物を、与えてくれる。 さて、Precision（精度）とRecall（再現率）の定義の 最後のコメントとして、 我らはPrecisionとRecallを 普通はy=1が 普通、より稀なクラスが存在している、という方を コンベンションを用いる。 つまり我らはレアな条件、例えば ガンなどを検出したい時、 それはレアな条件であって欲しい訳だが、 その時には、PrecisionとRecallは y=0に対してでは無くy=1に対して 定義される。 つまり我らが検出しようとする そのレアなクラスが存在する、という場合に 基づいて。 そしてPrecision(精度)とRecall(再現率)を使う事で、 起こる事というと、 とてもスキューしたクラスの 場合だとしても、 アルゴリズムが「チート」（ずる）して、 いかなる時もy=1を予測したり、 あるいはいかなる時でもy=0を 予測する事で、高いPrecision(精度)とかRecall(再現率)を得る事は 不可能である、という事だ。 そして特に、 もし分類器で高いPrecisionとRecallが得られたなら、 我らはアルゴリズムが とてもうまく機能しているだろう事に 実際にしっかりと確信が持てる、 たとえスキューしたクラスだったとしても。 つまり、スキューしたクラスの問題に関しては、 PrecisionとRecallは我らに アルゴリズムが実際にどうなってるか、についての より直接的な洞察を与えてくれて、 そしてこれはしばしば、 我らの学習アルゴリズムを評価する、より良い方法だ。 単に分類エラーや分類の正確さを見るだけに比べると。 クラスがとてもスキューしている時には。