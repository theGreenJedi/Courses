前回のビデオで、 我らはPrecision（精度）とRecall（再現率）という物を スキューしたクラスの分類問題に関する 評価指標として議論した。 多くの応用において、 PrecisionとRecallの間のトレードオフを なんらかの形で取りたいと思う事になる。 それをどうやってやれば良いのか、話していこう。 そしてまた、学習アルゴリズムの 評価メトリクスとして PrecisionとRecallを、 更に有効にする方法も お見せする。 まずは思い出してもらう為に これがら前回のビデオから抜き出した PrecisionとRecallの定義だ。 継続してガンの分類問題を使っていこう、 そこではもし患者がガンだったら y=1で、 それ以外だったらy=0だった。 そしてロジスティック回帰分類器を 訓練する事にし、それで 0と1の間の確率を出力する。 つまり、いつも通り、 1と予測する、つまりy =1と予測するのは hのxが 0.5以上の時で、 そして0と予測するのは 仮説の出力した値が 0.5未満だった時とする。 そしてこの分類器が我らに なんらかのPrecisionとRecallの値を与えてくれる訳だ。 しかしここで、我らが 患者がガンだと予測するのは 本当に彼らがガンだととても深く確信を持ってる時だけにしたい、 と仮定してみよう。 何故なら、知っての通り、 もしあなたが患者の所に行き 彼らがガンだ、と告げると、 その事は彼らにとって大ショックだろう、 何故ならこれはきわめて深刻な 知らせで、そして彼らは とても痛みを伴う治療過程を 受けるはめになりそうだから。 だから我らがある人に あなたはガンだと思う、と告げるのは とても確信がある時だけにしたい。 これを行う一つの方法としては アルゴリズムを修正して、 しきい値を0.5に 設定する代わりに 我らはy =1 と 予測するのを hのxが0.7以上の時だけにする、 という手が考えられる。 つまりこうして、 ある人にガンだと告げるのは 我らが彼らをガンだと思う確率が 70パーセント以上の 時だけにする訳だ。 そしてもしこのやり方をやれば、 あなたはある人をガンだと予測するのは、 あなたがより確信を持ってる時だけとなり、 それは結局、 より高いPrecisionの分類器を 得る事となる。 何故ならあなたが赴いて 「あなたはガンだろう」と告げる事になる 患者は全て、 それらの患者は全て、ここでは 彼らがひとたびその報告を聞けば、 彼らが実際にガンだととても確信が持てるのだから。 つまりはあなたがガンだと 予測した患者のより多くの割合が 実際にもガンとなる、 何故なら我らはそれらの予測に 極めて確信を持ってるから。 しかし対照的に、この分類器は より低いRecallとなる。何故なら ここでは我らは 予測をする時に、、、 我らはy=1と予測するのを、より少ない数の患者にしかしなくなるから。 ここでこれをさらに進めて、 しきい値を0.7に設定する代わりに これを0.9に設定して、 yが1と予測するのを 90パーセント以上確かに その患者がガンだと思う ときだけにする、という事も出来る。 その場合、それらの患者は 大部分が 実際にもガンだろう、つまり これは高いPrecisionで、より低いRecallの 分類器となるだろう、 何故なら我らは、それらの患者がガンだと、正しく検出したいから。 次に別の例を考えよう。 我らは実際のガンを 見逃しすぎるのを避けたいと仮定する。 つまりfalse negative（偽陰性）を避けたいとする。 具体的には、もし患者が実際に ガンの時に、もし我らがそうした人たちに あなたたちはガンだ、と言い損なったとすると、 それは極めてまずい事態となりうる。 何故ならもし我らが 患者に、あなたはガンじゃ無い、と 告げたとすると、 彼らはさらなる治療を受けには行かないだろう。 もし彼らが結局ガンだと 判明して、しかも彼らに ガンだと告げ損なったとすると、 彼らは全く治療を受けないだろう。 つまり、それは極めてまずい 結果と思われる。 何故なら彼は死んでしまうだろうから、 我らが彼らにガンでは無いと言ってしまったせいで、 彼らは治療を受け損ない、だが実際にガンだと のちに判明してしまったのだから。 疑わしい時には、 我らはy=1と予測したい。 つまり疑わしければ、 彼らはガンだ、と予想したい、 そうする事で少なくとも さらなる調査がなされるように。 そして最終的にガンだと判明したら 治療を受けさせる事が可能となるから。 この場合には、閾値を より高い確率に設定するのでは無く、 むしろこの値を より低い値に 設定する事になる。そうだなぁ、例えば 0.3とかそんな感じの。 そうする事で、我らはこう言ってる訳だ： 30%以上の確率で ガンだと思う時には、 念のため より保守的になって、 ガンかもしれない、と彼らに伝えた方がいい、と。 そうする事で、もし必要なら彼らが処置を受けられるように。 そしてこの場合、 我らが得る事になるのは、 より高いRecallの分類器だ。 何故なら我らは 実際にガンを持っている患者達全体の より高い割合を正しくフラグづけ する事になるから。 しかし我らは、 より低い Precisionとなる。 何故なら我らがガンだと言った患者のより高い割合が 彼らのうちより多くの割合の人が 実はガンで無かったと判明するだろうから。 ところで、ちょっと脇道にそれて、 ここまでの話を 他の生徒達にした所、 とても驚かれた。 生徒のうちの幾人かは どうしてこの話をどちら側でも出来るのか？と言った。 何故我らは高いPrecisionまたは高いRecallを 求める事があるのだろう？ そしてこの話はどちらの側でも、実際にありえそうに見える。 だが、アルゴリズムの詳細は 正しいと思うし、 より一般的な原理として、 あなたがどちらを望むかに応じて、 高いPrecisionで低いRecallを望むか、 はたまた高いRecallで低いPrecisionを望むかに応じて、 あなたはh(x)がある閾値より大きい時に y=1と予測する、という風にする事が出来る。 そしてそうする事で、一般に、 多くの分類器において、 PrecisionとRecallの間のトレードオフをとる事が出来る。 そしてこの閾値の値を ここに書いたこの値を、 変更していく事で、 あなたは実際に PrecisionとRecallのトレードオフを グラフにする事が出来る。 ここで、この点、 この値は、とても高い閾値の値に 対応している、 例えば閾値が0.99以上とかそういうの。 つまりそれの意味する事は、 99%以上の確信がある時しか y=1と予測しない、 最低でも99%の 確率の場合だ。 つまりとても高いPrecisionで、相対的に低いRecallとなる。 一方、この下の、ここの点、 これは閾値の値が ずっと小さい値、例えば0.01とかに 対応した点となる。 とても疑わしい時でも、y=1と言っておく。 そうしておけば、 結果としてより低いPrecisionで より高いRecallの分類器が得られる。 そしてこの閾値を変更していくにつれて、 もしお望みなら、あなたは実際に 分類器で、PrecisionとRecallの様々な値を得る事で、 このカーブを全てトレースする事が出来る。 ところで、Precision-Recall曲線は、 様々な形の曲線になりうる。 こんな形になる時もあるし、 こんな形になる時もある。 分類器の詳細に応じて Precision-Recall曲線は 様々な形となりうる。 これはもう一つ、 興味深い問を想起する： この閾値を、 自動的に選ぶ方法はあるだろうか？ またはより一般的に、 幾つかの異なるアルゴリズムや、 幾つかのアルゴリズムに対する異なるアイデアがあった時に、 どうやって異なるPrecision-Recallの値同士を比較したらいいだろうか？ 具体的には、 三つの異なるアルゴリズムがあると仮定しよう、 これらは本当に別々の 学習アルゴリズムでも良いし、 同一のアルゴリズムで別々の閾値、というだけでも良い。 どうやって、これらのアルゴリズムの中から、最良の物を決めたらいいだろうか？ 以前に話した事の中に、 単一実数値による評価指標の 重要さ、というのがあった。 それは、分類器が どれだけうまく機能しているかを 直接教えてくれる一つの数値を持つ、というアイデアだ。 だがPrecisionとRecallの指標にスイッチした時に、 我らは実は、それを失っている。 いまや我らは二つの実数値を持つのだ。 つまり、我らはしばしば 例えば二つのアルゴリズム、 アルゴリズム1とアルゴリズムを比較したい、という 状況に直面する。 すると結局我らは自分自身に、 Precisionが0.5でRecallが0.4というのは、 うーん、例えば Precisionが0.7でRecallが0.1より 良いのだろうか？悪いのだろうか？と 問いかけるハメになる。 もしあなたが新しいアルゴリズムを 試す都度、毎回 ぼーっと座って、 うーん、0.5がいいか0.4がいいか、 0.7や0.1よりは良いかもしれない、とか、 どうかな、分からん ー 私も分からん ー
とか考えなくてはならないとすると、 毎回ぼーっと座ってこれらの決断をしなくてはならないとすると、 それはあなたの意思決定プロセスを とてものろくしてしまう。 どの変更があなたのアルゴリズムにとって良いかに関する意思決定の。 それとは対照的に、もし我らが 単一実数値の評価指標を持っていたら、 アルゴリズム1とアルゴリズム2のどちらが良いかを 簡単に教えてくれるような数字があれば、 それはどのアルゴリズムで行ったらいいか、 もっと素早く決断する事を助けてくれて、 そしてアルゴリズムに対して 考えられる様々な変更を もっと素早く評価する事も 助けてくれる。 では、どうやって我らは 単一の実数値による評価指標を得られるだろうか？ 一つ自然に試そうと思える事として、 PrecisionとRecallの平均を 見てみる、というのが考えられるかもしれない。 つまり、PとRをPrecisionとRecallを 表す文字として、 あなたが行う事は、単純に平均を計算してみる、という事。 そしてどの分類器でこの平均値が 最大になるかを見てみるという事。 だがこれは、そんなに良いソリューションじゃない事が 判明する。何故なら、 前に見た例と同様に、 もしどんな時でも y=1と予測する分類器が あるとすると、 それでとても高いRecallが得られる。 それは結局、とても低いPrecisionにもなる訳だが。 逆にもし、どんな時でも y=0を予測する分類器があると、 つまりy=1と予測する時は とても慎重に行う分類器の場合、 これは前の段の記法で言う所の、 とても高い閾値の状況に対応する。 すると結局あなたは、 とても高いPrecisionでとても低いRecallとする事が出来る。 以上の二つの極端なケース、 とても高い閾値と、 とても低い閾値、 どちらも、特に良い分類器、という事は無いだろう。 その事は結局それらは とても低いPrecisionかとても低いRecallの どちらかとなる事を見れば 悟る事が出来る。 そしてもしあなたが単に(P+R)/2 で平均を取るだけだと、 この例でそれを実際に行うと、 平均はアルゴリズム3で最も高くなってしまう。 この種のパフォーマンスは どんな時でもy=1と予測するだけで 得られてしまう物だというのにも関わらずだ。 そしてそんな分類器は良い分類器では無かろう、でしょ？ いつでもy=1と予測する分類器は 単にそんなに役に立つ分類器ではなかろう、 いっつもy=1と画面に出力するだけなのだから。 だからアルゴリズム1とアルゴリズム2は 少なくともアルゴリズム３よりはマシだと 思われるのだが、 しかしこの例では、アルゴリズム3の方が PrecisionとRecallの平均の値は アルゴリズム1や2より大きくなってしまっている。 つまり、我らは普通、 PrecisionとRecallの平均というのは、 学習アルゴリズムを評価する良い指標とは、みなさない。 これとは対照的な、PrecisionとRecallを組み合わせる 別の方法がある。 それはFスコアと言われる物で、それはこんな式を使う。 この例では、Fスコアはこうなる。 そして我らはこのFスコアから、 こんな風に言える： 一番Fスコアが高いのは アルゴリズム1。 二番目に高いのがアルゴリズム2。 アルゴリズム3が最低。 つまり、Fスコアで判断すれば、 我らは他のアルゴリズムよりもアルゴリズム1を優先して選ぶ事になる。 Fスコアは、 F1スコアとも言われていて、 普通はこのようにF1スコアと 書くが、しかし単にFスコアと呼ぶ人も居る、 どちらの用語も使われているが、 これはPrecisionとRecallの 平均のような物を取る訳だが、 しかしPrecisionとRecallの 低い方の値に、 ー それがどちらであれ ー より大きな重みを与える。 つまり、分子を見ると、 FスコアはPrecisionとRecallの積を 持っている。 つまり、Precisionがイコール0か Recallがイコール0なら、 Fスコアもイコール0 となる。 つまり、ある意味これは、PrecisionとRecallを結合した物と言える。 しかし、Fスコアが大きくなる為には、 PrecisionとRecallの両方が かなり大きくならないといけない。 PrecisionとRecallを組み合わせるには、 これ以外にももっとたくさんの可能な式が あり得るという事は言っておくべきだろう。 このFスコアは単に、 もっとたくさんの可能性の中の たった一つの場合に過ぎない。 だが歴史的に、または慣例的に、 これが機械学習の人々が 使っている指標だ。 そしてFスコア、という用語には、 何の意味も無い。 だからなんでこれがFスコアとかF1スコアと呼ばれるか、とか 気にしないで欲しい。 しかしこれは通常、 望ましい振る舞いをする。 何故ならPrecisionが0か、 Recallが0なら、 これはとても低いFスコアを返すからだ。 つまり、高いFスコアを得るには、 いつも1とみなす、 という手は使えない。 具体的には、もしPが イコール0か、 Rがイコール0なら、 Ｆスコアは0となる。 一方で最高のFスコアは、 Precisionがイコール1で Recallもイコール1の時だから、 その時はFスコアは 1掛ける1を2で割った値に 2を掛けた物に 等しくなるのだから、 Fスコアは イコール1となる。 もし完璧なPrecisionとRecallを持つなら。 そして0と1の間の値は、 これは普通、異なる分類器同士の間の 合理的な順位の序列を提供してくれる。 このビデオでは、 PrecisionとRecallの間の トレードオフについて話してきた、 そしてy=1と予測するかy=0と予測するかの 閾値を変化させる事で そのトレードオフを選択する方法についても 話してきた。 この閾値は 我らがy=1と予測する為に 少なくともどれだけの確信が必要か、 70%以上の確信が必要、とするか、90%以上の確信が必要とするか、 それ以外のどんな値でも、とにかくそれ以上必要とする、という値を教える。 そして閾値を変更する事で、 PrecisionとRecallの間のトレードオフを コントロールする事が出来る。 最後に、Fスコアについて話したが、 これはPrecisionとRecallを用いて 単一の実数値による 評価指標を与える物だ。 そしてもちろん、もしあなたのゴールが y=1かy=0かを決める閾値を 自動的に設定する、 という物なら、 それを行う 一つのとても合理的な方法としては、 様々な範囲の閾値の値を試す、 という手が考えられて、 つまり、ある範囲の閾値を試してみて、 これらの様々な閾値を 例えばクロスバリデーションセットで 評価してみて、 クロスバリデーションセットに対して 一番高いFスコアを与えた 閾値を選ぶ、という方法が考えられる。 これは分類器の閾値を自動的に選ぶ、 極めて合理的な方法と言えるだろう。