En el video anterior hablé del análisis de errores y de la importancia de tener una métrica de errores; es decir, tener una métrica de evaluación con un número real simple para saber cómo se desempeña un algoritmo de aprendizaje. En el contexto de la evaluación y de la métrica de errores, hay un caso importante en el que es especialmente difícil obtener una métrica de errores o de evaluación apropiada para el algoritmo de aprendizaje. A este caso se le llama clases sesgadas. A continuación les diré que significa. Consideremos el problema de clasificación de cáncer, donde tenemos variables de pacientes médicos y queremos decidir si tienen cáncer o no. Este es como el ejemplo de clasificación de tumores malignos y benignos que vimos antes. Digamos que “y” es igual a 1 si el paciente tiene cáncer y “y” es igual a 0 si no tiene cáncer. Entrenaremos el clasificador de regresión logística y lo probaremos en el conjunto de pruebas y encontraremos un error del 1 por ciento. Por lo tanto, tenemos un 99% de diagnósticos correctos. Parece ser un resultado impresionante, ¿cierto? Estamos en lo correcto el 99% de las veces. Ahora, supongamos que sólo el 0.5% de los pacientes en nuestro conjunto de entrenamiento y de prueba tienen cáncer realmente; es decir, sólo medio punto porcentual de los pacientes en el proceso de selección tiene cáncer. En este caso, el 1% de error ya no parece tan impresionante. Aquí hay unas líneas de código, un código que no es de aprendizaje que toma las variables de entrada “x” y las ignora. Sólo ajusta “y” igual a 0 y siempre predice que nadie tiene cáncer. Este algoritmo arrojó un error del 0.5% que es mucho mejor que el 1% de error que estábamos obteniendo y este es un algoritmo que no es de aprendizaje y que predice que “y” es igual a 0 todo el tiempo. Este parámetro en el que la proporción de ejemplos positivos y negativos está muy cerca a alguno de los extremos, como en este caso, que el número de ejemplos positivos es mucho más pequeño que el número de ejemplos negativos porque “y” es igual a 1, esto es a lo que llamamos caso de clases sesgadas. Tenemos muchos más ejemplos para una clase que para la otra. Al predecir que “y” es igual a 0 todo el tiempo, o que “y” es igual a 1 todo el tiempo, el algoritmo puede desempeñarse muy bien. El problema cuando utilizamos el error de clasificación o la precisión de clasificación como nuestra métrica de evaluación es el siguiente: Digamos que tenemos un algoritmo que arroja una precisión del 99.2%. En otras palabras, un error de 0.8%. Digamos que haces algún cambio a tu algoritmo y ahora obtienes una precisión del 99.5%. Es decir, un error de 0.5%. ¿Esto sería una mejora para el algoritmo? Una de las ventajas de tener una métrica de evaluación con un número real simple es que nos ayuda a decidir rápidamente si necesitamos realizar cambios en el algoritmo o no. Al aumentar la precisión del algoritmo de 99.2% a 99.5% ¿hicimos algo útil, o simplemente remplazamos nuestro código con algo que predice que “y” es igual a 0 más seguido? Si tienes clases muy sesgadas se vuelve mucho más difícil utilizar la precisión de la clasificación, porque puedes obtener una precisión de clasificación alta y un error bajo pero no siempre queda claro si hacer esto mejora la calidad de tu clasificador, porque predecir que “y” es igual a 0 todo el tiempo no es un buen clasificador. Cuando predecimos que “y” es igual a 0 más seguido, podemos reducir el error tanto como hasta un 0.5%. Cuando nos encontramos con estas clases sesgadas queremos encontrar una métrica de errores o una métrica de evaluación diferente. Una de esas métricas de evaluación es lo que llamamos precisión/recuperación. A continuación explicaré qué es. Imaginemos que estamos evaluando un clasificador en el conjunto de prueba. Para este ejemplo, la clase real para el conjunto de prueba será ya sea cero o uno si es un problema de clasificación binaria. Lo que hará nuestro algoritmo de aprendizaje es predecir un valor para la clase. Nuestro algoritmo de aprendizaje predirá el valor de cada ejemplo en mi conjunto de prueba y el valor predicho también será ya sea uno o cero. Permítanme dibujar una tabla de dos por dos, como esta, dependiendo de estas entradas, o de la clase real y la clase predicha. Si tenemos un ejemplo en el que la clase real es uno y la clase predicha es uno, entonces le llamaremos un ejemplo positivo verdadero. Esto quiere decir que nuestro algoritmo predijo que es positivo y de hecho el ejemplo es positivo. Si nuestro algoritmo predijo que algo es negativo, clase cero, y la clase real es cero, entonces a esto le llamamos negativo verdadero. Predijimos que sería cero y es realmente cero. En las otras dos celdas, si nuestro algoritmo predice que la clase es uno, pero la clase real es cero, entonces lo llamamos un falso positivo. Esto quiere decir que el algoritmo pensó que el paciente tiene cáncer cuando en realidad no lo padece. Y finalmente, la última casilla es cero-uno. A esto se le llama falso negativo porque nuestro algoritmo predijo un cero, pero la clase real fue de uno. Entonces, tenemos esta tabla de dos por dos basada en la clase real y la clase predicha. Esta es una manera distinta de evaluar el desempeño de nuestro algoritmo. Calcularemos dos números. Al primero se le llama precisión. Lo que nos indica es, de todos los pacientes a los que les diagnosticamos cáncer ¿qué fracción tienen cáncer realmente? Voy a escribir esto: la precisión de un clasificador es el número de positivos verdaderos dividido entre el número que predijimos como positivo. ¿sí? De todos los pacientes a los que les dijimos “Creemos que usted puede tener cáncer”, ¿qué fracción realmente tiene cáncer? A esto se le llama precisión. Otra manera de escribir esto sería positivos verdaderos en el cociente el número de positivos predichos en el denominador. Esta sería la suma de las entradas en esta primera fila de la tabla. Entonces, tenemos los positivos verdaderos divididos entre los positivos, abreviaré positivo como "pos" más los falsos positivos, abreviando de nuevo positivo como "pos". A esto se le llama precisión. Como puedes ver, una precisión alta es deseable. Una alta precisión quiere decir que, de los pacientes a quienes les dijimos “creemos que tiene cáncer, lo sentimos mucho” la mayoría realmente tienen cáncer; por lo tanto, que hicimos las predicciones precisas acerca de ellos. El segundo número que calcularemos se llama recuperación. La recuperación nos dice, de todos los pacientes del conjunto de prueba o de validación cruzada, es decir, de todos los pacientes en el conjunto de datos que realmente tienen cáncer ¿en qué fracción de ellos detectamos el cáncer correctamente? Si todos los pacientes tienen cáncer, ¿a cuántos de ellos les dijimos correctamente que necesitan tratamiento? Ahora, escribamos esto: la recuperación se define como el número de positivos o el número de positivos verdaderos; es decir, el número de personas que tienen cáncer y que predijimos correctamente que tienen cáncer. Tomaremos esto y lo dividiremos entre el número de positivos verdaderos. Este será el número correcto de positivos verdaderos de todos los que sí sufren cáncer; en otras palabras, la fracción que marcamos y mandamos a tratamiento. Para escribir esto de manera diferente, el número real de positivos verdaderos estaría en el denominador y es la suma de las entradas de esta columna. Por lo tanto, este será el número de positivos verdaderos dividido entre el número de positivos verdaderos más el número de falsos negativos. Una vez más, tener una recuperación alta será bueno. Calcular la precisión y la recuperación nos dará un mejor entendimiento del desempeño de nuestro clasificador. De manera particular, si tenemos un algoritmo que predice que “y” es igual a 0 en todo momento; es decir, si predice que nadie tiene cáncer, este clasificador tendrá una recuperación igual a cero, porque no habrá ningún positivo verdadero. Esta es una manera rápida para reconocer que un clasificador que predice que “y” es igual a 0 todo el tiempo, no es un buen clasificador. De manera más general, aún para las situaciones en las que tenemos clases muy sesgadas, no es posible posible que un algoritmo “haga trampa” y obtenga una precisión o una recuperación muy alta haciendo algo tan simple como predecir que “y” es igual a 0 o que “y” es igual a 1 todo el tiempo. Estamos mucho más seguros de que un clasificador con una precisión alta o una recuperación alta es un buen clasificador. Esto nos provee una métrica de evaluación mucho más útil y directa para entender si nuestro algoritmo se está desempeñando bien. Un último comentario: En la definición de precisión y recuperación, utilizaríamos la convención, es decir, “y” igual a 1, en presencia de la clase más rara. De manera que si intentamos detectar condiciones raras, como el cáncer, la precisión y la recuperación se definen con “y” igual a 1 en vez de igual a 0 para representar la presencia de esta clase rara que intentamos detectar. Utilizando la precisión y la recuperación nos encontramos con que lo que pasa, aún si tenemos clases muy sesgadas, es que no es posible que un algoritmo “haga trampa” y prediga que “y” es igual a 1 todo el tiempo o prediga que “y” es igual a 0 todo el tiempo, y que obtengamos una precisión y una recuperación altas. Particularmente, si un clasificador arroja una precisión y una recuperación alta, entonces tendremos la certeza de que nuestro algoritmo se está desempeñando bien, aún si tenemos clases muy sesgadas. Para el problema de clases sesgadas, la precisión y la recuperación nos dan un entendimiento más directo sobre cómo se desempeña el algoritmo de aprendizaje y, a veces, es una manera mucho más efectiva de evaluar nuestros algoritmos de aprendizaje que el análisis del error de clasificación o la precisión de clasificación, cuando las clases están muy sesgadas.