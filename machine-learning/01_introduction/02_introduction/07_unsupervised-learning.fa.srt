1
00:00:00,380 --> 00:00:01,550
توی این ویدیو ، صحبت خواهیم کرد درباره

2
00:00:01,670 --> 00:00:02,690
دومین نوع اصلی مسایل یادگیری

3
00:00:03,010 --> 00:00:05,030
ماشین ، به نام یادگیری نظارت نشده .

4
00:00:06,300 --> 00:00:08,500
توی آخرین ویدیو ، درباره یادگیری نظارت شده حرف زدیم .

5
00:00:09,250 --> 00:00:10,700
برگردیم بهش ، مجموعه داده هارو فراخوانی کنیم

6
00:00:11,020 --> 00:00:12,670
که مثل اینه ، که هر

7
00:00:12,890 --> 00:00:15,150
نمونه برچسبی خورده یا

8
00:00:15,610 --> 00:00:16,900
به عنوان مثبت یا نمونه منفی .

9
00:00:17,530 --> 00:00:19,800
یا خوش خیم بود یا تومور بدخیم .

10
00:00:20,850 --> 00:00:21,920
خب برای هر نمونه در یادگیری نظارت شده

11
00:00:22,410 --> 00:00:24,270
به صورت ضمنی گفتیم چیزیکه

12
00:00:24,440 --> 00:00:25,760
جواب صحیح نامیده شد چی هست ،

13
00:00:26,490 --> 00:00:27,580
یا خوش خیمه یا بدخیم .

14
00:00:28,550 --> 00:00:30,210
در یادگیری نظارت نشده ،

15
00:00:30,540 --> 00:00:31,720
داده ایی دادیم که متفاوت به نظر میرسه

16
00:00:31,950 --> 00:00:32,910
از داده ایی مثل این

17
00:00:33,190 --> 00:00:34,600
که فاقد

18
00:00:34,720 --> 00:00:35,920
هرگونه برچسبی هست یا درکل

19
00:00:36,130 --> 00:00:37,460
برچسبی یکسان داره یا درواقع برچسبی نداره .

20
00:00:39,680 --> 00:00:40,740
پس مجموعه داده ایی دادیم و

21
00:00:40,980 --> 00:00:42,460
ذکر نکردیم چیکار کنه

22
00:00:42,560 --> 00:00:43,290
با اون و نگفتیم

23
00:00:43,640 --> 00:00:44,800
هر نقطه داده چی هست .

24
00:00:45,290 --> 00:00:47,190
بجاش فقط گفتیم ، بفرما این مجموعه داده است .

25
00:00:47,870 --> 00:00:49,650
میتونی تعدادی ساختار توی این داده پیدا کنی ؟

26
00:00:50,480 --> 00:00:51,670
مجموعه داده دراختیار گذاشته شده ، یک

27
00:00:52,350 --> 00:00:53,940
الگوریتم یادگیری نظارت تصمیم میگیره که

28
00:00:54,060 --> 00:00:56,090
داده توی دو تا خوشه متفاوت جا بگیره .

29
00:00:56,800 --> 00:00:57,960
و خب اینجا یه خوشه هستت

30
00:00:59,120 --> 00:00:59,910
و اینجا خوشه دیگه ایی .

31
00:01:01,110 --> 00:01:02,710
و بله ، الگوریتم یادگیری ماشین میتونه

32
00:01:03,040 --> 00:01:05,070
این داده ها رو بشکنه توی این دوتا 
خوشه مجزا .

33
00:01:06,410 --> 00:01:08,000
پس این الگوریتم خوشه بندی نامیده شده .

34
00:01:08,860 --> 00:01:10,310
و این میتونه توی جاهای بسیاری استفاده بشه .

35
00:01:11,930 --> 00:01:13,310
یک نمونه جایی که خوشه بندی

36
00:01:13,530 --> 00:01:14,860
استفاده شده در گوگل

37
00:01:15,060 --> 00:01:16,160
نیوز هست و اگه هنوز

38
00:01:16,360 --> 00:01:17,320
این رو قبلا ندیدین ، میتونین درواقع

39
00:01:18,210 --> 00:01:19,040
به این آدرس یوآرال برین 
news.google.com

40
00:01:19,830 --> 00:01:20,460
برای اینکه نگاهی بهش بندازین .

41
00:01:21,280 --> 00:01:22,970
کاری که گوگل نیوز انجام میده هرروز

42
00:01:23,480 --> 00:01:24,220
میره و به ده ها هزار

43
00:01:24,470 --> 00:01:25,430
یا صدها هزار

44
00:01:25,720 --> 00:01:26,740
موضوع جدید بر روی وب نگاه میندازه

45
00:01:26,800 --> 00:01:29,410
و اونارو توی موضوعات خبری به هم 
چسبیده گروه بندی میکنه .

46
00:01:30,730 --> 00:01:31,690
برای نمونه ، اینجا رو ببینین.

47
00:01:33,380 --> 00:01:35,370
اینجا  این یوآر ال لینک میشه

48
00:01:35,910 --> 00:01:37,260
به موضوع خبری متفاوتی

49
00:01:38,010 --> 00:01:40,110
درباره موضوع شرکت نفتی بی پی .

50
00:01:41,300 --> 00:01:42,160
خب بزارین کلیک کنیم روی

51
00:01:42,260 --> 00:01:43,090
یکی از این یوآرال ها و

52
00:01:43,550 --> 00:01:44,780
و همینطور کلیک روی این یوآرال ها .

53
00:01:45,100 --> 00:01:46,970
جاییکه من و میبره صفحه وبی شبیه اینه .

54
00:01:47,210 --> 00:01:48,390
روزنامه وال استریت

55
00:01:48,590 --> 00:01:50,180
مقاله ایی درباره ، میدونین ،

56
00:01:51,110 --> 00:01:52,530
موضوع شرکت نفتی بی پی هست

57
00:01:52,920 --> 00:01:54,350
"BP Kills Macondo",

58
00:01:54,590 --> 00:01:55,700
که نام

59
00:01:55,980 --> 00:01:57,960
محل نشت هست و اگر شما

60
00:01:58,020 --> 00:01:59,360
روی یوآرال دیگه ایی کلیک کنین

61
00:02:00,690 --> 00:02:02,500
از اون گروه ، بعدش احتمالا موضوع متفاوتی رو 
میبینید .

62
00:02:02,950 --> 00:02:04,760
اینجا گزارش سی ان ان درباره یک

63
00:02:04,820 --> 00:02:06,090
بازی هست ، 
the BP Oil Spill,

64
00:02:07,090 --> 00:02:08,180
و اگه بازم روش کلیک کنین

65
00:02:08,740 --> 00:02:10,990
لینک سومی ، بعدش احتمالا موضوع مختلفی 
رو میبینین .

66
00:02:11,440 --> 00:02:13,380
اینجا گزارش روزنامه گوآردین یو کی

67
00:02:13,940 --> 00:02:15,510
درباره  BP Oil Spill .

68
00:02:16,530 --> 00:02:17,790
پس کاری که گوگل نیوز انجام داده

69
00:02:17,990 --> 00:02:19,440
سرزدن به ده ها هزار

70
00:02:19,490 --> 00:02:22,170
موضوع خبری و به صورت خودکار
خوشه بندی اونا با همدیگه هست .

71
00:02:23,030 --> 00:02:24,660
پس ، موضوعات خبری ایی که همه

72
00:02:25,080 --> 00:02:27,010
درباره موضعی یکسان هستن باهمدیگه
نشون داده میشن .

73
00:02:27,210 --> 00:02:29,170
پیداست که

74
00:02:29,380 --> 00:02:31,020
الگوریتم های خوشه بندی و الگوریتم ها یادگیری نظارت نشده

75
00:02:31,530 --> 00:02:33,550
در مسایل بسیار دیگری هم بکار گرفته شده.

76
00:02:35,320 --> 00:02:36,690
اینجا یکی روی کشف داده های ژنومی داریم.

77
00:02:38,270 --> 00:02:40,510
اینجا نمونه ایی از داده های میکروآرایه دی ان ای داریم .

78
00:02:40,990 --> 00:02:42,230
ایده قراردادن

79
00:02:42,430 --> 00:02:44,360
گروهی از افراد متفاوت هست و

80
00:02:44,510 --> 00:02:45,590
برای هر کروم از اونا ، اندازه گیری میکنین

81
00:02:46,100 --> 00:02:48,580
چقدر از یک ژن خاص دارن یا ندارن .

82
00:02:49,050 --> 00:02:51,640
به صورت تخصصی اندازه گیری میکنین چه 
مقدار ژن خاص ارایه شده .

83
00:02:52,000 --> 00:02:54,190
خب این رنگ ها ، قرمز ، سبز ،

84
00:02:54,930 --> 00:02:56,210
خاکستری و غیره ،

85
00:02:56,340 --> 00:02:57,500
درجه ایی رو نشون میدن که

86
00:02:57,780 --> 00:02:59,440
هر فرد مختلف داره یا

87
00:02:59,510 --> 00:03:01,270
ژن خاصی نداره .

88
00:03:02,500 --> 00:03:03,400
و چیزیکه میتونین انجام بدین اینه بعدش

89
00:03:03,610 --> 00:03:05,070
الگوریتم خوشه بندی ایی رو اجرا کنین برای گروه بندی

90
00:03:05,380 --> 00:03:07,140
افراد توی دسته بندی های متفاوت

91
00:03:07,780 --> 00:03:08,810
یا توی گونه های مختلف مردم .

92
00:03:10,230 --> 00:03:11,660
پس این یادگیری نظارت نشده هست چون

93
00:03:11,930 --> 00:03:14,010
از پیش به الگوریتم نمیگیم

94
00:03:14,590 --> 00:03:15,690
که اینا نوع 1 افرادن ،

95
00:03:16,130 --> 00:03:17,420
اینا اشخاص نوع 2 ، اونا

96
00:03:17,560 --> 00:03:18,650
اشخاص نوع 3 و غیره

97
00:03:19,610 --> 00:03:22,390
و بجاش چیزیکه میگیم اینه که بله 
اینجا دسته ایی از داده هاست .

98
00:03:23,110 --> 00:03:24,030
نمیدونم چی توی این داده هست .

99
00:03:24,750 --> 00:03:25,870
نمیدونم کی و از چه نوعی هست .

100
00:03:26,150 --> 00:03:26,940
حتی نمیدونم چه گونه های مختلفی

101
00:03:27,260 --> 00:03:28,480
از افراد هستن ، ولی میتونی

102
00:03:28,610 --> 00:03:30,210
به صورت خودکار ساختاری رو توی

103
00:03:30,360 --> 00:03:31,260
داده های پیدا کنی از خوشه ایی که به صورت خودکار

104
00:03:32,180 --> 00:03:33,620
اشخاص رو توی این گونه هایی

105
00:03:33,870 --> 00:03:35,490
که از قبل نمیدونستم جا بدی ؟

106
00:03:35,890 --> 00:03:37,610
چون به الگوریتم

107
00:03:38,160 --> 00:03:40,140
جواب صحیح رو نمیدیم برای

108
00:03:40,370 --> 00:03:41,270
نمونه توی مجموعه داده ام

109
00:03:41,590 --> 00:03:43,090
این یادگیری نظارت نشده هست .

110
00:03:44,290 --> 00:03:47,040
یادگیری نظارت نشده یا خوشه بندی برای دسته ایی
از کاربردهای دیگه هم استفاده شده .

111
00:03:48,340 --> 00:03:50,340
در سازماندهی خوشه های کامپیوتر بزرگ
بکار گرفته شده .

112
00:03:51,390 --> 00:03:52,530
دوستانی دارم که توی

113
00:03:52,680 --> 00:03:53,970
مراکز داده بزرگ هستن ، اونجا

114
00:03:54,180 --> 00:03:55,970
خوشه های کامپیوتر بزرگی هست و سعی

115
00:03:56,230 --> 00:03:57,470
بر این دارن که مشخص کنن کدوم ماشین ها

116
00:03:57,590 --> 00:03:59,130
تمایل دارن باهم دیگه کار کنن و اگر

117
00:03:59,200 --> 00:04:00,270
بتونین اون ماشین ها رو کنار همدیگه بزارین ،

118
00:04:01,100 --> 00:04:03,220
میتونین مرکز داده ایی با کارایی موثر بیشتری
بسازین .

119
00:04:04,810 --> 00:04:06,820
این دومین کاربرد بر روی تجزیه-تحلیل شبکه های اجتماعی هست .

120
00:04:07,890 --> 00:04:09,230
خب شناخت داده شده درمورد اینکه به کدوم دوستان

121
00:04:09,630 --> 00:04:10,840
بیشتر ایمیل میزنین یا

122
00:04:10,880 --> 00:04:12,150
دوستان فیس بوکی تون یا

123
00:04:12,180 --> 00:04:14,150
دایره های گوگل پلاسی تون ، میتونیم

124
00:04:14,290 --> 00:04:16,380
به صورت خودکار مشخص کنیم کدومشون

125
00:04:16,450 --> 00:04:17,950
گروه های مرتبطی از دوستان هستن ،

126
00:04:18,460 --> 00:04:19,420
به علاوه کدوم ها گروه هایی از افراد هستن

127
00:04:20,230 --> 00:04:21,010
که همه همدیگر رو میشناسن ؟

128
00:04:22,540 --> 00:04:22,880
بخش بندی بازار .

129
00:04:24,680 --> 00:04:26,780
شرکت های بسیاری پایگاه داده های
عظیمی از اطلاعات مشتریان دارن .

130
00:04:27,700 --> 00:04:28,410
پس ، میتونین نگاهی بندازین به این

131
00:04:28,510 --> 00:04:30,000
مجموعه داده مشتری و به صورت خودکار

132
00:04:30,740 --> 00:04:32,340
بخش های بازار رو کشف کنین و به صورت خودکار

133
00:04:33,340 --> 00:04:35,290
دسته بندی کنین مشتریان تون رو توی

134
00:04:35,820 --> 00:04:37,400
بخش های بازار مختلف که بعدش

135
00:04:37,710 --> 00:04:39,490
میتونین به صورت خودکار و

136
00:04:39,650 --> 00:04:41,580
موثر تری بفروشین یا بازاریابی کنین

137
00:04:41,890 --> 00:04:43,250
بخش های مختلف بازار تون رو باهم دیگه ؟

138
00:04:44,260 --> 00:04:45,580
مجددا ، این یادگیری نظارت نشده هست

139
00:04:45,820 --> 00:04:46,720
چون همه این

140
00:04:46,900 --> 00:04:48,340
داده مشتریان رو داریم ، ولی نمیدونیم

141
00:04:48,590 --> 00:04:49,710
از قبل که

142
00:04:49,790 --> 00:04:51,270
بخش های بازار چیا هستن و برای

143
00:04:51,440 --> 00:04:52,570
مشتریان توی مجموعه داده مون

144
00:04:52,660 --> 00:04:53,590
میدونین ، نمیدونیم

145
00:04:53,690 --> 00:04:54,700
از قبل چه کسی توی

146
00:04:54,800 --> 00:04:55,840
بخش بازار یک هست ، چه کسی

147
00:04:55,940 --> 00:04:57,800
توی بخش باز دو هست و همینطور تا آخر .

148
00:04:57,930 --> 00:05:00,630
ولی به الگوریتم اجازه میدیم همه اینارو 
فقط از داده ها کشف کنه .

149
00:05:01,970 --> 00:05:03,140
درآخر ، پیداست که یادگیری نظارت نشده

150
00:05:03,690 --> 00:05:05,620
همچنین استفاده شده در

151
00:05:06,090 --> 00:05:08,060
تجزیه-تحلیل داده های شگفت آور نجومی

152
00:05:08,890 --> 00:05:10,390
و این الگوریتم های خوشه بندی

153
00:05:10,580 --> 00:05:12,440
تئوری های مفید جالب شگفت آوری

154
00:05:12,900 --> 00:05:15,610
از اینکه چطور کهکشان به وجود آمده به ما میدن .

155
00:05:15,880 --> 00:05:17,620
همه اینا نمونه هایی از خوشه بندی هستن .

156
00:05:18,400 --> 00:05:20,550
که فقط یه گونه از یادگیری نظارت نشده است .

157
00:05:21,530 --> 00:05:22,470
بزارین درباره یکی دیگه بهتون بگم .

158
00:05:23,200 --> 00:05:25,020
میخوام بهتون درباره مساله مهمونی مختلط 
بگم.

159
00:05:26,310 --> 00:05:28,270
خب ، قبلا توی یه مهمونی مختلط بودین ، درسته ؟

160
00:05:28,440 --> 00:05:30,080
خب ، میتونین تصور کنین یه

161
00:05:30,300 --> 00:05:31,690
مهمونی ، پر از آدم ، همه

162
00:05:31,870 --> 00:05:32,930
دور تا دور نشستن ، همه با هم حرف میزنن

163
00:05:32,970 --> 00:05:34,390
همزمان و

164
00:05:34,480 --> 00:05:36,230
هم پوشانی صداها وجود داره چون هرکسی

165
00:05:36,590 --> 00:05:37,920
داره توی یه زمان حرف میزنه ، و

166
00:05:38,070 --> 00:05:39,730
و تقریبا سخته که صدای شخصی رو که 
مقابل تون هست رو بشنوین .

167
00:05:40,690 --> 00:05:41,970
خب ممکنه توی

168
00:05:42,020 --> 00:05:43,990
یه مهمونی مختلط با دو نفر ،

169
00:05:45,690 --> 00:05:46,670
دو نفر همزمان حرف بزنن

170
00:05:46,770 --> 00:05:48,090
و این تا حدودی

171
00:05:48,740 --> 00:05:49,710
مهمونی مختلط کوچیکه .

172
00:05:50,690 --> 00:05:51,630
و قصد داریم دوتا

173
00:05:51,890 --> 00:05:53,080
میکروفون توی اتاق بزاریم پس

174
00:05:54,060 --> 00:05:55,640
میکروفون هایی هستن ، و چون

175
00:05:56,050 --> 00:05:57,430
این میکروفون ها دوتا هستن

176
00:05:57,560 --> 00:05:58,900
توی دوتا فاصله مختلف از

177
00:05:58,990 --> 00:06:01,250
بلندگوها هستن ، هر میکروفون ضبط میکنه

178
00:06:01,830 --> 00:06:04,720
ترکیب مختلفی از این دو صدای 
بلنگوها رو .

179
00:06:05,810 --> 00:06:06,970
شاید بلندگوی اول

180
00:06:07,120 --> 00:06:08,320
توی میکروفون اول بلند تر باشه

181
00:06:09,120 --> 00:06:10,680
و شاید بلندگوی دوم

182
00:06:10,800 --> 00:06:12,350
کمی توی میکروفون 2 بلند تر باشه

183
00:06:12,560 --> 00:06:14,040
چون 2 تا میکروفون

184
00:06:14,230 --> 00:06:15,950
توی موقعیت ارتباطی متفاوتی هستن

185
00:06:16,400 --> 00:06:19,020
نسبت به 2 بلندگو ، ولی هر

186
00:06:19,250 --> 00:06:20,390
میکروفون میتونه باعث هم پوشانی

187
00:06:20,970 --> 00:06:22,590
ترکیبی از هر دو صدای بلندگو باشه .

188
00:06:23,960 --> 00:06:25,500
خب اینجا صدای ضبط شده واقعی

189
00:06:26,520 --> 00:06:29,280
دو بلندگو هست که توسط محققی ضبط شده .

190
00:06:29,740 --> 00:06:30,950
بزارین براتون پخشش کنم

191
00:06:31,060 --> 00:06:32,760
اولی رو ، صدایی که اولین میکروفون میده مثل .

192
00:06:33,560 --> 00:06:34,800
One (uno), two (dos),

193
00:06:35,070 --> 00:06:36,590
three (tres), four (cuatro), five

194
00:06:37,060 --> 00:06:38,550
(cinco), six (seis), seven (siete),

195
00:06:38,990 --> 00:06:40,610
eight (ocho), nine (nueve), ten (y diez).

196
00:06:41,610 --> 00:06:42,650
بسیار خب ، شاید جالب ترین مهمونی مختلط نباشه

197
00:06:43,000 --> 00:06:44,270
دونفر هستن که

198
00:06:44,620 --> 00:06:45,670
از یک تا ده میشمارن

199
00:06:46,010 --> 00:06:47,880
توی دوتا زبون مختلف ولی میدونین .

200
00:06:48,870 --> 00:06:49,760
چیزیکه فقط شنیدین

201
00:06:49,820 --> 00:06:52,500
صدای ضبط شده اولین میکروفون بود ، اینجا
دومین صدای ضبط شده .

202
00:06:57,440 --> 00:06:58,040
Uno (one), dos (two), tres (three), cuatro

203
00:06:58,060 --> 00:06:58,730
(four), cinco (five), seis (six), siete (seven),

204
00:06:59,160 --> 00:07:00,900
ocho (eight), nueve (nine) y diez (ten).

205
00:07:01,860 --> 00:07:02,850
پس میتونیم انجام بدیم ،

206
00:07:03,380 --> 00:07:04,660
این صدای ضبط شده دوتا میکروفون رو گرفته و بهشون

207
00:07:04,980 --> 00:07:06,480
یه الگوریتم یادگیری نظارت نشده داده

208
00:07:07,010 --> 00:07:08,560
که اسمش رو الگوریتم مهمونی مختلط گذاشته ،

209
00:07:08,780 --> 00:07:09,910
و به الگوریتم گفته

210
00:07:10,450 --> 00:07:12,140
براتون توی این داده ساختاری رو پیدا کنه .

211
00:07:12,250 --> 00:07:14,010
و کاری که الگوریتم انجام میده

212
00:07:14,410 --> 00:07:15,730
گوش کردن به این

213
00:07:15,980 --> 00:07:17,990
صدای ضبط شدست و میگه ،

214
00:07:18,140 --> 00:07:19,020
میدونی این صداها شبیه

215
00:07:19,360 --> 00:07:20,950
دوتا صدای ضبط شده هستن

216
00:07:21,240 --> 00:07:22,450
که به هم دیگه اضافه شدن یا اینکه

217
00:07:22,670 --> 00:07:25,220
با هم جمع شدن برای ساخت این 
صداهای ضبط شده ایی که داریم .

218
00:07:25,990 --> 00:07:27,330
علاوه بر این ، چیزیکه الگوریتم مهمونی مختلط

219
00:07:27,710 --> 00:07:29,210
انجام خواهد داد جداکردن

220
00:07:29,570 --> 00:07:30,810
این دو تا منبع صدایی هست

221
00:07:31,480 --> 00:07:32,700
که اضافه شدن یا

222
00:07:33,000 --> 00:07:34,240
با همدیگه جمع شدن از

223
00:07:34,410 --> 00:07:35,600
صدای ضبط شده دیگه ای و ، در واقع ،

224
00:07:36,200 --> 00:07:38,630
اینجا اولین خروجی الکوریتم مهمانی مختلط هست .

225
00:07:39,790 --> 00:07:41,910
One, two, three, four,

226
00:07:42,590 --> 00:07:46,270
five, six, seven, eight, nine, ten.

227
00:07:47,630 --> 00:07:48,780
پس ، صداهای انگلیسی ها رو جدا کردم

228
00:07:49,240 --> 00:07:51,220
توی یه صدای ضبط شده .

229
00:07:52,460 --> 00:07:53,300
و اینجا دومیش هست .

230
00:07:53,380 --> 00:07:55,280
Uno, dos, tres, quatro, cinco,

231
00:07:55,980 --> 00:07:59,830
seis, siete, ocho, nueve y diez.

232
00:08:00,270 --> 00:08:01,180
بد نیست که

233
00:08:03,810 --> 00:08:05,270
نمونه بیشتر دیگه ایی بهتون بدم ، اینجا یکی دیگست

234
00:08:05,600 --> 00:08:07,370
صدای ضبط شده ایی از شرایط مشابه ،

235
00:08:08,060 --> 00:08:09,790
این اولین میکروفون : 
One

236
00:08:10,470 --> 00:08:12,430
two, three, four, five, six,

237
00:08:13,370 --> 00:08:15,710
seven, eight, nine, ten.

238
00:08:16,980 --> 00:08:17,920
بسیار خب صدای ضعیف شخص

239
00:08:18,180 --> 00:08:19,350
رفته خونه از مهمونی مختلط و

240
00:08:19,420 --> 00:08:21,880
اون حالا توی اتاقی نشسته و 
داره به رادیوش گوش میده .

241
00:08:23,090 --> 00:08:24,130
اینجا صدای ضبط شده میکروفون دومی .

242
00:08:28,810 --> 00:08:31,800
One, two, three, four, five, six, seven, eight, nine, ten.

243
00:08:33,310 --> 00:08:34,160
وقتیکه صدای ضبط شده این دوتا میکروفون

244
00:08:34,610 --> 00:08:35,530
رو به الگوریتم یکسانی میدین ،

245
00:08:36,360 --> 00:08:37,790
کاری که انجام میده ، دوباره میگه ،

246
00:08:38,380 --> 00:08:39,470
میدونی ، صدایی مثل این میده

247
00:08:39,690 --> 00:08:41,370
دوتا منبع صدا ، و بیشتر از اون

248
00:08:42,410 --> 00:08:43,820
آلبومی میگه ، اینجا

249
00:08:44,070 --> 00:08:46,010
اولین منبع صدا ، توش دارم

250
00:08:47,480 --> 00:08:49,300
One, two, three, four,

251
00:08:49,730 --> 00:08:53,430
five, six, seven, eight, nine, ten.

252
00:08:54,650 --> 00:08:56,110
خب خیلی عالی نبود ،

253
00:08:56,340 --> 00:08:57,360
صدا رو گرفت ، ولی

254
00:08:57,570 --> 00:08:59,070
همینطور صدای کمی از موزیک رو توش داره .

255
00:08:59,890 --> 00:09:01,360
بنابراین خروجی صدای دومی به الگوریتم هست .

256
00:09:10,020 --> 00:09:11,310
خیلی بد نیست ، توی دومین

257
00:09:11,540 --> 00:09:13,300
خروجی کلا صدای انسان رو ازش رد شده .

258
00:09:13,760 --> 00:09:14,850
و فقط ، میدونین

259
00:09:15,020 --> 00:09:17,380
موزیک خالص ، از شمردن از یک تا ده چشم پوشی کرده .

260
00:09:18,840 --> 00:09:20,090
پس باید نگاهی بندازین به

261
00:09:20,180 --> 00:09:21,750
یک الگوریتم یادگیری نظارت نشده مثل

262
00:09:21,950 --> 00:09:23,050
این و بپرسین چطور

263
00:09:23,250 --> 00:09:25,110
چیز پیچیده ای مثل این رو پیاده سازی میکنه ، درسته ؟

264
00:09:25,330 --> 00:09:26,560
مثل این می مونه که درخواست بدین ،

265
00:09:26,970 --> 00:09:28,870
میدونین ، این برنامه رو بسازین ، مثل

266
00:09:28,930 --> 00:09:30,550
که این برای اینکه پردازش صدا رو انجام بدین

267
00:09:30,670 --> 00:09:31,430
لازمه که یه تن کد نویسی کنین

268
00:09:32,240 --> 00:09:33,580
یا شاید وصلش کنین توی چیزی مثل یه

269
00:09:33,690 --> 00:09:35,380
دسته از سینت سایز کتابخونه های جاوا که

270
00:09:35,470 --> 00:09:37,150
پردازش صدا میکنن ، اینطور به نظر میرسه

271
00:09:37,240 --> 00:09:38,880
برنامه ایی واقعا پیچیده ، برای انجام

272
00:09:39,060 --> 00:09:41,040
این صدا ، جداسازی صدا و غیره .

273
00:09:42,460 --> 00:09:43,860
پیداست که الگوریتمی ، برای

274
00:09:44,070 --> 00:09:45,640
انجام چیزیکه فقط شنیدین ، که

275
00:09:45,900 --> 00:09:47,280
میتونه با یک خط  کد انجام بشه

276
00:09:47,530 --> 00:09:49,270
کد نویسی - اینجا نشون داده شده .

277
00:09:50,640 --> 00:09:52,350
زمان طولانی از محققین گرفته

278
00:09:52,610 --> 00:09:54,060
برای یافتن این خط کد .

279
00:09:54,490 --> 00:09:56,090
نمیگم که این مساله ایی سادست ،

280
00:09:57,080 --> 00:09:57,980
ولی پیداست که وقتی که شما

281
00:09:58,180 --> 00:10:00,330
محیط برنامه نویسی درستی رو استفاده کنین ، بسیاری از

282
00:10:00,670 --> 00:10:02,060
الگوریتم ها میتونه واقعا برنامه های کوچیکی باشه .

283
00:10:03,510 --> 00:10:04,700
پس این همینطور چیزی هست که چرا تو

284
00:10:04,840 --> 00:10:05,890
این کلاس میخوایم

285
00:10:06,010 --> 00:10:07,430
محیط برنامه نویسی اکتاو رو استفاده کنیم .

286
00:10:08,550 --> 00:10:09,910
اکتاو ، نرم افزاری منبع باز هست

287
00:10:10,120 --> 00:10:11,620
و استفاده میکنه از یه

288
00:10:11,670 --> 00:10:13,130
ابزاری مثل اکتاو یا متلب ،

289
00:10:14,000 --> 00:10:15,400
الگوریتم های بسیاری فقط تبدیل

290
00:10:15,690 --> 00:10:17,910
به چند خط کوتاه کد برای پیاده سازی شدن .

291
00:10:18,380 --> 00:10:19,400
بعدا توی این کلاس ، آموزش میدم بهتون

292
00:10:19,620 --> 00:10:20,570
کمی بیشتر درباره اینکه چطور

293
00:10:20,720 --> 00:10:21,920
اکتاو رو استفاده کنین و

294
00:10:22,050 --> 00:10:24,590
چندتا از این الگوریتم ها رو توی اکتاو پیاده سازی خواهید کرد .

295
00:10:24,980 --> 00:10:26,050
یا اگر متلب دارین میتونین از اونم استفاده کنین .

296
00:10:27,120 --> 00:10:28,500
پیداست که سیلیکون والی ، برای

297
00:10:28,620 --> 00:10:29,470
الگوریتم های یادگیری ماشین بسیاری هست

298
00:10:30,290 --> 00:10:31,310
چیزی که توی نمونه اول انجام دادیم

299
00:10:32,040 --> 00:10:33,900
نرم افزارمون اکتاو هست چون نرم افزار اکتاو

300
00:10:34,330 --> 00:10:35,250
به طور خارق العاده ایی سریع هست

301
00:10:35,540 --> 00:10:36,920
در پیاده سازی این الگوریتم های یادگیری .

302
00:10:38,230 --> 00:10:39,110
اینجا هریک از این توابع

303
00:10:39,720 --> 00:10:41,460
برای نمونه مثل SVD

304
00:10:41,680 --> 00:10:42,920
تابعی که سرنام 
singular

305
00:10:43,240 --> 00:10:44,520
value decomposition 
هست . : به معنی تجزیه مقدار منفرد . ولی

306
00:10:44,640 --> 00:10:45,690
پیداست که میتونه باشه یه

307
00:10:45,820 --> 00:10:48,420
روتین جبر خطی ، که فقط توی اکتاو ساخت شده .

308
00:10:49,500 --> 00:10:50,390
اگر میخواین این و انجام بدین

309
00:10:50,460 --> 00:10:51,490
توی سی پلاس پلاس یا جاوا

310
00:10:51,780 --> 00:10:53,040
خط های بسیار بسیار زیادی از

311
00:10:53,180 --> 00:10:55,680
کد خواهد بود با اتصال کتابخانه پیچیده 
سی پلاس پلاس یا جاوا .

312
00:10:56,440 --> 00:10:57,490
پس ، میتونین این جور چیزا رو  توی

313
00:10:57,680 --> 00:10:58,690
سی پلاس پلاس یا جاوا

314
00:10:59,050 --> 00:11:00,090
یا پایتون پیاده سازی کنین ، فقط مقداری

315
00:11:00,290 --> 00:11:02,090
پیچیده تر هست برای انجام دادن توی اون زبان ها .

316
00:11:03,750 --> 00:11:05,060
چیزیکه دیدم بعد از تدریس

317
00:11:05,300 --> 00:11:06,980
یادگیری ماشین برای تقریبا یه

318
00:11:07,210 --> 00:11:08,680
دهه اخیر ، اینه که ، شما

319
00:11:08,890 --> 00:11:10,340
سریعتر یاد میگیرین اگر

320
00:11:10,480 --> 00:11:11,700
اکتاو رو به عنوان

321
00:11:11,790 --> 00:11:14,070
محیط برنامه نویسی تون استفاده کنین ، و اگر

322
00:11:14,250 --> 00:11:15,570
اگر اکتاو رو به عنوان

323
00:11:16,260 --> 00:11:17,110
ابزار یادگیری تون استفاده کنین و به عنوان

324
00:11:17,240 --> 00:11:18,640
ابزار نمونه سازی ، اجازه میده

325
00:11:19,000 --> 00:11:21,280
یاد بگیرین و نمونه سازی کنین الگوریتم های یادگیری رو 
خیلی سریعتر .

326
00:11:22,640 --> 00:11:23,850
و در واقع چیزیکه خیلی از مردم

327
00:11:23,990 --> 00:11:25,390
در شرکت های بزرگ سیلیکون

328
00:11:25,730 --> 00:11:27,360
والی انجام میدن در واقع ، استفاده

329
00:11:27,560 --> 00:11:29,020
یک الگوریتم مثل اکتاو برای

330
00:11:29,370 --> 00:11:31,110
مدل سازی اولیه الگوریتم یادگیری هست ، و

331
00:11:31,510 --> 00:11:32,780
فقط بعد از اینکه یاد گرفتین

332
00:11:32,860 --> 00:11:33,820
چطور کار کنین باهاش ، بعدش منتقلش کنین

333
00:11:34,390 --> 00:11:35,910
توی سی پلاس پلاس یا جاوا یا هرچیز دیگه ای .

334
00:11:36,890 --> 00:11:37,960
پیداست که با انجام

335
00:11:38,220 --> 00:11:39,070
این راه ، اکثر مواقع میتونین

336
00:11:39,400 --> 00:11:40,440
الگوریتم تون رو راه بندازین

337
00:11:41,300 --> 00:11:43,050
خیلی سریعتر از جاییکه توی سی پلاس پلاس اونو 
شروع به کارکردن کنین .

338
00:11:44,440 --> 00:11:46,010
خب ، بنده این رو به عنوان یه

339
00:11:46,100 --> 00:11:47,490
استاد میدونم ، باید

340
00:11:47,570 --> 00:11:48,580
بگم " بهم اطمینان کنین توی

341
00:11:48,730 --> 00:11:49,790
این مورد " فقط یه دفعه

342
00:11:50,030 --> 00:11:51,420
ولی برای

343
00:11:51,560 --> 00:11:52,720
اون کسانی که هرگز استفاده نکردن این

344
00:11:53,330 --> 00:11:54,880
نوع محیط برنامه نویسی اکتاو رو قبلا ،

345
00:11:55,240 --> 00:11:56,070
ازتون میخوام که

346
00:11:56,130 --> 00:11:56,970
به من اطمینان کنین در این مورد ،

347
00:11:57,570 --> 00:11:58,950
و بهتون میگم که ،

348
00:11:59,700 --> 00:12:01,180
فکر میکنم زمان تون ، زمان توسعه تون

349
00:12:01,700 --> 00:12:03,100
یکی از با ارزش ترین منابع هست .

350
00:12:04,210 --> 00:12:05,570
و دیده شده اکثر

351
00:12:05,800 --> 00:12:06,850
مردم اینکار و میکنن ، فکر میکنم

352
00:12:07,190 --> 00:12:08,460
شما به عنوان محقق یادگیری ماشین

353
00:12:08,850 --> 00:12:09,990
یا توسعه دهنده یادگیری ماشین

354
00:12:10,830 --> 00:12:12,080
بسیار سازنده تر خواهید بود اگر

355
00:12:12,220 --> 00:12:13,010
یادبگیرین به شروع نمونه سازی اولیه ،

356
00:12:13,580 --> 00:12:15,250
در شروع کار در اکتاو ، توی برخی زبان های دیگه

357
00:12:17,570 --> 00:12:19,790
در نهایت ، برای تکمیل

358
00:12:20,090 --> 00:12:22,890
این ویدیو , پرسشی کوتاه از تون دارم .

359
00:12:24,400 --> 00:12:26,400
درباره یادگیری نظارت نشده حرف زدیم ، که

360
00:12:26,700 --> 00:12:27,670
یادگیری ایی است که شما

361
00:12:27,760 --> 00:12:28,730
الگوریتمی رو به یه تن

362
00:12:28,840 --> 00:12:30,120
داده میدین و فقط میخواین ازش

363
00:12:30,240 --> 00:12:32,900
ساختاری رو برامون توی اون داده پیدا کنه .

364
00:12:33,160 --> 00:12:35,170
از چهار نمونه پایین کدوم یک

365
00:12:35,490 --> 00:12:36,410
کدوم یکی از این چهار تا

366
00:12:36,870 --> 00:12:37,630
فکر میکنین میتونه یه

367
00:12:37,720 --> 00:12:39,520
الگوریتم یادگیری نظارت نشده باشه که

368
00:12:40,220 --> 00:12:41,950
مخالف مساله یادگیری نظارت نشده هست .

369
00:12:42,730 --> 00:12:43,590
برای هریک از این چهار تا

370
00:12:43,860 --> 00:12:44,850
چک باکس های سمت چپش

371
00:12:45,640 --> 00:12:46,900
تیک بزنین هرکدوم رو که

372
00:12:47,210 --> 00:12:49,400
فکر میکنین الگوریتم یادگیری نظارت نشده

373
00:12:49,700 --> 00:12:51,300
مناسب براش خواهد بود و

374
00:12:51,440 --> 00:12:53,930
بعدش برای بررسی جواب تون روی 
دکمه پایینی سمت راست کلیک کنین .

375
00:12:54,690 --> 00:12:57,030
پس وقتی ویدیو متوقف شد ، لطفا

376
00:12:57,370 --> 00:12:58,750
پرسش داخل اسلاید رو پاسخ بدین .

377
00:13:01,860 --> 00:13:03,950
خب ، امیدوارم مساله پوشه اسپم -هرز نامه- رو
یادتون مونده باشه .

378
00:13:04,710 --> 00:13:06,310
اگه داده برچسب خورده ایی دارین ، باید

379
00:13:06,450 --> 00:13:07,680
بشناسین ، با ایمیل اسپم و

380
00:13:07,800 --> 00:13:10,470
ایمیل غیر اسپم ، با این به عنوان مساله 
یادگیری نظارت شده برخورد میکنیم .

381
00:13:11,620 --> 00:13:13,870
مثال موضوعات خبری ، اون

382
00:13:14,100 --> 00:13:15,370
دقیقا نمونه گوگل نیوز هست

383
00:13:15,910 --> 00:13:16,600
که توی این ویدیو دیدیم ،

384
00:13:17,090 --> 00:13:17,950
دیدیم که چطور میتونین یه

385
00:13:18,080 --> 00:13:19,460
الگوریتم خوشه بندی رو استفاده کنین برای خوشه کردن

386
00:13:19,880 --> 00:13:21,980
این مقاله ها باهمدیگه پس اون یادگیری نظارت نشده هست .

387
00:13:23,250 --> 00:13:25,440
مثال بخش بندی بازار

388
00:13:25,510 --> 00:13:27,120
همین تازگی کمی دربارش حرف زدم ،

389
00:13:27,220 --> 00:13:29,110
میتونین اونو به عنوان مساله یادگیری نظارت نشده 
انجام بدین

390
00:13:29,970 --> 00:13:30,860
چون فقط میخوام

391
00:13:30,930 --> 00:13:32,340
داده الگوریتم خودم رو بگیرم و بخوام ازش

392
00:13:32,500 --> 00:13:34,340
به صورت خودکار بخش بندی های بازار رو کشف کنه .

393
00:13:35,610 --> 00:13:37,930
و آخرین نمونه ، دیابتی ها ، خب

394
00:13:38,070 --> 00:13:39,080
درواقع اون فقط شبیه

395
00:13:39,350 --> 00:13:41,480
مثال سرطان سینه مون از ویدیو قبلی بود .

396
00:13:42,190 --> 00:13:43,320
فقط بجای ، میدونین ،

397
00:13:43,600 --> 00:13:45,280
تومور سرطانی خوب و بد یا

398
00:13:45,550 --> 00:13:47,390
تومور خوش خیم و بدخیم

399
00:13:47,550 --> 00:13:49,270
بجاش دیابتی رو داریم یا

400
00:13:49,330 --> 00:13:50,440
نداریم و خب

401
00:13:50,700 --> 00:13:51,830
اونو رو به عنوان نظارت شده بکار میگیریم

402
00:13:52,370 --> 00:13:53,740
اونو به عنوان

403
00:13:53,870 --> 00:13:54,670
مساله یادگیری نظارت شده حل میکنیم مثل چیزیکه

404
00:13:54,730 --> 00:13:56,450
برای داده تومور سینه انجام دادیم .

405
00:13:58,270 --> 00:13:59,400
پس ، اون برای یادگیری نظارت نشده

406
00:14:00,100 --> 00:14:01,580
است و توی ویدیو

407
00:14:01,650 --> 00:14:02,940
بعدی ، بیشتر به سراغ

408
00:14:03,270 --> 00:14:04,600
جزییات الگوریتم های یادگیری میریم

409
00:14:05,550 --> 00:14:06,590
و صحبت درمورد شون رو شروع میکنیم

410
00:14:07,220 --> 00:14:08,750
که چطور این الگوریتم ها کار میکنن و

411
00:14:08,920 --> 00:14:11,270
چطور میتونیم به سراغ پیاده سازی شون بریم .