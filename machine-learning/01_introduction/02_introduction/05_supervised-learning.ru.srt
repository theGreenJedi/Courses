1
00:00:00,000 --> 00:00:04,620
В этом видео я дам определение, вероятно,
самому распространенному типу задач

2
00:00:04,620 --> 00:00:08,910
машинного обучения, а именно, обучению
с учителем.Я дам более формальное

3
00:00:08,910 --> 00:00:13,255
определение позднее, так как,
вероятно, лучше

4
00:00:13,255 --> 00:00:17,820
всего начать объяснение с
примера.Допустим, вы

5
00:00:17,820 --> 00:00:23,072
хотите предсказать цены на недвижимость.Некоторое время
назад один студент собрал данные в Институте

6
00:00:23,072 --> 00:00:28,745
Портленда, штат Орегон.Допустим, вы
создали графическое представление этих данных,

7
00:00:28,745 --> 00:00:34,347
и оно выглядит вот так.Здесь по горизонтальной
оси - площадь разных домов в

8
00:00:34,347 --> 00:00:39,879
квадратных футах, а по вертикальной
оси - цена домов в тысячах долларов.

9
00:00:39,879 --> 00:00:45,168
Итак.Учитывая эту информацию,
представьте, что у вас есть друг, у которого есть дом

10
00:00:45,168 --> 00:00:50,708
площадью, допустим, 750 квадратных футов,
он хочет продать этот дом, и ему надо

11
00:00:50,708 --> 00:00:56,116
понять, сколько денег он может получить за его продажу.Как в этой ситуации
вам может помочь алгоритм обучения?Например,

12
00:00:56,116 --> 00:01:01,524
алгоритм обучения мог бы построить прямую,
проходящую через данные, или

13
00:01:01,524 --> 00:01:07,111
подогнать прямую к данным и, на основании
этого сделать вывод, что дом,

14
00:01:07,111 --> 00:01:13,239
похоже, можно будет продать примерно за $150,000.Но, возможно,
это не единственный алгоритм обучения, который можно

15
00:01:13,239 --> 00:01:18,536
здесь применить.Возможно, есть вариант получше.Например,
вместо того, чтобы проводить через наши

16
00:01:18,536 --> 00:01:23,620
данные прямую, мы могли бы решить,
что эти данные лучше описывает

17
00:01:23,620 --> 00:01:29,110
квадратичная функция или полином второго
порядка.В этом случае

18
00:01:29,110 --> 00:01:34,667
похоже что оценка
дома ближе

19
00:01:34,667 --> 00:01:39,184
к $200,000.Позднее мы
поговорим о том, как выбрать, каким

20
00:01:39,184 --> 00:01:43,792
способом приближенного
описания данных воспользоваться, -

21
00:01:43,792 --> 00:01:48,631
прямой линией или квадратичной
функцией. Не так уж и просто сказать

22
00:01:48,631 --> 00:01:53,182
заранее, какой из методов даст более точный прогноз.Но каждый
из них будет хорошим примером алгоритма

23
00:01:53,182 --> 00:01:57,834
обучения.И это пример
алгоритма обучения с учителем.Этот

24
00:01:57,834 --> 00:02:03,736
тип обучения назван обучением с учителем
потому, что мы дали алгоритму набор данных,

25
00:02:03,736 --> 00:02:09,089
который уже содержит
«правильные ответы».То есть,

26
00:02:09,089 --> 00:02:14,580
мы задали ему в качестве базы данных список домов,
для каждого примера в этом

27
00:02:14,580 --> 00:02:20,002
перечне, мы рассказали, что
является правильной ценой, а что

28
00:02:20,002 --> 00:02:25,423
действительной стоимостью, за какую продается дом.Основной задачей
алгоритма является производство наибольшего числа правильных

29
00:02:25,423 --> 00:02:30,579
ответов, как, например, для данного дома, который, как вы знаете, пытается
продать ваш друг.Ещё одно определение:

30
00:02:30,579 --> 00:02:35,257
подобные задачи
также называются задачами регрессии. В

31
00:02:35,257 --> 00:02:40,467
задачах регрессии алгоритм должен прогнозировать значение
непрерывной величины.А именно - цены.

32
00:02:40,467 --> 00:02:44,720
На практике цены можно округлить
с точностью до цента.Так что

33
00:02:44,720 --> 00:02:49,246
цена может быть и дискретной величиной,
но на практике мы моделируем

34
00:02:49,246 --> 00:02:53,608
её как вещественную скалярную
непрерывную величину. Когда мы

35
00:02:53,608 --> 00:02:58,080
говорим о регрессии, мы имеем в
виду задачу, в которой необходимо

36
00:02:58,080 --> 00:03:02,060
предсказать значение, так сказать, непрерывной величины.Вот еще один
пример обучения с учителем, мы с друзьями

37
00:03:02,060 --> 00:03:06,427
действительно раньше над
ним работали.Предположим, на основе

38
00:03:06,427 --> 00:03:11,675
истории болезни вы пытаетесь определить, является ли
опухоль в груди злокачественной или доброкачественной.В

39
00:03:11,675 --> 00:03:16,856
случае обнаружения в груди опухоли, злокачественная
опухоль является вредной

40
00:03:16,856 --> 00:03:22,300
и опасной, а доброкачественная
не представляет опасности.

41
00:03:22,300 --> 00:03:27,876
Очевидно, что людей этот вопрос очень волнует.
Давайте представим собранные данные

42
00:03:27,876 --> 00:03:33,164
в таком виде, что значения по
горизонтальной оси - размер

43
00:03:33,164 --> 00:03:39,317
опухоли, а значения вертикальной оси
могут быть единицей или нулём, да или

44
00:03:39,317 --> 00:03:45,184
нет - является ли данная опухоль
злокачественной или

45
00:03:45,184 --> 00:03:50,392
доброкачественной.Предположим, наши данные
будут выглядеть так: опухоль этого

46
00:03:50,392 --> 00:03:56,283
размера оказалась доброкачественной.Одна такого
размера, одна такого.И так далее.И,

47
00:03:56,283 --> 00:04:02,227
к сожалению, мы также видим несколько злокачественных опухолей, одна такого размера,
одна такого, и одна такого

48
00:04:02,227 --> 00:04:08,572
размера.И так далее.Итак, в
нашем примере...У нас

49
00:04:08,572 --> 00:04:15,159
есть пять экземпляров доброкачественных опухолей, изображенных внизу,
и пять экземпляров злокачественных опухолей, изображенных со

50
00:04:15,159 --> 00:04:21,504
значением 1 по вертикальной оси.Предположим, у нас
есть подруга, у которой, к несчастью,

51
00:04:21,504 --> 00:04:28,097
в груди обнаружена опухоль, скажем,
примерно такого размера.Задача

52
00:04:28,097 --> 00:04:32,930
машинного обучения состоит в том, чтобы рассчитать
вероятности того, что опухоль

53
00:04:32,930 --> 00:04:37,819
является злокачественной, либо
доброкачественной.Подобные

54
00:04:37,819 --> 00:04:42,719
задачи называются задачами классификации
(classification).Классифицировать

55
00:04:42,719 --> 00:04:47,342
- значит пытаться предсказать дискретное
значение на выходе: 0 или

56
00:04:47,342 --> 00:04:52,321
1, доброкачественная
или злокачественная.В некоторых

57
00:04:52,321 --> 00:04:58,331
задачах классификации прогнозируемая величина
может принимать

58
00:04:58,331 --> 00:05:03,852
более двух возможных значений.Рассмотрим
конкретный пример. Пусть необходимо определить, какой из трёх видов рака груди

59
00:05:03,852 --> 00:05:09,947
у пациентки. Алгоритм будет выдавать
одно из четырех дискретных

60
00:05:09,947 --> 00:05:15,138
значений — 1, 2, 3, или 0, где 0 означает доброкачественную опухоль.
Доброкачественная опухоль означает отсутствие рака.

61
00:05:15,138 --> 00:05:19,836
Единица может означать первый тип рака, какой бы из трех
типов рака ни имелся

62
00:05:19,836 --> 00:05:24,654
в виду.Двойка
означает второй тип, и тройка - третий

63
00:05:24,654 --> 00:05:29,111
тип.Это
по-прежнему проблема классификации,

64
00:05:29,111 --> 00:05:33,929
поскольку предсказываемая
величина дискретна и означает, как я

65
00:05:33,929 --> 00:05:39,094
сказал, отсутствие рака или наличие и
один из трёх типов рака.В задачах классификации

66
00:05:39,094 --> 00:05:44,413
есть и другой способ графического представления этих данных.Позвольте вам
показать, что я имею в виду.Для визуализации

67
00:05:44,413 --> 00:05:49,206
этих данных я буду использовать немного другой
набор символов.Если размер

68
00:05:49,206 --> 00:05:54,303
опухоли предполагается использовать как атрибут, с помощью которого я буду
предсказывать злокачественность или доброкачественность

69
00:05:54,303 --> 00:05:58,975
опухоли, то эти данные можно изобразить вот так.Я собираюсь
использовать разные символы для обозначения доброкачественности и злокачественности,

70
00:05:58,975 --> 00:06:03,707
или отрицательных и положительных
результатов.И для

71
00:06:03,707 --> 00:06:11,595
обозначения доброкачественных опухолей я теперь вместо
крестиков буду рисовать кружки.Вот так.Злокачественные

72
00:06:11,595 --> 00:06:18,655
опухоли я продолжу обозначать крестиками.
Согласны?Я надеюсь, вам уже

73
00:06:18,655 --> 00:06:23,624
становится понятно.Я просто взял набор
данных с верхнего графика и спроецировал

74
00:06:23,624 --> 00:06:30,894
его вниз.На эту числовую прямую, вот так.И стал
использовать различные маркеры, кружки и крестики,

75
00:06:30,894 --> 00:06:35,828
для обозначения злокачественных и доброкачественных
экземпляров.В данном

76
00:06:35,828 --> 00:06:41,091
примере для предсказания злокачественности
опухоли мы используем только

77
00:06:41,091 --> 00:06:46,289
один
признак.В других задачах

78
00:06:46,289 --> 00:06:51,355
машинного обучения мы можем иметь более одного
признака или атрибута.Рассмотрим

79
00:06:51,355 --> 00:06:56,749
пример.Допустим, что
помимо размера опухоли, нам известен также возраст

80
00:06:56,749 --> 00:07:02,387
пациента.В этом

81
00:07:02,387 --> 00:07:08,562
случае наш набор данных будет выглядеть примерно так: здесь будет
первое множество пациентов, соответственно возрасту

82
00:07:08,562 --> 00:07:14,980
и размеру опухоли, их я обозначаю кружками,
и второе множество пациентов, со злокачественными

83
00:07:15,600 --> 00:07:23,968
опухолями, которые обозначены
крестиками.Итак, предположим,

84
00:07:23,968 --> 00:07:32,027
что у нас есть подруга, у которой,
к несчастью, обнаружена опухоль.И, возможно, размер

85
00:07:32,027 --> 00:07:37,657
опухоли и возраст пациентки находятся в этой области.Что может сделать
алгоритм обучения с подобным набором

86
00:07:37,657 --> 00:07:42,462
данных? Например, провести прямую
линию и постараться отделить

87
00:07:42,462 --> 00:07:47,710
злокачественные опухоли от доброкачественных.
Алгоритм может принять

88
00:07:47,710 --> 00:07:53,004
решение провести прямую вот так, отделяя два класса
опухолей друг от друга.

89
00:07:53,004 --> 00:07:57,644
Далее.Надеюсь,
глядя на этот рисунок, вы сможете предположить, что раз опухоль вашей подруги

90
00:07:57,644 --> 00:08:02,322
находится вот здесь, то наш алгоритм обучения скажет, что опухоль вашей
подруги лежит на доброкачественной части плоскости и, следовательно

91
00:08:02,322 --> 00:08:07,305
, более вероятно, что она является
доброкачественной, чем

92
00:08:07,305 --> 00:08:12,044
злокачественной.В этом
примере мы используем

93
00:08:12,044 --> 00:08:17,147
два признака: возраст пациента и
размер опухоли.В других задачах

94
00:08:17,147 --> 00:08:21,454
машинного обучения часто встречается
большее число характеристик, и мои друзья, работающие в этой области, в

95
00:08:21,454 --> 00:08:25,849
действительности используют,
например, такие характеристики: толщина сгустков, толщина определенного

96
00:08:25,849 --> 00:08:30,299
сгустка в опухоли.Однородность
размеров клеток опухоли.Однородность формы

97
00:08:30,299 --> 00:08:34,911
клеток опухоли, а также другие
признаки.И, оказывается,

98
00:08:34,911 --> 00:08:39,907
один из самых интересных алгоритмов
обучения, рассматриваемых в

99
00:08:39,907 --> 00:08:45,153
нашем курсе, может работать не просто
с двумя, тремя или пятью признаками

100
00:08:45,153 --> 00:08:50,150
, а с бесконечным числом признаков.На этом слайде я перечислил
пять разных признаков.

101
00:08:50,150 --> 00:08:54,482
Да, два - на осях и еще три - вот здесь.
Оказывается, что для некоторых

102
00:08:54,482 --> 00:08:58,497
задач обучения вам недостаточно использовать
три или, скажем, пять признаков.Вам

103
00:08:58,497 --> 00:09:02,566
нужно задействовать бесконечное
число параметров, чтобы

104
00:09:02,566 --> 00:09:06,211
алгоритм мог опираться на множество
различных характеристик или

105
00:09:06,211 --> 00:09:10,333
признаков при составлении
прогноза.Но как работать с

106
00:09:10,333 --> 00:09:14,439
бесконечным числом параметров?Как вы будете
хотя бы хранить бесконечный

107
00:09:14,439 --> 00:09:18,290
объем данных в компьютере, если память
в нем конечна?Оказывается,

108
00:09:18,290 --> 00:09:22,188
существует алгоритм, который называется
«метод опорных векторов» (Support

109
00:09:22,188 --> 00:09:26,675
Vector Machine, SVM). Он использует изящный математический
приём, который позволяет компьютеру

110
00:09:26,675 --> 00:09:31,214
использовать бесконечное число признаков.Представьте,
что я не просто записал два признака здесь

111
00:09:31,214 --> 00:09:35,487
и ещё три признака справа,а зафиксировал
очень длинный список, и продолжаю добавлять

112
00:09:35,487 --> 00:09:39,866
в него всё больше и больше
признаков.Как будто

113
00:09:39,866 --> 00:09:44,192
это бесконечный список признаков.Оказывается, мы
сможем предложить алгоритм, который

114
00:09:44,192 --> 00:09:49,701
сможет работать с таким списком.Итак, подведем итоги.
В этом курсе мы будем говорить

115
00:09:49,701 --> 00:09:54,167
об обучении с учителем.Особенность обучения
с учителем состоит в том, что для

116
00:09:54,167 --> 00:09:58,880
каждого экземпляра обучающей
выборки нам известен «правильный ответ». Мы бы хотели, чтобы обученный

117
00:09:58,880 --> 00:10:03,960
алгоритм прогнозировал
правильные ответы для этих экземпляров.Например,

118
00:10:03,960 --> 00:10:08,428
стоимость дома или вид опухоли - злокачественная
или доброкачественная.Мы также упоминали о

119
00:10:08,428 --> 00:10:13,202
проблеме регрессии.Под регрессией мы понимаем
задачу, в которой нашей целью является предсказание значений из

120
00:10:13,202 --> 00:10:17,977
некоторого непрерывного множества.Затем мы
обсудили задачу классификации, в которой

121
00:10:17,977 --> 00:10:22,690
целью является прогнозирование некоторой дискретной
величины.В завершение

122
00:10:22,690 --> 00:10:27,541
задам простой вопрос. Представьте, что Вы управляете компанией
и собираетесь разработать алгоритмы обучения для решения следующих

123
00:10:27,541 --> 00:10:32,618
двух задач.В первой
задаче у вас есть большой запас одинаковых

124
00:10:32,618 --> 00:10:38,113
предметов.Поэтому
представьте себе, что у вас есть несколько

125
00:10:38,113 --> 00:10:43,607
тысяч единиц какого-то товара на
продажу и вы хотели бы предсказать,

126
00:10:43,607 --> 00:10:49,172
какое их количество вы сможете продать за три месяца.Во второй задаче
у вас есть множество пользователей,

127
00:10:49,172 --> 00:10:54,145
и вы хотите написать программу,
которая будет проверять учетные записи

128
00:10:54,145 --> 00:10:59,193
каждого из ваших клиентов и определять,
была ли эта запись

129
00:10:59,193 --> 00:11:04,178
взломана
или скомпрометирована.Итак, для

130
00:11:04,178 --> 00:11:08,914
каждой из этих задач определите, является
она задачей классификации

131
00:11:08,914 --> 00:11:14,087
или регрессией?Когда
видео приостановится, пожалуйста, выберите

132
00:11:14,087 --> 00:11:20,884
мышкой тот из четырех ответов,
который вы считаете правильным.Надеюсь, что вы

133
00:11:20,884 --> 00:11:25,871
поняли, что это правильный ответ.Первую
задачу я бы решал как

134
00:11:25,871 --> 00:11:31,058
задачу регрессии, так как если исходное
количество предметов измеряется

135
00:11:31,058 --> 00:11:36,071
тысячами, то можно смело
моделировать его вещественной, непрерывной величиной.И

136
00:11:36,290 --> 00:11:41,837
также моделировать количество проданных предметов
непрерывной величиной.А вторую

137
00:11:41,837 --> 00:11:47,748
задачу я бы решал как задачу классификации,
так как можно предложить

138
00:11:47,748 --> 00:11:53,659
бинарное кодирование результирующей
величины: «ноль» в случае, если

139
00:11:53,659 --> 00:11:58,850
учётная запись не была взломана.
И «один» в случае, если она была взломана.Так же как

140
00:11:58,850 --> 00:12:03,287
в задаче о раке груди ноль означал доброкачественную
опухоль, а единица - злокачественную.Итак, я могу

141
00:12:03,287 --> 00:12:08,150
установить, что эти значения будут
равны 0 или 1 в зависимости от

142
00:12:08,150 --> 00:12:13,134
наличия взлома и постараюсь заставить алгоритм предсказывать
каждое из этих дискретных значений.Так как число

143
00:12:13,134 --> 00:12:17,693
значений прогнозируемой величины невелико, задачу
можно решать как

144
00:12:17,693 --> 00:12:23,075
задачу классификации.На этом закончим рассматривать  обучение с
учителем. В следующем видео я расскажу

145
00:12:23,075 --> 00:12:28,325
о другой важной категории алгоритмов
обучения - обучении без учителя.