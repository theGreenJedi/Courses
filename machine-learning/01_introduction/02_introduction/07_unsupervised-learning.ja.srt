1
00:00:00,380 --> 00:00:01,550
このビデオでは、

2
00:00:01,670 --> 00:00:02,690
二番目に大きな機械学習

3
00:00:03,010 --> 00:00:05,030
問題である「教師なし学習」についてお話します。

4
00:00:06,300 --> 00:00:08,500
前のビデオで教師あり学習についてお話しました。

5
00:00:09,250 --> 00:00:10,700
その時に、このようなデータセットを

6
00:00:11,020 --> 00:00:12,670
お見せしたことを思い返してください。個々の

7
00:00:12,890 --> 00:00:15,150
サンプルはラベル付けされていて

8
00:00:15,610 --> 00:00:16,900
陽性か陰性の手本のいずれかでした。

9
00:00:17,530 --> 00:00:19,800
良性かあるいは悪性の腫瘍、という風に。

10
00:00:20,850 --> 00:00:21,920
ですから、教師あり学習では、それぞれのサンプルには

11
00:00:22,410 --> 00:00:24,270
明示的に何が

12
00:00:24,440 --> 00:00:25,760
いわゆる「正解」が何か示されていました。

13
00:00:26,490 --> 00:00:27,580
良性か悪性かなどのように。

14
00:00:28,550 --> 00:00:30,210
教師なし学習で与えられるのは

15
00:00:30,540 --> 00:00:31,720
そのようなデータとは異なる

16
00:00:31,950 --> 00:00:32,910
このようなデータになります。

17
00:00:33,190 --> 00:00:34,600
ラベル付けされていないか

18
00:00:34,720 --> 00:00:35,920
全て同じ

19
00:00:36,130 --> 00:00:37,460
ラベルか、あるいはラベルがそもそもありません。

20
00:00:39,680 --> 00:00:40,740
そうしたデータセットを与えられ、

21
00:00:40,980 --> 00:00:42,460
それに対して何をすべきか

22
00:00:42,560 --> 00:00:43,290
指定がなく、それぞれの

23
00:00:43,640 --> 00:00:44,800
データポイントが何かも明示されていません。

24
00:00:45,290 --> 00:00:47,190
その代わりに、単に、データセットがあるだけです。

25
00:00:47,870 --> 00:00:49,650
このデータに何か構造を見つけることができますか？

26
00:00:50,480 --> 00:00:51,670
このデータセットでは、

27
00:00:52,350 --> 00:00:53,940
教師なし学習アルゴリズムは

28
00:00:54,060 --> 00:00:56,090
データが二つの異なるクラスターに属すると結論するかもしれません。

29
00:00:56,800 --> 00:00:57,960
ここに一つのクラスターがあり、

30
00:00:59,120 --> 00:00:59,910
ここに別のクラスターがあります。

31
00:01:01,110 --> 00:01:02,710
このように教師なし学習アルゴリズムは

32
00:01:03,040 --> 00:01:05,070
こうしたデータを二つの別々のクラスターに分けたりします。

33
00:01:06,410 --> 00:01:08,000
そしてこれはクラスタリング・アルゴリズムと呼ばれます。

34
00:01:08,860 --> 00:01:10,310
さらに、これは多くの場所で使われています。

35
00:01:11,930 --> 00:01:13,310
クラスタリングが使われている

36
00:01:13,530 --> 00:01:14,860
一例は、Google

37
00:01:15,060 --> 00:01:16,160
News です。もし、まだ

38
00:01:16,360 --> 00:01:17,320
見たことがなければ、実際に

39
00:01:18,210 --> 00:01:19,040
この URL news.google.com に行って

40
00:01:19,830 --> 00:01:20,460
見てください。

41
00:01:21,280 --> 00:01:22,970
Google News が行うのは、

42
00:01:23,480 --> 00:01:24,220
毎日、

43
00:01:24,470 --> 00:01:25,430
何万、

44
00:01:25,720 --> 00:01:26,740
何十万という数の報道記事を

45
00:01:26,800 --> 00:01:29,410
ウェブから集め、これを関連する記事にグループ分けします。

46
00:01:30,730 --> 00:01:31,690
例えば、これを見てみましょう。

47
00:01:33,380 --> 00:01:35,370
これらの URL がリンクしているのは

48
00:01:35,910 --> 00:01:37,260
BP 油井のニュースに関する

49
00:01:38,010 --> 00:01:40,110
別々の記事です。

50
00:01:41,300 --> 00:01:42,160
では、これらの

51
00:01:42,260 --> 00:01:43,090
URL の一つをクリックして

52
00:01:43,550 --> 00:01:44,780
これらの URL の一つをクリックします。

53
00:01:45,100 --> 00:01:46,970
すると、このようなウェブページに辿り着きます。

54
00:01:47,210 --> 00:01:48,390
これは ウォールストリート

55
00:01:48,590 --> 00:01:50,180
ジャーナルの記事、BP

56
00:01:51,110 --> 00:01:52,530
油井流出の報道で、

57
00:01:52,920 --> 00:01:54,350
「BP マコンドを閉鎖」

58
00:01:54,590 --> 00:01:55,700
これは流出が発生した

59
00:01:55,980 --> 00:01:57,960
油井の名前です。そして

60
00:01:58,020 --> 00:01:59,360
そのグループの違う URL をクリックすると

61
00:02:00,690 --> 00:02:02,500
違う記事に辿り着くかもしれません。

62
00:02:02,950 --> 00:02:04,760
これは、CNN の記事で、

63
00:02:04,820 --> 00:02:06,090
これもまた BP 原油流出に関するものです。

64
00:02:07,090 --> 00:02:08,180
そしてさらに三番目の

65
00:02:08,740 --> 00:02:10,990
リンクをクリックすると、また別の記事に行きます。

66
00:02:11,440 --> 00:02:13,380
これは イギリスのガーディアン紙の

67
00:02:13,940 --> 00:02:15,510
BP 原油流出に関する記事です。

68
00:02:16,530 --> 00:02:17,790
ですから Google News がしたのは、

69
00:02:17,990 --> 00:02:19,440
数万件もの報道記事を集めて、

70
00:02:19,490 --> 00:02:22,170
それをクラスタリングしてまとめたのです。

71
00:02:23,030 --> 00:02:24,660
これにより、同じ話題の報道記事は

72
00:02:25,080 --> 00:02:27,010
全てまとめて表示されています。

73
00:02:27,210 --> 00:02:29,170
実は、

74
00:02:29,380 --> 00:02:31,020
クラスタリング・アルゴリズムと教師なし学習

75
00:02:31,530 --> 00:02:33,550
アルゴリズムは他にも多くの問題で使われています。

76
00:02:35,320 --> 00:02:36,690
これは、ゲノミックスの理解に関する一例です。

77
00:02:38,270 --> 00:02:40,510
これは、DNA マイクロアレイのデータの例です。

78
00:02:40,990 --> 00:02:42,230
考え方としては、

79
00:02:42,430 --> 00:02:44,360
別々の個人からなるグループで

80
00:02:44,510 --> 00:02:45,590
一人一人、どれだけ特定の

81
00:02:46,100 --> 00:02:48,580
遺伝子を持っているかどうかを測定します。

82
00:02:49,050 --> 00:02:51,640
厳密には、特定の遺伝子がどれだけ発現しているかを測定します。

83
00:02:52,000 --> 00:02:54,190
様々な色、赤、緑

84
00:02:54,930 --> 00:02:56,210
灰色、などは、

85
00:02:56,340 --> 00:02:57,500
異なる個人間でどれぐらい

86
00:02:57,780 --> 00:02:59,440
特定の遺伝子が共通しているか

87
00:02:59,510 --> 00:03:01,270
していないかを表しています。

88
00:03:02,500 --> 00:03:03,400
これで何ができるかというと、

89
00:03:03,610 --> 00:03:05,070
クラスタリング・アルゴリズムを実行して、

90
00:03:05,380 --> 00:03:07,140
個人を異なるカテゴリ、

91
00:03:07,780 --> 00:03:08,810
異なるタイプの人々にグループ分けすることです。

92
00:03:10,230 --> 00:03:11,660
これは教師なし学習です。なぜなら

93
00:03:11,930 --> 00:03:14,010
前もってアルゴリズムに

94
00:03:14,590 --> 00:03:15,690
これらがタイプ 1 の人々で

95
00:03:16,130 --> 00:03:17,420
それらが タイプ 2 の人々で、それら

96
00:03:17,560 --> 00:03:18,650
がタイプ 3 の人々だなどと教えていないからです。

97
00:03:19,610 --> 00:03:22,390
代わりに、ここにいくらかデータがあり、

98
00:03:23,110 --> 00:03:24,030
そのデータに何が含まれているかは未知で、

99
00:03:24,750 --> 00:03:25,870
誰がどのタイプかは知らないし、

100
00:03:26,150 --> 00:03:26,940
そもそもどのような異なる

101
00:03:27,260 --> 00:03:28,480
タイプの人がいるのか分からないが、

102
00:03:28,610 --> 00:03:30,210
自動的に構造を

103
00:03:30,360 --> 00:03:31,260
データの中に見つけられないか、自動的に

104
00:03:32,180 --> 00:03:33,620
個人を前もって知られていないタイプに

105
00:03:33,870 --> 00:03:35,490
クラスタリングすることで、と言っているわけです。

106
00:03:35,890 --> 00:03:37,610
アルゴリズムに

107
00:03:38,160 --> 00:03:40,140
正解を

108
00:03:40,370 --> 00:03:41,270
データセット内のサンプルに対して与えていないため

109
00:03:41,590 --> 00:03:43,090
これは教師なし学習です。

110
00:03:44,290 --> 00:03:47,040
教師なし学習はあるいはクラスタリングは他にも幾つかの応用例があります。

111
00:03:48,340 --> 00:03:50,340
大規模なコンピュータ・クラスターを整理するために使われています。

112
00:03:51,390 --> 00:03:52,530
ある友人は大規模な

113
00:03:52,680 --> 00:03:53,970
データセンター、つまり

114
00:03:54,180 --> 00:03:55,970
大規模なコンピュータ・クラスターを調べて

115
00:03:56,230 --> 00:03:57,470
どのマシンが一緒に動作する

116
00:03:57,590 --> 00:03:59,130
傾向にあるかを知ろうとしていました。

117
00:03:59,200 --> 00:04:00,270
もしこうしたマシンを隣接させることができれば、

118
00:04:01,100 --> 00:04:03,220
データセンターの稼動効率が改善されます。

119
00:04:04,810 --> 00:04:06,820
二番目の応用例は、ソーシャルネットワーク分析です。

120
00:04:07,890 --> 00:04:09,230
ある知識が与えられていて、例えばどの友人に

121
00:04:09,630 --> 00:04:10,840
最も多くの電子メールを送るかとか

122
00:04:10,880 --> 00:04:12,150
Facebook フレンドリストあるいは

123
00:04:12,180 --> 00:04:14,150
Google+ サークルの情報を元に、

124
00:04:14,290 --> 00:04:16,380
自動的にどれが

125
00:04:16,450 --> 00:04:17,950
密接な友人のグループか、

126
00:04:18,460 --> 00:04:19,420
また、お互いが全て顔見知りの

127
00:04:20,230 --> 00:04:21,010
人々のグループはどれか、ということを特定できないか?

128
00:04:22,540 --> 00:04:22,880
マーケットセグメンテーション（市場細分化）。

129
00:04:24,680 --> 00:04:26,780
多くの企業は巨大な顧客情報データベースを保持しています。

130
00:04:27,700 --> 00:04:28,410
この顧客情報の

131
00:04:28,510 --> 00:04:30,000
データセットを見て、自動的に

132
00:04:30,740 --> 00:04:32,340
マーケットセグメントを発見し、自動的に

133
00:04:33,340 --> 00:04:35,290
顧客を異なる

134
00:04:35,820 --> 00:04:37,400
マーケットセグメントにグループ分けして

135
00:04:37,710 --> 00:04:39,490
自動的に、もっと

136
00:04:39,650 --> 00:04:41,580
効率的に、販売やマーケティングを

137
00:04:41,890 --> 00:04:43,250
異なるマーケットセグメントに一括して行うことができないか?

138
00:04:44,260 --> 00:04:45,580
これも、また、教師なし学習です。

139
00:04:45,820 --> 00:04:46,720
なぜなら、こうした

140
00:04:46,900 --> 00:04:48,340
顧客データは全てあるものの、

141
00:04:48,590 --> 00:04:49,710
前もってどのような

142
00:04:49,790 --> 00:04:51,270
マーケットセグメントがあるか知りませんし

143
00:04:51,440 --> 00:04:52,570
データセット内の顧客について

144
00:04:52,660 --> 00:04:53,590
前もって

145
00:04:53,690 --> 00:04:54,700
誰が

146
00:04:54,800 --> 00:04:55,840
マーケットセグメント 1 に属し、誰が

147
00:04:55,940 --> 00:04:57,800
マーケットセグメント 2 に属すかなどを知らないからです。

148
00:04:57,930 --> 00:05:00,630
しかしこれを全て単にデータから発見させなければなりません。

149
00:05:01,970 --> 00:05:03,140
最後に、実は教師なし学習が

150
00:05:03,690 --> 00:05:05,620
使われている用途には

151
00:05:06,090 --> 00:05:08,060
意外なことに、天文学データ解析もあります。

152
00:05:08,890 --> 00:05:10,390
そしてこうしたクラスタリング・アルゴリズムは

153
00:05:10,580 --> 00:05:12,440
意外にも銀河の発生について

154
00:05:12,900 --> 00:05:15,610
興味深い有用な理論を提供しています。

155
00:05:15,880 --> 00:05:17,620
これらは全てクラスタリングの事例です。

156
00:05:18,400 --> 00:05:20,550
そしてこれは教師なし学習の一種に過ぎません。

157
00:05:21,530 --> 00:05:22,470
別のものについてお話します。

158
00:05:23,200 --> 00:05:25,020
これからお話するのは、カクテルパーティー問題です。

159
00:05:26,310 --> 00:05:28,270
カクテルパーティーに行ったことはありますよね?

160
00:05:28,440 --> 00:05:30,080
想像できると思いますが、パーティーが

161
00:05:30,300 --> 00:05:31,690
あって、部屋に人がいっぱいいて、

162
00:05:31,870 --> 00:05:32,930
みな座って同時に会話を

163
00:05:32,970 --> 00:05:34,390
していて、

164
00:05:34,480 --> 00:05:36,230
みんなが同時に話をしているので

165
00:05:36,590 --> 00:05:37,920
話し声が重複しています。

166
00:05:38,070 --> 00:05:39,730
目の前にいる人の声を聞くのも困難なくらいです。

167
00:05:40,690 --> 00:05:41,970
そこで、仮に

168
00:05:42,020 --> 00:05:43,990
二人の人がカクテルパーティをしていて

169
00:05:45,690 --> 00:05:46,670
二人が同時に話をしているとします。

170
00:05:46,770 --> 00:05:48,090
これはやや

171
00:05:48,740 --> 00:05:49,710
小さなカクテルパーティーです。

172
00:05:50,690 --> 00:05:51,630
そして二つの

173
00:05:51,890 --> 00:05:53,080
マイクロフォンを部屋に設置します。

174
00:05:54,060 --> 00:05:55,640
これがマイクロフォンです。そして

175
00:05:56,050 --> 00:05:57,430
これらのマイクロフォンは二箇所の

176
00:05:57,560 --> 00:05:58,900
異なる距離の場所で

177
00:05:58,990 --> 00:06:01,250
話し手の声を拾いますので、それぞれのマイクロフォンは

178
00:06:01,830 --> 00:06:04,720
この二人の話し手の声を異なる組み合わせで録音します。

179
00:06:05,810 --> 00:06:06,970
例えば話し手 1 は

180
00:06:07,120 --> 00:06:08,320
マイクロフォン 1 ではやや大きな声に聞こえ、

181
00:06:09,120 --> 00:06:10,680
もしかして話し手 2 は

182
00:06:10,800 --> 00:06:12,350
マイクロフォン 2 に対してやや大きな声で聞こえるかもしれません。

183
00:06:12,560 --> 00:06:14,040
二つのマイクロフォンは

184
00:06:14,230 --> 00:06:15,950
相対的に異なる位置で

185
00:06:16,400 --> 00:06:19,020
二人の声を拾うので、それぞれの

186
00:06:19,250 --> 00:06:20,390
マイクロフォンが録音するのは、

187
00:06:20,970 --> 00:06:22,590
両方の話し手の声の重複した組み合わせです。

188
00:06:23,960 --> 00:06:25,500
さて、ここに実際に研究者が録音した

189
00:06:26,520 --> 00:06:29,280
二人の話し声があります。

190
00:06:29,740 --> 00:06:30,950
再生してみましょう。

191
00:06:31,060 --> 00:06:32,760
最初に第一のマイクロフォンが拾った音です。

192
00:06:33,560 --> 00:06:34,800
One (uno), two (dos),

193
00:06:35,070 --> 00:06:36,590
three (tres), four (cuatro),

194
00:06:37,060 --> 00:06:38,550
five (cinco), six (seis), seven (siete),

195
00:06:38,990 --> 00:06:40,610
eight (ocho), nine (nueve), ten (y diez).

196
00:06:41,610 --> 00:06:42,650
まあ、あんまり楽しそうなカクテル

197
00:06:43,000 --> 00:06:44,270
パーティーではなさそうですが、二人の人が

198
00:06:44,620 --> 00:06:45,670
1 から 10 まで

199
00:06:46,010 --> 00:06:47,880
二つの言語で数えています。お分かりの通り。

200
00:06:48,870 --> 00:06:49,760
たった今お聞き頂いたのは、

201
00:06:49,820 --> 00:06:52,500
第一のマイクロフォンの録音です。これが二番目の録音です。

202
00:06:57,440 --> 00:06:58,040
Uno (one), dos (two), tres (three),

203
00:06:58,060 --> 00:06:58,730
cuatro, (four), cinco (five), seis (six), siete (seven),

204
00:06:59,160 --> 00:07:00,900
ocho (eight), nueve (nine) y diez (ten).

205
00:07:01,860 --> 00:07:02,850
そこで私たちができるのは、

206
00:07:03,380 --> 00:07:04,660
この二つのマイクロフォンの録音を

207
00:07:04,980 --> 00:07:06,480
教師なし学習アルゴリズムに与えることです。

208
00:07:07,010 --> 00:07:08,560
これをカクテルパーティー・アルゴリズムと呼びます。

209
00:07:08,780 --> 00:07:09,910
そしてアルゴリズムに命令します

210
00:07:10,450 --> 00:07:12,140
このデータから構造を見つけなさいと。

211
00:07:12,250 --> 00:07:14,010
そしてアルゴリズムが行うのは、

212
00:07:14,410 --> 00:07:15,730
これらの

213
00:07:15,980 --> 00:07:17,990
音声録音を聞き、どうやら、

214
00:07:18,140 --> 00:07:19,020
この音声では、

215
00:07:19,360 --> 00:07:20,950
二つの音声録音が

216
00:07:21,240 --> 00:07:22,450
追加されているか、あるいは

217
00:07:22,670 --> 00:07:25,220
合計されてこうした録音が生み出されたと答えます。

218
00:07:25,990 --> 00:07:27,330
さらに、カクテルパーティー

219
00:07:27,710 --> 00:07:29,210
アルゴリズムが行うのは、

220
00:07:29,570 --> 00:07:30,810
二つの音源を

221
00:07:31,480 --> 00:07:32,700
追加される、あるいは

222
00:07:33,000 --> 00:07:34,240
合計されて最終的な

223
00:07:34,410 --> 00:07:35,600
録音となったものを分離します。実際に

224
00:07:36,200 --> 00:07:38,630
ここにカクテルパーティー・アルゴリズムの最初の出力結果を聞いてみましょう。

225
00:07:39,790 --> 00:07:41,910
One, two, three, four,

226
00:07:42,590 --> 00:07:46,270
five, six, seven, eight, nine, ten.

227
00:07:47,630 --> 00:07:48,780
このように英語の

228
00:07:49,240 --> 00:07:51,220
声が録音された音声の一つから分離されています。

229
00:07:52,460 --> 00:07:53,300
これが二番目の出力です。

230
00:07:53,380 --> 00:07:55,280
Uno, dos, tres, quatro, cinco,

231
00:07:55,980 --> 00:07:59,830
seis, siete, ocho, nueve y diez.

232
00:08:00,270 --> 00:08:01,180
なかなか悪くないですね。

233
00:08:03,810 --> 00:08:05,270
もう一つ例を出すと、ここに別の

234
00:08:05,600 --> 00:08:07,370
やはり似たような状況で録音されたものがあります。

235
00:08:08,060 --> 00:08:09,790
これが第一のマイクロフォンです。One,

236
00:08:10,470 --> 00:08:12,430
two, three, four, five, six,

237
00:08:13,370 --> 00:08:15,710
seven, eight, nine, ten.

238
00:08:16,980 --> 00:08:17,920
OK 可哀想にこの人は

239
00:08:18,180 --> 00:08:19,350
カクテルパーティーから帰ってきて

240
00:08:19,420 --> 00:08:21,880
自宅の部屋でラジオに向かって独り言を言っているようです。

241
00:08:23,090 --> 00:08:24,130
これが二番目のマイクロフォンの録音です。

242
00:08:28,810 --> 00:08:31,800
One, two, three, four, five, six, seven, eight, nine, ten.

243
00:08:33,310 --> 00:08:34,160
この二つのマイクロフォンの

244
00:08:34,610 --> 00:08:35,530
録音を同じアルゴリズムに与えると、

245
00:08:36,360 --> 00:08:37,790
それが行うのは、またこう答えます。

246
00:08:38,380 --> 00:08:39,470
この音声には、

247
00:08:39,690 --> 00:08:41,370
二つの音源があって、さらに、

248
00:08:42,410 --> 00:08:43,820
アルゴリズムの答えは、これが

249
00:08:44,070 --> 00:08:46,010
発見された最初の音源です。

250
00:08:47,480 --> 00:08:49,300
One, two, three, four,

251
00:08:49,730 --> 00:08:53,430
five, six, seven, eight, nine, ten.

252
00:08:54,650 --> 00:08:56,110
これは完璧ではありませんでした。

253
00:08:56,340 --> 00:08:57,360
声は復元されていますが、

254
00:08:57,570 --> 00:08:59,070
そこには少しだけ音楽も混じっています。

255
00:08:59,890 --> 00:09:01,360
そしてこれがアルゴリズムの二番目の出力です。

256
00:09:10,020 --> 00:09:11,310
悪くありません。二番目の

257
00:09:11,540 --> 00:09:13,300
出力では、声がなんとか完全に除去されています。

258
00:09:13,760 --> 00:09:14,850
そしてうまく

259
00:09:15,020 --> 00:09:17,380
音楽から 1 から 10 まで数える声を除去してきれいにしています。

260
00:09:18,840 --> 00:09:20,090
そこで、このような

261
00:09:20,180 --> 00:09:21,750
教師なし学習アルゴリズムを見て、

262
00:09:21,950 --> 00:09:23,050
思い浮かぶ疑問は、

263
00:09:23,250 --> 00:09:25,110
これを実装するのはどれだけ複雑か、ということですよね。

264
00:09:25,330 --> 00:09:26,560
一見して、

265
00:09:26,970 --> 00:09:28,870
このアプリケーションを構築するには、一見して

266
00:09:28,930 --> 00:09:30,550
この音声処理を行うには、

267
00:09:30,670 --> 00:09:31,430
たくさんのコードを書くか、

268
00:09:32,240 --> 00:09:33,580
あるいは、いくつかの

269
00:09:33,690 --> 00:09:35,380
音声処理用の C++ や Java ライブラリに

270
00:09:35,470 --> 00:09:37,150
リンクするか、一見して

271
00:09:37,240 --> 00:09:38,880
非常に複雑なプログラムのように見えます

272
00:09:39,060 --> 00:09:41,040
このように音声を処理し、音声を分離したりするのは。

273
00:09:42,460 --> 00:09:43,860
実は、このアルゴリズムが

274
00:09:44,070 --> 00:09:45,640
先ほどお聞き頂いたものを出力するのに

275
00:09:45,900 --> 00:09:47,280
実行するのは単に一行の

276
00:09:47,530 --> 00:09:49,270
コードだけです。ここにある通り。

277
00:09:50,640 --> 00:09:52,350
研究者が

278
00:09:52,610 --> 00:09:54,060
この一行のコードを開発するのには大変時間がかかりました。

279
00:09:54,490 --> 00:09:56,090
ですから、これが簡単な問題であるというつもりはありません。

280
00:09:57,080 --> 00:09:57,980
しかし、実は、

281
00:09:58,180 --> 00:10:00,330
適切なプログラミング環境を用いると、多くの学習

282
00:10:00,670 --> 00:10:02,060
アルゴリズムは非常に短いプログラムになります。

283
00:10:03,510 --> 00:10:04,700
ですから、これが

284
00:10:04,840 --> 00:10:05,890
このクラスでは、

285
00:10:06,010 --> 00:10:07,430
Octave プログラミング環境を利用する理由です。

286
00:10:08,550 --> 00:10:09,910
Octave は無料のオープンソース

287
00:10:10,120 --> 00:10:11,620
ソフトウェアです。そして

288
00:10:11,670 --> 00:10:13,130
Octave や MATLAB のようなツールを使うと

289
00:10:14,000 --> 00:10:15,400
多くの学習アルゴリズムが単に

290
00:10:15,690 --> 00:10:17,910
数行のコードとして実装できます。

291
00:10:18,380 --> 00:10:19,400
このクラスでは後で

292
00:10:19,620 --> 00:10:20,570
少しどのように

293
00:10:20,720 --> 00:10:21,920
Octave を使うか講義します。

294
00:10:22,050 --> 00:10:24,590
そしてこうしたアルゴリズムを Octave で実装していくことになります。

295
00:10:24,980 --> 00:10:26,050
もし MATLAB をお持ちであれば、それを使うことも出来ます。

296
00:10:27,120 --> 00:10:28,500
実は、シリコンバレーでは、

297
00:10:28,620 --> 00:10:29,470
多くの機械学習アルゴリズムにおいて

298
00:10:30,290 --> 00:10:31,310
まず最初に行うのは、ソフトウェアの試作を

299
00:10:32,040 --> 00:10:33,900
Octave で行います。Octave の

300
00:10:34,330 --> 00:10:35,250
ソフトウェアを使うと信じられないほど速く

301
00:10:35,540 --> 00:10:36,920
こうした学習アルゴリズムを実装できるからです。

302
00:10:38,230 --> 00:10:39,110
ここの、これらの関数のそれぞれ、

303
00:10:39,720 --> 00:10:41,460
例えば svd

304
00:10:41,680 --> 00:10:42,920
関数は、特異値分解 singular

305
00:10:43,240 --> 00:10:44,520
value decomposition の略です。

306
00:10:44,640 --> 00:10:45,690
これは実は

307
00:10:45,820 --> 00:10:48,420
Octave では単に内蔵された線型代数ルーチンです。

308
00:10:49,500 --> 00:10:50,390
これを

309
00:10:50,460 --> 00:10:51,490
C++ か Java で行おうとすると

310
00:10:51,780 --> 00:10:53,040
これは、多くの行数の

311
00:10:53,180 --> 00:10:55,680
複雑な C++ や Java のライブラリにリンクするコードとなります。

312
00:10:56,440 --> 00:10:57,490
つまり、こうしたものを

313
00:10:57,680 --> 00:10:58,690
C++ や Java

314
00:10:59,050 --> 00:11:00,090
あるいは Pythonで実装可能ですが、

315
00:11:00,290 --> 00:11:02,090
こうした言語ではもっと複雑なことになります。

316
00:11:03,750 --> 00:11:05,060
私がこれまで

317
00:11:05,300 --> 00:11:06,980
機械学習を教えて

318
00:11:07,210 --> 00:11:08,680
ほぼ十年間見てきたことは、

319
00:11:08,890 --> 00:11:10,340
プログラミング環境として

320
00:11:10,480 --> 00:11:11,700
Octave を使うと、

321
00:11:11,790 --> 00:11:14,070
非常に速く学ぶことができるということです。

322
00:11:14,250 --> 00:11:15,570
Octave を

323
00:11:16,260 --> 00:11:17,110
学習のため、そして

324
00:11:17,240 --> 00:11:18,640
試作のためのツールとして使うと

325
00:11:19,000 --> 00:11:21,280
学習アルゴリズムをもっと手早く学び、試作ができるということです。

326
00:11:22,640 --> 00:11:23,850
そして、事実、多くの人が

327
00:11:23,990 --> 00:11:25,390
大手のシリコン

328
00:11:25,730 --> 00:11:27,360
バレーの企業で行っているのは、

329
00:11:27,560 --> 00:11:29,020
Octave のアルゴリズムを使って最初に

330
00:11:29,370 --> 00:11:31,110
学習アルゴリズムを試作し、そして

331
00:11:31,510 --> 00:11:32,780
それが実際に機能するようにした後にのみ

332
00:11:32,860 --> 00:11:33,820
それを

333
00:11:34,390 --> 00:11:35,910
C++ あるいは Java などに移植します。

334
00:11:36,890 --> 00:11:37,960
結果的に、こうした

335
00:11:38,220 --> 00:11:39,070
やり方をすると、

336
00:11:39,400 --> 00:11:40,440
アルゴリズムが実際に機能するように

337
00:11:41,300 --> 00:11:43,050
するのに、C++ で始めるよりずっと速くなります。

338
00:11:44,440 --> 00:11:46,010
もちろん、私は承知しています。

339
00:11:46,100 --> 00:11:47,490
教師として

340
00:11:47,570 --> 00:11:48,580
「これについては私の言うことを

341
00:11:48,730 --> 00:11:49,790
信じなさい」と言えるのはほんのわずかの限定された

342
00:11:50,030 --> 00:11:51,420
回数のみであると。

343
00:11:51,560 --> 00:11:52,720
しかし、こうした

344
00:11:53,330 --> 00:11:54,880
Octave のようなプログラミング環境を使ったことがない方に

345
00:11:55,240 --> 00:11:56,070
お願いしたいことは、

346
00:11:56,130 --> 00:11:56,970
これについては私の言うことを信じてください、ということです。

347
00:11:57,570 --> 00:11:58,950
そして、言いたいのは、皆さんは、

348
00:11:59,700 --> 00:12:01,180
皆さんの時間、皆さんの開発

349
00:12:01,700 --> 00:12:03,100
時間は、もっとも貴重な資源の一つであると。

350
00:12:04,210 --> 00:12:05,570
そして多くの

351
00:12:05,800 --> 00:12:06,850
人がこれを実践するのを見てきて言えるのは、

352
00:12:07,190 --> 00:12:08,460
機械学習の

353
00:12:08,850 --> 00:12:09,990
研究者あるいは機械学習の開発者として

354
00:12:10,830 --> 00:12:12,080
あなたの生産性がずっと高くなるのは、

355
00:12:12,220 --> 00:12:13,010
試作から始めることを習得し、

356
00:12:13,580 --> 00:12:15,250
他の言語ではなく Octave から始める場合ですと。

357
00:12:17,570 --> 00:12:19,790
最後に、この

358
00:12:20,090 --> 00:12:22,890
ビデオのまとめとして、一つだけ復習のための問題があります。

359
00:12:24,400 --> 00:12:26,400
教師なし学習について学んできました。これは

360
00:12:26,700 --> 00:12:27,670
学習の設定として、

361
00:12:27,760 --> 00:12:28,730
アルゴリズムに非常に多くの

362
00:12:28,840 --> 00:12:30,120
データを与え、そして単に

363
00:12:30,240 --> 00:12:32,900
データから構造を見つけなさいと求めるというものです。

364
00:12:33,160 --> 00:12:35,170
以下の四つの例のうち、

365
00:12:35,490 --> 00:12:36,410
この四つのどれが

366
00:12:36,870 --> 00:12:37,630
あなたの考えでは

367
00:12:37,720 --> 00:12:39,520
教師なし学習アルゴリズムで、

368
00:12:40,220 --> 00:12:41,950
教師ありアルゴリズムではないと思いますか。

369
00:12:42,730 --> 00:12:43,590
左の四つの

370
00:12:43,860 --> 00:12:44,850
チェックボックスそれぞれ

371
00:12:45,640 --> 00:12:46,900
チェックして頂きたいのは、

372
00:12:47,210 --> 00:12:49,400
教師なし学習

373
00:12:49,700 --> 00:12:51,300
アルゴリズムが適切だと思われるものです。

374
00:12:51,440 --> 00:12:53,930
そして、右下にあるボタンをクリックして正解を見てください。

375
00:12:54,690 --> 00:12:57,030
ビデオが停止したら、どうぞ

376
00:12:57,370 --> 00:12:58,750
スライドにある質問に答えてください。

377
00:13:01,860 --> 00:13:03,950
さて、スパムフォルダの問題で覚えていて頂きたいのは、

378
00:13:04,710 --> 00:13:06,310
ラベルがついたデータがある場合は、

379
00:13:06,450 --> 00:13:07,680
電子メールがスパムか

380
00:13:07,800 --> 00:13:10,470
非スパムかなど、これは教師あり学習として取り扱います。

381
00:13:11,620 --> 00:13:13,870
報道記事の例は、これは

382
00:13:14,100 --> 00:13:15,370
正にこのビデオで見た Google News の例と

383
00:13:15,910 --> 00:13:16,600
同じですので、

384
00:13:17,090 --> 00:13:17,950
既に見たとおり、

385
00:13:18,080 --> 00:13:19,460
クラスタリング・アルゴリズムを使って

386
00:13:19,880 --> 00:13:21,980
こうした記事をクラスタリングしてまとめることが出来ますので、これは教師なし学習です。

387
00:13:23,250 --> 00:13:25,440
マーケットセグメンテーションの例は

388
00:13:25,510 --> 00:13:27,120
少し前にお話したもので、

389
00:13:27,220 --> 00:13:29,110
これも教師なし学習問題として扱うことが出来ます。

390
00:13:29,970 --> 00:13:30,860
なぜなら、単に

391
00:13:30,930 --> 00:13:32,340
アルゴリズムにはデータを与えるだけで、

392
00:13:32,500 --> 00:13:34,340
後はそれに自動的にマーケットセグメントを発見させるからです。

393
00:13:35,610 --> 00:13:37,930
最後の糖尿病の例は、

394
00:13:38,070 --> 00:13:39,080
これは実は、

395
00:13:39,350 --> 00:13:41,480
前のビデオの乳癌の例にちょうど似ています。

396
00:13:42,190 --> 00:13:43,320
単に、

397
00:13:43,600 --> 00:13:45,280
良性と悪性の癌腫瘍、

398
00:13:45,550 --> 00:13:47,390
良性と悪性の腫瘍ではなく

399
00:13:47,550 --> 00:13:49,270
その代わりに、糖尿病か

400
00:13:49,330 --> 00:13:50,440
そうでないかということですので、

401
00:13:50,700 --> 00:13:51,830
これは教師あり

402
00:13:52,370 --> 00:13:53,740
これを解決するのには

403
00:13:53,870 --> 00:13:54,670
教師あり学習問題として扱います。

404
00:13:54,730 --> 00:13:56,450
ちょうど乳房の腫瘍のデータと同じように。

405
00:13:58,270 --> 00:13:59,400
さて、ここまでが教師なし

406
00:14:00,100 --> 00:14:01,580
学習です。次の

407
00:14:01,650 --> 00:14:02,940
ビデオでは、もっと

408
00:14:03,270 --> 00:14:04,600
特定の学習アルゴリズムを掘り下げていき、

409
00:14:05,550 --> 00:14:06,590
実際にこうした

410
00:14:07,220 --> 00:14:08,750
アルゴリズムがどのように動くのか

411
00:14:08,920 --> 00:14:11,270
そしてそれをどのように実装するのかについてお話していきます。