1
00:00:00,240 --> 00:00:01,690
前回までのビデオで

2
00:00:01,820 --> 00:00:02,990
まずは映画のフィーチャーが与えられた時

3
00:00:03,140 --> 00:00:04,570
それを用いて

4
00:00:04,780 --> 00:00:06,210
ユーザーのパラメータのデータを

5
00:00:06,920 --> 00:00:08,610
学習する事が出来る、という話をした。

6
00:00:09,490 --> 00:00:11,400
次に、ユーザーのパラメータを与えられたら、

7
00:00:11,920 --> 00:00:13,570
それを使って映画のフィーチャーを学習する事が出来る、という話をした。

8
00:00:14,480 --> 00:00:15,550
このビデオでは、これら二つのアイデアを

9
00:00:15,650 --> 00:00:16,670
用いて、それらを合わせて

10
00:00:16,850 --> 00:00:18,130
協調フィルタのアルゴリズムに

11
00:00:18,280 --> 00:00:20,130
たどり着く。

12
00:00:21,250 --> 00:00:22,450
つまり以前やった事の

13
00:00:22,520 --> 00:00:23,640
一つには

14
00:00:23,680 --> 00:00:24,510
映画のフィーチャーがあれば、

15
00:00:24,600 --> 00:00:25,740
最小化問題を解く事で

16
00:00:26,070 --> 00:00:27,590
ユーザーのパラメータである

17
00:00:27,950 --> 00:00:30,010
シータを見つける事が出来た。

18
00:00:30,730 --> 00:00:32,260
そしてその次に、

19
00:00:32,640 --> 00:00:33,960
パラメータであるシータがあれば、

20
00:00:34,360 --> 00:00:37,440
それを用いて

21
00:00:38,080 --> 00:00:38,990
フィーチャーxを推計する事が

22
00:00:39,170 --> 00:00:40,800
出来るのだった。

23
00:00:40,870 --> 00:00:42,980
それは最小化問題を解く事で出来るのだった。

24
00:00:44,310 --> 00:00:45,720
だから取りうる手段として一つ考えられるのは

25
00:00:45,880 --> 00:00:47,360
行ったり来たりして実行する事だ。

26
00:00:47,870 --> 00:00:50,230
ランダムに初期化されたパラメータで、

27
00:00:50,510 --> 00:00:51,350
シータについて解き、

28
00:00:51,780 --> 00:00:52,690
xについて解き、シータについて解き、

29
00:00:52,870 --> 00:00:54,330
xについて解く。

30
00:00:54,420 --> 00:00:55,220
だが、シータとxを

31
00:00:55,400 --> 00:00:56,760
行ったり来たりしなくても良い

32
00:00:56,980 --> 00:00:57,910
もっと効率的なアルゴリズムがある事が

33
00:00:58,110 --> 00:00:59,700
知られている、それは行ったり来たりする代わりに

34
00:00:59,730 --> 00:01:00,670
シータとxを同時に

35
00:01:01,300 --> 00:01:04,250
解く事が出来る。

36
00:01:05,160 --> 00:01:06,310
それはこんなだ。
我らがやる事は、基本的には

37
00:01:06,600 --> 00:01:08,990
これら二つの最適化の目的関数をとり、

38
00:01:09,130 --> 00:01:10,640
それを一つの目的関数に突っ込む、という事だ。

39
00:01:11,550 --> 00:01:12,590
つまり私は、新しい最適化の目的関数Jを

40
00:01:12,730 --> 00:01:15,010
定義する、

41
00:01:15,250 --> 00:01:16,540
それはフィーチャーxと

42
00:01:16,640 --> 00:01:17,630
パラメータのシータに関する

43
00:01:18,050 --> 00:01:19,150
関数としての、

44
00:01:19,790 --> 00:01:20,750
コスト関数だ。

45
00:01:21,660 --> 00:01:23,050
そして基本的には上の所に

46
00:01:23,520 --> 00:01:24,920
二つの最適化の目的関数があったが、それを一つにくっつけた。

47
00:01:26,270 --> 00:01:27,760
これを説明する為に、

48
00:01:28,060 --> 00:01:31,140
まず以下の二つの項が同じという事を指摘したい：

49
00:01:31,400 --> 00:01:33,420
この項、この二乗誤差の項と、

50
00:01:33,820 --> 00:01:35,490
この二乗誤差の項、

51
00:01:35,920 --> 00:01:39,250
これ。

52
00:01:39,760 --> 00:01:40,880
和の取り方がちょっと違って見えるが、

53
00:01:41,050 --> 00:01:42,940
この和が実際に何をやっているかを見てみよう。

54
00:01:43,800 --> 00:01:45,090
最初の和は、

55
00:01:45,480 --> 00:01:48,280
全てのユーザーjに渡ってとっている、

56
00:01:48,380 --> 00:01:50,590
そして次にそのユーザーによってレーティングされた全ての映画について和を取る。

57
00:01:51,890 --> 00:01:53,240
つまり、これは実際は、ユーザーにレーティングされた

58
00:01:53,470 --> 00:01:55,950
全ての映画に対応した

59
00:01:56,510 --> 00:01:57,830
iとjのペアに渡って和を取っている。

60
00:01:58,550 --> 00:01:59,960
jに渡って取る和は、各ユーザに対して、と言っていて

61
00:02:00,150 --> 00:02:01,520
そしてそのユーザーがレーティングした

62
00:02:01,740 --> 00:02:03,110
全ての映画に渡って和を取る。

63
00:02:04,250 --> 00:02:07,340
この下の和は、それを反対の順番でやるだけだ。

64
00:02:07,630 --> 00:02:08,710
これが言っているのは、各映画iに対して

65
00:02:09,050 --> 00:02:11,140
その映画をレーティングした全てのユーザーj

66
00:02:11,340 --> 00:02:12,480
に渡って

67
00:02:12,690 --> 00:02:14,580
和をとる、つまり、

68
00:02:14,690 --> 00:02:16,100
これらの和は、これらは両方とも、

69
00:02:16,220 --> 00:02:18,150
r i jがイコール1な

70
00:02:18,930 --> 00:02:21,150
全てのi jのペアに渡って

71
00:02:21,440 --> 00:02:24,620
和を取るだけ。

72
00:02:24,660 --> 00:02:26,580
それはようするに、

73
00:02:27,180 --> 00:02:29,810
レーティングを持っている全てのユーザーと映画の組に渡って和を取る。

74
00:02:30,840 --> 00:02:32,230
つまりこれら二つの項は

75
00:02:32,600 --> 00:02:34,740
この最初の項と

76
00:02:34,930 --> 00:02:36,460
完全に一致している。

77
00:02:36,500 --> 00:02:38,310
そして私は和を明示的に書いた、

78
00:02:39,310 --> 00:02:40,290
それはようするに、r i j がイコール1となる

79
00:02:40,580 --> 00:02:42,290
全てのi jのペアに渡って

80
00:02:42,540 --> 00:02:45,060
和を取る、と言っている。

81
00:02:45,310 --> 00:02:46,800
我らがやりたい事は

82
00:02:46,940 --> 00:02:48,790
xとシータを同時に解く為に

83
00:02:49,130 --> 00:02:51,410
最小化をする対象となる、

84
00:02:51,670 --> 00:02:53,290
最適化の複合目的関数を

85
00:02:53,550 --> 00:02:55,700
定義するという事だ。

86
00:02:56,970 --> 00:02:58,040
そして最適化の目的関数に残ってる

87
00:02:58,070 --> 00:03:00,250
他の項はこれだ。

88
00:03:00,570 --> 00:03:02,870
これはシータの正規化項だ。

89
00:03:03,770 --> 00:03:05,830
それはここに来る。

90
00:03:06,290 --> 00:03:08,190
そして最後のピースとなるのは

91
00:03:08,900 --> 00:03:10,690
この項で、これは

92
00:03:10,850 --> 00:03:12,970
xの為の最適化の目的関数の中に残ってる物で

93
00:03:13,170 --> 00:03:16,180
それはここに来る。

94
00:03:16,500 --> 00:03:18,020
そしてこの最適化の目的関数Jは

95
00:03:18,720 --> 00:03:19,730
興味深い性質を持っている、それは

96
00:03:20,240 --> 00:03:20,950
xを定数に

97
00:03:21,410 --> 00:03:23,070
固定してみて、

98
00:03:23,260 --> 00:03:25,490
シータに関して最小化してみると、

99
00:03:25,670 --> 00:03:27,040
するとまさにこの問題を解いている事になる。

100
00:03:27,840 --> 00:03:28,450
一方、その反対をやると、

101
00:03:28,620 --> 00:03:29,590
シータを仮に定数で

102
00:03:29,690 --> 00:03:31,310
固定してみて、Jを

103
00:03:31,670 --> 00:03:32,650
xに関してだけ

104
00:03:32,750 --> 00:03:34,920
最小化してみると、

105
00:03:35,230 --> 00:03:36,780
この項か、またはこの項が

106
00:03:37,060 --> 00:03:38,860
xに関してかシータに関してのどちらかに関してだけ最小化するなら、

107
00:03:38,970 --> 00:03:40,510
定数になるから。

108
00:03:40,920 --> 00:03:43,680
だから、これが、xに関するコスト関数と

109
00:03:44,640 --> 00:03:46,840
シータに関するコスト関数を

110
00:03:47,440 --> 00:03:50,230
くっつけた最適化の目的関数だ。

111
00:03:51,620 --> 00:03:53,050
そして一つの最適化問題に

112
00:03:53,470 --> 00:03:54,750
帰着される為には、

113
00:03:55,090 --> 00:03:56,130
我らがやるべき事は、

114
00:03:56,280 --> 00:03:57,590
このコスト関数を

115
00:03:58,430 --> 00:03:59,850
フィーチャーxとユーザーのパラメータシータの

116
00:03:59,880 --> 00:04:00,890
両方に関する

117
00:04:01,410 --> 00:04:02,540
関数として

118
00:04:03,180 --> 00:04:05,020
扱う事だ。

119
00:04:05,140 --> 00:04:06,570
そして単純に全体を

120
00:04:06,740 --> 00:04:07,830
xとシータの関数として、

121
00:04:08,120 --> 00:04:10,210
最小化すれば良い。

122
00:04:11,300 --> 00:04:12,400
そして実際、これと

123
00:04:12,540 --> 00:04:13,800
以前のアルゴリズムの

124
00:04:14,160 --> 00:04:15,650
唯一の違いは、

125
00:04:15,980 --> 00:04:17,340
行ったり来たりする代わりに、

126
00:04:17,840 --> 00:04:20,110
以前のはシータに関して最小化した後

127
00:04:20,420 --> 00:04:22,130
次にxについて最小化して、

128
00:04:22,260 --> 00:04:23,370
その後シータについて最小化して、

129
00:04:23,900 --> 00:04:25,270
xについて最小化して、、、などとやったのだったが、

130
00:04:26,130 --> 00:04:28,090
この新しいバージョンでは、

131
00:04:28,560 --> 00:04:30,020
xとシータという二つのパラメータのセットの間を

132
00:04:30,220 --> 00:04:31,880
順番に行ったり来たりする代わりに

133
00:04:32,180 --> 00:04:32,940
たんに両方の

134
00:04:33,230 --> 00:04:34,600
パラメータに関して

135
00:04:34,780 --> 00:04:36,410
同時に最小化する、という事をする。

136
00:04:39,750 --> 00:04:41,290
最後に一つ詳細な話だが、

137
00:04:42,030 --> 00:04:44,380
フィーチャーをこういう風に学ぶ場合、

138
00:04:45,110 --> 00:04:46,410
以前はx0がイコール1となる

139
00:04:46,840 --> 00:04:49,290
コンベンションを使っていて、

140
00:04:49,470 --> 00:04:50,540
これは切片項に

141
00:04:50,740 --> 00:04:52,940
対応した物だった。

142
00:04:54,140 --> 00:04:55,530
この種の定式化を用いる時には、

143
00:04:55,760 --> 00:04:57,790
実際にはフィーチャーも学習する事になるので、

144
00:04:58,300 --> 00:05:00,200
このコンベンション無しでいける。

145
00:05:01,400 --> 00:05:04,220
だから我らの学習する事になるフィーチャーxはR nだ。

146
00:05:05,430 --> 00:05:06,650
一方、以前はフィーチャーxは

147
00:05:06,810 --> 00:05:09,770
R n+1だった、切片項を含んでいたから。

148
00:05:10,390 --> 00:05:13,390
そこからx0を取り除いたから、xはR nとなる。

149
00:05:14,880 --> 00:05:16,520
そして同様に、

150
00:05:16,590 --> 00:05:17,780
パラメータのシータは

151
00:05:17,850 --> 00:05:19,260
同じ次元なので、

152
00:05:19,510 --> 00:05:21,010
シータも R nとなる。

153
00:05:21,540 --> 00:05:23,340
何故ならx0が無いなら、

154
00:05:23,710 --> 00:05:24,580
シータ0も同様に

155
00:05:25,370 --> 00:05:26,880
不要だからだ。

156
00:05:27,960 --> 00:05:28,880
そしてこのコンベンション無しで

157
00:05:29,160 --> 00:05:30,390
いける理由としては、

158
00:05:31,010 --> 00:05:32,610
いまや我らはフィーチャーを全て学習する事になった、よね？

159
00:05:32,820 --> 00:05:34,280
だからいつも1と等しくなるフィーチャーを

160
00:05:34,420 --> 00:05:36,650
ハードコードする必要が無い。

161
00:05:37,170 --> 00:05:38,310
何故ならもしアルゴリズムが

162
00:05:38,600 --> 00:05:39,450
いつも1となるフィーチャーを本当に必要としているなら、

163
00:05:40,060 --> 00:05:41,830
それは自身で勝手にそう学習するはずだからだ。

164
00:05:42,290 --> 00:05:43,430
だからもしアルゴリズムがそう選べば

165
00:05:43,720 --> 00:05:45,330
x1=1とセットされる。

166
00:05:45,670 --> 00:05:47,010
だからフィーチャーx0を1と

167
00:05:47,260 --> 00:05:48,300
ハードコードする必要は無い。

168
00:05:48,440 --> 00:05:50,060
アルゴリズムはそれを自分自身で学習する事が出来るだけの

169
00:05:50,340 --> 00:05:55,890
柔軟性がある。

170
00:05:56,420 --> 00:05:58,410
では全部を合わせると、

171
00:05:58,780 --> 00:05:59,910
これが協調フィルタリングのアルゴリズムだ。

172
00:06:01,460 --> 00:06:02,330
まず、xとシータを

173
00:06:03,010 --> 00:06:05,580
ある小さなランダムの値で

174
00:06:05,820 --> 00:06:07,290
初期化する。

175
00:06:08,450 --> 00:06:09,200
これはちょっと

176
00:06:09,310 --> 00:06:11,700
ニューラルネットワークのトレーニングに似てるね。

177
00:06:11,720 --> 00:06:14,240
そこでもニューラルネットワークのパラメータを全て小さなランダムの数で初期化したんだった。

178
00:06:16,640 --> 00:06:17,730
次に、コスト関数を、

179
00:06:17,950 --> 00:06:20,110
最急降下法なりアドバンスドな最適化のアルゴリズムなりを使って

180
00:06:20,500 --> 00:06:23,360
最小化する。

181
00:06:24,610 --> 00:06:25,890
だから微分をとれば、

182
00:06:26,020 --> 00:06:27,460
こんな感じの

183
00:06:27,590 --> 00:06:29,320
最急降下法の更新ルールが得られる。

184
00:06:29,630 --> 00:06:31,160
この項は、コスト関数を

185
00:06:31,660 --> 00:06:33,890
偏微分した物。

186
00:06:35,140 --> 00:06:35,940
全部書いたりはしないが、

187
00:06:36,110 --> 00:06:37,860
それをフィーチャーx(i) kで

188
00:06:38,070 --> 00:06:40,020
偏微分した物だ。

189
00:06:41,020 --> 00:06:42,430
同様に、この項もまた、

190
00:06:43,030 --> 00:06:44,660
コスト関数を偏微分した物で

191
00:06:44,730 --> 00:06:46,480
今度はパラメータであるシータに関して

192
00:06:46,930 --> 00:06:48,950
偏微分した物で、それに関して最小化する。

193
00:06:50,210 --> 00:06:51,410
ちょっと注意を。

194
00:06:51,760 --> 00:06:52,920
この式では、もはや

195
00:06:53,130 --> 00:06:54,760
x0イコール1が無い、

196
00:06:54,970 --> 00:06:56,740
だからxはR nとなり、

197
00:06:57,010 --> 00:07:00,010
シータもR nとなる。

198
00:07:01,480 --> 00:07:03,100
この新しい定式化では、各パラメータ、シータも、

199
00:07:03,760 --> 00:07:05,220
各パラメータx nも、正規化している。

200
00:07:07,400 --> 00:07:09,060
もはやシータ0の特別扱いのケースも

201
00:07:09,480 --> 00:07:11,850
存在しない。シータ0は異なる風に

202
00:07:12,210 --> 00:07:13,760
正規化していたんだった。

203
00:07:13,860 --> 00:07:15,440
それはシータ1からシータnのようには

204
00:07:15,560 --> 00:07:17,650
正規化しないんだった。

205
00:07:18,370 --> 00:07:19,710
もはやシータ0は存在しないので、

206
00:07:20,070 --> 00:07:21,150
そんな訳だからこのアップデートでも

207
00:07:21,400 --> 00:07:22,450
k=0の特別なケースの

208
00:07:22,700 --> 00:07:24,080
場合分けをしていない。

209
00:07:26,070 --> 00:07:27,230
そして最急降下法を使って

210
00:07:27,740 --> 00:07:28,710
コスト関数Jを

211
00:07:29,090 --> 00:07:30,260
フィーチャーxとパラメータシータに関して

212
00:07:30,390 --> 00:07:32,000
最小化していく。

213
00:07:33,160 --> 00:07:35,050
そして最後に、あるユーザーに対して

214
00:07:35,140 --> 00:07:36,320
ユーザーがあるパラメータ、シータを

215
00:07:36,570 --> 00:07:38,920
持っているなら、

216
00:07:39,410 --> 00:07:40,540
そして映画にはある種の

217
00:07:40,690 --> 00:07:41,980
学習されたフィーチャーxがあるなら、

218
00:07:42,580 --> 00:07:43,720
そのユーザーが映画に

219
00:07:43,970 --> 00:07:44,940
星いくつのレーティングをするかを

220
00:07:45,030 --> 00:07:46,200
シータjの転置と、、、

221
00:07:47,010 --> 00:07:48,780
または、前の表記と

222
00:07:48,860 --> 00:07:50,370
合わせると、

223
00:07:50,640 --> 00:07:52,250
もしユーザーjがまだ

224
00:07:52,630 --> 00:07:53,780
映画iをレーティング

225
00:07:54,010 --> 00:07:55,980
していなければ、

226
00:07:56,170 --> 00:07:57,300
ユーザーjは映画iを

227
00:07:58,150 --> 00:07:59,120
シータjの転置 xi と

228
00:07:59,710 --> 00:08:01,420
レーティングすると

229
00:08:02,300 --> 00:08:04,230
予測する。

230
00:08:06,650 --> 00:08:08,010
以上が協調フィルタリングアルゴリズムだ。

231
00:08:08,810 --> 00:08:10,170
もしこのアルゴリズムを

232
00:08:10,310 --> 00:08:12,230
実装すれば、おそらく全ての映画の

233
00:08:12,730 --> 00:08:14,080
良いフィーチャーと

234
00:08:15,060 --> 00:08:16,770
全てのユーザーのパラメータを同時に

235
00:08:17,110 --> 00:08:18,460
学習する、かなり良いアルゴリズムを

236
00:08:18,570 --> 00:08:19,890
得る事が出来る。

237
00:08:20,050 --> 00:08:21,290
そして別々のユーザーが別々の映画を

238
00:08:21,440 --> 00:08:23,060
どうレーティングするかの

239
00:08:23,290 --> 00:08:25,890
かなり良い予測を与える事も期待出来る。