En los últimos dos vídeos, hablamos sobre las ideas de cómo, en primer lugar, si se les han proporcionado variables para sus películas, pueden usar eso para saber los parámetros teta para los usuarios. Y en segundo lugar, si se les proporcionaron los parámetros para los usuarios, pueden usar eso para saber las variables para las películas. En este vídeo, vamos a tomar esas ideas y las vamos a unir para presentar un algoritmo de filtrado colaborativo. Así es que una de las cosas con las que trabajamos anteriormente, es que si tienen las variables para las películas, entonces pueden resolver este problema de minimización para encontrar la parámetros teta para sus usuarios. Y luego también resolvimos que si se les dan los parámetros teta, también pueden usar eso para estimar las variables x, y pueden hacer eso solucionando este problema de minimización. Así que algo que podrían hacer es en realidad ir y venir. Quizás inicializar aleatoriamente los parámetros y luego despejar para teta, despejar para x, despejar para teta, despejar para x. Pero resulta que existe un algoritmo más eficiente que no necesita ir y venir entre las x’s y las teta, pero que puede resolver para teta y x de manera simultánea. Y aquí está. Lo que haremos es básicamente tomar ambos objetivos de optimización y colocarlos en el mismo objetivo. Así es que voy a definir el nuevo objetivo de optimización j, el cual es una función de costos, que es una función de mis variables x y una función de mi parámetros teta. Y son básicamente los dos objetivos de optimización que tenía en la parte superior, pero los puse juntos. Entonces, con el fin de explicar esto, primero quiero señalar que este término aquí, este término de error al cuadrado es el mismo que este término de error al cuadrado, y las sumatorias se ven un poco diferente, pero vamos a ver lo que las sumas están haciendo en realidad. La primera suma es la suma sobre todos los usuarios j y después, la suma sobre todas las películas calificadas por ese usuario. Por lo tanto, esto está realmente sumando sobre todos los pares ij, que corresponden a una película que fue calificada por un usuario. La suma sobre j dice, para cada usuario, la suma de todas las películas calificadas por ese usuario. Esta suma aquí abajo, simplemente hace las cosas en el orden inverso. Esto dice que para cada película i, suma sobre todos los los usuarios j que han calificado esa película, así que estas sumas, ambas son sólo sumas sobre todos los pares ij para las que r de i,j es igual a 1. Es solamente sumar sobre todas los pares de películas del usuario para las que tienen una calificación, de modo que estos dos términos allí son sólo exactamente este primer término, y acabo de escribir la suma aquí de forma explícita, en donde sólo digo que la suma de todos los pares ij, de tal forma que r(i,j) es igual a 1. Así que lo que vamos hacer es definir un objetivo de optimización combinado que queremos minimizar a fin de resolver de forma simultánea para x y teta. Y luego, los otros términos en el objetivo de optimización son estos, que es una regularización en función de teta, así que esto va aquí abajo, y la pieza final es este término, que es mi objetivo de optimización para las x, y que se convirtió en esto. Y este objetivo de optimización. j en realidad tiene una propiedad interesante, que si mantuvieran la constante de x y simplemente minimizaran con respecto a teta, entonces estarían resolviendo exactamente este problema, mientras que si hicieran lo contrario, si fueran a mantener la constante de teta y minimizar j sólo con respecto a las x, entonces se convertiría en equivalente a ésta, ya sea porque este término o este término es una constante si están minimizando sólo respecto a las x , o sólo respecto a las tetas. Así que aquí está un objetivo de optimización objetivo que reúne mis funciones de costo en términos de x, y en términos de teta. Y con el fin de llegar a un solo problema de optimización, lo que vamos a hacer, es tratar esta función de costos como una función de mis variables x y de los parámetros de mi usuario teta y simplemente minimizar todo esto como una función tanto de las x’s, como de los tetas. Y realmente la única diferencia entre éste y el algoritmo anterior es que, en lugar de ir y venir, ya saben, anteriormente hablamos de minimizar con respecto a teta y después minimizar con respecto a x, mientras minimizamos con respecto a teta, minimizamos con respecto a x, y así sucesivamente. En esta nueva versión, en lugar de ir de manera secuencial entre los 2 grupos de parámetros x y teta, lo que vamos a hacer es minimizar con respecto a ambos conjuntos de parámetros de manera simultánea. Finalmente, un último detalle, es que cuando estamos aprendiendo las variables de esta forma, anteriormente habíamos usado esta convención de que tenemos una variable x0 igual a x0 uno, que corresponde a un término de interceptor. Cuando usamos esta especie de formulación en el que estamos realmente conociendo las variables, realmente vamos a eliminar esta convención. Y así las variables que vamos a aprender, x, estarán en Rn, mientras que anteriormente teníamos las variables x y Rn + 1, incluyendo el término de intercepción al deshacernos de x0, ahora sólo tenemos x en Rn. Y de manera similar, debido a que los parámetros teta están en la misma dimensión, ahora también tenemos a teta en Rn porque, si no hay x0, entonces no hay necesidad del parámetro teta 0 tampoco. Y la razón por la que eliminamos esta convención es porque ahora estamos conociendo todas las variables, ¿verdad? De modo que no hay necesidad para codificar la variable que  siempre es igual a uno. Porque si el algoritmo realmente desea una variable que sea siempre igual a 1, puede optar por aprender una por sí mismo. Así que si el algoritmo lo elige, puede establecer la variable x1 es igual a 1. de modo que no es necesario codificar la variable de 001, el algoritmo ahora tiene la flexibilidad para simplemente aprender por sí mismo. Así, uniendo todo, aquí está nuestro algoritmo de filtrado colaborativo. Primero, vamos a inicializar x y teta a pequeños valores aleatorios. Y esto es un poco como el entrenamiento de la red neuronal, en donde también inicializamos todos los parámetros de una red neuronal a pequeños valores aleatorios. Después, vamos a minimizar la función de costos usando descensos de gradiente o uno de los algoritmos de optimización avanzados. Por lo tanto, si toman las derivadas, encontrarán las actualizaciones de gradiente en descenso como estas y por tanto este término aquí es la derivada parcial de la función de costos - no voy a escribir eso - con respecto al valor de variable x(i)k y, de manera similar, este término aquí también es un valor de la derivada parcial de la función de costos con respecto al parámetro teta que estamos minimizando. Y sólo como recordatorio, en esta fórmula ya no tenemos esta x0 que es igual a 1 y tenemos que x está en Rn y teta es una Rn. En esta nueva formulación, estamos regularizando cada uno de nuestros parámetros teta, ya saben, cada uno de nuestros parámetros x. Ya no existe el caso especial teta cero, que se regularizó de manera diferente, o que no se regularizó en comparación con la parámetros teta 1 hasta teta n. Así que ahora ya no hay una teta 0, razón por la que, en estas actualizaciones, no desglosé un paréntesis especial para k es igual a 0. De modo que entonces usamos el gradiente de descenso minimizar la función de costos j con respecto a las variables x y con respecto a los parámetros teta. Finalmente, dado un usuario, si un usuario tiene algunos parámetros, teta, y si hay una película con algún tipo de variables aprendidas x, entonces podríamos predecir que a esa película se le daría una calificación de estrella por parte de ese usuario de transposición teta j. O sólo para llenarlos, entonces diríamos que si el usuario j aún no ha calificado la película i, entonces lo que hacemos es predecir que el usuario j va a calificar la película i de acuerdo con teta j transpone xi. Así que ese es el algoritmo de filtrado colaborativo. Si implementan este algoritmo, obtienen en realidad un algoritmo muy decente que aprenderá de manera simultánea buenas variables para, con suerte, todas las películas, así como conocer los parámetros para todos los usuarios y dar muy buenas predicciones sobre cómo los diferentes usuarios calificarán diferentes películas que aún no hayan calificado