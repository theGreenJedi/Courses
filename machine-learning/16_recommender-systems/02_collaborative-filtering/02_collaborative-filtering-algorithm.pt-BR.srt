1
00:00:00,240 --> 00:00:01,690
Nos últimos vídeos,

2
00:00:01,820 --> 00:00:02,990
falamos sobre como,

3
00:00:03,140 --> 00:00:04,570
se você receber características

4
00:00:04,780 --> 00:00:06,210
determinadas para filmes,

5
00:00:06,920 --> 00:00:08,610
pode usá-las para aprender parâmetros para usuários.

6
00:00:09,490 --> 00:00:11,400
Também, se você receber parâmetros para os usuários,

7
00:00:11,920 --> 00:00:13,570
você pode usá-los para aprender características para os filmes.

8
00:00:14,480 --> 00:00:15,550
Neste vídeo, vamos

9
00:00:15,650 --> 00:00:16,670
pegar essas ideias e

10
00:00:16,850 --> 00:00:18,130
juntá-las para chegar

11
00:00:18,280 --> 00:00:20,130
a um algoritmo de filtragem colaborativa.

12
00:00:21,250 --> 00:00:22,450
Uma das coisas que descobrimos

13
00:00:22,520 --> 00:00:23,640
antes é que, se você

14
00:00:23,680 --> 00:00:24,510
tiver características

15
00:00:24,600 --> 00:00:25,740
para os filmes, pode resolver

16
00:00:26,070 --> 00:00:27,590
este problema de minimização para

17
00:00:27,950 --> 00:00:30,010
encontrar os parâmetros "θ" dos usuários.

18
00:00:30,730 --> 00:00:32,260
Também descobrimos que,

19
00:00:32,640 --> 00:00:33,960
se você receber os

20
00:00:34,360 --> 00:00:37,440
parâmetros "θ", você

21
00:00:38,080 --> 00:00:38,990
pode usá-los

22
00:00:39,170 --> 00:00:40,800
para estimar as características "x",

23
00:00:40,870 --> 00:00:42,980
resolvendo este problema de minimização.

24
00:00:44,310 --> 00:00:45,720
Uma coisa que você

25
00:00:45,880 --> 00:00:47,360
pode fazer é ir e voltar.

26
00:00:47,870 --> 00:00:50,230
Talvez inicializando aleatoriamente e

27
00:00:50,510 --> 00:00:51,350
solucionar em "θ",

28
00:00:51,780 --> 00:00:52,690
depois em "x",

29
00:00:52,870 --> 00:00:54,330
depois "θ", depois "x".

30
00:00:54,420 --> 00:00:55,220
Mas existe

31
00:00:55,400 --> 00:00:56,760
um algoritmo mais eficiente que

32
00:00:56,980 --> 00:00:57,910
não precisa ir e voltar

33
00:00:58,110 --> 00:00:59,700
entre os "x" e os "θ",

34
00:00:59,730 --> 00:01:00,670
mas consegue

35
00:01:01,300 --> 00:01:04,250
encontrar soluções para "θ" e "x" simultaneamente.

36
00:01:05,160 --> 00:01:06,310
Aqui está ele. Basicamente,

37
00:01:06,600 --> 00:01:08,990
pegamos ambos os objetivos de minimização

38
00:01:09,130 --> 00:01:10,640
e juntamos no mesmo objetivo.

39
00:01:11,550 --> 00:01:12,590
Definirei o novo

40
00:01:12,730 --> 00:01:15,010
objetivo de minimização "J",

41
00:01:15,250 --> 00:01:16,540
uma função custo, que é

42
00:01:16,640 --> 00:01:17,630
uma função das

43
00:01:18,050 --> 00:01:19,150
características "x" e

44
00:01:19,790 --> 00:01:20,750
dos parâmetros "θ".

45
00:01:21,660 --> 00:01:23,050
Assim, é basicamente a

46
00:01:23,520 --> 00:01:24,920
junção dos dois objetivos.

47
00:01:26,270 --> 00:01:27,760
Para explicar isso,

48
00:01:28,060 --> 00:01:31,140
primeiro preciso mostrar que

49
00:01:31,400 --> 00:01:33,420
este termo aqui, o termo

50
00:01:33,820 --> 00:01:35,490
de erro quadrático, é

51
00:01:35,920 --> 00:01:39,250
o mesmo termo que está aqui.

52
00:01:39,760 --> 00:01:40,880
As somas parece um pouco

53
00:01:41,050 --> 00:01:42,940
diferentes, mas vamos ver o que elas estão fazendo.

54
00:01:43,800 --> 00:01:45,090
A primeira soma é

55
00:01:45,480 --> 00:01:48,280
sobre todos os usuários "j", e a soma interna

56
00:01:48,380 --> 00:01:50,590
é sobre todos os filmes avaliados pelo usuário.

57
00:01:51,890 --> 00:01:53,240
Ou seja, uma soma por todos

58
00:01:53,470 --> 00:01:55,950
os pares "(i, j)" que correspondem a um filme

59
00:01:56,510 --> 00:01:57,830
avaliado por um usuário.

60
00:01:58,550 --> 00:01:59,960
A soma em "j" determina,

61
00:02:00,150 --> 00:02:01,520
para cada usuário, a soma de

62
00:02:01,740 --> 00:02:03,110
todos os filmes avaliados por ele.

63
00:02:04,250 --> 00:02:07,340
A soma aqui embaixo faz o mesmo, mas na ordem inversa.

64
00:02:07,630 --> 00:02:08,710
Ela determina,

65
00:02:09,050 --> 00:02:11,140
para cada filme "i", a soma

66
00:02:11,340 --> 00:02:12,480
por todos os usuários "j"

67
00:02:12,690 --> 00:02:14,580
que avaliaram o filme,

68
00:02:14,690 --> 00:02:16,100
portanto, ambas as somas

69
00:02:16,220 --> 00:02:18,150
são por todos os pares "(i, j)"

70
00:02:18,930 --> 00:02:21,150
para os quais

71
00:02:21,440 --> 00:02:24,620
"r(i, j) = 1".

72
00:02:24,660 --> 00:02:26,580
É uma soma por todos os

73
00:02:27,180 --> 00:02:29,810
pares "(i, j)" para os quais existe uma avaliação.

74
00:02:30,840 --> 00:02:32,230
Assim, os dois termos

75
00:02:32,600 --> 00:02:34,740
de cima são exatamente

76
00:02:34,930 --> 00:02:36,460
este primeiro termo,

77
00:02:36,500 --> 00:02:38,310
em que escrevi a soma explicitamente,

78
00:02:39,310 --> 00:02:40,290
por todos os

79
00:02:40,580 --> 00:02:42,290
pares "(i, j)"

80
00:02:42,540 --> 00:02:45,060
para os quais "r(i, j) = 1".

81
00:02:45,310 --> 00:02:46,800
Então, o que fazemos

82
00:02:46,940 --> 00:02:48,790
é definir um

83
00:02:49,130 --> 00:02:51,410
objetivo de otimização combinado

84
00:02:51,670 --> 00:02:53,290
que queremos minimizar para

85
00:02:53,550 --> 00:02:55,700
achar soluções em "x" e "θ" simulaneamente.

86
00:02:56,970 --> 00:02:58,040
Os outros termos

87
00:02:58,070 --> 00:03:00,250
no objetivo de minimização são: este,

88
00:03:00,570 --> 00:03:02,870
que é um termo de regularização para "θ",

89
00:03:03,770 --> 00:03:05,830
que veio aqui embaixo,

90
00:03:06,290 --> 00:03:08,190
e a última parte é esta,

91
00:03:08,900 --> 00:03:10,690
que está no

92
00:03:10,850 --> 00:03:12,970
objetivo de minimização

93
00:03:13,170 --> 00:03:16,180
para os "x", que veio para cá.

94
00:03:16,500 --> 00:03:18,020
Esse objetivo de otimização "J"

95
00:03:18,720 --> 00:03:19,730
tem uma propriedade

96
00:03:20,240 --> 00:03:20,950
interessante,

97
00:03:21,410 --> 00:03:23,070
onde, deixando os "x" constantes

98
00:03:23,260 --> 00:03:25,490
e minimizando em "θ", você estará resolvendo

99
00:03:25,670 --> 00:03:27,040
exatamente este problema,

100
00:03:27,840 --> 00:03:28,450
enquanto se fizer

101
00:03:28,620 --> 00:03:29,590
o contrário,

102
00:03:29,690 --> 00:03:31,310
deixando "θ" constante e minimizar

103
00:03:31,670 --> 00:03:32,650
"J" somente

104
00:03:32,750 --> 00:03:34,920
em relação aos "x", torna-se equivalente a isto.

105
00:03:35,230 --> 00:03:36,780
Isso porque este termo

106
00:03:37,060 --> 00:03:38,860
ou aquele será constante se minimizar

107
00:03:38,970 --> 00:03:40,510
somente em "x" ou somente em "θ".

108
00:03:40,920 --> 00:03:43,680
Então, este é um objetivo

109
00:03:44,640 --> 00:03:46,840
de otimização que junta

110
00:03:47,440 --> 00:03:50,230
as funções custo em termos de "x" e "θ".

111
00:03:51,620 --> 00:03:53,050
Para chegar

112
00:03:53,470 --> 00:03:54,750
ao problema de otimização

113
00:03:55,090 --> 00:03:56,130
único, o que

114
00:03:56,280 --> 00:03:57,590
faremos será tratar

115
00:03:58,430 --> 00:03:59,850
a função custo como

116
00:03:59,880 --> 00:04:00,890
uma função das

117
00:04:01,410 --> 00:04:02,540
características "x"

118
00:04:03,180 --> 00:04:05,020
e os parâmetros de usuário "θ"

119
00:04:05,140 --> 00:04:06,570
para minimizar isso tudo,

120
00:04:06,740 --> 00:04:07,830
como uma função

121
00:04:08,120 --> 00:04:10,210
tanto dos "x" quanto dos "θ".

122
00:04:11,300 --> 00:04:12,400
Na verdade, a única

123
00:04:12,540 --> 00:04:13,800
diferença entre este

124
00:04:14,160 --> 00:04:15,650
e o algoritmo anterior é

125
00:04:15,980 --> 00:04:17,340
que, em vez de ir e voltar,

126
00:04:17,840 --> 00:04:20,110
minimizando em "θ", depois

127
00:04:20,420 --> 00:04:22,130
minimizando em "x",

128
00:04:22,260 --> 00:04:23,370
depois em "θ",

129
00:04:23,900 --> 00:04:25,270
depois em "x", e assim por diante.

130
00:04:26,130 --> 00:04:28,090
Nesta nova versão, em vez de

131
00:04:28,560 --> 00:04:30,020
ir sequencialmente pelos

132
00:04:30,220 --> 00:04:31,880
dois conjuntos de parâmetros, "x" e "θ",

133
00:04:32,180 --> 00:04:32,940
o que faremos

134
00:04:33,230 --> 00:04:34,600
será simplesmente minimizar

135
00:04:34,780 --> 00:04:36,410
em ambos os conjuntos simultaneamente.

136
00:04:39,750 --> 00:04:41,290
Finalmente, um último detalhe

137
00:04:42,030 --> 00:04:44,380
é que, quando aprendemos assim,

138
00:04:45,110 --> 00:04:46,410
anteriomente tínhamos

139
00:04:46,840 --> 00:04:49,290
essa convenção de uma característica

140
00:04:49,470 --> 00:04:50,540
"x₀ = 1"

141
00:04:50,740 --> 00:04:52,940
que corresponde a um termo de intercepção.

142
00:04:54,140 --> 00:04:55,530
Quando usamos esse

143
00:04:55,760 --> 00:04:57,790
formalismo onde estamos aprendendo as características,

144
00:04:58,300 --> 00:05:00,200
vamos nos livrar dessa convenção.

145
00:05:01,400 --> 00:05:04,220
Assim, as características "x" que vamos aprender estão em "ℝⁿ",

146
00:05:05,430 --> 00:05:06,650
em comparação

147
00:05:06,810 --> 00:05:09,770
com as características em "ℝⁿ⁺¹" com o termo de intercepção.

148
00:05:10,390 --> 00:05:13,390
Nos livrando de "x₀", temos "x ∈ ℝⁿ".

149
00:05:14,880 --> 00:05:16,520
Da mesma forma, como

150
00:05:16,590 --> 00:05:17,780
os parâmetros "θ"

151
00:05:17,850 --> 00:05:19,260
têm a mesma dimensão,

152
00:05:19,510 --> 00:05:21,010
também temos "θ ∈ ℝⁿ",

153
00:05:21,540 --> 00:05:23,340
porque, se não existe

154
00:05:23,710 --> 00:05:24,580
"x₀", também

155
00:05:25,370 --> 00:05:26,880
não há necessidade para o "θ₀".

156
00:05:27,960 --> 00:05:28,880
Agora, estamos

157
00:05:29,160 --> 00:05:30,390
aprendendo todas

158
00:05:31,010 --> 00:05:32,610
as características, certo?

159
00:05:32,820 --> 00:05:34,280
Assim, não há necessidade

160
00:05:34,420 --> 00:05:36,650
de estabelecer uma característica sempre igual a 1.

161
00:05:37,170 --> 00:05:38,310
Isso porque, se o algoritmo

162
00:05:38,600 --> 00:05:39,450
realmente quiser

163
00:05:40,060 --> 00:05:41,830
uma característica igual a 1, pode aprender por conta própria.

164
00:05:42,290 --> 00:05:43,430
Assim, se ele escolher,

165
00:05:43,720 --> 00:05:45,330
pode colocar "x₁ = 1".

166
00:05:45,670 --> 00:05:47,010
Não há necessidade

167
00:05:47,260 --> 00:05:48,300
de forçar

168
00:05:48,440 --> 00:05:50,060
"x₀ = 1", o algoritmo

169
00:05:50,340 --> 00:05:55,890
agora tem a flexibilidade para aprender isso por conta própria.

170
00:05:56,420 --> 00:05:58,410
Juntando tudo, aqui está nosso algoritmo

171
00:05:58,780 --> 00:05:59,910
de filtragem colaborativa.

172
00:06:01,460 --> 00:06:02,330
Primeiro,

173
00:06:03,010 --> 00:06:05,580
inicializamos "x" e "θ"

174
00:06:05,820 --> 00:06:07,290
a valores aleatórios pequenos.

175
00:06:08,450 --> 00:06:09,200
Isso é um pouco

176
00:06:09,310 --> 00:06:11,700
como treinar redes neurais,

177
00:06:11,720 --> 00:06:14,240
onde também inicializamos todos os parâmetros a valores aleatórios pequenos.

178
00:06:16,640 --> 00:06:17,730
A seguir, vamos

179
00:06:17,950 --> 00:06:20,110
minimizar a função custo usando

180
00:06:20,500 --> 00:06:23,360
gradiente descendente ou um dos algoritmos de otimização avançados.

181
00:06:24,610 --> 00:06:25,890
Encontrando as derivadas,

182
00:06:26,020 --> 00:06:27,460
você descobre que as

183
00:06:27,590 --> 00:06:29,320
atualizações do gradiente descendente

184
00:06:29,630 --> 00:06:31,160
são estas, onde este termo

185
00:06:31,660 --> 00:06:33,890
é a derivada parcial da função custo,

186
00:06:35,140 --> 00:06:35,940
com respeito

187
00:06:36,110 --> 00:06:37,860
à característica

188
00:06:38,070 --> 00:06:40,020
"x⁽ⁱ⁾ₖ", e, similarmente,

189
00:06:41,020 --> 00:06:42,430
este termo aqui também

190
00:06:43,030 --> 00:06:44,660
é uma derivada parcial da

191
00:06:44,730 --> 00:06:46,480
função custo no

192
00:06:46,930 --> 00:06:48,950
parâmetro "θ" que estamos minimizando.

193
00:06:50,210 --> 00:06:51,410
Só como um lembrete,

194
00:06:51,760 --> 00:06:52,920
nesta fórmula não

195
00:06:53,130 --> 00:06:54,760
temos mais esse "x₀ = 1",

196
00:06:54,970 --> 00:06:56,740
ou seja,

197
00:06:57,010 --> 00:07:00,010
"x ∈ ℝⁿ" e "θ ∈ ℝⁿ".

198
00:07:01,480 --> 00:07:03,100
Neste novo formalismo, estamos regularizando

199
00:07:03,760 --> 00:07:05,220
todos os parâmetros "θ" e "x".

200
00:07:07,400 --> 00:07:09,060
Não existe mais o caso

201
00:07:09,480 --> 00:07:11,850
especial "θ₀", que era regularizado

202
00:07:12,210 --> 00:07:13,760
de forma diferente,

203
00:07:13,860 --> 00:07:15,440
ou que não era regularizado

204
00:07:15,560 --> 00:07:17,650
comparado aos parâmetros "θ₁" a "θₙ".

205
00:07:18,370 --> 00:07:19,710
Agora não existe mais

206
00:07:20,070 --> 00:07:21,150
o "θ₀", e é por isso

207
00:07:21,400 --> 00:07:22,450
que não separei um

208
00:07:22,700 --> 00:07:24,080
caso especial para "k = 0".

209
00:07:26,070 --> 00:07:27,230
Então, usamos gradiente

210
00:07:27,740 --> 00:07:28,710
descendente para

211
00:07:29,090 --> 00:07:30,260
minimizar a função custo

212
00:07:30,390 --> 00:07:32,000
"J" com respeito a "x" e "θ".

213
00:07:33,160 --> 00:07:35,050
E, finalmente, dado

214
00:07:35,140 --> 00:07:36,320
um usuário

215
00:07:36,570 --> 00:07:38,920
com parâmetros "θ",

216
00:07:39,410 --> 00:07:40,540
se existir um filme com

217
00:07:40,690 --> 00:07:41,980
características aprendidas "x",

218
00:07:42,580 --> 00:07:43,720
estimaríamos que o

219
00:07:43,970 --> 00:07:44,940
filme receberia

220
00:07:45,030 --> 00:07:46,200
uma nota igual a

221
00:07:47,010 --> 00:07:48,780
"θᵀ · x" do usuário.

222
00:07:48,860 --> 00:07:50,370
Ou seja,

223
00:07:50,640 --> 00:07:52,250
se o usuário "j"

224
00:07:52,630 --> 00:07:53,780
ainda não

225
00:07:54,010 --> 00:07:55,980
avaliou o filme "i"

226
00:07:56,170 --> 00:07:57,300
estimamos que

227
00:07:58,150 --> 00:07:59,120
o usuário "j"

228
00:07:59,710 --> 00:08:01,420
avaliará o filme "i"

229
00:08:02,300 --> 00:08:04,230
com a nota "(θ⁽ʲ⁾ᵀ) · (x⁽ⁱ⁾)".

230
00:08:06,650 --> 00:08:08,010
Esse é o algoritmo

231
00:08:08,810 --> 00:08:10,170
de filtragem colaborativa, e

232
00:08:10,310 --> 00:08:12,230
se você implementá-lo, obterá um algoritmo

233
00:08:12,730 --> 00:08:14,080
bem decente, que simultaneamente

234
00:08:15,060 --> 00:08:16,770
aprende boas características para

235
00:08:17,110 --> 00:08:18,460
todos os filmes, espero,

236
00:08:18,570 --> 00:08:19,890
além dos parâmetros para

237
00:08:20,050 --> 00:08:21,290
todos os usuários,

238
00:08:21,440 --> 00:08:23,060
e pode fornecer estimativas boas

239
00:08:23,290 --> 00:08:25,890
para as avaliações de usuários diferentes em filmes que ainda não viram.