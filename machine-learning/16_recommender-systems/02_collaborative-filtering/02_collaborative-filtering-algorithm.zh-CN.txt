在前面几个视频里 我们谈到几个概念 首先 如果给你几个特征表示电影 我们可以使用这些资料去获得用户的参数数据 第二 如果给你用户的参数数据 你可以使用这些资料去获得电影的特征 本节视频中 我们将会使用这些概念 并且将它们合并成 协同过滤算法 (Collaborative Filtering Algorithm) 我们之前做过的事情 其中之一是 假如你有了电影的特征 你就可以解出 这个最小化问题 为你的用户找到参数 θ 然后我们也 知道了 如果你拥有参数 θ 你也可以用该参数 通过解一个最小化问题 去计算出特征 x 所以你可以做的事 是不停地重复这些计算 或许是随机地初始化这些参数 然后解出 θ 解出 x 解出 θ 解出 x 但实际上呢 存在一个更有效率的算法 让我们不再需要再这样不停地 计算 x 和 θ 而是能够将 x 和 θ 同时计算出来 下面就是这种算法 我们所要做的 是将这两个优化目标函数 给合为一个 所以我要来定义 这个新的优化目标函数 J 它依然是一个代价函数 是我特征 x 和参数 θ 的函数 它其实就是上面那两个优化目标函数 但我将它们给合在一起 为了把这个解释清楚 首先 我想指出 这里的这个表达式 这个平方误差项 和下面的这个项是相同的 可能两个求和看起来有点不同 但让我们来看看它们到底到底在做什么 第一个求和运算 是所有用户 J 的总和 和所有被用户评分过的电影总和 所以这其实是正在将 所有关于 (i,j) 对的项全加起来 表示被用户评分过的电影 关于 j 的求和 意思是 对每个用户 关于该用户评分的电影的求和 而下面的求和运算只是用相反的顺序去进行计算 这写着关于每部电影 i 求和 关于的是 所有曾经对它评分过的 用户 j 所以这些求和运算 这两种都是对所有 (i,j) 对的求和 其中 r(i,j) 是等于1的 这只是所有你有评分的用户 和电影对而已 因此 这两个式子 其实就是 这里的第一个式子 我已经给出了这个求和式子 这里我写着 其为所有 r(i,j) 值为1的 (i,j) 对求和 所以我们要做的 是去定义 一个我们想将其最小化的 合并后的优化目标函数 让我们能同时解出 x 和 θ 然后在这些优化目标函数里的 另一个式子是这个 其为 θ 所进行的正则化 它被放到这里 最后一部分 是这项式 是我 x 的优化目标函数 然后它变成这个 这个优化目标函数 J 它有一个很有趣的特性 如果你假设 x 为常数 并关于 θ 优化的话 你其实就是在计算这个式子 反过来也一样 如果你把 θ 作为常量 然后关于 x 求 J 的最小值的话 那就与第二个式子相等 因为不管是这个部分 还是这个部分 将会变成常数 如果你将它化简成只以 x 或 θ 表达的话 所以这里是 一个将我的 x 和 θ 合并起来的代价函数 为了按照 为了解出 这个优化目标问题 我们所要做的是 将这个代价函数视为 特征 x 和用户参数 θ 的 函数 然后全部化简为 一个既关于 x 也关于 θ 的函数 这和 前面的算法之间 唯一的不同是 不需要反复计算 就像我们之前所提到的 先关于 θ 最小化 然后关于 x 最小化 然后再关于 θ 最小化 再关于 x 最小化... 在新版本里头 不需要不断地在 x 和 θ 这两个参数之间不停折腾 我们所要做的是 将这两组参数 同时化简 最后一件事是 当我们以这样的方法学习特征量时 之前我们所使用的 前提是 我们所使用的特征 x0 等于1 对应于一个截距 当我们以 这种形式真的去学习特征量时 我们必须要去掉这个前提 所以这些我们将学习的特征量 x 是 n 维实数 而先前我们所有的 特征值x 是 n+1 维 包括截距 删除掉x0 我们现在只会有 n 维的 x 同样地 因为参数 θ 是 在同一个维度上 所以 θ 也是 n 维的 因为如果没有 x0 那么 θ0 也不再需要 我们将这个前提移除的理由是 因为我们现在是在 学习所有的特征 对吧? 所以我们没有必要 去将这个等于一的特征值固定死 因为如果算法真的需要 一个特征永远为1 它可以选择靠自己去获得1这个数值 所以如果这算法想要的话 它可以将特征值 x1 设为1 所以没有必要 去将1 这个特征定死 这样算法有了 灵活性去自行学习 所以 把所有讲的这些合起来 即是我们的协同过滤算法 首先我们将会把 x 和 θ 初始为小的随机值 这有点像 神经网络训练 我们也是将所有神经网路的参数用小的随机数值来初始化 接下来 我们要用 梯度下降 或者某些其他的高级优化算法 把这个代价函数最小化 所以如果你求导的话 你会发现梯度下降法 写出来的更新式是这样的 这个部分就是 代价函数 这里我简写了 关于特征值 x(i)k 的偏微分 然后相同地 这部分 也是代价函数 关于我们正在最小化的参数 θ 所做的偏微分 提醒一下 这公式里 我们不再有这等于1 的 x0 项 所以 x 是 n 维 θ 也是n 维 在这个新的表达式里 我们将所有的参数 θ 和 xn 做正则化 不存在 θ0 这种特殊的情况 会需要不同地正则化 或者说是 跟 θ1 到 θn 的正则化 不同的 θ0 的正则化 所以现在不存在 θ0 这就是为什么 在这些更新式里 我并没有分出 k 等于0的特殊情况 所以我们使用梯度下降 来最小化这个 代价函数 J 关于特征 x 和参数 θ 最后 给你一个用户 如果这个用户 具有一些参数 θ 以及给你一部电影 带有已知的特征 x 我们可以预测 这部电影会被 θ 转置乘以 x 给出怎样的评分 或者将这些直接填入 那我们可以说 如果用户 j 尚未对电影 i 评分 那我们可以预测 这个用户 j 将会根据 θ(j) 转置乘以 x(i) 对电影 i 评分 所以这就是 协同过滤算法 如果你使用这个算法 你可以得到一个十分有用的算法 可以同时学习 几乎所有电影的特征 和所有用户参数 然后有很大机会 能对不同用户会如何对他们尚未评分的电影做出评​​价 给出相当准确的预测 【教育无边界字幕组】翻译: 爱发呆的羊 校对/审核: 所罗门捷列夫