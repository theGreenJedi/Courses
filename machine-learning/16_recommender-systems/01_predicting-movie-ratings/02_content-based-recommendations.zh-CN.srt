1
00:00:01,370 --> 00:00:02,420
在过去的视频中 我们谈到

2
00:00:02,740 --> 00:00:04,200
推荐系统的问题

3
00:00:05,030 --> 00:00:06,270
举个例子

4
00:00:06,380 --> 00:00:07,810
假如你有一些电影

5
00:00:07,940 --> 00:00:09,140
还有一些观众

6
00:00:09,810 --> 00:00:10,960
他们每个人都对某些电影

7
00:00:11,670 --> 00:00:13,170
进行了一些评价打分

8
00:00:13,370 --> 00:00:14,340
把电影按照1星到5星 

9
00:00:14,500 --> 00:00:15,460
或者0到5星评分

10
00:00:15,630 --> 00:00:16,830
我想做的

11
00:00:17,200 --> 00:00:18,170
是通过这些

12
00:00:18,240 --> 00:00:19,720
用户的评价来预测出

13
00:00:19,910 --> 00:00:22,540
他们会怎样给还没看过的电影打分

14
00:00:23,530 --> 00:00:24,540
在这段视频中

15
00:00:24,600 --> 00:00:25,950
我想介绍第一种构造

16
00:00:26,430 --> 00:00:28,190
推荐系统的方法

17
00:00:28,360 --> 00:00:30,100
这种方法叫做“基于内容的推荐”

18
00:00:31,460 --> 00:00:32,690
这是我们之前的数据集

19
00:00:33,310 --> 00:00:34,470
我想提醒一下

20
00:00:34,550 --> 00:00:35,780
关于这个符号表示

21
00:00:36,690 --> 00:00:37,870
我是用n_u来表示

22
00:00:38,030 --> 00:00:39,110
用户的数量

23
00:00:39,290 --> 00:00:40,990
在这里等于4

24
00:00:41,990 --> 00:00:44,780
n_m来表示电影的数量 这里是5部电影

25
00:00:47,230 --> 00:00:48,140
那么我应该如何预测

26
00:00:48,960 --> 00:00:50,950
这些缺少的值呢？

27
00:00:52,490 --> 00:00:53,520
我们假设

28
00:00:53,700 --> 00:00:55,500
对这些电影的每一部

29
00:00:55,540 --> 00:00:57,460
我都用一些特征来描述

30
00:00:57,910 --> 00:00:58,990
具体来说

31
00:00:59,690 --> 00:01:00,850
我们假设每部电影有两种特征

32
00:01:01,920 --> 00:01:03,500
分别用x1和x2代表

33
00:01:04,080 --> 00:01:05,700
x1表示这部电影

34
00:01:06,130 --> 00:01:07,450
属于爱情电影的程度

35
00:01:07,650 --> 00:01:09,270
x2表示这部电影

36
00:01:09,810 --> 00:01:12,080
是动作电影的程度

37
00:01:12,840 --> 00:01:13,700
因此 对于电影

38
00:01:14,470 --> 00:01:16,490
《爱到最后》

39
00:01:16,800 --> 00:01:17,960
那么这部电影

40
00:01:18,030 --> 00:01:19,190
是爱情电影的比率为0.9

41
00:01:19,260 --> 00:01:20,850
这是一部绝对的爱情电影

42
00:01:20,920 --> 00:01:22,400
但是它属于动作电影的比率为0

43
00:01:22,520 --> 00:01:24,390
表示几乎没有动作内容

44
00:01:24,540 --> 00:01:25,860
《浪漫永远》 爱情比率1.0

45
00:01:26,230 --> 00:01:27,610
大部分都是爱情内容 动作比率0.01

46
00:01:27,860 --> 00:01:29,790
我也不知道为什么

47
00:01:30,700 --> 00:01:32,650
可能出了一场车祸什么的

48
00:01:33,630 --> 00:01:35,580
所以有一丁点动作成分

49
00:01:35,610 --> 00:01:36,760
我们跳过一些

50
00:01:37,860 --> 00:01:39,630
看最后这个 《剑与空手道》

51
00:01:39,870 --> 00:01:41,110
爱情比率为0

52
00:01:41,520 --> 00:01:42,780
表示没有爱情成分

53
00:01:43,250 --> 00:01:46,040
估计全片都是动作情节

54
00:01:46,300 --> 00:01:47,120
 同样的 《无尽狂飙》

55
00:01:47,220 --> 00:01:48,390
可能里面有一丁点爱情

56
00:01:48,500 --> 00:01:49,800
但主要是动作

57
00:01:50,460 --> 00:01:51,560
《小爱犬》

58
00:01:51,680 --> 00:01:52,730
也算爱情电影

59
00:01:53,510 --> 00:01:54,410
基本没有动作成分

60
00:01:55,990 --> 00:01:57,150
所以 如果每部电影

61
00:01:57,550 --> 00:01:59,220
我们有这样的一些特征

62
00:01:59,800 --> 00:02:01,510
那么可以用一个特征矩阵表示

63
00:02:02,380 --> 00:02:03,810
对于电影1

64
00:02:04,020 --> 00:02:06,210
我们暂且用电影1,2,3,4,5来指代这些电影

65
00:02:06,630 --> 00:02:08,180
所以对于电影1

66
00:02:08,520 --> 00:02:09,810
《爱到最后》

67
00:02:10,170 --> 00:02:11,710
我的两个特征值

68
00:02:12,180 --> 00:02:12,950
分别是0.9和0

69
00:02:13,380 --> 00:02:16,170
这就是特征变量 x1 和 x2

70
00:02:16,340 --> 00:02:17,270
我们还是像以前一样

71
00:02:17,790 --> 00:02:18,780
加一个额外的特征变量

72
00:02:19,350 --> 00:02:21,640
截距特征变量 x0 其值为1

73
00:02:22,680 --> 00:02:23,810
把三个特征变量放在一起

74
00:02:24,700 --> 00:02:26,150
这样我就有了特征 x(1)

75
00:02:26,970 --> 00:02:28,420
这个上标括号(1)

76
00:02:28,510 --> 00:02:29,430
表示这是我第一部电影的

77
00:02:29,770 --> 00:02:30,720
特征向量

78
00:02:30,980 --> 00:02:32,520
这个向量内部是这样的

79
00:02:33,190 --> 00:02:34,880
第一个值是截距项1

80
00:02:35,740 --> 00:02:37,010
然后是两个特征值0.9和0

81
00:02:37,260 --> 00:02:39,330
就这样

82
00:02:40,370 --> 00:02:41,360
所以特征x(1)

83
00:02:41,550 --> 00:02:43,470
就是电影《爱到最后》的特征向量

84
00:02:44,480 --> 00:02:46,220
对电影《浪漫永远》

85
00:02:46,340 --> 00:02:47,510
我们有另外一个特征向量 x(2)

86
00:02:47,800 --> 00:02:49,310
等等 以此类推

87
00:02:49,380 --> 00:02:50,780
最后电影《剑与空手道》

88
00:02:51,510 --> 00:02:54,050
有另外一个向量 x(5)

89
00:02:56,150 --> 00:02:57,460
同样地 为了跟

90
00:02:57,680 --> 00:02:59,090
我们之前的符号表达

91
00:02:59,300 --> 00:03:00,220
保持一致

92
00:03:00,490 --> 00:03:02,130
我们仍然用 n 表示特征变量数

93
00:03:02,360 --> 00:03:03,530
不包括 x0

94
00:03:03,810 --> 00:03:05,320
这样

95
00:03:05,420 --> 00:03:06,600
n就等于2

96
00:03:06,790 --> 00:03:08,180
表示两个特征变量x1和x2

97
00:03:08,890 --> 00:03:10,140
分别对应每部电影的

98
00:03:10,640 --> 00:03:11,980
爱情程度

99
00:03:12,630 --> 00:03:14,270
和动作程度

100
00:03:14,560 --> 00:03:17,930
为了进行预测 我们可以这么做

101
00:03:19,230 --> 00:03:20,980
我们可以把对

102
00:03:21,160 --> 00:03:22,340
每个观众打分的预测

103
00:03:23,250 --> 00:03:26,210
当成一个独立的线性回归问题

104
00:03:26,440 --> 00:03:27,660
具体来说

105
00:03:27,920 --> 00:03:29,170
比如每一个用户j

106
00:03:29,270 --> 00:03:30,860
我们都学习出一个参数θ(j)

107
00:03:31,340 --> 00:03:33,030
在这里是一个三维向量

108
00:03:33,540 --> 00:03:35,730
更一般的情况是

109
00:03:35,950 --> 00:03:37,960
θ(j)为一个n+1维向量

110
00:03:38,340 --> 00:03:39,460
n是特征数

111
00:03:39,700 --> 00:03:42,170
不包括截距项x0

112
00:03:42,440 --> 00:03:43,880
然后我们要根据参数向量θ

113
00:03:44,050 --> 00:03:45,780
与特征x(i)的内积

114
00:03:46,000 --> 00:03:47,390
来预测用户j

115
00:03:47,860 --> 00:03:50,590
对电影i的评分

116
00:03:51,830 --> 00:03:53,680
我们来看一个具体的例子吧

117
00:03:55,130 --> 00:03:56,700
我们来看用户1

118
00:03:59,600 --> 00:04:01,120
Alice

119
00:04:01,380 --> 00:04:02,700
与Alice对应的

120
00:04:02,830 --> 00:04:03,990
参数向量

121
00:04:04,810 --> 00:04:06,210
就应该是θ(1)

122
00:04:06,520 --> 00:04:07,610
第二个用户Bob

123
00:04:07,720 --> 00:04:08,600
就是跟第二个向量

124
00:04:08,970 --> 00:04:10,290
θ(2)对应的

125
00:04:10,800 --> 00:04:12,190
Carol则对应

126
00:04:12,300 --> 00:04:13,360
另一个参数向量θ(3)

127
00:04:13,660 --> 00:04:14,790
第四个用户Dave

128
00:04:15,750 --> 00:04:17,670
对应另一个参数向量θ(4)

129
00:04:18,090 --> 00:04:18,990
现在假如我们想预测

130
00:04:19,320 --> 00:04:21,040
Alice对电影

131
00:04:21,240 --> 00:04:22,450
《小爱犬》

132
00:04:22,690 --> 00:04:24,640
是如何评价的

133
00:04:24,810 --> 00:04:25,670
那么这部电影

134
00:04:26,810 --> 00:04:29,180
有一个参数向量x(3)

135
00:04:29,410 --> 00:04:30,400
x(3)是等于[1 0.99 0]

136
00:04:30,430 --> 00:04:32,460
其中1是截距项

137
00:04:32,650 --> 00:04:34,580
然后是两个特征

138
00:04:34,800 --> 00:04:37,220
0.99和0

139
00:04:38,560 --> 00:04:39,680
假如说

140
00:04:39,810 --> 00:04:41,040
对于这个例子

141
00:04:41,190 --> 00:04:42,890
你已经知道Alice的

142
00:04:43,290 --> 00:04:44,600
参数向量θ(1)

143
00:04:44,830 --> 00:04:45,700
后面我们还会

144
00:04:45,850 --> 00:04:47,560
详细讲到

145
00:04:47,800 --> 00:04:48,520
这个参数是怎么得到的

146
00:04:48,600 --> 00:04:50,530
但现在就假设

147
00:04:50,710 --> 00:04:52,000
你已经知道了

148
00:04:52,150 --> 00:04:53,560
用某种学习算法得到的

149
00:04:54,040 --> 00:04:55,040
参数向量θ(1)

150
00:04:55,180 --> 00:04:56,970
它的值等于

151
00:04:57,120 --> 00:04:59,260
[0 5 0]

152
00:05:00,150 --> 00:05:02,010
因此 我们对

153
00:05:02,270 --> 00:05:04,130
这一项的预测

154
00:05:04,260 --> 00:05:06,930
就等于θ(1)——

155
00:05:07,440 --> 00:05:08,760
θ(1)是Alice的参数向量

156
00:05:09,620 --> 00:05:11,450
——的转置 乘以x(3)

157
00:05:11,620 --> 00:05:13,730
x(3)是3号电影

158
00:05:14,170 --> 00:05:16,050
《小爱犬》的特征向量

159
00:05:16,250 --> 00:05:17,200
因此 这两个

160
00:05:17,470 --> 00:05:18,470
向量的内积

161
00:05:19,910 --> 00:05:21,780
就等于 5 × 0.99

162
00:05:23,980 --> 00:05:26,340
等于4.95

163
00:05:27,360 --> 00:05:28,940
因此我对这个值的预测

164
00:05:29,130 --> 00:05:30,930
其结果将为4.95

165
00:05:31,970 --> 00:05:33,110
这看起来算是一个

166
00:05:33,230 --> 00:05:34,660
比较合理的预测

167
00:05:36,130 --> 00:05:37,830
对于参数向量θ(1)来说

168
00:05:38,950 --> 00:05:40,290
因此 我们做的事情

169
00:05:40,520 --> 00:05:42,710
实际上就是对每个用户

170
00:05:42,930 --> 00:05:44,480
应用不同的线性回归模型

171
00:05:44,760 --> 00:05:46,020
并且我们预测

172
00:05:46,230 --> 00:05:47,610
Alice 会做的是

173
00:05:47,820 --> 00:05:48,880
Alice 对应一个参数θ(1)

174
00:05:49,160 --> 00:05:50,400
我们要用这个参数

175
00:05:51,410 --> 00:05:52,380
来预测Alice

176
00:05:53,310 --> 00:05:54,770
对这部电影的评价

177
00:05:54,950 --> 00:05:56,190
这个评价和电影的

178
00:05:56,470 --> 00:05:57,540
爱情程度和动作程度是相关的

179
00:05:58,210 --> 00:05:59,600
然后Bob Carol 和 Dave

180
00:05:59,740 --> 00:06:01,010
他们每个人

181
00:06:01,220 --> 00:06:03,170
都有一个不同的线性方程

182
00:06:03,330 --> 00:06:04,700
来算出电影的爱情性和动作性

183
00:06:05,220 --> 00:06:06,510
或者说某部电影的爱情程度

184
00:06:07,580 --> 00:06:08,030
和动作程度

185
00:06:08,820 --> 00:06:11,300
这就是预测它们评分结果的方法

186
00:06:14,820 --> 00:06:16,330
我们可以把这个问题

187
00:06:16,610 --> 00:06:17,920
写成如下更正式一些的形式

188
00:06:19,260 --> 00:06:20,320
我们用r(i,j)=1

189
00:06:20,690 --> 00:06:21,600
来表示用户j

190
00:06:21,680 --> 00:06:22,910
对电影i进行了评分

191
00:06:23,380 --> 00:06:24,630
y(i,j)则表示用户j

192
00:06:25,850 --> 00:06:28,010
对电影i的评分值

193
00:06:29,540 --> 00:06:30,520
如果这个用户

194
00:06:31,030 --> 00:06:32,830
对这部电影进行过评价

195
00:06:33,330 --> 00:06:34,360
在前面的幻灯片中

196
00:06:34,650 --> 00:06:36,540
我们还定义了θ(j)

197
00:06:36,740 --> 00:06:38,790
表示用户j对应的参数向量

198
00:06:39,150 --> 00:06:40,830
x(i)是某部电影i的

199
00:06:41,220 --> 00:06:42,370
特征向量

200
00:06:42,850 --> 00:06:43,780
那么某个用户对

201
00:06:44,300 --> 00:06:45,620
某部电影的评分就是这样的

202
00:06:47,230 --> 00:06:49,560
现在 我临时介绍一个

203
00:06:49,650 --> 00:06:51,600
额外的表示符号

204
00:06:51,860 --> 00:06:53,530
m(j)

205
00:06:53,760 --> 00:06:54,980
我们用m(j)

206
00:06:55,070 --> 00:06:56,140
来表示用户j评价过的电影数

207
00:06:56,400 --> 00:06:57,350
我们只在这一页中

208
00:06:57,580 --> 00:06:59,890
使用m(j)这个符号

209
00:07:00,160 --> 00:07:01,700
为了学习参数向量θ(j)

210
00:07:01,760 --> 00:07:03,720
我们应该怎么做呢？

211
00:07:04,410 --> 00:07:06,380
这是一个基本的线性回归问题

212
00:07:06,930 --> 00:07:07,980
因此我们要做的

213
00:07:08,290 --> 00:07:09,810
就是选择一个参数向量θ(j)

214
00:07:10,520 --> 00:07:12,100
使得预测的结果

215
00:07:12,570 --> 00:07:13,620
这里

216
00:07:13,980 --> 00:07:15,280
尽可能接近

217
00:07:15,800 --> 00:07:18,760
我们在训练集中的观测值

218
00:07:19,900 --> 00:07:21,390
我们来写一下

219
00:07:22,290 --> 00:07:24,320
为了学习出

220
00:07:24,380 --> 00:07:26,960
参数向量θ(j)

221
00:07:27,170 --> 00:07:28,510
我们要关于θ(j)最小化

222
00:07:29,400 --> 00:07:30,360
下面这个求和值

223
00:07:31,920 --> 00:07:32,860
这个求和值

224
00:07:33,290 --> 00:07:34,900
是用户j

225
00:07:35,240 --> 00:07:36,930
对电影的所有评价

226
00:07:37,270 --> 00:07:38,290
所以求和范围是所有的i值

227
00:07:39,100 --> 00:07:42,000
即r(i,j)=1时的所有i

228
00:07:43,870 --> 00:07:45,970
这个求和序列的读法是

229
00:07:46,370 --> 00:07:48,280
对所有满足

230
00:07:48,470 --> 00:07:49,550
r(i,j)=1的这些i

231
00:07:49,780 --> 00:07:51,180
进行求和运算

232
00:07:51,210 --> 00:07:52,470
这样就求出了所有用户j

233
00:07:52,560 --> 00:07:54,670
所评价过的电影

234
00:07:56,230 --> 00:07:57,000
然后我要计算的内容

235
00:07:58,150 --> 00:07:59,910
是θ(j)的转置

236
00:08:01,810 --> 00:08:04,450
乘以x(i)

237
00:08:04,610 --> 00:08:06,740
这就是用户j

238
00:08:07,030 --> 00:08:08,390
对电影i评分的预测值

239
00:08:09,230 --> 00:08:10,960
减去y(i,j)

240
00:08:11,700 --> 00:08:13,700
这是实际观测到的评分值 然后平方

241
00:08:15,190 --> 00:08:16,790
在求和式前面

242
00:08:17,260 --> 00:08:18,650
除以用户j评价过的

243
00:08:19,040 --> 00:08:20,990
所有电影的数量

244
00:08:21,380 --> 00:08:23,910
也就是乘上1/2m(j)

245
00:08:24,000 --> 00:08:25,460
这其实就很像

246
00:08:25,690 --> 00:08:27,620
最小二乘回归

247
00:08:28,210 --> 00:08:29,550
或者线性回归

248
00:08:30,170 --> 00:08:31,170
我们要选择一个

249
00:08:31,320 --> 00:08:34,480
最佳的参数θ(j)来最小化这个平方误差项

250
00:08:34,510 --> 00:08:35,090
当然如果你愿意的话

251
00:08:36,330 --> 00:08:39,580
你也可以加上正则化项

252
00:08:39,980 --> 00:08:41,870
加上λ/2m

253
00:08:43,780 --> 00:08:44,930
实际上应该是2m(j)

254
00:08:45,420 --> 00:08:47,760
因为我们有m(j)个样本对吧？

255
00:08:47,920 --> 00:08:49,330
因为如果用户j

256
00:08:49,650 --> 00:08:50,910
对这么多电影进行了评分的话

257
00:08:51,050 --> 00:08:53,340
那么我们就需要这么多数据点

258
00:08:53,680 --> 00:08:55,790
来拟合参数θ(j)

259
00:08:56,650 --> 00:08:57,390
然后 我还是加上

260
00:08:58,340 --> 00:09:00,260
我一般使用的正则化项

261
00:09:00,460 --> 00:09:02,530
θk(j)的平方关于k求和

262
00:09:03,110 --> 00:09:04,270
同样地

263
00:09:04,840 --> 00:09:05,980
这里的求和是从k等于1到n

264
00:09:06,330 --> 00:09:08,670
所以这里的θ(j)

265
00:09:08,880 --> 00:09:10,050
将是一个

266
00:09:10,520 --> 00:09:12,400
n+1维的向量

267
00:09:12,620 --> 00:09:14,630
在我们之前那个例子中n为2

268
00:09:15,320 --> 00:09:17,090
但更一般地

269
00:09:17,260 --> 00:09:20,980
n应该是每一部电影的特征数

270
00:09:21,730 --> 00:09:22,270
按照惯例 我们还是不对θ(0)进行正则化

271
00:09:22,390 --> 00:09:23,710
我们不对偏差项

272
00:09:23,910 --> 00:09:24,750
进行正则化

273
00:09:24,930 --> 00:09:28,590
因为这个求和是从k等于1到n

274
00:09:28,760 --> 00:09:30,430
因此 如果你对这个式子

275
00:09:30,570 --> 00:09:31,780
关于θ(j)求最小值的话

276
00:09:31,900 --> 00:09:33,010
你会得到一个解

277
00:09:33,180 --> 00:09:35,330
你会得到一个很好的θ(j)的估计值

278
00:09:36,490 --> 00:09:37,200
预测出用户j

279
00:09:37,940 --> 00:09:39,460
对电影的评分值

280
00:09:40,820 --> 00:09:42,250
对于推荐系统

281
00:09:42,520 --> 00:09:44,140
我要把符号稍微变化一下

282
00:09:44,500 --> 00:09:46,130
为了让数学更简单

283
00:09:46,690 --> 00:09:48,440
我要去掉这个m(j)

284
00:09:49,570 --> 00:09:50,720
这就一个常数

285
00:09:50,970 --> 00:09:52,140
我可以去掉这一项

286
00:09:53,000 --> 00:09:54,310
不改变θ(j)的

287
00:09:54,430 --> 00:09:55,840
最优化结果

288
00:09:56,010 --> 00:09:57,030
所以你可以把这个式子

289
00:09:57,220 --> 00:09:58,850
看成是算出整个表达式

290
00:09:59,010 --> 00:10:00,290
然后乘以一个m(j)项

291
00:10:00,870 --> 00:10:02,540
然后去掉这个常数项

292
00:10:02,950 --> 00:10:04,110
当我最小化的时候

293
00:10:04,200 --> 00:10:06,590
我还是会得到同样的θ(j)

294
00:10:06,710 --> 00:10:07,780
再重复一下

295
00:10:08,440 --> 00:10:10,060
我们前一页写的

296
00:10:10,340 --> 00:10:12,250
这是我们的最优化目标

297
00:10:12,580 --> 00:10:13,620
为了学习θ(j)

298
00:10:13,990 --> 00:10:15,080
θ(j)是用户j对应的参数向量

299
00:10:15,790 --> 00:10:17,570
我们要关于θ(j)最小化

300
00:10:17,770 --> 00:10:19,820
这个最优化目标

301
00:10:20,100 --> 00:10:21,360
这是我们通常的

302
00:10:21,720 --> 00:10:24,830
平方误差项 这是我们的正则化项

303
00:10:26,050 --> 00:10:27,410
当然 在构建

304
00:10:27,690 --> 00:10:28,790
推荐系统的时候

305
00:10:29,030 --> 00:10:29,800
我们也不想只对某一个用户

306
00:10:30,420 --> 00:10:31,500
学习出参数向量

307
00:10:31,650 --> 00:10:33,140
我们想对所有的用户都学习出θ

308
00:10:33,490 --> 00:10:35,640
因为我有n_u个用户

309
00:10:35,760 --> 00:10:36,730
所以我希望学习出

310
00:10:36,950 --> 00:10:38,920
所有的参数

311
00:10:39,060 --> 00:10:39,830
那么我要做的是

312
00:10:40,140 --> 00:10:42,320
将这个最优化目标

313
00:10:42,500 --> 00:10:45,480
另外再加上一个求和

314
00:10:45,800 --> 00:10:47,610
所以 这里的表达式

315
00:10:48,410 --> 00:10:49,200
前面是1/2

316
00:10:49,240 --> 00:10:50,510
这实际上就跟

317
00:10:50,780 --> 00:10:52,520
我们上面的一样

318
00:10:52,950 --> 00:10:53,980
唯一不同的是

319
00:10:54,090 --> 00:10:55,670
现在不是只对一个θ(j)

320
00:10:55,960 --> 00:10:57,270
现在我要对所有的用户

321
00:10:57,680 --> 00:10:59,340
求这个目标函数的和

322
00:10:59,490 --> 00:11:00,940
然后对整个优化目标

323
00:11:01,260 --> 00:11:03,700
求最小值

324
00:11:04,320 --> 00:11:05,570
最小化整个这个代价函数

325
00:11:06,730 --> 00:11:09,200
当我关于θ(1) θ(2)

326
00:11:09,380 --> 00:11:10,560
一直到θ(n_u)

327
00:11:11,360 --> 00:11:12,400
最小化

328
00:11:12,600 --> 00:11:14,130
这个函数时

329
00:11:14,270 --> 00:11:15,750
我就会得到对每个用户

330
00:11:16,030 --> 00:11:17,340
不同的参数向量

331
00:11:17,450 --> 00:11:18,720
然后我可以用它们

332
00:11:19,090 --> 00:11:20,460
来对所有的用户

333
00:11:20,530 --> 00:11:21,610
所有这n_u个用户

334
00:11:21,720 --> 00:11:23,150
来作出预测了

335
00:11:24,520 --> 00:11:26,560
把所有这些放在一起

336
00:11:27,180 --> 00:11:28,730
最上面的就是

337
00:11:28,880 --> 00:11:29,940
我们的最优化目标

338
00:11:30,170 --> 00:11:31,070
给这个项

339
00:11:31,930 --> 00:11:33,480
起一个名字吧

340
00:11:33,630 --> 00:11:35,520
就叫它J(θ(1), ... ,θ(n_u))

341
00:11:36,050 --> 00:11:37,280
同样地

342
00:11:37,590 --> 00:11:39,830
J还是我们要最小化的最优化目标函数

343
00:11:41,330 --> 00:11:42,500
接下来 为了

344
00:11:42,880 --> 00:11:44,310
求出这个最小值

345
00:11:44,500 --> 00:11:45,840
如果你想要用

346
00:11:46,150 --> 00:11:47,410
梯度下降来更新的话

347
00:11:47,530 --> 00:11:48,720
你可能会用到这些式子

348
00:11:49,900 --> 00:11:51,300
你可能会用

349
00:11:51,750 --> 00:11:53,310
θ(j)k

350
00:11:53,430 --> 00:11:56,190
减去学习速率α 乘以右边这一项

351
00:11:56,280 --> 00:11:57,540
对于k=0和k≠0的情况

352
00:11:58,160 --> 00:11:59,660
我们的式子有一点点区别

353
00:11:59,840 --> 00:12:01,460
因为我们的正则化项

354
00:12:01,960 --> 00:12:04,380
只对k不为0的θ(j)k

355
00:12:04,910 --> 00:12:06,430
进行正则化

356
00:12:06,610 --> 00:12:07,690
因此 我们不对

357
00:12:07,830 --> 00:12:09,470
θ0进行正则化

358
00:12:10,090 --> 00:12:11,610
因此这里在更新的时候

359
00:12:12,270 --> 00:12:13,580
k=0和k≠0会有一点区别

360
00:12:14,680 --> 00:12:16,080
这里这一项

361
00:12:16,250 --> 00:12:18,090
这实际上是

362
00:12:18,520 --> 00:12:20,790
你的最优化目标函数

363
00:12:21,090 --> 00:12:24,300
对你的参数

364
00:12:25,350 --> 00:12:28,270
求偏微分 对吧？

365
00:12:28,790 --> 00:12:30,280
因此 这实际上就是

366
00:12:30,680 --> 00:12:33,000
梯度下降法

367
00:12:33,230 --> 00:12:35,440
我已经算出了偏微分然后放在这里了

368
00:12:36,560 --> 00:12:39,580
如果你觉得这个

369
00:12:40,570 --> 00:12:41,810
梯度下降的更新

370
00:12:41,980 --> 00:12:42,870
看起来跟之前

371
00:12:43,050 --> 00:12:44,700
线性回归差不多的话

372
00:12:44,880 --> 00:12:47,250
那是因为这其实就是线性回归

373
00:12:48,190 --> 00:12:49,510
唯一的一点区别

374
00:12:49,780 --> 00:12:51,120
是在线性回归中

375
00:12:51,580 --> 00:12:52,600
我们有1/m项

376
00:12:52,990 --> 00:12:54,710
实际上这里我们也有

377
00:12:54,810 --> 00:12:56,770
1/m(j)项

378
00:12:57,550 --> 00:12:59,230
但在前面我们

379
00:12:59,370 --> 00:13:00,780
推导最优化目标函数时

380
00:13:01,270 --> 00:13:03,540
我们忽略了这个项 因此这里就没有了

381
00:13:04,440 --> 00:13:05,880
除此之外 实际上就是

382
00:13:06,080 --> 00:13:08,350
对所有的训练样本求和

383
00:13:08,530 --> 00:13:09,890
预测误差乘以

384
00:13:10,230 --> 00:13:13,390
xk 加上正则化项

385
00:13:14,900 --> 00:13:16,550
组成了这个导数项

386
00:13:18,120 --> 00:13:19,040
所以 如果你用

387
00:13:19,200 --> 00:13:20,360
梯度下降法的话

388
00:13:20,680 --> 00:13:22,140
你可以这样

389
00:13:22,440 --> 00:13:23,880
最小化代价函数J

390
00:13:24,110 --> 00:13:25,490
来学习出所有的参数

391
00:13:25,640 --> 00:13:26,980
当然 用这些微分项

392
00:13:27,090 --> 00:13:28,240
如果你愿意的话

393
00:13:28,440 --> 00:13:29,710
你也可以把它们用在

394
00:13:30,290 --> 00:13:31,710
更高级的优化算法里 比如聚类下降

395
00:13:31,810 --> 00:13:33,730
或者L-BFGS(Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm)

396
00:13:33,940 --> 00:13:35,930
或者别的方法 来最小化代价函数J

397
00:13:37,360 --> 00:13:38,450
通过这节课 你应该知道了

398
00:13:38,750 --> 00:13:40,510
怎样应用一种

399
00:13:41,000 --> 00:13:42,820
事实上是线性回归的一个变体

400
00:13:42,950 --> 00:13:45,460
来预测不同用户对不同电影的评分值

401
00:13:46,350 --> 00:13:47,510
这种具体的算法叫

402
00:13:48,030 --> 00:13:49,930
”基于内容的推荐“

403
00:13:50,040 --> 00:13:51,980
或者”基于内容的方法“

404
00:13:52,130 --> 00:13:53,200
因为我们假设

405
00:13:53,650 --> 00:13:55,430
我们有不同电影的特征

406
00:13:56,150 --> 00:13:57,330
我们有了电影

407
00:13:57,490 --> 00:13:58,610
内容的特征

408
00:13:58,700 --> 00:14:00,260
比如电影的爱情成分有多少？

409
00:14:01,280 --> 00:14:03,050
动作成分有多少？

410
00:14:03,430 --> 00:14:04,690
我们就是用电影的这些特征

411
00:14:04,780 --> 00:14:06,910
来进行预测

412
00:14:08,350 --> 00:14:09,770
但事实上

413
00:14:09,920 --> 00:14:11,300
对很多电影 我们并没有这些特征

414
00:14:11,820 --> 00:14:13,630
或者说 很难得到

415
00:14:13,850 --> 00:14:14,970
所有电影的特征

416
00:14:15,050 --> 00:14:16,160
很难知道 我们要卖的产品

417
00:14:16,460 --> 00:14:17,800
有什么样的特征

418
00:14:18,880 --> 00:14:20,430
所以在下一段视频中

419
00:14:20,590 --> 00:14:21,530
我们将谈到

420
00:14:22,010 --> 00:14:23,290
一种不基于内容的推荐系统

421
00:14:23,570 --> 00:14:24,710
我们应该如何处理

422
00:14:24,980 --> 00:14:26,090
也就是 没有别人

423
00:14:26,670 --> 00:14:28,420
给我们这些电影数据的特征信息

424
00:14:28,880 --> 00:14:30,300
【教育无边界字幕组】翻译：所罗门捷列夫 校对：竹二个 审核：Naplessss