Nos últimos vídeos, falamos sobre um algoritmo de filtragem colaborativa. Nesse vídeo, quero falar sobre a implementação vetorizada desse algoritmo, e sobre outras coisas que podemos fazer com ele. Por exemplo, uma das coisas que podemos fazer é: dado um produto, você pode encontrar outros produtos que sejam relacionados? Por exemplo, se o usuário procurou por um produto recentemente, há outros produtos relacionados, que você recomendaria a esse usuário? Vamos ver como podemos fazer isso. O que eu quero fazer é escrever, de forma alternativa, predições do algoritmo de filtragem colaborativa. Para começar, aqui está o nosso conjunto de dados, com 5 filmes, e eu vou pegar as avaliações de todos os usuários e agrupá-las em uma matriz. Então, aqui eu tenho 5 filmes e 4 usuários, e, portanto, essa matriz "Y" será uma matriz 5x4. Eu estou apenas pegando todos os dados, incluindo pontos de interrogação, e agrupando nessa matriz. O elemento "(i, j)" dessa matriz é, na verdade, o que previamente estávamos escrevendo como "y⁽ⁱʲ⁾", A avaliação do filme "i", dada pelo usuário "j". Dada a matriz "Y" de todas as avaliações que temos, há um modo alternativo de escrever todas as avaliações estimadas pelo algoritmo. Se você olhar a predição de um certo usuário, para um certo filme, o que o usuário "j" prediz para o filme "i", é dado por essa fórmula. Então, se você tiver uma matriz de avaliações preditas, você terá uma matriz como essa, onde a avaliação correspondente à entrada "(i, j)" é o que estimamos que o usuário "j" dará para o filme "i", é exatamente igual a isso: "(θ⁽ʲ⁾)ᵀ · (x⁽ⁱ⁾)". Essa é uma matriz onde o primeiro elemento, o elemento "(1, 1)" é uma predição da avaliação do usuário 1 para o filme 1. O elemento "(1, 2)" é a avaliação predita do usuário 2 para o filme 1. E essa é a avaliação predita do usuário 1 para o último filme. Esta avaliação é o que nós teríamos predito para esse valor, aquela avaliação é o que teríamos predito para aquele valor, e assim por diante. Agora, dada essa matriz de avaliações preditas, há uma maneira mais simples, vetorizada, de escrever isso. Se eu definir uma matriz "X", muito parecida com a que tivemos anteriormente, para regressão linear: "(X⁽¹⁾)ᵀ", "(X⁽²⁾)ᵀ" até "(X⁽ⁿᵐ⁾)ᵀ". Tomo todos os atributos dos filmes, e empilho-los em linhas. Assim, se pensarmos em cada filme como um exemplo, e empilharmos todos os atributos de diferentes filmes em linhas, e encontrarmos uma matriz "Θ", formada pelo vetor de parâmetros de cada usuário, empilhado em linhas, dessa forma. Então esse é "(θ⁽¹⁾)ᵀ", o vetor de parâmetros para o usuário 1, esse é "(θ⁽²⁾)ᵀ". Então, você os empilha em linhas dessa forma, para definir uma matriz "Θ". Eu tenho "nᵤ" vetores de parâmetros, todos empilhados em linhas. Agora, dada essa definição para matriz "X" e essa definição para a matriz "Θ", para encontrar uma maneira vetorizada de computar a matriz de todas as predições, basta calcular "X · (Θᵀ)", que é uma maneira vetorizada de computar essa matriz acima. Dando o nome ao algoritmo de filtragem colaborativa que estamos utilizando, ele é também chamado "fatoração de matriz de baixo posto". Portanto, se você ouvir alguém falar sobre fatoração de matriz de baixo posto, é exatamente o algoritmo sobre o qual estamos falando. Esse termo vem de uma propriedade que essa matriz "X · (Θᵀ)" tem, uma propriedade matemática da álgebra linear chamada "baixo posto". Daí vem o nome "fatoração de matriz de baixo posto" para esse algoritmo. Por causa dessa propriedade de baixo posto da matriz "X · (Θᵀ)". Caso você não saiba o que significa "baixo posto", ou uma "matriz de baixo posto", não se preocupe. Você não precisa saber isso para usar esse algoritmo. Mas se você for um especialista em álgebra linear, é isso que dá a esse algoritmo esse outro nome de "fatoração de matriz de baixo posto". Finalmente, tendo executado esse algoritmo de filtragem colaborativa, você pode usar os atributos aprendidos para encontrar filmes semelhantes. Assim, para cada produto "i", ou filme "i", nós aprendemos um vetor de atributos "x⁽ⁱ⁾". Então, quando você aprende um conjunto de atributos, você não sabe, a princípio, quais serão esses atributos. Mas, quando você roda o algoritmo, idealmente, essas variáveis vão capturar os aspectos importantes desses filmes, desses produtos, que fazem com que alguns usuários gostem de certos filmes e com que outros gostem de outros filmes. Assim, talvez você termine aprendendo um vetor onde "x₁ = romance", "x₂ = ação", e, similarmente a um vídeo anterior, talvez você aprenda outra variável, "x₃", que diz se isso é uma comédia, e outra variável "x₄", que é outra coisa. Você tem um total de "n" variáveis, e, depois de aprender esses atributos, costuma ser difícil dar a eles uma interpretação do que essas variáveis realmente são, de forma que possamos entender. Mas na prática, embora seja difícil visualizar essas variáveis, é difícil descobrir o que elas são, normalmente, as variáveis aprendidas são muito significativas para capturar as propriedades mais importantes de um filme, que fazem com que você goste ou não desse filme. Agora, digamos que você queira resolver o seguinte problema: digamos que você tenha um filme "i", e você queira encontrar outros filmes "j" que sejam relacionados ao filme "i". Por que você ia querer fazer isso? Bom, talvez você tenha um usuário navegando pelos filmes, que está assistindo ao filme "j", qual é a recomendação adequada para ele, após assistir ao filme "j"? Ou, se alguém comprou o filme "j" recentemente, que filme seria razoável recomendar a ele, para que ele considere comprá-lo? Então, após aprendermos esses vetores de atributos, isso nos dá uma maneira conveniente de medir a similaridade entre filmes. Se o filme "i" tem um vetor de atributos "x⁽ⁱ⁾", e se você puder encontrar um outro filme "j", tal que a distância entre "x⁽ⁱ⁾" e "x⁽ⁱ⁾" seja pequena, isso é uma forte indicação de que os filmes "j" e "i" são similares, pelo menos no sentido em que alguém que goste do filme "i" talvez goste também do filme "j". Então, apenas recapitulando, se o seu usuário está assitindo a algum filme "i", e você quer encontrar os 5 filmes mais similares a esse filme, para recomendar 5 novos filmes a esse usuário, você pode encontrar os 5 filmes "j" com as menores distâncias entre os atributos desses filmes. Isso sugere alguns filmes para recomendar. Com isso, espero que agora você saiba como usar uma implementação vetorizada para calcular todas as classificações estimadas de todos os usuários e todos os filmes, e também fazer coisas como usar as variáveis aprendidas para encontrar quais são os filmes ou produtos que estão relacionados entre si.
Tradução: Pablo de Morais Andrade