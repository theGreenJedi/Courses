1
00:00:00,530 --> 00:00:01,650
在上几节视频中

2
00:00:01,730 --> 00:00:03,890
我们谈到了协同过滤算法

3
00:00:04,830 --> 00:00:05,890
本节视频中我将会

4
00:00:05,970 --> 00:00:07,120
讲到有关

5
00:00:07,490 --> 00:00:09,090
该算法的向量化实现

6
00:00:09,980 --> 00:00:12,670
以及说说有关该算法你可以做的其他事情

7
00:00:13,340 --> 00:00:14,520
举一个例子

8
00:00:14,600 --> 00:00:15,830
其中一个你可以做得是

9
00:00:16,180 --> 00:00:17,390
当给出一件产品时

10
00:00:17,770 --> 00:00:19,160
你能否找到与之相关的其它产品

11
00:00:19,270 --> 00:00:20,210
我们不妨再举一个例子

12
00:00:20,490 --> 00:00:23,140
一位用户最近看上一件产品

13
00:00:23,650 --> 00:00:24,990
有没有其它相关的产品

14
00:00:25,520 --> 00:00:27,170
你可以推荐给他

15
00:00:27,620 --> 00:00:28,980
好的 让我们看看我们能做什么

16
00:00:30,170 --> 00:00:31,190
我将要做的是

17
00:00:31,550 --> 00:00:33,520
实现一种选择的方法

18
00:00:33,740 --> 00:00:35,710
写出协同过滤算法的预测情况

19
00:00:37,370 --> 00:00:38,590
首先

20
00:00:38,960 --> 00:00:40,440
我们有关于五部电影

21
00:00:40,750 --> 00:00:41,880
的数据集

22
00:00:42,160 --> 00:00:43,150
我将要做的是

23
00:00:43,390 --> 00:00:44,520
将这些用户的电影评分

24
00:00:44,850 --> 00:00:46,500
进行分组并存到

25
00:00:47,080 --> 00:00:48,800
一个矩阵中

26
00:00:49,200 --> 00:00:51,390
看这里我们有五部电影

27
00:00:51,670 --> 00:00:53,390
以及四位用户

28
00:00:53,670 --> 00:00:54,550
那么 这个矩阵 Y

29
00:00:54,910 --> 00:00:57,110
就是一个5行4列的矩阵

30
00:00:57,340 --> 00:00:58,770
它将这些电影的用户评分数据都存在矩阵里

31
00:00:59,820 --> 00:01:02,390
包括问号标注出的 将这些数据分组到这个矩阵中

32
00:01:03,290 --> 00:01:04,470
当然 矩阵中的这些元素

33
00:01:04,650 --> 00:01:06,400
在(i, j)位置的元素

34
00:01:06,500 --> 00:01:07,860
其实是

35
00:01:08,060 --> 00:01:09,710
我们先前写的

36
00:01:10,520 --> 00:01:12,090
y(i, j)

37
00:01:12,220 --> 00:01:13,480
这个评分是用户 j

38
00:01:14,140 --> 00:01:15,640
对电影 i 给出的评分

39
00:01:16,070 --> 00:01:17,290
由这个矩阵 Y 中所有的

40
00:01:17,430 --> 00:01:18,520
评分数据

41
00:01:18,700 --> 00:01:20,500
就可以选择用另一种方法

42
00:01:20,880 --> 00:01:23,340
写出我们对于所有的预测评分来

43
00:01:24,320 --> 00:01:26,210
具体来说

44
00:01:26,430 --> 00:01:27,540
如果你看到某位

45
00:01:27,920 --> 00:01:29,480
用户预测

46
00:01:29,690 --> 00:01:31,250
某部电影的评分

47
00:01:31,950 --> 00:01:35,540
这个公式就是用户 j 对电影 i 的预测评分

48
00:01:37,010 --> 00:01:38,570
所以 如果你有

49
00:01:39,440 --> 00:01:40,330
预测评分矩阵

50
00:01:40,910 --> 00:01:42,000
你就会有

51
00:01:42,180 --> 00:01:43,600
以下的这个

52
00:01:45,030 --> 00:01:48,140
有着(i, j)位置数据的矩阵

53
00:01:49,650 --> 00:01:51,440
它对应的评分是

54
00:01:52,000 --> 00:01:54,020
我们对用户j

55
00:01:54,460 --> 00:01:55,690
对电影 i 的评分的预测值

56
00:01:57,130 --> 00:01:58,440
准确说来其值等于

57
00:01:58,790 --> 00:02:00,680
θ(j)转置乘x(i)

58
00:02:00,900 --> 00:02:01,940
你应该也要明白

59
00:02:02,520 --> 00:02:04,310
这个矩阵中的第一个元素

60
00:02:04,750 --> 00:02:05,930
即我们第一行第一列的元素

61
00:02:06,220 --> 00:02:07,450
是第一位用户

62
00:02:07,760 --> 00:02:09,360
对第一部电影的预测分数

63
00:02:09,560 --> 00:02:11,070
这个第一行第二列的

64
00:02:11,430 --> 00:02:12,680
元素的预测评分

65
00:02:13,470 --> 00:02:14,640
是第二位用户

66
00:02:14,930 --> 00:02:16,070
对第一部电影的打分等等

67
00:02:16,630 --> 00:02:18,670
下面的这个

68
00:02:19,000 --> 00:02:20,130
预测评分是第一位用户

69
00:02:20,930 --> 00:02:23,380
对最后一部电影的评分

70
00:02:23,640 --> 00:02:25,100
你应该知道

71
00:02:25,400 --> 00:02:26,870
这个评分

72
00:02:27,020 --> 00:02:28,050
是我们之前对于这个值预测的结果

73
00:02:29,050 --> 00:02:32,470
同时这个评分

74
00:02:32,650 --> 00:02:33,570
是我们对另外一个值预测的结果

75
00:02:33,910 --> 00:02:35,080
等等

76
00:02:36,180 --> 00:02:37,480
现在 我们给出

77
00:02:37,560 --> 00:02:39,290
预测评分矩阵

78
00:02:39,610 --> 00:02:42,670
这里给出一个更简化的向量化的方法

79
00:02:43,640 --> 00:02:44,640
具体来说

80
00:02:45,120 --> 00:02:46,850
如果我定义这个矩阵 X

81
00:02:46,970 --> 00:02:48,090
这就会有点

82
00:02:48,370 --> 00:02:50,980
类似我们先前在线性回归里面的矩阵

83
00:02:52,070 --> 00:02:53,820
x(1)转置

84
00:02:55,050 --> 00:02:57,060
x(2)转置一直到

85
00:02:58,530 --> 00:03:01,740
x(nm)的转置

86
00:03:02,420 --> 00:03:03,320
我会把这些所有

87
00:03:04,210 --> 00:03:05,670
有关电影的特征

88
00:03:06,140 --> 00:03:07,260
按行堆叠起来

89
00:03:07,950 --> 00:03:08,860
所以 你可以把

90
00:03:08,980 --> 00:03:09,810
每一部电影想象成一个范例

91
00:03:10,350 --> 00:03:11,200
把这些范例即特征 按行堆叠起来

92
00:03:11,670 --> 00:03:13,460
每一部电影就是一行

93
00:03:14,290 --> 00:03:16,160
然后我们要做的是

94
00:03:16,280 --> 00:03:18,550
定义一个大写的Θ矩阵

95
00:03:19,870 --> 00:03:20,840
接下来我要做的是将

96
00:03:21,180 --> 00:03:22,490
接下来我要做的是将

97
00:03:22,750 --> 00:03:25,780
每一位用户的参数向量θ(j)

98
00:03:26,280 --> 00:03:28,520
像这样按行堆叠起来

99
00:03:28,790 --> 00:03:29,690
这是θ(1)

100
00:03:30,220 --> 00:03:31,880
是第一位用户的参数向量

101
00:03:33,430 --> 00:03:36,100
这个是θ(2)

102
00:03:37,040 --> 00:03:38,100
所以你需要

103
00:03:38,360 --> 00:03:39,470
像这样将这些用户的参数向量按行堆叠

104
00:03:39,650 --> 00:03:41,530
以定义这个大写Θ矩阵

105
00:03:42,070 --> 00:03:43,830
在这个矩阵里

106
00:03:45,870 --> 00:03:48,410
我们有nu个参数向量都像这样按行堆叠起来

107
00:03:50,000 --> 00:03:51,390
现在我们已经给出了

108
00:03:52,080 --> 00:03:53,400
针对大写X矩阵以及

109
00:03:53,590 --> 00:03:54,870
大写Θ矩阵的定义

110
00:03:55,820 --> 00:03:56,970
为了能有

111
00:03:57,290 --> 00:03:59,330
一个向量化的方法以计算

112
00:03:59,420 --> 00:04:00,330
这些矩阵的预测值

113
00:04:01,060 --> 00:04:03,570
你可以计算大写X矩阵乘

114
00:04:04,710 --> 00:04:07,050
大写Θ矩阵的转置

115
00:04:07,160 --> 00:04:08,380
这样就给出了一种向量化的方法

116
00:04:08,570 --> 00:04:10,530
以计算这个矩阵

117
00:04:11,680 --> 00:04:12,460
我们用的这个协同过滤

118
00:04:12,480 --> 00:04:15,220
算法还有另外一个名字

119
00:04:16,070 --> 00:04:17,190
我们现在正在使用的这个算法

120
00:04:17,660 --> 00:04:19,840
也被称作是

121
00:04:21,240 --> 00:04:22,540
低秩矩阵分解

122
00:04:24,280 --> 00:04:25,410
所以如果你听到

123
00:04:25,620 --> 00:04:26,760
人们说道低秩矩阵分解

124
00:04:27,210 --> 00:04:29,490
准确说来

125
00:04:30,390 --> 00:04:32,100
就应该是我们现在正在讲的这个算法

126
00:04:32,590 --> 00:04:33,900
这一项来自

127
00:04:33,990 --> 00:04:36,100
这个矩阵 

128
00:04:36,770 --> 00:04:38,880
乘Θ的转置

129
00:04:39,110 --> 00:04:40,780
其有一个数学属性

130
00:04:41,030 --> 00:04:42,410
在代数中被称为

131
00:04:42,670 --> 00:04:43,820
低秩矩阵

132
00:04:44,720 --> 00:04:45,800
同时

133
00:04:46,060 --> 00:04:47,190
 这也是对于这个算法给出

134
00:04:47,340 --> 00:04:48,570
这个低秩矩阵分解名字的原因

135
00:04:48,930 --> 00:04:50,240
因为这个

136
00:04:50,410 --> 00:04:53,580
矩阵X乘Θ的转置的低秩属性

137
00:04:54,830 --> 00:04:55,640
如果你不懂什么是

138
00:04:55,910 --> 00:04:57,310
低秩或者低秩矩阵

139
00:04:57,620 --> 00:04:59,770
也不用担心

140
00:04:59,970 --> 00:05:02,820
你真的不需要为了使用这个算法而知道这些

141
00:05:03,740 --> 00:05:04,790
但若你对

142
00:05:04,890 --> 00:05:06,110
线性代数很熟悉

143
00:05:06,320 --> 00:05:07,580
那你就会知道这个算法是如何给出的

144
00:05:07,850 --> 00:05:12,370
以及有关低秩矩阵分解的含义

145
00:05:12,620 --> 00:05:14,090
最后

146
00:05:14,300 --> 00:05:16,350
在运行了协同过滤算法后

147
00:05:17,310 --> 00:05:18,160
仍有一些你可以去做的事

148
00:05:18,530 --> 00:05:20,060
比如说

149
00:05:20,320 --> 00:05:23,510
为了找到某部相关的电影来学习特征

150
00:05:25,060 --> 00:05:26,810
准确说来 对于每一个产品 

151
00:05:27,050 --> 00:05:27,810
好比每一部电影 

152
00:05:28,810 --> 00:05:30,970
我们有特征向量x(i)

153
00:05:31,740 --> 00:05:32,880
那么 你就会知道 当你

154
00:05:32,930 --> 00:05:34,220
学习某个特征

155
00:05:34,590 --> 00:05:35,420
其实并不需要知道

156
00:05:35,610 --> 00:05:37,850
这些不同的特征将会变成什么样的

157
00:05:37,940 --> 00:05:39,550
 但如果你运行这个算法

158
00:05:39,990 --> 00:05:41,690
这些特征将会完美地捕获

159
00:05:41,930 --> 00:05:43,490
这些重要的方面

160
00:05:43,730 --> 00:05:45,340
在你对不同电影或者产品的打分上面

161
00:05:45,480 --> 00:05:47,120
到底什么是造成

162
00:05:47,610 --> 00:05:48,600
某些用户

163
00:05:48,930 --> 00:05:49,830
喜欢某些电影以及

164
00:05:50,210 --> 00:05:51,670
是什么造成了这些电影其它不同的电影呢

165
00:05:52,470 --> 00:05:53,380
也许你刚进行完

166
00:05:53,540 --> 00:05:55,050
特征参数学习

167
00:05:55,260 --> 00:05:56,550
你有爱情指数x1

168
00:05:57,060 --> 00:05:59,180
动作指数x2

169
00:05:59,460 --> 00:06:00,590
这与之前我们的视频内容类似

170
00:06:00,710 --> 00:06:02,100
或许你的参数特征里有x3

171
00:06:02,210 --> 00:06:04,520
是一个表现喜剧程度的指数

172
00:06:05,330 --> 00:06:07,000
还有特征x4等其它参数特征

173
00:06:07,270 --> 00:06:09,750
这时你就有了 n 个

174
00:06:09,940 --> 00:06:11,600
参数特征

175
00:06:12,610 --> 00:06:14,420
在你完成了这些参数特征的学习后

176
00:06:14,750 --> 00:06:16,030
实际中参数特征的学习

177
00:06:16,420 --> 00:06:18,120
是比较困难的

178
00:06:18,390 --> 00:06:19,980
并提出一个人类能理解

179
00:06:20,810 --> 00:06:22,850
对于这些特征实际是什么的解释更困难

180
00:06:22,950 --> 00:06:24,540
但在实践中

181
00:06:24,620 --> 00:06:27,480
这些特征是很难以可视化的

182
00:06:28,100 --> 00:06:29,570
也很难计算出这些特征到底是什么

183
00:06:31,070 --> 00:06:32,160
通常来说

184
00:06:32,410 --> 00:06:33,400
特征学习对于

185
00:06:33,960 --> 00:06:35,250
捕获哪些是

186
00:06:35,870 --> 00:06:37,120
电影的重要或显著的属性

187
00:06:37,880 --> 00:06:39,300
是很有意义的

188
00:06:39,710 --> 00:06:41,800
也正是这样才有了你对某些电影的喜欢或不喜欢

189
00:06:41,860 --> 00:06:44,950
现在我们来解决下面这个问题

190
00:06:45,970 --> 00:06:47,410
你有一部电影 i

191
00:06:47,790 --> 00:06:48,980
你想找到

192
00:06:49,120 --> 00:06:50,750
与电影 i 相关的

193
00:06:51,620 --> 00:06:52,680
其它某部电影 j

194
00:06:53,150 --> 00:06:54,770
那么是什么原因让你想用这个方法呢

195
00:06:54,920 --> 00:06:56,120
可能有一位用户

196
00:06:56,320 --> 00:06:57,840
正在浏览一些电影

197
00:06:58,360 --> 00:07:00,210
他们当前看的电影是 i

198
00:07:00,550 --> 00:07:01,820
一个什么样的理由才能让

199
00:07:02,350 --> 00:07:04,110
他们看完电影 i 之后被推荐另一部电影 j 呢

200
00:07:04,530 --> 00:07:06,040
或者说某人最近买了一部电影 

201
00:07:06,330 --> 00:07:07,470
一部什么样的其它电影

202
00:07:07,730 --> 00:07:11,000
我们有理由推荐他们进行下一次购买呢

203
00:07:12,190 --> 00:07:13,000
现在既然你已经

204
00:07:13,080 --> 00:07:14,540
对特征参数向量进行了学习

205
00:07:14,640 --> 00:07:16,080
那么我们就会有一个很方便的方法

206
00:07:16,250 --> 00:07:17,930
来度量两部电影之间的相似性

207
00:07:18,670 --> 00:07:20,530
例如说

208
00:07:21,460 --> 00:07:22,340
电影i有一个特征向量x(i)

209
00:07:23,290 --> 00:07:24,200
你是否能找到一部

210
00:07:24,640 --> 00:07:27,500
不同的电影 j

211
00:07:27,710 --> 00:07:29,300
保证两部电影的特征向量

212
00:07:29,780 --> 00:07:30,800
之间的距离x(i)和x(j)很小

213
00:07:33,080 --> 00:07:34,010
那就能

214
00:07:34,430 --> 00:07:36,980
很有力地表明

215
00:07:37,830 --> 00:07:41,360
电影 i 和电影 j 在某种程度上有相似

216
00:07:42,320 --> 00:07:44,080
至少在某种意义上 某些人喜欢电影 i

217
00:07:44,200 --> 00:07:46,950
或许更有可能也对电影 j 感兴趣

218
00:07:47,760 --> 00:07:49,940
总结一下

219
00:07:50,590 --> 00:07:52,130
当用户在看

220
00:07:52,510 --> 00:07:53,710
某部电影 i 的时候

221
00:07:54,150 --> 00:07:55,060
如果你想找5部

222
00:07:55,310 --> 00:07:56,640
与电影 非常相似的电影

223
00:07:56,900 --> 00:07:57,860
为了能给用户推荐

224
00:07:58,230 --> 00:07:59,590
5部新电影

225
00:07:59,690 --> 00:08:00,650
你需要做的是

226
00:08:00,970 --> 00:08:02,260
找出电影 j

227
00:08:02,340 --> 00:08:03,880
在这些不同的电影中

228
00:08:04,190 --> 00:08:05,680
与我们要找的电影 i 的距离最小

229
00:08:06,550 --> 00:08:09,220
这样你就能给你的用户推荐几部不同的电影了

230
00:08:10,010 --> 00:08:11,500
通过这个方法

231
00:08:11,680 --> 00:08:13,350
希望你能知道

232
00:08:13,700 --> 00:08:15,930
如何进行一个向量化的计算来

233
00:08:16,560 --> 00:08:18,130
对所有的用户和所有的电影

234
00:08:18,210 --> 00:08:20,280
进行评分计算

235
00:08:20,390 --> 00:08:21,720
同时希望你也能掌握

236
00:08:21,920 --> 00:08:23,300
通过学习特征参数

237
00:08:23,930 --> 00:08:25,360
来找到相关电影

238
00:08:25,480 --> 00:08:27,490
和产品的方法 【教育无边界字幕组】翻译：Yuens 校对/审核：所罗门捷列夫