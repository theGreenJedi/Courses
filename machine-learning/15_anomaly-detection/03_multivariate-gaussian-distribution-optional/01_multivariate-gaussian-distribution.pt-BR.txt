Neste vídeo e no próximo, quero falar sobre uma possível extensão para o algoritmo de detecção de anomalias que desenvolvemos até agora. Essa extensão usa algo chamado distrubuição gaussiana multivariada, e ela tem algumas vantagens e desvantagens, e ela consegue notar algumas anomalias que o algoritmo anterior não conseguia. Como motivação, vamos começar com um exemplo. Digamos que nossos dados parecem com o que desenhei aqui. Vou usar o exemplo de máquinas de monitoramento em um data center, monitorando seus computadores. Meus recursos são "x₁", a utilização da CPU, e "x₂", que pode ser a utilização de memória. Se eu tomar meus dois recursos, "x₁" e "x₂", e modelá-los como gaussianas, aqui está um gráfico dos meus recursos "x₁" e os recursos "x₂", e se eu ajustar uma gaussiana a eles, talvez ache uma gaussiana como esta, então aqui está "p(x₁)", que depende dos parâmetros "μ₁" e "σ₁²", este é o uso de memória e encontro uma gaussiana mais ou menos como essa, e este é "p(x₂)", que depende de "μ₂" e "σ₂²". Então é assim a forma como o algoritmo de detecção de anomalias modela "x₁" e "x₂". Agora, digamos que no conjunto de teste existe um exemplo como este. Na localização daquele "X" em verde, o valor de "x₁" é cerca de 0.4, e o valor de "x₂" é cerca de 1.5. Agora, se você olhar para os dados, parece que a maior parte dos dados fica nesta região, e o "X" verde está bem longe dos outros exemplos que já vi. Parece que ele deveria ser detectado como uma anomalia. Nos meus dados, dos exemplos normais, parece que a utilização da CPU e da memória crescem mais ou menos linearmente um com o outro. Assim, se uma máquina está usando muita CPU, a utilização de memória também será alta, enquanto neste exemplo em verde a utilização de CPU é bem baixa, mas a de memória é muito alta, e eu não vi isso nenhuma vez no conjunto de trieno. Isso deveria ser reconhecido como anomalia. Mas vamos ver o que o algoritmo de detecção de anomalias vai fazer. Bom, na utilização de CPU, ele está por aqui, perto de 0.5, e essa probabilidade é razoavelmente alta, não muito diferente dos outros exemplos que vimos, 0.5 está próximo de outros exemplos. Para a utilização de memória, o valor é cerca de 1.5, por aqui. Novamente, está mais para o final da gaussiana, mas os valores aqui e aqui não são muito diferentes de outros exemplos que já vimos, e, assim, "p(x₁)" será alta, razoavelmente alta. "p(x₂)" também. Se você olhar para o gráfico, este ponto aqui não parece muito ruim, e se olhar para o outro gráfico, por aqui, também não parece muito ruim. Afinal, temos exemplos com utilização de memória ainda maior, ou com uso de CPU menor, e esse exemplo não parece tão anômalo. Assim, um algoritmo de detecção de anomalias falhará em detectar esse ponto como anomalia. O que ocorre é que o algoritmo de detecção de anomalias não está conseguindo perceber que esta elipse em azul é a região de alta probabilidade, e que exemplos aqui têm alta probabilidade, e os exemplos no outro círculo têm probabilidade menor, e exemplos no mais externo têm probabilidade ainda menor, e o algoritmo pensa que o "X" em verde tem alta probabilidade. Na verdade, ele tende a pensar que tudo nessa região, tudo na circunferência que estou mostrando tem probabilidade mais ou menos igual. Ele não percebe que algo por aqui tem, na verdade, probabilidade muito menor que algo por aqui. Para resolver esse problema, vamos desenvolver uma versão modificada do algoritmo de detecção de anomalias, usando algo chamado "distribuição gaussiana multivariada", também chamada distribuição normal multivariada. Vamos fazer o seguinte. Temos recursos "x" em ℝⁿ, e, em vez de "p(x₁)" e "p(x₂)" separademente, vamos modelar "p(x)", tudo de uma vez, um modelo para "p(x)", com o vetor "x" inteiro. Assim, os parâmetros da distribuição gaussiana multivariada são μ, um vetor, e σ, que é uma matriz "n" por "n", chamada matriz de covariância. E essa é parecida com a matriz de covariância que vimos quando estávamos trabalhando com PCA, o algoritmo de análise de componentes principais. Para completar, vou escrever a fórmula para a distribuição gaussiana multivariada. Dado um vetor "x", e com a distribuição parametrizada pelos vetores μ e Σ, a probabilidade de "x" (novamente, não existe necessidade nenhuma de decorar essa fórmula, é só pesquisar quando você precisar usá-la), mas esta é a expressão para a probabilidade de "x". a expressão para a probabilidade de "x". a expressão para a probabilidade de "x". E isto aqui, o valor absoluto de "Σ", isto aqui quando escrevemos este símbolo, isso é chamado o determinante de "Σ", uma função matemática que recebe uma matriz, mas você não precisa saber bem o que é o determinante de uma matriz, tudo o que você precisa é saber que você pode calculá-la no Octave usando o comando "det(Σ)". o comando "det(Σ)". Novamente, só para deixar claro: Nessa expressão, esses "Σ" aqui, são matrizes "n" por "n". Isto não é uma somatória, o sigma é uma matriz "n" por "n". Essa é a fórmula para "p(x)", porém, e mais interessante e importante, qual é o formato de "p(x)"? Vamos dar uma olhada em alguns exemplos de distribuições gaussianas multivariadas. Vamos dar uma olhada exemplo com duas dimensões, digamos, "n = 2", eu tenho dois recursos, "x₁" e "x₂". Vamos tomar "μ = 0" e Σ é igual a esta matriz aqui, com "1" na diagonal e "0" nos elementos fora da diagonal. Essa matriz também é chamada matriz identidade. Nesse caso, "p(x)" vai ser isto, e o que estou mostrando na figura é para um valor específico de "x₁" e para um valor específico de "x₂", e a altura dessa superfície é o valor de "p(x)". Com essa configuração, "p(x)" é a maior possível quando "x₁ = x₂ = 0", que é o pico dessa distribuição gaussiana, e a probabilidade decai com essa gaussiana bidimensional, essa superfície bidimensional em formato de sino. Abaixo está o mesmo gráfico, mas traçado utilizando curvas de nível, usando cores diferentes, assim, o vermelho intenso no meio corresponde aos valores mais altos, e os valores decrescem com o amarelo representando um valor pouco menor, ciano valores ainda menores, e o azul escuro representando os menores valores, assim, isso é a mesma figura porém vista de cima, utilizando cores. Portanto, com essa distribuição, pode-se ver que a maior probabilidade está em tordo de "(0,0)" e à medida que se afasta de "(0,0)", a probabilidade de "x₁" e "x₂" diminui. Agora vamos tentar variar alguns dos parâmetros e ver o que acontece. Vamos mudar Σ, diminui-lo um pouquinho. Σ é uma matriz de covariância, que mede a variância, a variabilidade dos recursos "x₁" e "x₂". Se diminuirmos Σ, o que acontece é uma diminuição na largura dessa elevação, e a altura também aumenta um pouco, porque o volume abaixo da superfície é igual a 1. Ou seja, a integral, o volume abaixo da superfície, é igual a 1, porque distribuições de probabilidade devem ter integral igual a 1. Mas se você diminuir a variância, é mais ou menos como diminuir σ², você acaba com uma distribuição mais estreita e mais alta. Dá pra ver aqui que as elipses concêntricas diminuíram um pouco. Em contraste, se aumentarmos o valor de Σ para os valores 2 e 2 na diagonal, ou seja, duas vezes a identidade, você acaba com uma gaussiana muito mais larga e baixa. Ou seja, a largura aumenta bastante. É difícil ver, mas isso ainda é uma elevação em forma de sino, só que muito achatada, ela ficou mais larga, e a variância, ou variabilidade de "x₁" e "x₂" aumenta. Aqui estão mais alguns exemplos. Agora, vamos tentar variar um dos elementos de Σ de cada vez. Digamos que mudo Σ para o valor 0.6 aqui, e 1 ali. O que isso faz é reduzir a variância do primeiro recurso, "x₁", mantendo a variância do segundo recurso, "x₂", igual. Com essa configuração de parâmetros, você pode modelar coisas como isso. "x₁" tem variância menor, e "x₂" tem variância maior. Agora se eu fizer isto, se alterar essa matriz com os valores 2 e 1, também será possível modelar exemplos nos quais "x₁" pode tomar uma gama rande de valores enquanto "x₂" tem uma gama relativamente menor. Isso se reflete na figura também, onde a distribuição decresce mais devagar quando "x₁" se afasta do 0, e diminui muito rapidamente à medida que "x₂" se afasta de 0. Similarmente, se fôssemos modificar este elemento da matriz, parecido com o slide anterior, exceto que variar aqui quer dizer que "x₂" pode assumir uma gama pequena de valores, assim, se isto é 0.6, notamos agora que "x₂" tende a assumir uma gama menor que no exemplo original, enquanto se colocássemos esse valor igual a 2, implica que "x₂" tem uma gama muito maior de valores. Agora, uma das coisas legais da distribuição gaussiana multivariada é que você também pode usá-la para modelar correlações entre dados. Ou seja, podemos usá-la para modelar o fato de "x₁" e "x₂" tenderem a ser altamente correlacionados entre si, por exemplo. Especificamente, se você começar a mudar as entradas fora da diagonal da matriz de covariância, você pode encontrar tipos diferentes de distribuições. Assim, à medida que aumento as entradas fora da diagonal de 0.5 para 0.8, encontro uma distribuição que tem um pico cada vez mais fino ao longo da reta definida pela equação "x = y". Aqui, o gráfico mostra que "x" e "y" tendem a crescer juntos, e as regiões com alta probabilidade são ou "x₁" é grande e "x₂" é grande, ou "x₁" é pequeno e "x₂" é pequeno. Ou algo entre os dois. E à medida que esta entrada, 0.8, fica maior, você encontra distribuições gaussianas onde toda a probabilidade está concentrada nessa região estreita, onde "x₁" é aproximadamente igual a "x₂". Essa é uma distribuição muito alta e fina, ao longo dessa região central onde "x₁" é próximo de "x₂". Isso no caso em que as entradas são positivas. Se elas fossem negativas, como de "-0.5" a "-0.8", o que encontramos é um modelo onde colocamos a maioria da probabilidade nessa região de correlação negativa entre "x₁" e "x₂". A maior parte da probabilidade está nessa região, onde "x₁" é aproximadamente "-x₂", em vez de "x₁ = x₂". Isso captura uma correlação negativa entre "x₁" e "x₂". Então é assim isso te dê uma noção das diferentes distribuições que a distribuição gaussiana multivariada consegue capturar. Além de variar a matriz de covariância Σ, a outra coisa que podemos fazer é variar também o parâmetro μ, assim, originalmente tínhamos "μ = [0;0]", e a distribuição estava centrada em torno de "x₁ = 0", "x₂ = 0", o pico da distribuição está aqui, enquanto se variarmos o valor de μ, o pico da distribuição varia. Se "μ = [0; 0.5]", o pico estará em "x₁ = 0" e "x₂ = 0.5", portanto, o pico, ou centro, da distribuição foi deslocado, e se "μ = [1.5; -0.5]", similarmente, o pico da distribuição foi deslocado para outro lugar, correspondente a "x₁ = 1.5" e "x₂ = -0.5", assim, variar o parâmetro μ simplesmente desloca o centro da distribuição toda. Espero que, olhando para todas essas figuras, você tenha uma ideia do tipo de distribuições de probabilidade que a distribuição gaussiana multivariada possibilida capturar. A grande vantagem é que ela permite capturar quando você poderia esperar que dois recursos estejam positivamente ou negativamente correlacionados. No próximo vídeo iremos pegar essa distribuição gaussiana multivariada e aplicá-la em detecção de anomalias.