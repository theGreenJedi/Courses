No último vídeo, falamos sobre a Distribuição Gaussiana Multivariada, e vimos alguns exemplos de distribuições que podemos modelar, variando os parâmetros Mu (μ) e Sigma (Σ). Neste vídeo, vamos usar essas ideias e aplicá-las para desenvolver outro algoritmo de Detecção de Anomalias. Relembrando, a distribuição Gaussiana Multivariada, ou Normal Multivariada, tem dois parâmetros, "μ" e "Σ". Onde, "μ" é um vector n-dimensional, e "Σ", a matriz de covariância, uma matriz n x n. Aqui está a fórmula para a probabilidade de "x", parametrizada por "μ" e "Σ". E, à medida que variamos "μ" e "Σ", obtemos uma série de diferentes distribuições. Aqui estão 3 exemplos que vimos no vídeo anterior. Então, vamos falar sobre o ajuste dos parâmetros, ou a estimação dos parâmetros. A questão, como de costume, é: tendo um conjunto de exemplos x⁽¹⁾ até x⁽ᵐ⁾, onde cada um é um vector n-dimensional. E, acredito que meus exemplos vêm de uma distribuição Gaussiana multivariada, como podemos estimar os parâmetros "μ" e "Σ"? A fórmula usual para estimá-los, é definir "μ" como a média dos exemplos de treinamento, e "Σ" igual a essa fórmula. Na verdade isto é igual ao "Σ" que escrevemos quando usamos o PCA, ou o algoritmo de Análise dos Componentes Principais. Plugamos os valores nas duas fórmulas, e o resultado será a estimativa para os parâmetros "μ" e "Σ". É assim que se estima "μ" e "Σ" a partir de um conjunto de dados. Vamos plugar esse método em um algoritmo de Detecção de Anomalias. Então, como juntamos tudo isto para desenvolver um algoritmo de Detecção de Anomalias? Fazemos assim: Primeiro tomamos o conjunto de treino, e calibramos o modelo, ajustando p(x), ajustando "μ" e "Σ", como no slide anterior. Depois, quando temos um novo exemplo "x". Vamos tomar esse exemplo de teste, em verde, aqui em cima. Um exemplo de teste. Dado o novo exemplo "x", nós vamos computar "p(x)", usando essa fórmula para a distribuição Gaussiana multivariada. E, se "p(x)" for muito pequena, marcamos como uma anomalia. Caso contrário, se "p(x)" for maior que um parâmetro "ε", não marcamos como uma anomalia. Ajustando uma distribuição Gaussiana multivariada a esses dados, ou seja, usando os pontos em vermelho, teremos uma distribuição Gaussiana com alta probabilidade na região central, com probabilidade nos afastamos do centro. nos afastamos do centro. Tendo probabilidade muito baixa nesse ponto verde. Então, se utilizarmos a distribuição Gaussiana multivariada para esses dados, esse exemplo, em verde, será corretamente rotulado como anomalia. Finalmente, vale a pena dizer algumas palavras sobre a relação entre o modelo de distribuição Gaussiana multivariada, e o modelo original, onde modelamos "p(x)" como esse produto de p(x₁), p(x₂), até p(xn). p(x₁), p(x₂), até p(xn). Acontece que você pode provar matematicamente, eu não vou provar, mas você pode provar matematicamente que essa é a relação entre o modelo de Gaussiana multivariada e e o modelo original. E, o modelo original corresponde ao modelo de Gaussiana multivariada, onde as curvas de contorno da Gaussiana estão sempre alinhadas aos eixos. Então, esses são três exemplos de distribuição Gaussiana que você pode ajustar usando o modelo original. Portanto, eles são casos especiais da distribuição Gaussiana Multivariada, onde as curvas de contorno são alinhas aos eixos. Esse modelo, na verdade, corresponde a um caso especial da distribuição Gaussiana multivariada. E esse caso especial é definido restringindo a distribuição "p(x)", a distribuição Gaussiana multivariada de "p(x)", tal que, as curvas de contorno da função de densidade de probabilidade são alinhados aos eixos. E então, você pode ter uma "p(x)" que é uma distribuição Gaussiana multivariada que pareça com esta, essa, ou aquela. E note que nesses 3 exemplos, essas elipses que estou desenhando, têm seus eixos alinhados com os eixos x₁ e x₂. E o que nós NÃO temos, são contornos em ângulo, ou inclinados. E isso corresponde a exemplos onde há elementos não-zero fora da diagonal principal, por exemplo, Σ=[1 0.8; 0.8 1] É possível mostrar matematicamente que É possível mostrar matematicamente que esse modelo é, na verdade, o mesmo que uma distribuição Gaussiana Multivariada, mas com uma limitação. E essa limitação é que a matriz de covariância, Σ, precisa ter 0's como elementos fora da diagonal. Então, a matriz de covariância, Σ, terá na diagonal os valores: σ₁², σ₂², até σ²n. E, fora da diagonal, todos os elementos, acima e abaixo da diagonal principal, serão todos zero (0). E, na verdade, se você tomar esses valores, do modelo original: σ₁², σ₂², até σ²n, e plugar aqui, na diagonal da matriz covariância, os dois modelos serão idênticos. Isso é, o novo modelo, usando a distribuição Gaussiana Multivariada, corresponde exatamente ao modelo antigo, se a matriz de covariância, Σ, tiver todos os elementos fora da diagonal iguais a 0. E isso corresponde, graficamente, a ter uma distribuição Gaussiana, onde os contornos dessa função de distribuição são alinhadas aos eixos. Então, você não pode modelar a correlação entre as variáveis. Nesse sentido, o modelo original é um caso especial do modelo de Gaussiana Multivariada. Então, quando você usaria cada um desses modelos? Quando você usaria o modelo original? Quando você usaria o modelo de Gaussiana Multivariada? O modelo original é, provavelmente, usado mais frequentemente, e o modelo com distribuição Gaussiana Multivariada é usado com menos frequência, mas tem a vantagem de poder capturar a correlação entre as variáveis. Suponha que você queira capturar anomalias, onde há diferentes variáveis, digamos x₁ e x₂, que tomam combinações incomuns de valores. No exemplo anterior, tínhamos anomalias onde a carga da CPU, e uso de memória, tinham uma combinação incomum de valores. Se você quiser usar o modelo original para capturar isso, você precisa criar novas variáveis, por exemplo, "x₃=x₁/x₂": carga da CPU por uso de memória. Você precisará criar novas variáveis, se as variáveis x₁ e x₂ tiverem uma combinação incomum de valores, embora os valores de x₁, sozinho, e de x₂, sozinho, sejam perfeitamente normais. Mas, se você estiver disposto a gastar tempo criando variáveis extras, como essa x₃, o modelo original funcionará bem. Ao contrário, o modelo de Gaussiana Multivariada pode capturar automaticamente a correlação entre as diferentes variáveis. Mas, o modelo original tem outras vantagens. Uma vantagem do modelo original é ser computacionalmente mais barato. Assim, ele funcionará para valores muito grandes de "n", onde "n" é o número de variáveis. Mesmo que "n" seja igual a 1,000, ou "n=10,000" variáveis, ou até mesmo para "n=100,000" variáveis, o modelo original funcionará bem. Em contraste, para a modelo de Gaussiana Multivariada, note que, por exemplo, precisamos calcular (Σ ⁻¹), o inverso da matriz Σ, onde Σ é uma matriz "n x n". E, calcular o inverso de Σ, se Σ for, por exemplo, uma matriz 100,000 x 100,000; será computacionalmente muito caro. Então, o modelo de Gaussiana Multivariada não escalona tão bem, para altos valores de "n". Finalmente, o modelo original funciona bem, mesmo que você tenha um conjunto de treinamento pequeno. Aqui está um conjunto pequeno de exemplos não-rotulados, usado para modelar "p(x)", e funciona bem, mesmo para "m" igual a 50 ou 100. Já para a Gaussiana Multivariada, um tipo de propriedade Matemática do algoritmo é que você precisa ter "m>n",  ou seja, o número de exemplos tem que ser maior que o número de variáveis. E se isso não for verdade,  se "m≤n", essa matriz Σ não será nem invertível. Essa matriz será singular, e você não poderá nem usar o modelo de Gaussiana Multivariada. A regra que eu sigo é: eu uso o modelo de Gaussiana Multivariada apenas de "m" for muito maior que "n". O requisito matemático é "m>n", mas na prática, eu usaria o modelo de Gaussiana Multivariada apenas se "m" for bem maior que "n". Se "m" for maior ou igual a 10 vezes "n", seria uma regra razoável. E se isso não for satisfeito, o modelo de Gaussiana Multivariada terá muitos parâmetros, já que a matriz de covariância Σ é "n x n", tendo aproximadamente "n²" parâmetros. Por ser simétrica, na verdade, está mais próximo de "n²/2" parâmetros, mas ainda são muitos parâmetros. Você precisa garantir que tem um valor alto de "m", garantir que você tem dados o suficiente para ajustar todos esses parâmetros. E, "m≥10*n" está razoável para estimar a matriz de covariância Σ. Então, na prática, o modelo original, à esquerda, é mais utilizado. E se você suspeitar que precisa capturar a correlação entre as varáveis, você pode criar, manualmente, variáveis extras, para capturar combinações incomuns de valores específicas. Mas, em problemas onde você tiver um conjunto de treinamento grande, com "m" muito grande, e "n" não muito grande, vale a pena considerar o modelo de Gaussiana Multivariada. E você não precisa criar variáveis extras, manualmente, caso as anomalias sejam capturadas por combinações incomuns de valores das variáveis. Finalmente, eu só quero mencionar uma propriedade técnica: Se você estiver ajustando um modelo de Gaussiana Multivariada, e encontrar que a matriz de covariância Σ é singular, ou não-invertível, normalmente você está em um de 2 casos. O primeiro é que a condição "m>n" não é satisfeita, e o segundo é que você tem variáveis redundantes. "Variáveis redundantes" quer dizer: 2 variáveis iguais. De alguma forma, você acidentalmente fez 2 cópias da mesma variável, por exemplo, "x₁=x₂". Variáveis redundantes pode ser também algo como: a variável "x₃" é igual a "x₄+x₅". Então, se você tiver variáveis muito redundantes, como essas. Porque, se "x₃=x₄+x₅", "x₃" não tem nenhuma informação extra. Você apenas somou duas outras variáveis. E se você tiver esse tipo de variáveis redundantes, variáveis duplicadas, ou esse tipo de variáveis, então Σ pode ser não-invertível. Esse tipo de problema é muito raro, você provavelmente não encontrará esse tipo de situação. Mas, se você implementar um modelo de Gaussiana Multivariada, e encontrar que "Σ" é não-invertível, primeiro garanta que "m" é bem maior que "n". E, segundo, veja se há variáveis redundantes. Se você tiver 2 variáveis iguais, elimine uma delas. Ou, se você tiver variáveis redundantes, como essa: "x₃=x₄+x₅", elimine a variável redundante, e o modelo deve funcionar. Para aqueles que conhecem Álgebra Linear, por "variáveis redundantes", o que quero dizer é: "variáveis linearmente dependentes". Mas, na prática, isso significa que, se você garantir que suas variáveis são não-redundantes, isso deve resolver o problema de "Σ" ser não-invertível. Mas, novamente, as chances de encontrar um problema como esse são muito pequenas, então, você pode aplicar o modelo de Gaussiana Multivariada, sem se preocupar se "Σ" é não-invertível, desde que "m" seja maior ou igual a "n". Então, é isso, para Detecção de Anomalia com distribuição Gaussiana Multivariada. E, se você aplicar esse método, você terá um algoritmo de Detecção de Anomalias que automaticamente captura correlações entre as variáveis, e marca uma anomalia, caso haja uma combinação incomum de valores das variáveis.
Tradução: Pablo de Morais Andrade