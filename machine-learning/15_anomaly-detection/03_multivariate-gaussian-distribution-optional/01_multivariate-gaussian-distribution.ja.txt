このビデオとこの次のビデオで、 ここまでに開発してきたアノマリー検出のアルゴリズムの 考えられる一つの拡張を お話しよう。 この拡張は、多変量ガウス分布と 呼ばれる物を使う。 そしてそれはある長所があり、 そしてある短所もある。 そしてそれは、前のアルゴリズムで捉えられなかったアノマリーを検出出来る事がある。 この動機付けを理解する為に、具体例から始めよう。 ここにプロットしたようなラベル無しデータがあるとする。 そしてデータセンターの マシンのモニタリングの例を 使っていく事にする。
データセンターのコンピュータをモニタリングする。 つまり私の二つのフィーチャーは x1がCPUロードで、 x2が例えばメモリの使用量。 そこで二つのフィーチャー、 x1とx2に対して、 ガウス分布でモデリングすると、 これがx1フィーチャーによる プロットで、 これがx2フィーチャーによるプロットだ。 ここにガウス分布を フィッティングすると、 こんな感じのガウス分布が得られるだろう。 これがp(x1)で、 これはパラメータの ミュー1とシグマ1に 依存している。 そしてこれがメモリ使用量、 そのガウス分布は例えば こんな感じ。これがp(x2)で、 それはミュー2とシグマ二乗2に依存する。 さて、以上が アノマリー検出のアルゴリズムが x1とx2をモデリングする方法だ。 ここで、テストセットに こんな手本が あったとする。 この緑のバッテンの位置、 つまりx1の値がだいたい0.4で、 x2の値がだいたい1.5くらいの位置。 今、データを見てみると、 ぱっと見た感じ 多くのデータは この範囲に存在しているので、 この緑のバッテンは 観測されているデータのどれとも、かなり離れている。 これはアノマリーとして 提起されるべきに見える。 つまり、私のデータにおいては、 私の正常な手本のデータでは、 CPUロードと メモリ使用量は、 お互いに線形に 上昇する傾向にあるように見える。 つまり、たくさんのCPUを 使っているマシンがあれば、 そのマシンのメモリ使用量もおそらく高い。 一方でこの例、 この緑の例では、 CPUロードはとても低いが、 メモリ使用量は とても高い。 こんな物はトレーニングセットでは見られなかった物だ。 これはアノマリーだとみなすべきだろう。 だが、アノマリー検出のアルゴリズムが何をするか、見てみよう。 CPUロードに関しては、 だいたいこの辺となる、0.5とか。 これはリーズナブルに高い確率の範囲で、 これまで見た手本の集団から それ程離れた場所では無い。 一方、メモリ使用量に関しては、 ここの点は0.5のあたりだが、 一方でメモリ使用量は、 1.5くらいで、それはここ。 これもまた、 ガウス分布の尻尾の方ではあるが、 でもこの値とこの値は、観測されている その他のたくさんの手本と比べても、 そんなに違う物では無い。 つまりp(x1)は かなり大きくなる、 リーズナブルな程度には大きくなるだろうし、 p(x2)もリーズナブルな程度には大きい。 つまり、この右側のプロットを見ると、 この、ここの点は、 そんなに悪くは見えないし、 そしてこのプロットを見ると ここのバッテンを見ると、そんなに悪くも見えない。 つまり、もっとメモリ使用量が多い手本も ある訳だし、また、 もっと少ないCPU使用量の手本もある。 つまり、この手本はそんなにアノマリーっぽくは見えない。 つまり、アノマリー検出のアルゴリズムは この点をアノマリーだとフラグ付けするのに失敗するだろう。 我らのアノマリー検出のアルゴリズムが やっている事は、 結局の所、 この青い楕円が 高い確率の範囲を示しているという事を 認識していないで、 ここの手本が、高い確率だと 考える。 そしてこの隣の円は、 一段低い確率となり、 そしてここの手本は、 さらに低い確率となり、そして、 ここの緑のバッテンは、 かなり高い確率となってしまう。 そして具体的には、 この領域は全て、 私が円でくくってる直線上の点は全て、 だいたい同じ確率だ、と考える。 そして、この辺の外れた何かは 実際にはこの辺よりは もっと低い確率だ、という事を認識出来ていない。 そこで、これを修正する為に、 修正版のアノマリー検出の アルゴリズムを 開発していこう、 多変量ガウス分布とか、多変量正規分布と 呼ばれる物を用いて。 これが、我らがやる事だ。 我らはフィーチャーとして Rnのxを持つ。 p(x1), p(x2)と別々にする代わりに、 p(x)、という風に 全部をいっぺんにモデリングする。 つまり全部を一度に、p(x)をモデリングする。 さて、多変量ガウス分布の パラメータとしては、まずミュー、 これはベクトル。そしてシグマ、 これはn掛けるn行列で、共分散行列とも呼ばれる。 そしてこれはPCAの時に 使った共分散行列と 同様の物だ。 主成分分析の アルゴリズムの時の。 だが完全を期して、 多変量ガウス分布の式を 書いてみよう。 さて、xの確率は、 パラメータのミューとシグマで パラメトライズされていて、 そのxの確率は イコール、 もう一度言うが、この式を暗記する必要は まったく、これっぽっちも無い。 使う必要がある時に 調べれば十分だ。 だがこれが、xの確率が どうなるか、だ。 ふんふん、転置の、シグマの逆行列に、 x マイナスの ミュー。 そしてここのこれ、 シグマの絶対値、 ここのこれ、 この記号を書くと、 これはシグマの行列式(determinant)と呼ばれる物で、 これは行列に対する、数学的な関数だ。 そしてあなたは、行列の行列式が何かを 実際にしっている必要は 全くない。 あなたが実際に 知る必要があるのは、 それをOctaveで計算するには コマンドdet(Sigma)を使う、 という事だけだ。 よし。もう一度確認しておこう。 この式において、これらのシグマ、 これらは単なるn掛けるn行列だ。 これは和の記号じゃない。 そしてこのシグマは、n掛けるn行列。 さて、以上がp(x)の式だ。 だが、もっと興味深い事としては、 あるいはもっと重要な事としては、 p(x)は実際に、どんな感じになるのだろうか？ 多変量のガウス分布の例を 幾つか見ていこう。 さて、二次元の例を 考えよう。 n=2なら、 二つのフィーチャー、x1とx2を持つ。 ミューをイコール0に セットして、 シグマをこの行列としよう。 対角成分が1で、非対角成分が0。 この行列はまた、単位行列と呼ばれる事もある。 この場合、pのxは こんな見た目となる。 そしてこの図で 私が示しているのは、 ある特定のx1の値と ある特定のx2の値の時に、 この表面の高さ、 これがpのxの値だ。 そしてこのパラメータの 設定では、 x1とx2がイコール0の時 もっとも高くなる、 つまりそこがガウス分布のピークとなり、 そして確率分布は、 こんな感じの二次元ガウス分布として、 あるいはこの二次元のベル型の平面として減衰していく。 この下には、同じ事を 代わりに等高線プロットを用いて プロットしてみた、等高線は様々な色を使う事で表現される。 つまりこの真中の、 赤が激しく集中している所は、 もっとも高い値に対応している。 そして黄色くなっている所は 値がもうちょっと低くなっている所で、 シアン色になってる所は さらに値が低くなっている所で、 そしてこの深い青は、もっとも低い値となっている所だ。 つまり、全く同じ図を 真上から見て、色を使ってプロットした物となっている。 さて、この分布では、 確率の大半は 0,0のそばにあり、 そして0, 0から外に出て行くと、 x1とx2の確率は下がっていく。 ここでパラメータの幾つかを 変更していき、 何が起こるか見ていこう。 シグマを変えてみよう。 シグマをちょっと縮めてみよう。 シグマは 共分散行列なので、 フィーチャーx1, x2の分散、または 変わりやすさを測った物だ。 だからシグマを縮めると、 得られるのは、、、 得られるのは、 このコブの幅が減少し、 高さもちょっとだけ 増加する。何故なら、 平面の下の体積は1だから。 つまり平面の下の体積の 積分結果は、イコール1だ。 何故なら確率分布の積分は 1にならなくてはいけないから。 だがもし分散を縮めると、 それはシグマ二乗を 縮める事に相当し、 より狭い分布が得られる、そしてその背の高さはちょっとだけ高くなる。 そして見て分かるように、 同心の楕円もちょっとだけ縮む。 一方で、対照的に、もしシグマの対角成分を 2, 2に増加させると、 するとこれは単位行列の2倍となり、 もっと幅広くて、もっと平坦な ガウス分布が得られる。 つまり、この幅はもっと広くなる。 これは見づらいが、 これもまだベル型のコブになっていて、 ただ凄い平坦になっているだけだ、 それはより幅広くなった、 つまり、x1とx2の分散、あるいは変わりやすさは 単により広くなった。 もうちょっと例を挙げておこう。 今度は、一度にシグマの要素の一方だけを 変更していこう。 シグマのこっちを0.6に こっちを1にしてみよう。 こうすると、 最初のフィーチャーx1の 分散が減少し、同時に フィーチャーx2の分散は 同一に保たれている。 すると、このパラメータの設定では、こんな物をモデリング出来る。 x1はより小さな分散となり、 x2はより大きな分散となる。 他方、もし私が この行列、2, 1を セットすると、 今度は以下のような手本を モデリングする事が出来る。それは x1が広い範囲の 値をとり、 一方x2は相対的に狭い範囲の値を、取る、というような。 そしてその事は、この図にも 反映されている。ここでは、 分布はx1から離れるに連れて よりゆっくりと 減衰していて、 そしてx2が0から離れると 急激に減衰している。 そして同様に、 代わりに 行列のこの要素を変更すると、 前のスライドと似ているが、 違いとしては、 ここでいじっている所は、 x2がとても小さい範囲の 値をとるようになるという事で、 つまりここでは、 これが0.6だと、 x2がオリジナルの例よりも より小さな範囲を取るようになるという事に気づく。 一方で、もしシグマを イコール2にセットしたら、 それはx2がもっと幅広い範囲の値を取る、という事だ。 ここで、多変量ガウス分布の クールな事の一つには、 データ同士の相関を モデリングするのに 使う事が出来る、という事がある。 つまり、多変量ガウス分布を用いて、 例えばx1とx2が 互いに高く相関する傾向にある、という事実を モデリング出来る。 具体的には、 この共分散行列の 非対角成分を変更する事で、 異なる種類のガウス分布を得る事が出来る。 つまり、 非対角成分の要素を0.5から0.8に 増加させると、 この分布が得られる。 これはこの、x=yの直線に沿った より狭い範囲に山があるような分布。 つまり、ここの等高線は、 xとyがともに増加する傾向にある、 と主張していて、 x1が大きくてx2も大きい、という時と x1が小さくてx2も小さい、 という時は 大きな確率と なっている。 あるいはその両者の間も高い確率となっている。 そしてこの要素、0.8が 大きくなると、 この種の狭い領域に 全ての確率が存在するような ガウス分布が得られ、 xはだいたいイコールyとなる。 これはとても背が高く、 薄い分布で、 だいたいこの直線に沿った、 xがyと近い領域を中心とした 直線に沿った。 以上が、これらの要素を 正に設定した時だ。 逆に、もしこれらの値を 負の値にセットすると、 これを-0.5から-0.8まで 減少させていくと、 以下のようなモデルが得られる： 確率の多くを いわゆるx1とx2が負の相関となっている範囲に 置くようなモデル。 つまり、確率のほとんどが、 今度はこの範囲に存在し、 そこはx1がだいたいイコール -x2 となっている、x1 = x2の 代わりに。 つまり、これは、いわゆる x1とx2の間の負の相関を 捉えている。 以上で、 あなたも多変量ガウス分布が 捉える事が出来る、様々な分布が、 感覚的につかめただろうか。 ここまでの所、我らは 共分散行列のシグマを変化させてきたが、 他にも出来る事として、 平均のパラメータ、ミューを 変えていく、という事がある。 もともとは、ミューはイコール 0, 0だった。 つまり分布はx1=0とx2=0を 中心として分布していた。 つまり分布のピークは ここだ。 一方、ミューの値を変えていくと、 分布のピークが 変わっていって、 もしミューがイコール、 0, 0.5なら、 ピークはちょうど x1=0でx2=0.5の所にあり、 つまり分布の ピーク、あるいは中心は、 シフトする。 そしてもしミューが1.5, -0.5なら、 ふたたび同様に、 分布のピークは 今度は別の場所に シフトする。 そこはx1が1.5で、 x2が-0.5の点に 対応した場所。 つまり、ミューのパラメータを変えていく事で、 分布全体の中心をシフトする事になる。 これら全ての 別々の図を見る事で、 多変量のガウス分布で 捉える事の出来る、確率分布の 感じがつかめたかな。 そしてそのキーとなる利点としては、 それを用いると 二つの別々のフィーチャーが 正の相関を持っている、とか負の相関を持っている、という事が期待される時に それらを捕捉出来るようになる、という事だ。 次のビデオでは、 多変量ガウス分布を、 アノマリー検出に適用する。