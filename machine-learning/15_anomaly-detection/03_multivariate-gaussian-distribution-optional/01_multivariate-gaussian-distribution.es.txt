En este video y en el siguiente, me gustaría hablarles sobre una posible extensión del algoritmo de detección de anomalías que hemos desarrollado hasta ahora. Esta extensión utiliza algo llamado: distribución Gaussiana multivariada, tiene algunas ventajas y algunas desventajas, a veces puede captar algunas anomalías que el algoritmo anterior no pudo. Para motivar esto, vamos a empezar con un ejemplo, digamos que nuestros datos valores no asignados se parecen a lo que he trazado aquí y voy a usar el ejemplo de monitoreo de las máquinas en el centro de datos, la supervisión de equipos en el centro de datos. Así que mis dos variables son x1, que es la carga de CPU y x2, que es, tal vez, el uso de la memoria. Si tomo mis dos variables x1 y x2 y las modelo como Gaussianas, entonces aquí está un trazo de mis variables x1, aquí se muestra un trazo de mis variables x2, y si coloco una gaussiana en esto, quizá obtenga una una gaussina como esta, por lo que aquí está p(x1), que depende de los parámetros M1 y «sigma» cuadrada 1 y aquí está mi memoria usada y, como sabe, tal vez voy a conseguir una Gaussiana que se parezca a esto, y esta es mi p(x2) que depende de M2 y «sigma» cuadrada 2, entonces Y entonces esto es algoritmo de detección de anomalías modela x1 y x2. Ahora, digamos que en los conjuntos de prueba tengo un ejemplo que se parece a esto. La ubicación de esa cruz verde, en donde el valor de x1 es de aproximadamente 0.4 y el valor de x2 es de aproximadamente 1.5. Ahora, si observamos los datos, parece como, sí, la mayoría de los datos están en esta región, de modo que la cruz verde está bastante lejos de cualquiera de los datos que he visto. Parece que debe resaltarse como una anomalía. Así que en mis datos, en mi, en los los datos de mis buenos ejemplos, parece que, ya sabe, la carga del CPU y el uso de la memoria, de alguna manera crecen linealmente uno con el otro. Si tengo una máquina con una gran parte del CPU en uso, el uso de memoria también será alto, mientras que en este ejemplo, este ejemplo en verde aquí, la carga del CPU es muy baja pero el uso de memoria es muy alto, y no he visto eso antes en mi conjunto de entrenamiento. Parece que eso es una anomalía pero vamos a ver lo que el algoritmo de detección hará. Bueno, para la carga del CPU, se pone alrededor de 0.5 y está probabilidad razonablemente alta, no está tan lejos de otros ejemplos que hemos visto, tal vez, mientras que, para el uso de la memoria, esta asignación 0.5, para el uso de la memoria, es aproximadamente 1.5, que se muestra allí. Una vez más, como sabe, depende de nosotros, no es terriblemente Gaussiano pero este valor y este otro no son tan diferentes de otros ejemplos que hemos visto, entonces p(x1) será bastante alta, razonablemente alta, p(x2) razonablemente alta. Quiero decir, si nos fijamos bien en este trazo, este punto aquí, no luce tan mal y si nos fijamos en este trazo, por esta zona, no se ve tan mal. Quiero decir, he tenido ejemplos con aún más memoria utilizada o incluso con un menor uso de CPU y este ejemplo no parece tan anómalo. Y así, un algoritmo de detección de anomalías fallará en marcar este punto como una anomalía y resulta que lo que nuestro algoritmo de detección de anomalías está haciendo es que no se está dando cuenta de que este elipse azul muestra la región de alta probabilidad, es que, una de las cosas es que, los ejemplos aquí muestran una alta probabilidad y los ejemplos ejemplos del siguiente círculo, una menor probabilidad y estos ejemplos son de una probabilidad aún menor y de alguna manera, aquí hay cosas que son, esa cruz verde, es una probabilidad bastante alta, y en particular, se tiende a pensar que, ya sabe, todo en esta región, todo lo que está en esta línea que estoy circulando, tiene aproximadamente la misma probabilidad y no se da cuenta de que algo aquí afuera tiene realmente una probabilidad mucho menor que algo por allá. Así que para arreglar esto podemos, vamos a desarrollar una versión modificada del algoritmo de detección de anomalías usando algo llamado: distribución Gaussiana multivariada, también conocida como distribución normal multivariada. Así que esto es lo que vamos a hacer: tenemos variables "x" que están en Rn y en lugar de p(x1), p(x2), por separado, vamos a modelar p(x) de una sola vez, entonces modelamos p(x), ya sabe, todo al mismo tiempo. Los parámetros de la distribución Gaussiana multivariada son M, que es un vector, y «sigma», que es una matriz nxn, llamada una matriz de covarianza, esto es muy similar a la matriz de covarianza que vimos cuando estábamos trabajando con el ACP, con el algoritmo de componentes principales. Para complementar esto, déjeme escribir la fórmula de la distribución Gaussiana multivariada, decimos que es la probabilidad de "x", y esto se parametriza por medio de parámetros: letra griega «Mu» y «sigma», por lo que la probabilidad de "x", es igual una vez más, no hay absolutamente ninguna necesidad de memorizar esta fórmula. Como sabe, puede revisarla siempre que necesite usarla pero así es como luce la probabilidad de "x": Transversal, segunda inversa, x- «Mu». Y esto aquí, el valor absoluto de «sigma», esta figura, cuando quiere escribir este símbolo, esto se llama determinante de «sigma» y es una función matemática de una matriz y realmente no necesita saber lo que es una determinante de una matriz, todo lo que necesita saber es que puede calcularlo en Octave, utilizando el comando "det" de «sigma». Bien y de nuevo, sólo para aclarar ¿de acuerdo? En esta expresión, estos «sigma»s aquí, son sólo una matriz nxn. Esto no es una suma y usted sabe, el «sigma» es una matriz nxn. Así que esa es la fórmula para p(x) pero es aún más interesante, o más importante cómo luce p(x) en realidad. Veamos algunos ejemplos de distribuciones Gaussianas multivariadas. Así que tomemos el mismo ejemplo dimensional, digamos que si tengo n=2, tengo dos variables, x1 y x2. Digamos que puse que «Mu» es igual a 0 y «sigma» es igual a esta matriz. Con 1s en las diagonales y 0s fuera de las diagonales, esta matriz a veces también se llama la matriz de identidad. En ese caso, p(x) se verá así y lo que les muestro en esta figura es, ya sabe, para un valor específico de x1 y para un valor específico de x2, la altura de esta superficie, el valor de p(x) Y con este ajuste de los parámetros p(x) es más alta cuando x1 y x2 son iguales a cero 0, ese es el pico de esta distribución de Gauss y la probabilidad disminuye con este tipo de Gaussiana bidimensional o esta superficie en forma de campana bidimensional. Abajo aparece lo mismo pero trazado usando un trazo de contorno en lugar de usar diferentes colores y este rojo intenso a la mitad, corresponde a los valores más altos y a continuación, los valores disminuyen con el amarillo, siendo valores ligeramente menores, el cian muestra los valores más bajos y este azul profundo, los valores aún más bajos, por lo que esta es en realidad la misma figura, pero se traza vista desde la parte superior y con colores. Con esta distribución, puede ver que se enfrenta la mayor parte de la probabilidad cerca de 0,0 y luego, a medida que sale de 0,0, la probabilidad de x1 y x2 se reduce. Ahora vamos a tratar de variar algunos de los parámetros y veamos lo que sucede. Entonces, vamos a tomar «sigma» y lo cambiamos, así que digamos que «sigma» se reduce un poco. «sigma» es una matriz de covarianza, por tanto, mide la varianza o la variabilidad de las variables x1, x2. Si «sigma» se reduce, entonces lo que tiene, lo que se obtiene es que el ancho de esta protuberancia disminuye, la altura también se incrementa un poco, porque el área bajo la superficie es igual a 1. Entonces, la integral del volumen bajo la superficie es igual a 1, porque la distribución de la probabilidad se debe integrar a uno. Pero, si reduce la varianza, es un poco como la reducción de «sigma» al cuadrado, usted termina con una distribución más estrecha, y una que es un poco más alta. Y por lo que se ve aquí también las elipses concéntricas se han reducido un poco, mientras que por el contrario, si usted fuera a aumentar «sigma» a 2, 2 en las diagonales, esto es ahora dos veces la identidad, entonces termina con una Gaussiana mucho más amplia y plana, la anchura de esto es mucho más amplia. Esto es difícil de ver pero esta todavía es una protuberancia en forma de campana, sólo se ha aplanado mucho, se ha vuelto mucho más amplia y la varianza o la variabilidad de x1 y x2, sólo se hace más amplia. Aquí hay algunos ejemplos más: ahora vamos a tratar de variar uno de los elementos de «sigma» a la vez. Digamos que envío «sigma» a 0.6 allí, y 1 por allá. Lo que esto hace es reducir la varianza de la primera variable, x1 mientras mantiene la varianza de la segunda variable x2, la misma. Y así, con este ajuste de parámetros, puede modelar cosas de esta forma, x1 tiene menor varianza, y x2 tiene mayor varianza. Mientras que si hago esto, si coloco esta matriz en 2, 1 entonces usted también puede modelar ejemplos en los que, vamos a decir que x1 puede tener un gran rango de valores, mientras que x2 toma un rango relativamente estrecho de valores, Eso se refleja en esta figura también, ya sabe, donde la distribución cae más lentamente a medida que x1 se aleja de 0 y cae muy rápidamente a medida que x2 se aleja de 0. De manera similar, si fuéramos a modificar este elemento de la matriz por el contrario, similar a la diapositiva anterior, excepto que aquí, donde se están desplazando, digamos que x2 puede tomar un rango muy pequeño de valores, entonces, si esto es 0.6, notamos ahora que x2 tiende a tomar un rango de valores menor que el ejemplo original, mientras que si tuviéramos que establecer «sigma» para ser igual a 2, es como decir que x2 tiene un alcance mucho más amplio de valores. Ahora, uno de los mejores aspectos sobre la distribución Gaussiana multivariada es que también puede usarla para modelar correlaciones entre los datos. Es decir, podemos utilizarla para modelar el hecho de que x1 y x2 tienden a estar altamente correlacionados entre sí, por ejemplo. Específicamente, si comienza a cambiar las entradas fuera de la diagonal de esta matriz de covarianza, puede obtener un tipo diferente de distribución de Gauss. Y a medida que incremento las entradas fuera de la diagonal 0.5 a 0.8, lo que consigo es esta distribución que tiene un pico cada vez más delgado a lo largo de esta línea de "x=y". Entonces, aquí el contorno indica que "x" y "y" tienden a crecer juntas y las cosas que tienen una gran probabilidad son, ya sea que x1 sea grande y y2 sea grande o x1 sea pequeña y y2 sea pequeña. o algún punto intermedio y a medida que esta entrada 0.8 se hace grande, se obtiene una distribución de Gauss, que es como en lugar donde toda la probabilidad se encuentra en este tipo de región estrecha, donde "x" es aproximadamente igual a de "y". Esta es una distribución muy alta y delgada, la línea en su mayoría a lo largo de esta región central, donde "x" está cerca de "y". Así que esta ponemos estas entradas como entradas positivas, en contraste, si colocamos estos en valores negativos, a medida que lo reduzco de - 0.5 hasta - 0.8, a continuación, lo que tenemos es un modelo en el que ponemos la mayor parte de la probabilidad en este tipo de región de correlación negativa x1 y x2 y así, la mayor parte de la probabilidad ahora se encuentra en esta región, donde x1 es aproximadamente igual a -x2, en lugar de x1 igual a x2. Y así, esto captura una especie de correlación negativa entre x1 y x2. Y entonces esto es espero que esto le de una idea de las diferentes distribuciones que la distribución Gaussiana multivariada puede capturar. Al continuar con la variación de la matriz de covarianza «sigma», lo que también puede hacer es variar el parámetro medio M, y de forma operativa, tenemos «Mu» =0.0, por lo que la distribución se centró en torno a x1=0, x2=0, de modo que el pico de la distribución está aquí, mientras que, si variamos los valores de M, a continuación, esto varía el pico de la distribución y así, si «Mu» es igual a 0, 0.5, el pico está en, ya sabe, x1=0, y x2=0.5, de modo que el pico o el centro de esta distribución se ha cambiado y si «Mu» era 1.5 menos 0.5 entonces está bien, de manera similar, el pico de la distribución ahora se ha desplazado a una ubicación diferente, correspondiente al lugar donde, ya sabe, x1 es 1.5 y x2 es -0.5, y al variar el parámetro «Mu», sólo se mueve alrededor del centro de toda esta distribución. Espero que el mirar todas estas imágenes le de una impresión de los tipos de distribuciones de probabilidad que la distribución Gaussiana multivariada permite capturar y la principal ventaja de esto es que le permite capturar cuándo puede esperar que dos variables distintas se correlacionen de forma positiva o tal vez negativa. En el siguiente video, tomaremos esta distribución Gaussiana multivariada y la aplicaremos a la detección de anomalías.