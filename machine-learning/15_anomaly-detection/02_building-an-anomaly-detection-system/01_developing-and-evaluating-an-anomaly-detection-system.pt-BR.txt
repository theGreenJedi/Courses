No último vídeo nós desenvolvemos um algoritmo de detecção de anomalias. Neste vídeo, eu gostaria de falar sobre como desenvolver um especifico algoritmo de detecção de anomalias para um problema em particular. Vamos focar no problema de como avaliar este algoritmo de detecção. Nos vídeos anteriores, nós já falamos sobre a importância da avaliação com números reais e isso captura a ideia de que quando você está desenvolvendo um algoritmo de aprendizado você geralmente precisa fazer várias escolhas como, escolher quais características para usar, etc. E criar essas decisões de qual escolher geralmente se torna mais fácil se você tem um jeito de avaliar seu algoritmo com uma função que retorna apenas um número. Se você estiver tentando decidir por exemplo, eu tenho uma ideia de uma característica extra, eu incluo ela ou não? Se você puder rodar o algoritmo com aquela nova característica, e rodar o mesmo sem ela e pegar apenas um número de volta que diz minha performance melhorou ou piorou quando eu adicionei esta característica? Isto nos dá uma maneira melhor e muito simples com a qual podemos decidir se incluímos ou não essa nova característica. Então, para desenvolvermos um sistema de detecção de anomalia rapidamente, seria de grande ajuda ter um jeito de avaliar este sistema de detecção. Para fazermos isso, avaliar este algoritmo, nós vamos assumir que temos alguns dados já rotulados. Nós vamos tratar a detecção de anomalias como um algoritmo de aprendizado não supervisionado com dados não rotulados. Mas nós teremos alguns dados rotulados que especifiquem quais são anômalos e quais exemplos não são. Isto é como nós vamos realmente usar estes rótulos para avaliar o algoritmo de detecção. Por exemplo, vamos pegar o exemplo do motor de avião. Vamos dizer que temos alguns dados rotulados que são considerados anomalias de motores de aviões que foram fabricados no passado e agora são considerados problemáticos. Se tornaram em falhas ou estranhos em alguma forma. Vamos dizer também que temos alguns exemplos normais, alguns exemplos em perfeita condição. Eu vou usar ( y = 0 ) para denotar os exemplos normais ou sem anomalia e ( y = 1 ) para denotar os anômalos exemplos. O processo de desenvolver e avaliar o algoritmo de detecção de anomalia é o seguinte. Nós vamos pensar sobre o conjunto como um conjunto de treino, nós vamos falar sobre validação cruzada no conjunto de testes depois, mas o conjunto de treino nós ainda consideramos ele como um conjunto de treino não rotulado. Então, este é nosso grande conjunto de normais, não anômalos exemplos. E geralmente pensamos como isto sendo não anômalos mas não tem problemas se algumas anomalias acabem dentro de seu conjunto de treinos não rotulados. O próximo para nós vamos definir um conjunto de validação cruzada, e um conjunto de teste com o qual vamos poder avaliar em particular nosso algoritmo de detecção. Então especificamente, para ambos os conjuntos de validação cruzada nós vamos assumir que, bem você sabe, nós podemos incluir alguns exemplos no nosso conjunto de validação cruzada e também no conjunto de teste alguns exemplos que são considerados anômalos. Então o conjunto de testes vai ter alguns poucos exemplos com ( y = 1 ) que corresponde a alguns motores de aviões com problemas. Aqui temos um exemplo especifico Vamos dizer que tudo junto, este são os dados que temos. Nós fabricamos 10.000 exemplos de de motores que até onde sabemos são motores perfeitos, motores de avião em perfeito funcionamento. E novamente, é perfeitamente normal se alguns destes sejam motores falhos acabem caindo dentro do conjunto dos 10.000, mas nós assumimos que a grande maioria destes 10.000 exemplos são, bons e normais, não anômalos exemplos. E vamos dizer que historicamente, você sabe, por um bom tempo esta fabrica esta produzindo, e geralmente acabamos tendo exemplos, de 24 até 28 exemplos de motores falhos também. E para uma típica aplicação de de detecção de anomalias, o número de não anômalos exemplos, em que ( y = 1 ) pode ser qualquer num range de 20 a 50 por exemplo. É bem normal ter este intervalo de exemplos em temos ( y = 1). Até porque geralmente temos um número muito maior de exemplos bons. Então, dado este conjunto de dados uma maneira justa e típica de separar os dados em conjunto de treinos, validação cruzada e conjunto de testes seria a seguinte: Vamos pegar nossos 10.000 motores bons e colocar 6.000 deles no conjunto de treino não rotulados. Eu estou chamando este exemplo de não rotulado mas sabemos que todos estes exemplos são realmente aqueles que correspondem a ( y = 0 ), até onde sabemos. E então vamos usar este conjunto para ajustar p(x). Vamos usar estes 6000 motores para ajustar p ( x ), este p(x) é: parametrizado por mu1 p(x₁; μ₁ , σ₁²), ...., até p(xn; μn , σn²). E então serão estes 6000 exemplos que nós vamos usar para estimar os parâmetros Mu 1, sigma squared 1, up to Mu N, sigma squared N. And so that's our training set of all, you know, good, or the vast majority of good examples. Próximo passo vamos pegar todos os motores de avião bons e colocar alguns deles no conjunto de validação cruzada e outros vamos adicionar no conjunto de testes. Temos então: 6000 + 2000 + 2000, é assim que separamos nossos 10.000 motores de avião que são considerados bons. Se nós tivermos 20 motores de aeronave com problemas, nós vamos pegar e separar elas, colocar 10 delas no conjunto de validação cruzada e 10 delas no conjunto de testes. No próximo slide nós vamos falar sobre como realmente usar isto para avaliar o algoritmo de detecção de falhas. O que eu acabei de descrever aqui para você é a maneira recomendada de separar os dados rotulados e não rotulados. Os motores de aeronave bons e ruins. Nós usamos algo como 60, 20 e 20% para os exemplos de motores bons, e os motores com falhas nós apenas colocamos no conjunto de validação cruzada e apenas no conjunto de testes. em seguida veremos o porque dessa separação. Apenas como um complemento, se você olhar como as pessoas normalmente aplicam esses algoritmos de detecção, você vai diferentes formas de separar os dados. Uma outra alternativa, não uma alternativa muito recomendada, mas algumas pessoas pegam seus 10 exemplos separam 6000 exemplos para o conjunto de testes. e colocam os restantes 4000 exemplos no conjunto de validação cruzada e os mesmos no conjunto de testes. Mas como você deve saber, gostamos de pensar como conjunto de validação e conjunto de testes como sendo separados um do outro. Mas em detecção de anomalias, algumas vezes a gente vê pessoas usando o mesmo conjunto de bons motores no conjunto de validação e conjunto de teste, e algumas vezes você vê pessoas usando o mesmo conjunto de motores anômalos  no conjunto de validação cruzada e no conjunto de teste. Porém todos estes casos não são considerados boas práticas, e são desencorajados. Certamente usando o mesmo conjunto de dados na validação e no conjunto de testes não é considerado uma boa prática de aprendizado de máquina. Mas algumas vezes você as pessoas fazendo mesmo assim. Enfim, dado o conjunto treinamento, validação cruzada e teste, é assim que vamos avaliar ou como vamos desenvolver e avaliar nosso algoritmo. Primeiro nós pegamos o conjunto de treinamento e ajustamos o modelo p(x) Nós ajustamos todos esses Gaussianos para os meus m's exemplos não rotulados de motores de aeronave, estes eu estou chamando de não rotulados,  mas na realidade estes são os exemplos que estamos assumindo serem nossos exemplos de aeronave sem defeito. Então imagine que seu algoritmo está realmente fazendo uma predição. Então, na validação cruzada com o conjunto de teste, dado o exemplo X, pense no algoritmo como predizendo que "y=1", se p(x)<ε e predizendo "y=0", se p(x)≥ε. Dado "x", tenta predizer o rótulo. É y=1, correspondendo a uma anomalia, ou y=0, correspondendo a um exemplo normal? Dados os conjuntos de treinamento, validação, e teste, como desenvolver um algoritmo? Ou, como você avalia um algoritmo de Detecção de Anomalias? O primeiro passo é pegar o conjunto de treinamento não-rotulado, e ajustar o modelo de p(x) a esses dados de treinamento. Então, pegamos esse conjunto de treinamento não-rotulado, que são esses exemplos, e estamos assumindo que a maioria dos motores das aeronaves são normais, que não são anomalias. E vamos ajustar o modelo "p(x)", ajustar todos os parâmetros de todas as Gaussianas, a esses dados. Depois, na validação cruzada do conjunto de testes, vamos pensar que o algoritmo de Detecção de Anomalia quer predizer o valor de "y". Então, para cada exemplo de teste, temos x⁽ᴵ⁾ de teste, e y⁽ᴵ⁾ de teste, onde "y" é igual a 0 ou 1, dependendo se o exemplo é uma anomalia, ou não. Então, dada a entrada "x" no conjunto de teste, meu algoritmo de Detecção de Anomalia quer predizer "y=1", se "p(x)<ε", predizer isso como uma anomalia. E queremos que o algoritmo preveja "y=0", se "p(x)" for maior ou igual a "ε". Assim, predizendo como um exemplo normal, se "p(x)" for razoavelmente grande. Então, podemos pensar no algoritmo de Detecção de Anomalia como fazendo predições sobre os valores de "y" no conjunto de teste, ou de validação cruzada. Isso nos coloca numa situação semelhante ao Aprendizado Supervisionado, certo? Onde temos um conjunto de teste rotulado, e o algoritmo está fazendo predições sobre esses rótulos, e podemos avaliar, vendo quantas vezes o algoritmo acerta o rótulo. Claro que esses rótulo serão bem enviesados, porque "y=0", exemplos normais, são muito mais comuns do que "y=1", exemplos anomalias. Mas, isso está mais perto da avaliação que usamos em Aprendizado Supervisionado Então, o que é uma boa medida de avaliação? Bom, como os dados são muito enviesados,  já que "y=0" é muito mais comum, a precisão da classificação não será uma boa medida de avaliação. Já falamos sobre isso em outro vídeo. Se você tiver dados muito enviesados, predizer sempre "y=0" terá uma elevada precisão de classificação. Ao invés disso, deveríamos usar métricas de avaliação como: fração de verdadeiro-positivos, falso-positivos, falsos-negativos, e verdadeiros-negativos. Ou calcular a precisão, e recall do algoritmo. Ou o "f1 score", que é um número, que sumariza: a precisão e número de recalls. Essas são formas de avaliar o algoritmo com dados de validação cruzada, ou de teste. Finalmente, mais cedo, no algoritmo de Detecção de Anomalias, também tínhamos esse parâmetro "ε". Então, "ε" era esse limiar que usaríamos para decidir quando rotular algo como uma anomalia. Então, se você tiver um conjunto de validação cruzada, uma forma de escolher esse "ε", seria testar vários valores diferentes "ε", e então escolher o valor de "ε" que, por exemplo, maximiza o "f1 score". Ou que se sai bem com seu conjunto de validação cruzada. Em termos mais gerais, a forma de se usar os conjuntos de treinamento, validação e teste, é: quando tentamos tomar decisões, como, que variáveis incluir, ou melhorar a predição do parâmetro "ε", avaliaremos continuamente o algoritmo no conjunto de validação cruzada. Tomar todas as decisões, que variáveis usar, como fixar "ε", avaliar o algoritmo no conjunto de validação cruzada, e então, após termos escolhido o conjunto de variáveis, e estarmos satisfeitos com o valor de "ε", podemos tomar o modelo final e avaliar ele, usando o conjunto de teste. Então, nesse vídeo, falamos sobre o processo de como avaliar um algoritmo de Detecção de Anomalias. E, ser capaz de avaliar um algoritmo usando um único número, como o "f1 score", permite que você faça uso muito mais eficiente seu tempo quando você está desenvolvendo um sistema de Detecção de Anomalia. E tem que tomar decisões, como: escolher "ε", que variáveis incluir, e assim por diante. Nesse vídeo, começamos a usar dados rotulados para avaliar o algoritmo de Detecção de Anomalias, e isso nos deixa mais próximos da configuração de Aprendizado Supervisionado. No próximo vídeo, vou falar um pouco sobre isso. Em especial, vamos falar sobre quando deveríamos usar um algoritmo de Detecção de Anomalias, e quando deveríamos usar Aprendizado Supervisionado, e quais as diferenças.