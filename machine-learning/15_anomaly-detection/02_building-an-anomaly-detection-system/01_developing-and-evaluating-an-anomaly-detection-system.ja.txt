前回のビデオでは アノマリー検出のアルゴリズムを開発した。 このビデオでは、具体的な問題に対して どのようにアノマリー検出を 適用するか、 そのプロセスを 議論していく。特に、 今回はアノマリー検出アルゴリズムの評価を どう行うかについてフォーカスしていきたい。 前回のビデオでは、実数による評価の 重要性について話してきた。 これは、特定の応用に関する 学習アルゴリズムを 開発しようとしている時に、 通常たくさんの選択を 行う必要がある、たとえばどのフィーチャーを使うか、 などを選択する必要がある、という発想を捉えている。 これらの選択の全てについて、 決断を下すには、 あなたの学習アルゴリズムを評価する 数字が得られる方が、 より簡単になる事がしばしばある。 たとえば、もう一つ追加の フィーチャーを加えるかについて、 何か追加のフィーチャーについて心当たりがある時、 もしそのフィーチャーを加えて アルゴリズムを走らせて、 さらにフィーチャー無しでアルゴリズムを走らせて そこから数字を得たら、 それがこのフィーチャーを加えた事で パフォーマンスが改善したか悪化したかを教えてくれる。 それはつまり、そのフィーチャーを 含めるべきかどうかを決定する為の より良い、よりシンプルな方法を提供してくれる。 だから、アノマリー検出の システムを手早く 開発する為には、 そのアノマリー検出のシステムを 評価する方法がある事は、とても有用だ。 これを行う為に、 アノマリー検出のシステムを 評価する為に、 実際にはあるラベル付けされたデータを仮定する。 ここまでの所、アノマリー検出を 教師なし学習として 扱ってきた、ラベル付けされていない データを扱って。 でも、もしどれがアノマリーのサンプルかを 示すラベルのついたデータが あれば、そしてどれが 非アノマリーのサンプルかを示すデータがあれば、 これは普通の、アノマリー検出のアルゴリズムを 評価する方法と考える事が出来る。 航空機野エンジンの例を ふたたび考えよう。 ラベル付けされたデータで、 そのうちのちょっとだけが アノマリーの航空機エンジンの サンプル、つまり過去に製造されていてアノマリーだと、 欠陥品か、とにかく何らかの意味で異常な物だと判明しているとする。 また、非アノマリーの 完璧にオーケーな サンプルも幾つか あるとしよう。 y=0をノーマル、 または非アノマリーのサンプルを 表すのに使い、 そしてy=1を、アノマリーのサンプルを表すのに使う。 アノマリー検出のアルゴリズムを開発し、 評価していくプロセスは以下のようになる。 まずはトレーニングセットについて、 ところでクロスバリデーションセットと テストセットについては後で話すが、 まずはトレーニングセットについて。 これは普通、ラベルづけされてない トレーニングセットと考える。 これは普通の、 アノマリーで無いデータが 大量に集まっている、と考える。 普通はこれを非アノマリーと 考えるが、 だが実際はちょっとアノマリーなのが 紛れ込むくらいはオーケーだ。 それがラベル無しトレーニングセットに入っちゃってても。 次に、クロスバリデーションセットと テストセットを 定義する。 それで個々のアノマリー検出アルゴリズムの評価を行う。 つまり、具体的には、クロスバリデーションと テストセットではどちらも、 アノマリーだと分かっている サンプルが含まれているという 前提だ。 クロスバリデーションにも テストセットにも。 つまりテストセットには y=1となるサンプルが 幾つか含まれている。 それはアノマリーな航空機エンジンに対応している。 具体的にはこうだ。 これらを合わせて、我らの持ってる データだとしよう。 1万機の、分かってる範囲では 完全に正常な 航空機エンジンを製造したとしよう。 完全に良い航空機エンジン。 そしてここでも、ちょっとの 欠陥品がこの1万の中に 紛れ込んでたとしても、 実際はオーケーだ。 だがこれらの中の大多数は 良い、普通の、アノマリーでない エンジンである、と仮定する。 そしてこれまでに 長い期間工場を運営してきて、 フィーチャーを計測していて だいたい20機の 欠陥エンジン、 アノマリーのエンジンを得ているとする。 ところでアノマリー検出の きわめて典型的な適用ケースでは、 アノマリーのサンプルの数は、つまりy=1となるサンプルの数は だいたい20から50ってあたりだ。 この辺が 典型的なy=1となる 数の範囲。 そして通常は、もっとずっと多くの 正常なサンプルがある。 だからデータセットが与えられた時の 極めて標準的な トレーニングセット、クロスバリデーションセット、 テストセットの分割方法は以下のようになる。 1万の良品の航空機エンジンから 6000をラベル無しの トレーニングセットとして取り分ける。 これをラベル無しと言ったが、 これらは全てy=0に対応した サンプルだ、 我らの知る限りは。 そしてこれをp(x)をフィットするのに 使う。 つまり我らはこれら6000のエンジンを p(x)をフィットするのに使う、 ここでp(x1)は ミュー1とシグマ二乗1で パラメトライズされていて、 これはp(xn)が ミューnとシグマ二乗nでパラメトライズされている所まで続く。 つまりこれら6000の手本を使って パラメータの ミュー1、シグマ二乗1から ミューn、シグマ二乗nまでを 推計する。 以上が我らのトレーニングセットで それらは全て正常な、 または少なくとも大部分が正常なサンプルだ。 次に、正常な 航空機エンジンの中から いくらかをクロスバリデーションセットに、 さらにまた幾らかをテストセットに 入れます。 例えば6000+2000+2000。 こんな風に1万機の 正常な航空機エンジンを分割します。 次に、20機の不良エンジンが あるとして、 それを、例えば 10個ずつに分けて、 クロスバリデーションセットと テストセットにそれぞれ入れます。 そして次のスライドで これらを実際に、どのように使って アノマリー検出のアルゴリズムを評価するかを 議論していく。 ここまでで私が 書いてきた事は、 ラベル付けされたデータとラベル付けされていないデータの 分割する望ましい方法だ。 航空機エンジンのうち、良品と欠陥品の。 我らは良品に関しては 60、20，20%に それぞれ分割し、 欠陥品に関しては クロスバリデーションセットと テストセットだけに入れた。 次のスライドでどうしてそうしたかが分かる。 補足になるが、たまに世の中の人々が アノマリー検出のアルゴリズムを適用するやり方として、 たまにデータを異なったやり方で 分割してるのを見かける事もあるかもしれない。 これはあまりオススメしない代替案だが 代替案としては、 ある人々は1万個の良品を 6000個をトレーニングセットに、 残りの4000をクロスバリデーションセットと テストセットの両方に 入れる、という事を やるかもしれない。 でもご存知の通り、クロスバリデーションセットと テストセットを 完全に異なるデータと 我々は思いたいのだった。 だがアノマリー検出では、 たまにテストセットと クロスバリデーションセットの 両方に同じ良品のエンジンを 使うのを見かける場合があり、 時には欠陥品の方のエンジンまで 全く同じセットを クロスバリデーションセットと テストセットに使ってるのを見る事すらある。 これらは皆、より悪いやり方だと みなす事が出来るので、決してオススメしない。 確実に、クロスバリデーションセットと テストセットで 同じデータを使うのは 機械学習における良い習慣では無い。 でもたまにこれをやってる人を見かけるだろう。 さて、トレーニング、クロスバリデーション、そしてテストセットが 与えられたとして、 これが評価方法、、、いや、 これがアルゴリズムの開発方法と評価方法だ。 まず、トレーニングセットに対し、 モデルp(x)をフィッティングする。 これら全てをガウス分布に フィッティングする。 m個のラベル無し航空機エンジンのサンプルを。 ところでこれらをラベル無しサンプルと 呼んだが、実のところこれらは、 良品、正常な航空機エンジンであると 想定している。 そして次に、アノマリー検出のアルゴリズムが 実際に予測をしているのを想像してみよう。 つまりクロスバリデーションとテストのセットに対し、 サンプルxを検定する。 p(x)がエプシロンより 小さければ、 アルゴリズムはy=1を 予測したと考え、 もしp(x)が エプシロン以上なら y =0に違いない、と考える。 つまりxが与えられた時に、ラベルがどちらかを 予測しようとする。y=1は アノマリーに対応し、 y=0は通常のサンプルに対応する。 ではトレーニング、クロスバリデーション、テストセットがそれぞれ与えられたとして、 どうアルゴリズムを開発していくか？ より詳細に言うと、 アノマリー検出のアルゴリズムをどうやって評価するか？ これらを元に、 最初のステップは ラベル無しのトレーニングセットに対し モデルp(x)をフィッティングする。 つまりこれを取るのだが、 これはラベル無しトレーニングセットと言っているが、 実際は、それらの大多数は 通常の航空機エンジンだと 想定している物だった。 それらはアノマリーでは無さそうだから それにモデルp(x)を フィッティングする訳だ。 このパラメータ全部で、、、 これらガウス分布のデータ全てでフィッティングする。 次にクロスバリデーションとテストセットに対して、 アノマリー検出の アルゴリズムを、 yの値を予測する物と みなす。 つまり各テストのサンプルに 対して、 これらxi testと yi testがある訳だが、 ここでyは、これがアノマリーかどうかに応じて 1か0のどちらかの値を取る。 テストセットから入力xが与えられた時、 このアノマリー検出のアルゴリズムを p(x)がイプシロンより小さかったら yが1と予測している、とみなす。 つまりそれがアノマリーと予測するという事は、そんなサンプルとなる確率がとても低いという事だ。 そしてアルゴリズムは、 p(x)がイプシロンより大きいか等しい場合に y=0を予測しているとみなす。 つまりp(x)が普通の感覚で十分に大きければ それらを正常なサンプルとみなす。 つまり、これでアノマリー検出のアルゴリズムを これらのテストセット、クロスバリデーションセットの yラベルの値を 予測するものと みなす事が 出来た訳だ。 これはようするに、教師有り学習に 似たようなセッティングになる、でしょ？ そこではラベル付きのテストセットがあって、 我らのアルゴリズムは これらのラベルに対して予測を行う。 つまりこれらのラベルを どれだけ当てられるかで、それを評価する訳だ。 もちろんこれらのラベルは とても歪んだ割合となっている、 何故ならy=0、それは正常なサンプルだが、 このケースの方が通常は y=1、つまりアノマリーのサンプルとなるよりも ずっと一般的だからだ。 だがそれも、教師有り学習の 評価で使った評価指標と 似た事情だ。 ではどの評価指標を使うのが良いか？ データはとても歪んでいるのだから y =0の方がずっと一般的なのだから、 分類の精度はきっと 良い指標では無いだろう。 この件は以前のビデオで議論した。 もしとても歪んだ データセットに対するなら、 いつもy=0と予測するだけで、 とても高い分類精度が得られてしまう。 その代わりに、以下のような評価指標を使うべきだ: 真陽性（true positive）、偽陽性（false  positive）、 偽陰性（false negative）、真陰性（true nagative）の 比率の計算結果とか、 適合率（precision）と再現率（recall）を 計算するとか、 F1スコアなど、、、これは 適合率と再現率を 一つの実数で要約するような物だったが、 それを計算するなどする。 これらがクロスバリデーションセットとテストセットで アノマリー検出のアルゴリズムを 評価する方法となる。 最後に、 アノマリー検出のアルゴリズムには このイプシロン、というバラメータもあった。 イプシロンはある物を アノマリーとフラグ付けするかを決定する 閾値となる。 だからクロスバリデーションセットが ある時に、このパラメータ イプシロンを選ぶ、もう一つの方法は、 たくさんの異なる イプシロンの値を、 ほんとにたくさん試して、 そして例えばf1スコアを最大化する イプシロンを選ぶという方法が考えられる。 そうすれば、クロスバリデーションセットでは良い結果が得られるから。 より一般的には、 トレーニング、テスト、そして クロスバリデーションセットを減らす方法としては、 意思決定をしようとする時には、 例えばどのフィーチャーを含めるべきかとか、 またはパラメータのイプシロンをチューンしたい時には、 クロスバリデーションセットに対して 継続的にアルゴリズムを 評価して、 それらの決定は全て、、、 例えばどのフィーチャーを含めるかとか イプシロンを幾つに設定するかとか、 そういう時はクロスバリデーションセットに対してアルゴリズムを評価し、 そしてフィーチャーを選んだり、 これでいい！と思うイプシロンの値を 選んだ時には、 その最終的にモデルで、 それをテストセットに対し アルゴリズムの評価を 行う。 このビデオでは、 アノマリー検出の アルゴリズムの評価をどうやるか、の 手順を議論した。 ここでも、アルゴリズムを 評価する時には、 単一の実数による評価、 例えばF1スコアみたいな物は、 しばしばあなたの時間を より効率的に使わせてくれる、 アノマリー検出のシステムを 開発しようとしている時には。 そしてこの種の意思決定、たとえば イプシロンを選ぶとか、どのフィーチャーを含めるかとか、そういう意思決定をしたい時には。 このビデオでは、アノマリー検出の アルゴリズムを評価する為に、 ラベル付けされたデータを 少量使う。 これはちょっとだけ 教師有り学習に近くなる。 次のビデオでは、 その事ついてもうすこし説明します。 特に、どういう時はアノマリー検出アルゴリズムを 使うべきで、 どういう時はその代わりに教師有り学習を 採用すべきか、そしてこれら2つの形式化の違いについても扱います。