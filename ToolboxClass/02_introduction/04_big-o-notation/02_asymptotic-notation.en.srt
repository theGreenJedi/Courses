1
00:00:05,170 --> 00:00:06,170
Hello, everybody.

2
00:00:06,170 --> 00:00:07,100
Welcome back.

3
00:00:07,100 --> 00:00:09,430
Today we're going to start talking
about asymptotic notation.

4
00:00:10,690 --> 00:00:14,620
So here we're going to sort of just
introduce this whole idea of asymptotic

5
00:00:14,620 --> 00:00:18,210
notation and describe some of
the advantages of using it.

6
00:00:19,460 --> 00:00:23,960
So last time we ran into this really
interesting problem that computing

7
00:00:23,960 --> 00:00:28,870
runtimes is hard, in that if you really,
really want to know how long

8
00:00:28,870 --> 00:00:33,670
a particular program will take to run on
a particular computer, it's a huge mess.

9
00:00:33,670 --> 00:00:37,890
It depends on knowing all kinds of fine
details about how the program works.

10
00:00:37,890 --> 00:00:41,440
And all kinds of fine details about how
the computer works, how fast it is,

11
00:00:41,440 --> 00:00:43,550
what kind of system architecture it is.

12
00:00:43,550 --> 00:00:44,850
It's a huge mess.

13
00:00:44,850 --> 00:00:48,640
And we don't want to go through this
huge mess every single time we try to

14
00:00:48,640 --> 00:00:50,030
analyze an algorithm.

15
00:00:50,030 --> 00:00:55,920
So, we need something that's maybe
a little bit less precise but much

16
00:00:55,920 --> 00:00:59,920
easier to work with, and we're going to
talk about the basic idea behind that.

17
00:01:01,600 --> 00:01:04,470
And the basic idea is the following.

18
00:01:04,470 --> 00:01:10,150
That, there are lots of factors that
have an effect on the final runtime but,

19
00:01:10,150 --> 00:01:14,120
most of them will only change
the runtimes by a constant.

20
00:01:14,120 --> 00:01:17,220
If you're running on a computer
that's a hundred times faster,

21
00:01:17,220 --> 00:01:21,430
it will take one-one hundreth of the time,
a constant multiple.

22
00:01:21,430 --> 00:01:25,980
If your system architecture has
multiplications that take three times as

23
00:01:25,980 --> 00:01:30,810
long as additions, then if your program
is heavy on multiplications instead of

24
00:01:30,810 --> 00:01:36,315
additions, it might take three times as
long, but it's only a factor of three.

25
00:01:36,315 --> 00:01:39,505
If your memory hierarchy is
arranged in a different way,

26
00:01:39,505 --> 00:01:42,515
you might have to do disk
lookups instead of RAM lookups.

27
00:01:42,515 --> 00:01:45,975
And those will be a lot slower,
but only by a constant multiple.

28
00:01:47,190 --> 00:01:51,840
So the key idea is if we come up with
a measure of runtime complexity that

29
00:01:51,840 --> 00:01:57,310
ignores all of these constant multiples,
where running in time n and

30
00:01:57,310 --> 00:02:02,110
in running in time 100 times n are sort
of considered to be the same thing, then

31
00:02:02,110 --> 00:02:07,290
we don't have to worry about all of these
little, bitty details that affect runtime.

32
00:02:08,940 --> 00:02:11,290
Of course there's
a problem with this idea,

33
00:02:11,290 --> 00:02:16,580
if you look at it sort of by itself,
that if you have runtimes of one second or

34
00:02:16,580 --> 00:02:21,130
one hour or one year,
these only differ by constant multiples.

35
00:02:21,130 --> 00:02:25,070
A year is just something
like 30 million seconds.

36
00:02:25,070 --> 00:02:30,020
And so, if you don't care about factors of
30 million, you can't tell the difference

37
00:02:30,020 --> 00:02:33,400
between a runtime of a second and
a runtime of a year.

38
00:02:33,400 --> 00:02:34,950
How do we get around this problem?

39
00:02:35,950 --> 00:02:39,090
Well, there's a sort of
weird solution to this.

40
00:02:39,090 --> 00:02:42,750
We're not going to actually consider
the runtimes of our programs on

41
00:02:42,750 --> 00:02:44,680
any particular input.

42
00:02:44,680 --> 00:02:47,810
We're going to look at what
are known as asymptotic runtimes.

43
00:02:47,810 --> 00:02:51,890
These ask, how does the runtime
scale with input size?

44
00:02:51,890 --> 00:02:55,080
As the input size n gets larger,

45
00:02:55,080 --> 00:03:00,390
does the output scale proportional to n,
maybe proportional to n squared?

46
00:03:00,390 --> 00:03:02,210
Is it exponential in n?

47
00:03:02,210 --> 00:03:04,350
All these things are different.

48
00:03:04,350 --> 00:03:09,790
And in fact they're sort of so different
that as long as n is sufficiently large,

49
00:03:09,790 --> 00:03:12,420
the difference between n runtime and

50
00:03:12,420 --> 00:03:17,930
n squared runtime is going to be
worse than any constant multiple.

51
00:03:18,960 --> 00:03:22,470
If you've got a constant multiple of 1000,

52
00:03:22,470 --> 00:03:26,250
1000n might be pretty bad with
that big number in front.

53
00:03:26,250 --> 00:03:29,880
But, when n becomes big,
it's still better than n squared.

54
00:03:31,020 --> 00:03:34,900
And so, by sort of only caring about
what happens in this sort of long

55
00:03:34,900 --> 00:03:39,710
scale behavior, we will be able to do
this without seeing these constants,

56
00:03:39,710 --> 00:03:41,370
without having to care
about these details.

57
00:03:42,830 --> 00:03:44,580
And in fact, this sort of asymptotic,

58
00:03:44,580 --> 00:03:49,070
large scale behavior is actually what you
care about a lot of the time, because

59
00:03:49,070 --> 00:03:52,900
you really want to know: what happens when
I run my program on very large inputs?

60
00:03:54,330 --> 00:04:00,160
And these different sorts of scalings do
make a very large difference on that.

61
00:04:00,160 --> 00:04:05,160
So suppose that we have an algorithm whose
runtime is roughly proportional to n and

62
00:04:05,160 --> 00:04:09,340
we want it to run it on a machine
that runs at about a gigahertz.

63
00:04:09,340 --> 00:04:14,260
How large an input can we handle such that
we'll finish the computation in a second?

64
00:04:15,380 --> 00:04:20,210
Well if it runs at about size n,
you can handle about a billion

65
00:04:20,210 --> 00:04:22,970
sized inputs,
before it takes more than a second.

66
00:04:24,230 --> 00:04:27,567
If instead of n,
it's n log n it's a little bit slower,

67
00:04:27,567 --> 00:04:30,941
you can only handle inputs
the size about 30 million.

68
00:04:30,941 --> 00:04:33,210
If it runs like n squared,
it's a lot worse.

69
00:04:33,210 --> 00:04:36,040
You can only handle inputs
of size about 30,000

70
00:04:36,040 --> 00:04:37,740
before it starts taking
more than a second.

71
00:04:38,990 --> 00:04:41,230
If the inputs are of size 2 to the n,

72
00:04:41,230 --> 00:04:46,330
it's incredibly bad, you can only handle
inputs of size about 30 in a second.

73
00:04:46,330 --> 00:04:50,830
Inputs of size 50 already take two weeks,
inputs of size 100

74
00:04:50,830 --> 00:04:53,030
you'll never ever finish.

75
00:04:53,030 --> 00:04:55,700
And so the difference between n and
n squared and

76
00:04:55,700 --> 00:04:59,080
2 to the n is actually really,
really significant.

77
00:04:59,080 --> 00:05:03,120
It's often more significant
than these factors of 5 or

78
00:05:03,120 --> 00:05:04,640
100 that you're seeing from other things.

79
00:05:07,250 --> 00:05:11,255
Now just to give you another feel of sort
of how these sort of different types of

80
00:05:11,255 --> 00:05:16,240
runtimes behave, let's look at some
sort of common times that you might see.

81
00:05:16,240 --> 00:05:19,422
There's log n,
which is much smaller than root n,

82
00:05:19,422 --> 00:05:23,418
which is much smaller than n,
which is much smaller than n log n,

83
00:05:23,418 --> 00:05:28,160
which is much smaller than n squared,
which is much smaller than 2 to the n.

84
00:05:28,160 --> 00:05:29,570
So, if we graph all of these,

85
00:05:29,570 --> 00:05:34,620
you can see that these graphs sort
of separate out from each other.

86
00:05:34,620 --> 00:05:38,673
If you just look at them at small inputs,
it's maybe a little bit hard to tell which

87
00:05:38,673 --> 00:05:42,070
ones are bigger, there's a bit of
jostling around between each other.

88
00:05:42,070 --> 00:05:46,754
But if we extend the graph outwards a bit,
it becomes much more clear.

89
00:05:46,754 --> 00:05:50,712
2 to the n starts after
about 4 really taking off.

90
00:05:50,712 --> 00:05:54,229
Really just 2 to the n just shoots
up thereafter and becomes 20 or 30,

91
00:05:54,229 --> 00:05:56,414
it just leaves everyone
else in the dust.

92
00:05:56,414 --> 00:06:02,015
N squared keeps up a pretty sizable
advantage though against everyone else.

93
00:06:02,015 --> 00:06:06,610
N log n and n also are pretty
well separated from the others.

94
00:06:06,610 --> 00:06:11,063
In this graph, root n and log n seem
to be roughly equal to each other, but

95
00:06:11,063 --> 00:06:14,641
if you kept extending,
if you let n get larger and larger,

96
00:06:14,641 --> 00:06:17,796
they'd very quickly
differentiate themselves.

97
00:06:17,796 --> 00:06:21,338
Square root of 1 million is about 1,000.

98
00:06:21,338 --> 00:06:23,754
Log of 1 million is about 20.

99
00:06:23,754 --> 00:06:28,067
And so really as you keep going out,
very quickly the further out you go

100
00:06:28,067 --> 00:06:31,799
the further separated these
things become from each other,

101
00:06:31,799 --> 00:06:35,558
and that's really the key idea
behind sort of asymptotics.

102
00:06:35,558 --> 00:06:38,240
We don't care so much about the constants,

103
00:06:38,240 --> 00:06:42,215
we care about what happens as your inputs
get very large, how do they scale.

104
00:06:43,615 --> 00:06:45,085
So that's it for today.

105
00:06:45,085 --> 00:06:46,015
Come back next lecture.

106
00:06:46,015 --> 00:06:50,415
We'll talk about in sort of detail
what this actually means and

107
00:06:50,415 --> 00:06:53,435
how to actually get it to work.

108
00:06:53,435 --> 00:06:54,145
So until next time.